<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 09:48:41 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Entropy Minimization and Compression</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS1" title="In 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Entropy and Coding Rate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS2" title="In 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Differential Entropy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS3" title="In 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Minimizing Coding Rate</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Compression via Denoising</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="In 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Diffusion and Denoising Processes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="In 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Learning and Sampling a Distribution via Iterative Denoising</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px1" title="In 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Step 1: different discretizations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px2" title="In 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Step 2: different noise models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px3" title="In 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Step 3: optimizing training pipelines.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px4" title="In 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">(Optional) Step 4: changing the estimation target.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Compression via Lossy Coding</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="In 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Necessity of Lossy Coding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="In 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Rate Distortion and Data Geometry</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="In 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Lossy Coding Rate for a Low-Dimensional Gaussian</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS4" title="In 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Clustering a Mixture of Low-Dimensional Gaussians</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS4.SSS0.Px1" title="In 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">The clustering problem.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS4.SSS0.Px2" title="In 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Clustering via lossy compression.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS4.SSS0.Px3" title="In 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Optimization strategies to cluster.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Maximizing Information Gain</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS0.SSS0.Px1" title="In 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">How to measure the goodness of representations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="In 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Linear Discriminative Representations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px1" title="In 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Encoding class information via cross entropy.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px2" title="In 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Minimal discriminative features via information bottleneck.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px3" title="In 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Linear discriminative representations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="In 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>The Principle of Maximal Coding Rate Reduction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS2.SSS0.Px1" title="In 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Coding rate of features.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="In 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Optimization Properties of Coding Rate Reduction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px1" title="In 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Regularized MCR<sup class="ltx_sup">2</sup>.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Summary and Notes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S6" title="In Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">Entropy Minimization and Compression</a><div class="mini-toc-sub"><a href="#S1.SS1">Entropy and Coding Rate</a><a href="#S1.SS2">Differential Entropy</a><a href="#S1.SS3">Minimizing Coding Rate</a></div></li><li><a href="#S2">Compression via Denoising</a><div class="mini-toc-sub"><a href="#S2.SS1">Diffusion and Denoising Processes</a><a href="#S2.SS2">Learning and Sampling a Distribution via Iterative Denoising</a></div></li><li><a href="#S3">Compression via Lossy Coding</a><div class="mini-toc-sub"><a href="#S3.SS1">Necessity of Lossy Coding</a><a href="#S3.SS2">Rate Distortion and Data Geometry</a><a href="#S3.SS3">Lossy Coding Rate for a Low-Dimensional Gaussian</a><a href="#S3.SS4">Clustering a Mixture of Low-Dimensional Gaussians</a></div></li><li><a href="#S4">Maximizing Information Gain</a><div class="mini-toc-sub"><a href="#S4.SS1">Linear Discriminative Representations</a><a href="#S4.SS2">The Principle of Maximal Coding Rate Reduction</a><a href="#S4.SS3">Optimization Properties of Coding Rate Reduction</a></div></li><li><a href="#S5">Summary and Notes</a></li><li><a href="#S6">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">We compress to learn, and we learn to compress</span>.”
<br class="ltx_break"/>   — High-dimensional Data Analysis, Wright and Ma, 2022</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">In Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>, we have shown how to learn simple classes of distributions whose supports are assumed to be either a single or a mixture of low-dimensional subspaces or low-rank Gaussians. For further simplicity, the different (hidden) linear or Gaussian modes are assumed to be orthogonal or independent<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Or can be easily reduced to such idealistic cases.</span></span></span>, as illustrated in Figure <a class="ltx_ref" href="Ch2.html#F4" title="Figure 2.4 ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.4</span></a>. As we have shown, for such special distributions, one can derive rather simple and effective learning algorithms with correctness and efficiency guarantees. The geometric and statistical interpretation of operations in the associated algorithms is also very clear.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">In practice, both linearity and independence are rather idealistic assumptions that distributions of real-world high-dimensional data rarely satisfy. The only thing that we may assume is that the intrinsic dimension of the distribution is very low compared to the dimension of the ambient space in which the data are embedded. Hence, in this chapter, we show how to learn a more general class of low-dimensional distributions in a high-dimensional space that is not necessarily (piecewise) linear.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">It is typical that the distribution of real data often contains multiple components or modes, say corresponding to different classes of objects in the case of images. These modes might not be statistically independent and they may even have different intrinsic dimensions. It is also typical that we have access to only a finite number of samples of the distribution. Therefore, in general, we may assume our data are distributed on a mixture of (nonlinear) low-dimensional submanifolds in a high-dimensional space. Figure <a class="ltx_ref" href="#F1" title="Figure 3.1 ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.1</span></a> illustrates an example of such a distribution.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p">To learn such a distribution under such conditions, there are several fundamental questions that we need to address:</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p">What is a general approach to learn a general low-dimensional distribution in a high-dimensional space and represent the learned distribution?</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p">How do we measure the complexity of the resulting representation so that we can effectively exploit the low dimensionality to learn?</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i3.p1">
<p class="ltx_p">How do we make the learning process computationally tractable and even scalable, as the ambient dimension is usually high and the number of samples typically large?</p>
</div>
</li>
</ul>
<p class="ltx_p">As we will see, the fundamental idea of <span class="ltx_text ltx_font_italic">compression</span>, or <span class="ltx_text ltx_font_italic">dimension reduction</span>, which has been shown to be very effective for the linear/independent case, still serves as a general principle for developing effective computational models and methods for learning general low-dimensional distributions.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">Due to its theoretical and practical significance, we will study in greater
depth how this general framework of learning low-dimensional distributions via compression substantiates when the distribution of interest can be well-modeled or approximated by a mixture of low-dimensional subspaces or low-rank Gaussians.</p>
</div>
<figure class="ltx_figure" id="F1"><img alt="Figure 3.1 : Data distributed on a mixture of low-dimensional submanifolds ∪ j ℳ j \cup_{j}\mathcal{M}_{j} ∪ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in a very high-dimensional ambient space, say ℝ D \mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ." class="ltx_graphics ltx_img_landscape" height="186" id="F1.g1" src="chapters/chapter3/figs/mixed-manifolds.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.1</span>: </span><span class="ltx_text" style="font-size:90%;">Data distributed on a mixture of low-dimensional submanifolds <math alttext="\cup_{j}\mathcal{M}_{j}" class="ltx_Math" display="inline" id="F1.m3"><semantics><mrow><msub><mo>∪</mo><mi>j</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\cup_{j}\mathcal{M}_{j}</annotation><annotation encoding="application/x-llamapun">∪ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in a very high-dimensional ambient space, say <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="F1.m4"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.1 </span>Entropy Minimization and Compression</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.1 </span>Entropy and Coding Rate</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p">In Chapter <a class="ltx_ref" href="Ch1.html" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1</span></a>, we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complexity but it is not computable and not associated with any implementable scheme that can actually reproduce the data. Hence we need an alternative, computable, and realizable, measure of complexity. That leads us to the notion of <span class="ltx_text ltx_font_italic">entropy</span>, introduced by Shannon in 1948 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx242" title="">Sha48</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p">To illustrate the constructive nature of entropy, let us start with the simplest case. Suppose that we have a discrete random variable that takes <math alttext="N" class="ltx_Math" display="inline" id="S1.SS1.p2.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> distinct values, or <span class="ltx_text ltx_font_italic">tokens</span>, <math alttext="\{\bm{x}_{1},\ldots,\bm{x}_{N}\}" class="ltx_Math" display="inline" id="S1.SS1.p2.m2"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{1},\ldots,\bm{x}_{N}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math> with equal probability <math alttext="1/N" class="ltx_Math" display="inline" id="S1.SS1.p2.m3"><semantics><mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1/N</annotation><annotation encoding="application/x-llamapun">1 / italic_N</annotation></semantics></math>. Then we could encode each token <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.p2.m4"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> using the <math alttext="\log_{2}N" class="ltx_Math" display="inline" id="S1.SS1.p2.m5"><semantics><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\log_{2}N</annotation><annotation encoding="application/x-llamapun">roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_N</annotation></semantics></math>-bit binary representation of <math alttext="i" class="ltx_Math" display="inline" id="S1.SS1.p2.m6"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>. This coding scheme could be generalized to encoding arbitrary discrete distributions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>: Given a distribution <math alttext="p" class="ltx_Math" display="inline" id="S1.SS1.p2.m7"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> such that <math alttext="\sum_{i=1}^{N}p(\bm{x}_{i})=1" class="ltx_Math" display="inline" id="S1.SS1.p2.m8"><semantics><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{N}p(\bm{x}_{i})=1</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 1</annotation></semantics></math>, one could assign each token <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.p2.m9"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with probability <math alttext="p(\bm{x}_{i})" class="ltx_Math" display="inline" id="S1.SS1.p2.m10"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}_{i})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> to a binary code of size <math alttext="\log_{2}[1/p(\bm{x}_{i})]=-\log_{2}p(\bm{x}_{i})" class="ltx_Math" display="inline" id="S1.SS1.p2.m11"><semantics><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mo rspace="0.167em">−</mo><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\log_{2}[1/p(\bm{x}_{i})]=-\log_{2}p(\bm{x}_{i})</annotation><annotation encoding="application/x-llamapun">roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT [ 1 / italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> bits. Hence the average number of bits, or <span class="ltx_text ltx_font_italic">the coding rate</span>, needed to encode any sample from the distribution <math alttext="p(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.p2.m12"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p ( ⋅ )</annotation></semantics></math> is given by the expression:<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>By the convention of Information Theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>, the <math alttext="\log" class="ltx_Math" display="inline" id="footnote2.m1"><semantics><mi>log</mi><annotation encoding="application/x-tex">\log</annotation><annotation encoding="application/x-llamapun">roman_log</annotation></semantics></math> here is to the base <math alttext="2" class="ltx_Math" display="inline" id="footnote2.m2"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation><annotation encoding="application/x-llamapun">2</annotation></semantics></math>. Hence entropy is measured in (binary) bits.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(\bm{x})\doteq\mathbb{E}[\log 1/p(\bm{x})]=-\sum_{i=1}^{N}p(\bm{x}_{i})\log p(\bm{x}_{i})." class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(\bm{x})\doteq\mathbb{E}[\log 1/p(\bm{x})]=-\sum_{i=1}^{N}p(\bm{x}_{i})\log p(\bm{x}_{i}).</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_x ) ≐ blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is known as the <span class="ltx_text ltx_font_italic">entropy</span> of the (discrete) distribution <math alttext="p(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.p2.m13"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p ( ⋅ )</annotation></semantics></math>. Note that this entropy is always nonnegative and it is zero if and only if <math alttext="p(\bm{x}_{i})=1" class="ltx_Math" display="inline" id="S1.SS1.p2.m14"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p(\bm{x}_{i})=1</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 1</annotation></semantics></math> for some <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.p2.m15"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="i\in[N]" class="ltx_Math" display="inline" id="S1.SS1.p2.m16"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">i\in[N]</annotation><annotation encoding="application/x-llamapun">italic_i ∈ [ italic_N ]</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Here notice that we use the fact <math alttext="\lim_{p\rightarrow 0}p\log p=0" class="ltx_Math" display="inline" id="footnote3.m1"><semantics><mrow><mrow><msub><mo>lim</mo><mrow><mi>p</mi><mo stretchy="false">→</mo><mn>0</mn></mrow></msub><mrow><mi>p</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lim_{p\rightarrow 0}p\log p=0</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_p → 0 end_POSTSUBSCRIPT italic_p roman_log italic_p = 0</annotation></semantics></math>.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.2 </span>Differential Entropy</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p">When the random variable <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is continuous and has a probability density <math alttext="p" class="ltx_Math" display="inline" id="S1.SS2.p1.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>, one may view that the limit of the above sum (<a class="ltx_ref" href="#S1.E1" title="Equation 3.1.1 ‣ 3.1.1 Entropy and Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) is related to an integral:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x})\doteq\operatorname{\mathbb{E}}[\log 1/p(\bm{x})]=-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}." class="ltx_Math" display="block" id="S1.E2.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{x})\doteq\operatorname{\mathbb{E}}[\log 1/p(\bm{x})]=-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}.</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) ≐ blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log italic_p ( bold_italic_ξ ) roman_d bold_italic_ξ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">More precisely, given a continuous variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS2.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we may quantize it with a quantization size <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S1.SS2.p1.m4"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>. Denote the resulting discrete variable as <math alttext="\bm{x}^{\epsilon}" class="ltx_Math" display="inline" id="S1.SS2.p1.m5"><semantics><msup><mi>𝒙</mi><mi>ϵ</mi></msup><annotation encoding="application/x-tex">\bm{x}^{\epsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT</annotation></semantics></math>. Then one can show that <math alttext="H(\bm{x}^{\epsilon})+\log(\epsilon)\approx h(\bm{x})" class="ltx_Math" display="inline" id="S1.SS2.p1.m6"><semantics><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>ϵ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≈</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{x}^{\epsilon})+\log(\epsilon)\approx h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_x start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) + roman_log ( italic_ϵ ) ≈ italic_h ( bold_italic_x )</annotation></semantics></math>. Hence, when <math alttext="\epsilon" class="ltx_Math" display="inline" id="S1.SS2.p1.m7"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small, the differential entropy <math alttext="h(\bm{x})" class="ltx_Math" display="inline" id="S1.SS2.p1.m8"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x )</annotation></semantics></math> can be negative. Interested readers may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite> for a more detailed explanation.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Entropy of Gaussian Distributions)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">Through direct calculation, it is possible to show that the entropy of a Gaussian distribution <math alttext="x\sim\mathcal{N}(\mu,\sigma^{2})" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><mrow><mi>x</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>μ</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x\sim\mathcal{N}(\mu,\sigma^{2})</annotation><annotation encoding="application/x-llamapun">italic_x ∼ caligraphic_N ( italic_μ , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(x)=\frac{1}{2}\log(2\pi\sigma^{2})+\frac{1}{2}." class="ltx_Math" display="block" id="S1.E3.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(x)=\frac{1}{2}\log(2\pi\sigma^{2})+\frac{1}{2}.</annotation><annotation encoding="application/x-llamapun">italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_π italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It is also known that the Gaussian distribution achieves the maximal entropy
for all distributions with the same variance <math alttext="\sigma^{2}" class="ltx_Math" display="inline" id="Thmexample1.p1.m2"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^{2}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. The entropy of a multivariate Gaussian distribution <math alttext="\bm{x}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample1.p1.m3"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ caligraphic_N ( bold_italic_μ , bold_Σ )</annotation></semantics></math> in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexample1.p1.m4"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x})=\frac{D}{2}(1+\log(2\pi))+\frac{1}{2}\log\det(\bm{\Sigma})." class="ltx_Math" display="block" id="S1.E4.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mi>D</mi><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>π</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{x})=\frac{D}{2}(1+\log(2\pi))+\frac{1}{2}\log\det(\bm{\Sigma}).</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) = divide start_ARG italic_D end_ARG start_ARG 2 end_ARG ( 1 + roman_log ( 2 italic_π ) ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_Σ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p1.m5"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p">Similar to the entropy for a discrete distribution, we would like the
differential entropy to be associated with the coding rate of some realizable
coding scheme. For example, as above, we may discretize the domain of the distribution with a grid of size <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S1.SS2.p2.m1"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>. The coding rate of the resulting discrete distribution can be viewed as an approximation to the differential entropy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p">Be aware that there are some caveats associated with the definition of
differential entropy. For a distribution in a high-dimensional space, when its
support becomes degenerate (low-dimensional), its differential entropy diverges
to <math alttext="-\infty" class="ltx_Math" display="inline" id="S1.SS2.p3.m1"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation><annotation encoding="application/x-llamapun">- ∞</annotation></semantics></math>. This fact is proved in
<a class="ltx_ref" href="A2.html#Thmtheorem1" title="Theorem B.1. ‣ B.1 Differential Entropy of Low-Dimensional Distributions ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">B.1</span></a> (we also recall the
maximum entropy characterization of the Gaussian distribution mentioned above in
<a class="ltx_ref" href="A2.html#Thmtheorem1" title="Theorem B.1. ‣ B.1 Differential Entropy of Low-Dimensional Distributions ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">B.1</span></a>) but even in the simple explicit case of Gaussian
distributions (<a class="ltx_ref" href="#S1.E4" title="Equation 3.1.4 ‣ Example 3.1 (Entropy of Gaussian Distributions). ‣ 3.1.2 Differential Entropy ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.1.4</span></a>), when the covariance
<math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="S1.SS2.p3.m2"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math> is singular, we can see that <math alttext="\log\det(\bm{\Sigma})=-\infty" class="ltx_Math" display="inline" id="S1.SS2.p3.m3"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">\log\det(\bm{\Sigma})=-\infty</annotation><annotation encoding="application/x-llamapun">roman_log roman_det ( bold_Σ ) = - ∞</annotation></semantics></math> so we have <math alttext="h(\bm{x})=-\infty" class="ltx_Math" display="inline" id="S1.SS2.p3.m4"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x})=-\infty</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) = - ∞</annotation></semantics></math>. In such a situation, it is not obvious how to properly quantize or encode such a distribution. Nevertheless, degenerate (Gaussian) distributions are precisely the simplest possible, and arguably the most important, instances of low-dimensional distributions in a high-dimensional space. In this chapter, we will discuss a complete resolution to this seeming difficulty with degeneracy.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.3 </span>Minimizing Coding Rate</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p">Remember that the learning problem entails the recovery of a (potentially continuous) distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p1.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> from a set of samples <math alttext="\{\bm{x}_{1},\ldots,\bm{x}_{N}\}" class="ltx_Math" display="inline" id="S1.SS3.p1.m2"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{1},\ldots,\bm{x}_{N}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math> drawn from the distribution. For ease of exposition, we write <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S1.SS3.p1.m3"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> . Given that the distributions of interest here are (nearly) low-dimensional, we should expect that their (differential) entropy is very small. But unlike the situations that we have studied in the previous chapter, in general we do not know the family of (analytical) low-dimensional models to which the distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p1.m4"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> belongs. So checking whether the entropy is small seems to be the only guideline that we can rely on to identify and model the distribution.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p">Now given the samples alone without knowing what <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> is, in theory they could be interpreted as samples from any generic distribution. In particular, they could be interpreted as any of the following cases:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">as samples from the empirical distribution <math alttext="p^{\bm{X}}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1"><semantics><msup><mi>p</mi><mi>𝑿</mi></msup><annotation encoding="application/x-tex">p^{\bm{X}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT bold_italic_X end_POSTSUPERSCRIPT</annotation></semantics></math> itself, which assigns <math alttext="1/N" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m2"><semantics><mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1/N</annotation><annotation encoding="application/x-llamapun">1 / italic_N</annotation></semantics></math> probability each of the <math alttext="N" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> samples <math alttext="\bm{x}_{i},i=1,\ldots,N" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m4"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{i},i=1,\ldots,N</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , … , italic_N</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">as samples from a standard normal distribution <math alttext="\bm{x}^{n}\sim p^{n}\doteq\mathcal{N}(\bm{0},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1"><semantics><mrow><msup><mi>𝒙</mi><mi>n</mi></msup><mo>∼</mo><msup><mi>p</mi><mi>n</mi></msup><mo>≐</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}^{n}\sim p^{n}\doteq\mathcal{N}(\bm{0},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∼ italic_p start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ≐ caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> with a variance <math alttext="\sigma^{2}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m2"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^{2}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> large enough (say larger than the sample norms);</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">as samples from a normal distribution <math alttext="\bm{x}^{e}\sim p^{e}\doteq\mathcal{N}(\bm{0},\hat{\bm{\Sigma}})" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m1"><semantics><mrow><msup><mi>𝒙</mi><mi>e</mi></msup><mo>∼</mo><msup><mi>p</mi><mi>e</mi></msup><mo>≐</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mover accent="true"><mi>𝚺</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}^{e}\sim p^{e}\doteq\mathcal{N}(\bm{0},\hat{\bm{\Sigma}})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ∼ italic_p start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ≐ caligraphic_N ( bold_0 , over^ start_ARG bold_Σ end_ARG )</annotation></semantics></math> with a covariance <math alttext="\hat{\bm{\Sigma}}=\frac{1}{N}\bm{X}\bm{X}^{T}" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m2"><semantics><mrow><mover accent="true"><mi>𝚺</mi><mo>^</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mi>T</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{\Sigma}}=\frac{1}{N}\bm{X}\bm{X}^{T}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_Σ end_ARG = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> being the empirical covariance of the samples;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p">as samples from a distribution <math alttext="\hat{\bm{x}}\sim\hat{q}(\bm{x})" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∼</mo><mrow><mover accent="true"><mi>q</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}\sim\hat{q}(\bm{x})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ∼ over^ start_ARG italic_q end_ARG ( bold_italic_x )</annotation></semantics></math> that closely approximates the ground truth distribution <math alttext="p" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>.</p>
</div>
</li>
</ol>
<p class="ltx_p">Now the question is which one is better, in what sense? Suppose that you believe these data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> are drawn from a particular distribution <math alttext="q(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m3"><semantics><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_x )</annotation></semantics></math>, which may be one of the above distributions considered. Then we could encode the data points with the optimal code book for the distribution <math alttext="q(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m4"><semantics><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_x )</annotation></semantics></math>. The required average coding length (or coding rate) is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{1}{N}\sum_{i=1}^{N}-\log q(\bm{x}_{i})\quad\approx\quad-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}" class="ltx_Math" display="block" id="S1.E5.m1"><semantics><mrow><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mrow><mo lspace="0em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>q</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1em"></mspace><mo>≈</mo><mspace width="1em"></mspace><mrow><mo>−</mo><mrow><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>q</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{N}\sum_{i=1}^{N}-\log q(\bm{x}_{i})\quad\approx\quad-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT - roman_log italic_q ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≈ - ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log italic_q ( bold_italic_ξ ) roman_d bold_italic_ξ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">as the number of samples <math alttext="N" class="ltx_Math" display="inline" id="S1.SS3.p2.m5"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> becomes large. If we have identified the correct distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m6"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>, the coding rate is given by the entropy <math alttext="-\int p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}" class="ltx_Math" display="inline" id="S1.SS3.p2.m7"><semantics><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">-\int p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">- ∫ italic_p ( bold_italic_ξ ) roman_log italic_p ( bold_italic_ξ ) roman_d bold_italic_ξ</annotation></semantics></math>. It turns out that the above coding length <math alttext="-\int p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}" class="ltx_Math" display="inline" id="S1.SS3.p2.m8"><semantics><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>q</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">-\int p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">- ∫ italic_p ( bold_italic_ξ ) roman_log italic_q ( bold_italic_ξ ) roman_d bold_italic_ξ</annotation></semantics></math> is always larger than or equal to the entropy unless <math alttext="q(\bm{x})=p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m9"><semantics><mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">q(\bm{x})=p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_x ) = italic_p ( bold_italic_x )</annotation></semantics></math>. Their difference, denoted as</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx13">
<tbody id="S1.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{\mathsf{KL}}(p\;\|\;q)" class="ltx_Math" display="inline" id="S1.E6.m1"><semantics><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>q</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{\mathsf{KL}}(p\;\|\;q)</annotation><annotation encoding="application/x-llamapun">sansserif_KL ( italic_p ∥ italic_q )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\doteq" class="ltx_Math" display="inline" id="S1.E6.m2"><semantics><mo>≐</mo><annotation encoding="application/x-tex">\displaystyle\doteq</annotation><annotation encoding="application/x-llamapun">≐</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}-\Big{(}-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}\Big{)}" class="ltx_Math" display="inline" id="S1.E6.m3"><semantics><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>q</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log q(\bm{\xi})\mathrm{d}\bm{\xi}-\Big{(}-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log p(\bm{\xi})\mathrm{d}\bm{\xi}\Big{)}</annotation><annotation encoding="application/x-llamapun">- ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log italic_q ( bold_italic_ξ ) roman_d bold_italic_ξ - ( - ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log italic_p ( bold_italic_ξ ) roman_d bold_italic_ξ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.6)</span></td>
</tr></tbody>
<tbody id="S1.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S1.E7.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{p(\bm{\xi})}{q(\bm{\xi})}\mathrm{d}\bm{\xi}" class="ltx_Math" display="inline" id="S1.E7.m2"><semantics><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{p(\bm{\xi})}{q(\bm{\xi})}\mathrm{d}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log divide start_ARG italic_p ( bold_italic_ξ ) end_ARG start_ARG italic_q ( bold_italic_ξ ) end_ARG roman_d bold_italic_ξ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is known as the <span class="ltx_text ltx_font_italic">Kullback-Leibler</span> (KL) divergence, or relative entropy. This quantity is always non-negative.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Information Inequality)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="p(\bm{x}),q(\bm{x})" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m1"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}),q(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ) , italic_q ( bold_italic_x )</annotation></semantics></math> be two probability density functions (that have the same
support). Then <math alttext="\operatorname{\mathsf{KL}}(p\;\|\;q)\geq 0" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m2"><semantics><mrow><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>q</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{KL}}(p\;\|\;q)\geq 0</annotation><annotation encoding="application/x-llamapun">sansserif_KL ( italic_p ∥ italic_q ) ≥ 0</annotation></semantics></math>, where the inequality becomes equality if and only if <math alttext="p=q" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m3"><semantics><mrow><mi>p</mi><mo>=</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">p=q</annotation><annotation encoding="application/x-llamapun">italic_p = italic_q</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">4</span></span><span class="ltx_text ltx_font_upright">Technically, this equality should be taken to mean “almost everywhere”, i.e., except possibly on a set of zero measure (volume), since this set would not impact the value of any integral.</span></span></span></span></span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S1.SS3.p3">
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx14">
<tbody id="S1.Ex2">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle-\operatorname{\mathsf{KL}}(p\;\|\;q)" class="ltx_Math" display="inline" id="S1.Ex1.m1"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>q</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle-\operatorname{\mathsf{KL}}(p\;\|\;q)</annotation><annotation encoding="application/x-llamapun">- sansserif_KL ( italic_p ∥ italic_q )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S1.Ex1.m2"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{p(\bm{\xi})}{q(\bm{\xi})}\mathrm{d}\bm{\xi}=\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{q(\bm{\xi})}{p(\bm{\xi})}\mathrm{d}\bm{\xi}" class="ltx_Math" display="inline" id="S1.Ex1.m3"><semantics><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle-\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{p(\bm{\xi})}{q(\bm{\xi})}\mathrm{d}\bm{\xi}=\int_{\mathbb{R}^{D}}p(\bm{\xi})\log\frac{q(\bm{\xi})}{p(\bm{\xi})}\mathrm{d}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">- ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log divide start_ARG italic_p ( bold_italic_ξ ) end_ARG start_ARG italic_q ( bold_italic_ξ ) end_ARG roman_d bold_italic_ξ = ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) roman_log divide start_ARG italic_q ( bold_italic_ξ ) end_ARG start_ARG italic_p ( bold_italic_ξ ) end_ARG roman_d bold_italic_ξ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\leq" class="ltx_Math" display="inline" id="S1.Ex2.m1"><semantics><mo>≤</mo><annotation encoding="application/x-tex">\displaystyle\leq</annotation><annotation encoding="application/x-llamapun">≤</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\log\int_{\mathbb{R}^{D}}p(\bm{\xi})\frac{q(\bm{\xi})}{p(\bm{\xi})}\mathrm{d}\bm{\xi}=\log\int_{\mathbb{R}^{D}}q(\bm{\xi})\mathrm{d}\bm{\xi}=\log 1=0," class="ltx_Math" display="inline" id="S1.Ex2.m2"><semantics><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mn>1</mn></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\log\int_{\mathbb{R}^{D}}p(\bm{\xi})\frac{q(\bm{\xi})}{p(\bm{\xi})}\mathrm{d}\bm{\xi}=\log\int_{\mathbb{R}^{D}}q(\bm{\xi})\mathrm{d}\bm{\xi}=\log 1=0,</annotation><annotation encoding="application/x-llamapun">roman_log ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) divide start_ARG italic_q ( bold_italic_ξ ) end_ARG start_ARG italic_p ( bold_italic_ξ ) end_ARG roman_d bold_italic_ξ = roman_log ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_q ( bold_italic_ξ ) roman_d bold_italic_ξ = roman_log 1 = 0 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where the first inequality follows from <span class="ltx_text ltx_font_italic">Jensen’s inequality</span> and the
fact that the function <math alttext="\log(\cdot)" class="ltx_Math" display="inline" id="S1.SS3.p3.m1"><semantics><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(\cdot)</annotation><annotation encoding="application/x-llamapun">roman_log ( ⋅ )</annotation></semantics></math> is strictly concave. The equality holds
if and only if <math alttext="p=q" class="ltx_Math" display="inline" id="S1.SS3.p3.m2"><semantics><mrow><mi>p</mi><mo>=</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">p=q</annotation><annotation encoding="application/x-llamapun">italic_p = italic_q</annotation></semantics></math> .
∎</p>
</div>
</div>
<div class="ltx_para" id="S1.SS3.p4">
<p class="ltx_p">Hence, given a set of sampled data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, to determine which case is better among <math alttext="p^{n}" class="ltx_Math" display="inline" id="S1.SS3.p4.m2"><semantics><msup><mi>p</mi><mi>n</mi></msup><annotation encoding="application/x-tex">p^{n}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="p^{e}" class="ltx_Math" display="inline" id="S1.SS3.p4.m3"><semantics><msup><mi>p</mi><mi>e</mi></msup><annotation encoding="application/x-tex">p^{e}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\hat{q}" class="ltx_Math" display="inline" id="S1.SS3.p4.m4"><semantics><mover accent="true"><mi>q</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{q}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG italic_q end_ARG</annotation></semantics></math>, we may compare their coding rates for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p4.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and see which one gives the lowest rate. We know from the above that the (theoretically achievable) coding rate for a distribution is closely related to its entropy. In general, we have:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x}^{n})&gt;h(\bm{x}^{e})&gt;h(\hat{\bm{x}})." class="ltx_Math" display="block" id="S1.E8.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>n</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>e</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{x}^{n})&gt;h(\bm{x}^{e})&gt;h(\hat{\bm{x}}).</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) &gt; italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) &gt; italic_h ( over^ start_ARG bold_italic_x end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, if the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p4.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> were encoded by the code book associated with each of these distributions, the coding rate for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p4.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> would in general decrease in the same order:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\bm{x}^{n})\rightarrow p(\bm{x}^{e})\rightarrow p(\hat{\bm{x}})." class="ltx_Math" display="block" id="S1.E9.m1"><semantics><mrow><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>n</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>e</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p(\bm{x}^{n})\rightarrow p(\bm{x}^{e})\rightarrow p(\hat{\bm{x}}).</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) → italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) → italic_p ( over^ start_ARG bold_italic_x end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.1.9)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S1.SS3.p5">
<p class="ltx_p">This observation gives us a general guideline on how we may be able to pursue a distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p5.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> which has a low-dimensional structure. It suggests two possible approaches:</p>
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p">Starting with a general distribution (say a normal distribution) with high entropy, gradually transforming the distribution towards the (empirical) distribution of the data by reducing entropy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p">Among a large family of (parametric or non-parametric) distributions with explicit coding schemes that encode the given data, progressively search for better coding schemes that give lower coding rates.</p>
</div>
</li>
</ol>
<p class="ltx_p">Conceptually, both approaches are essentially trying to do the same thing. For the first approach, we need to make sure such a path of transformation exists and is computable. For the second approach, it is necessary that the chosen family is rich enough and can closely approximate (or contain) the ground truth distribution. For either approach, we need to ensure that solutions with lower entropy or better coding rates can be efficiently computed and converge to the desired distribution quickly.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Say the distribution of real-world data such as images and texts.</span></span></span> We will explore both approaches in the two remaining sections of this chapter.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.2 </span>Compression via Denoising</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">In this section, we will describe a <span class="ltx_text ltx_font_italic">natural</span> and <span class="ltx_text ltx_font_italic">computationally tractable</span> way to learn a distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S2.p1.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> by way of learning a parametric encoding of our distribution such that the representation has the minimum entropy or coding rate, then using this encoding to transform high-entropy samples from a standard Gaussian into low-entropy samples from the target distribution, as illustrated in <a class="ltx_ref" href="#F2" title="In 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. This presents a methodology that utilizes both approaches above in order to learn and sample from the distribution.</p>
</div>
<figure class="ltx_figure" id="F2"><img alt="Figure 3.2 : Illustration of an iterative denoising process that, starting from an isotropic Gaussian distribution, converges to an arbitrary data distribution." class="ltx_graphics ltx_img_landscape" height="118" id="F2.g1" src="chapters/chapter3/figs/diffusion_pipeline.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.2</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of an iterative denoising process that, starting from an isotropic Gaussian distribution, converges to an arbitrary data distribution. </span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2.1 </span>Diffusion and Denoising Processes</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">We first want to find a procedure to decrease the entropy of a given very noisy sample into a lower-entropy sample from the data distribution. Here, we describe a potential approach—one of many, but perhaps the most natural way to attack this problem. First, we find a way to <span class="ltx_text ltx_font_italic">gradually increase</span> the entropy of existing samples from the data distribution. Then, we find an <span class="ltx_text ltx_font_italic">approximate inverse</span> of this process. But in general, the operation of increasing entropy does not have an inverse, as information from the original distribution may be destroyed. We will thus tackle a special case where (1) the operation of adding entropy takes on a simple, computable, and reversible form; (2) we can obtain a (parametric) encoding of the data distribution, as alluded to in the above pair of approaches. As we will see, the above two factors will ensure that our approach is possible.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">We will increase the entropy in arguably the simplest possible way, i.e., <span class="ltx_text ltx_font_italic">adding isotropic Gaussian noise</span>. More precisely, given the random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we can consider the <span class="ltx_text ltx_font_italic">stochastic process</span> <math alttext="(\bm{x}_{t})_{t\in[0,T]}" class="ltx_Math" display="inline" id="S2.SS1.p2.m2"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></msub><annotation encoding="application/x-tex">(\bm{x}_{t})_{t\in[0,T]}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT</annotation></semantics></math> which adds gradual noise to it, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{t}\doteq\bm{x}+t\bm{g},\qquad\forall t\in[0,T]," class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>≐</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{t}\doteq\bm{x}+t\bm{g},\qquad\forall t\in[0,T],</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ bold_italic_x + italic_t bold_italic_g , ∀ italic_t ∈ [ 0 , italic_T ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="T\in[0,\infty]" class="ltx_Math" display="inline" id="S2.SS1.p2.m3"><semantics><mrow><mi>T</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">T\in[0,\infty]</annotation><annotation encoding="application/x-llamapun">italic_T ∈ [ 0 , ∞ ]</annotation></semantics></math> is a time horizon and <math alttext="\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S2.SS1.p2.m4"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> is drawn independently of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. This process is an example of a <span class="ltx_text ltx_font_italic">diffusion process</span>, so-named because it spreads out the probability mass out over all of <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.p2.m6"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> as time goes on, increasing the entropy over time. This intuition is confirmed graphically by <a class="ltx_ref" href="#F3" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, and rigorously via the following theorem.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Simplified Version of <a class="ltx_ref" href="A2.html#Thmtheorem2" title="Theorem B.2 (Diffusion Increases Entropy). ‣ B.2.1 Diffusion Process Increases Entropy Over Time ‣ B.2 Diffusion and Denoising Processes ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">B.2</span></a>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="(\bm{x}_{t})_{t\in[0,T]}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m1"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></msub><annotation encoding="application/x-tex">(\bm{x}_{t})_{t\in[0,T]}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT</annotation></semantics></math> follows the model (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). For any <math alttext="t\in(0,T]" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m2"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in(0,T]</annotation><annotation encoding="application/x-llamapun">italic_t ∈ ( 0 , italic_T ]</annotation></semantics></math>, the random variable <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m3"><semantics><msub><mi>𝐱</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> has differential entropy <math alttext="h(\bm{x}_{t})&gt;-\infty" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m4"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t})&gt;-\infty</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) &gt; - ∞</annotation></semantics></math>. Moreover, under certain technical conditions on <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m5"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\mathrm{d}}{\mathrm{d}t}h(\bm{x}_{t})&gt;0,\qquad\forall t\in(0,T]," class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mrow><mrow><mfrac><mi mathvariant="normal">d</mi><mrow><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\mathrm{d}}{\mathrm{d}t}h(\bm{x}_{t})&gt;0,\qquad\forall t\in(0,T],</annotation><annotation encoding="application/x-llamapun">divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) &gt; 0 , ∀ italic_t ∈ ( 0 , italic_T ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">showing that the entropy of the noised <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m6"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> increases over time <math alttext="t" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m7"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">The proof is elementary, but it is rather long, so we postpone it to <a class="ltx_ref" href="A2.html#S2.SS1" title="B.2.1 Diffusion Process Increases Entropy Over Time ‣ B.2 Diffusion and Denoising Processes ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.2.1</span></a>. The main as-yet unstated implication of this result is that <math alttext="h(\bm{x}_{t})&gt;h(\bm{x})" class="ltx_Math" display="inline" id="S2.SS1.p3.m1"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t})&gt;h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) &gt; italic_h ( bold_italic_x )</annotation></semantics></math> for every <math alttext="t&gt;0" class="ltx_Math" display="inline" id="S2.SS1.p3.m2"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t&gt;0</annotation><annotation encoding="application/x-llamapun">italic_t &gt; 0</annotation></semantics></math>. To see this, note that if <math alttext="h(\bm{x})=-\infty" class="ltx_Math" display="inline" id="S2.SS1.p3.m3"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x})=-\infty</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) = - ∞</annotation></semantics></math> then <math alttext="h(\bm{x}_{t})&gt;-\infty" class="ltx_Math" display="inline" id="S2.SS1.p3.m4"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t})&gt;-\infty</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) &gt; - ∞</annotation></semantics></math> for all <math alttext="t&gt;0" class="ltx_Math" display="inline" id="S2.SS1.p3.m5"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t&gt;0</annotation><annotation encoding="application/x-llamapun">italic_t &gt; 0</annotation></semantics></math>, and if <math alttext="h(\bm{x})&gt;-\infty" class="ltx_Math" display="inline" id="S2.SS1.p3.m6"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x})&gt;-\infty</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) &gt; - ∞</annotation></semantics></math> then <math alttext="h(\bm{x}_{t})=h(\bm{x})+\int_{0}^{t}[\frac{\mathrm{d}}{\mathrm{d}s}h(\bm{x}_{s})]\mathrm{d}s&gt;h(\bm{x})" class="ltx_Math" display="inline" id="S2.SS1.p3.m7"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.055em">+</mo><mrow><msubsup><mo rspace="0em">∫</mo><mn>0</mn><mi>t</mi></msubsup><mrow><mrow><mo stretchy="false">[</mo><mrow><mfrac><mi mathvariant="normal">d</mi><mrow><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>s</mi></mrow></mrow></mrow></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t})=h(\bm{x})+\int_{0}^{t}[\frac{\mathrm{d}}{\mathrm{d}s}h(\bm{x}_{s})]\mathrm{d}s&gt;h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_h ( bold_italic_x ) + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT [ divide start_ARG roman_d end_ARG start_ARG roman_d italic_s end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ] roman_d italic_s &gt; italic_h ( bold_italic_x )</annotation></semantics></math> by the fundamental theorem of calculus, so in both cases <math alttext="h(\bm{x}_{t})&gt;h(\bm{x})" class="ltx_Math" display="inline" id="S2.SS1.p3.m8"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t})&gt;h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) &gt; italic_h ( bold_italic_x )</annotation></semantics></math> for every <math alttext="t&gt;0" class="ltx_Math" display="inline" id="S2.SS1.p3.m9"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t&gt;0</annotation><annotation encoding="application/x-llamapun">italic_t &gt; 0</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F3"><img alt="Figure 3.3 : Diffusing a mixture of Gaussians. From left to right, we observe the evolution of the density as t t italic_t grows from 0 to 10 10 10 , along with some representative samples. Each region is colored by its density ( 0.0 0.0 0.0 is completely white, &gt; 0.01 &gt;0.01 &gt; 0.01 is very dark blue, every other value maps to some shade of blue in between.) We observe that the probability mass gets less concentrated as t t italic_t increases, signaling that entropy increases." class="ltx_graphics ltx_img_landscape" height="104" id="F3.g1" src="chapters/chapter3/figs/ve_forward_diffusion_density.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Diffusing a mixture of Gaussians.<span class="ltx_text ltx_font_medium"> From left to right, we observe the evolution of the density as <math alttext="t" class="ltx_Math" display="inline" id="F3.m7"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> grows from <math alttext="0" class="ltx_Math" display="inline" id="F3.m8"><mn>0</mn></math> to <math alttext="10" class="ltx_Math" display="inline" id="F3.m9"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation><annotation encoding="application/x-llamapun">10</annotation></semantics></math>, along with some representative samples. Each region is colored by its density (<math alttext="0.0" class="ltx_Math" display="inline" id="F3.m10"><semantics><mn>0.0</mn><annotation encoding="application/x-tex">0.0</annotation><annotation encoding="application/x-llamapun">0.0</annotation></semantics></math> is completely white, <math alttext="&gt;0.01" class="ltx_Math" display="inline" id="F3.m11"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">&gt;0.01</annotation><annotation encoding="application/x-llamapun">&gt; 0.01</annotation></semantics></math> is very dark blue, every other value maps to some shade of blue in between.) We observe that the probability mass gets less concentrated as <math alttext="t" class="ltx_Math" display="inline" id="F3.m12"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> increases, signaling that entropy increases.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p">The inverse operation to adding noise is known as <span class="ltx_text ltx_font_italic">denoising</span>. It is a classical and well-studied topic in signal processing and system theory, such as the Wiener filter and the Kalman filter. The several problems discussed in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>, such as PCA, ICA, and Dictionary Learning, are specific instances of the denoising problem. For a fixed <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p4.m1"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> and the additive Gaussian noise model (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), the denoising problem can be formulated as attempting to learn a function <math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.m2"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> which forms the best possible approximation (in expectation) of the true random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p4.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, given both <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p4.m4"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p4.m5"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)\in\operatorname*{arg\ min}_{\bar{\bm{x}}(t,\cdot)}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bm{x}-\bar{\bm{x}}(t,\bm{x}_{t})\|_{2}^{2}." class="ltx_Math" display="block" id="S2.E3.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)\in\operatorname*{arg\ min}_{\bar{\bm{x}}(t,\cdot)}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bm{x}-\bar{\bm{x}}(t,\bm{x}_{t})\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ ) ∈ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over¯ start_ARG bold_italic_x end_ARG ( italic_t , ⋅ ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x - over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The solution to this problem, when optimizing <math alttext="\bar{\bm{x}}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.m6"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG ( italic_t , ⋅ )</annotation></semantics></math> over all possible (square-integrable) functions, is the so-called <span class="ltx_text ltx_font_italic">Bayes optimal denoiser</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{\xi})\doteq\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]." class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{\xi})\doteq\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}].</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_ξ ) ≐ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p">This expression justifies the notation <math alttext="\bar{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS1.p5.m1"><semantics><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, which is meant to compute a conditional expectation (i.e., conditional mean or conditional average). In short, it attempts to remove the noise from the noisy input, outputting the best possible guess (in expectation and w.r.t. the <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S2.SS1.p5.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-distance) of the (de-noised) original random variable.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Denoising Gaussian Noise from a Mixture of Gaussians)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample2.p1">
<p class="ltx_p">In this example we compute the Bayes optimal denoiser for an incredibly important class of distributions, the Gaussian mixture model. To start, let us fix parameters for the distribution: mixture weights <math alttext="\bm{\pi}\in\mathbb{R}^{K}" class="ltx_Math" display="inline" id="Thmexample2.p1.m1"><semantics><mrow><mi>𝝅</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\pi}\in\mathbb{R}^{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_π ∈ blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>, component means <math alttext="\{\bm{\mu}_{k}\}_{k=1}^{K}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexample2.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{\mu}_{k}\}_{k=1}^{K}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, and component covariances <math alttext="\{\bm{\Sigma}_{k}\}_{k=1}^{K}\subseteq\mathsf{PSD}(D)" class="ltx_Math" display="inline" id="Thmexample2.p1.m3"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝚺</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo>⊆</mo><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\{\bm{\Sigma}_{k}\}_{k=1}^{K}\subseteq\mathsf{PSD}(D)</annotation><annotation encoding="application/x-llamapun">{ bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ⊆ sansserif_PSD ( italic_D )</annotation></semantics></math>, where <math alttext="\mathsf{PSD}(D)" class="ltx_Math" display="inline" id="Thmexample2.p1.m4"><semantics><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{PSD}(D)</annotation><annotation encoding="application/x-llamapun">sansserif_PSD ( italic_D )</annotation></semantics></math> is the set of <math alttext="D\times D" class="ltx_Math" display="inline" id="Thmexample2.p1.m5"><semantics><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">D\times D</annotation><annotation encoding="application/x-llamapun">italic_D × italic_D</annotation></semantics></math> symmetric positive semidefinite matrices. Now, suppose <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is generated by the following two-step procedure:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p">First, an index (or <span class="ltx_text ltx_font_italic">label</span>) <math alttext="y\in[K]" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m1"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y\in[K]</annotation><annotation encoding="application/x-llamapun">italic_y ∈ [ italic_K ]</annotation></semantics></math> is sampled such that <math alttext="y=k" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m2"><semantics><mrow><mi>y</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">y=k</annotation><annotation encoding="application/x-llamapun">italic_y = italic_k</annotation></semantics></math> with probability <math alttext="\pi_{k}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m3"><semantics><msub><mi>π</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\pi_{k}</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">Second, <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is sampled from the normal distribution <math alttext="\operatorname{\mathcal{N}}(\bm{\mu}_{y},\bm{\Sigma}_{y})" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m2"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝁</mi><mi>y</mi></msub><mo>,</mo><msub><mi>𝚺</mi><mi>y</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{\mu}_{y},\bm{\Sigma}_{y})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p">Then <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has distribution</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\sim\sum_{k=1}^{K}\pi_{k}\operatorname{\mathcal{N}}(\bm{\mu}_{k},\bm{\Sigma}_{k})," class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo rspace="0.111em">∼</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝚺</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}\sim\sum_{k=1}^{K}\pi_{k}\operatorname{\mathcal{N}}(\bm{\mu}_{k},\bm{\Sigma}_{k}),</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and so</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{t}=\bm{x}+t\bm{g}\sim\sum_{k=1}^{K}\pi_{k}\operatorname{\mathcal{N}}(\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})." class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo rspace="0.111em">∼</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\bm{x}+t\bm{g}\sim\sum_{k=1}^{K}\pi_{k}\operatorname{\mathcal{N}}(\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I}).</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g ∼ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let us define <math alttext="\varphi(\bm{x};\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample2.p1.m8"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\bm{x};\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">italic_φ ( bold_italic_x ; bold_italic_μ , bold_Σ )</annotation></semantics></math> as the probability density of <math alttext="\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample2.p1.m9"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_italic_μ , bold_Σ )</annotation></semantics></math> evaluated at <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. In this notation, the density of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample2.p1.m11"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{t}(\bm{x}_{t})=\sum_{k=1}^{K}\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})." class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{t}(\bm{x}_{t})=\sum_{k=1}^{K}\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I}).</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Thmexample2.p2">
<p class="ltx_p">Conditioned on <math alttext="y" class="ltx_Math" display="inline" id="Thmexample2.p2.m1"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>, the variables are jointly Gaussian: if we say that <math alttext="\bm{x}=\bm{\mu}_{y}+\bm{\Sigma}_{y}^{1/2}\bm{u}" class="ltx_Math" display="inline" id="Thmexample2.p2.m2"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><msub><mi>𝝁</mi><mi>y</mi></msub><mo>+</mo><mrow><msubsup><mi>𝚺</mi><mi>y</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{\mu}_{y}+\bm{\Sigma}_{y}^{1/2}\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT bold_italic_u</annotation></semantics></math> where <math alttext="(\cdot)^{1/2}" class="ltx_Math" display="inline" id="Thmexample2.p2.m3"><semantics><msup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">(\cdot)^{1/2}</annotation><annotation encoding="application/x-llamapun">( ⋅ ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> is the matrix square root and <math alttext="\bm{u}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="Thmexample2.p2.m4"><semantics><mrow><mi>𝒖</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{u}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_u ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> independently of <math alttext="y" class="ltx_Math" display="inline" id="Thmexample2.p2.m5"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> (and <math alttext="\bm{g}" class="ltx_Math" display="inline" id="Thmexample2.p2.m6"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_g</annotation></semantics></math>), then we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\end{bmatrix}=\begin{bmatrix}\bm{\mu}_{y}\\
\bm{\mu}_{y}\end{bmatrix}+\begin{bmatrix}\bm{\Sigma}_{y}^{1/2}&amp;\bm{0}\\
\bm{\Sigma}_{y}^{1/2}&amp;t\bm{I}\end{bmatrix}\begin{bmatrix}\bm{u}\\
\bm{g}\end{bmatrix}." class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒙</mi></mtd></mtr><mtr><mtd><msub><mi>𝒙</mi><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝝁</mi><mi>y</mi></msub></mtd></mtr><mtr><mtd><msub><mi>𝝁</mi><mi>y</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>+</mo><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msubsup><mi>𝚺</mi><mi>y</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mi>y</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup></mtd><mtd><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒖</mi></mtd></mtr><mtr><mtd><mi>𝒈</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\end{bmatrix}=\begin{bmatrix}\bm{\mu}_{y}\\
\bm{\mu}_{y}\end{bmatrix}+\begin{bmatrix}\bm{\Sigma}_{y}^{1/2}&amp;\bm{0}\\
\bm{\Sigma}_{y}^{1/2}&amp;t\bm{I}\end{bmatrix}\begin{bmatrix}\bm{u}\\
\bm{g}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] + [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_t bold_italic_I end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_u end_CELL end_ROW start_ROW start_CELL bold_italic_g end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This shows that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample2.p2.m8"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are jointly Gaussian (conditioned on <math alttext="y" class="ltx_Math" display="inline" id="Thmexample2.p2.m9"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>) as claimed. Thus we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\end{bmatrix}\sim\operatorname{\mathcal{N}}\left(\begin{bmatrix}\bm{\mu}_{y}\\
\bm{\mu}_{y}\end{bmatrix},\begin{bmatrix}\bm{\Sigma}_{y}&amp;\bm{\Sigma}_{y}\\
\bm{\Sigma}_{y}&amp;\bm{\Sigma}_{y}+t^{2}\bm{I}\end{bmatrix}\right)." class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒙</mi></mtd></mtr><mtr><mtd><msub><mi>𝒙</mi><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝝁</mi><mi>y</mi></msub></mtd></mtr><mtr><mtd><msub><mi>𝝁</mi><mi>y</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>y</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mi>y</mi></msub></mtd></mtr><mtr><mtd><msub><mi>𝚺</mi><mi>y</mi></msub></mtd><mtd><mrow><msub><mi>𝚺</mi><mi>y</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\end{bmatrix}\sim\operatorname{\mathcal{N}}\left(\begin{bmatrix}\bm{\mu}_{y}\\
\bm{\mu}_{y}\end{bmatrix},\begin{bmatrix}\bm{\Sigma}_{y}&amp;\bm{\Sigma}_{y}\\
\bm{\Sigma}_{y}&amp;\bm{\Sigma}_{y}+t^{2}\bm{I}\end{bmatrix}\right).</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∼ caligraphic_N ( [ start_ARG start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] , [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I end_CELL end_ROW end_ARG ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus the conditional expectation of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p2.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample2.p2.m11"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> (i.e., the
Bayes optimal denoiser conditioned on <math alttext="y" class="ltx_Math" display="inline" id="Thmexample2.p2.m12"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>) is famously
(<a class="ltx_ref" href="#Thmexercise2" title="Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>)</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y]=\bm{\mu}_{y}+\bm{\Sigma}_{y}(\bm{\Sigma}_{y}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{y})." class="ltx_Math" display="block" id="S2.E10.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝝁</mi><mi>y</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝚺</mi><mi>y</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><msub><mi>𝝁</mi><mi>y</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y]=\bm{\mu}_{y}+\bm{\Sigma}_{y}(\bm{\Sigma}_{y}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{y}).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ] = bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ( bold_Σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">To find the overall Bayes optimal denoiser, we use the law of iterated expectation, obtaining</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx15">
<tbody id="S2.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})" class="ltx_Math" display="inline" id="S2.E11.m1"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S2.E11.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.11)</span></td>
</tr></tbody>
<tbody id="S2.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{\mathbb{E}}[\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y]\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S2.E12.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{\mathbb{E}}[\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y]\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ] ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.12)</span></td>
</tr></tbody>
<tbody id="S2.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{k=1}^{K}\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y=k]." class="ltx_Math" display="inline" id="S2.E13.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mrow><mi>ℙ</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>y</mi><mo>=</mo><mrow><mi>k</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi></mrow></mrow><mo>=</mo><mi>k</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{k=1}^{K}\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y=k].</annotation><annotation encoding="application/x-llamapun">= ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT blackboard_P [ italic_y = italic_k ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y = italic_k ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The probability can be dealt with as follows. Let <math alttext="p_{t\mid y}" class="ltx_Math" display="inline" id="Thmexample2.p2.m13"><semantics><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">p_{t\mid y}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t ∣ italic_y end_POSTSUBSCRIPT</annotation></semantics></math> be the probability density of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample2.p2.m14"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> conditioned on the value of <math alttext="y" class="ltx_Math" display="inline" id="Thmexample2.p2.m15"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>. Then</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx16">
<tbody id="S2.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S2.E14.m1"><semantics><mrow><mi>ℙ</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>y</mi><mo>=</mo><mrow><mi>k</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_P [ italic_y = italic_k ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{p_{t\mid y}(\bm{x}_{t}\mid k)\pi_{k}}{p_{t}(\bm{x}_{t})}" class="ltx_Math" display="inline" id="S2.E14.m2"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mi>y</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>π</mi><mi>k</mi></msub></mrow><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{p_{t\mid y}(\bm{x}_{t}\mid k)\pi_{k}}{p_{t}(\bm{x}_{t})}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG italic_p start_POSTSUBSCRIPT italic_t ∣ italic_y end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ italic_k ) italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.14)</span></td>
</tr></tbody>
<tbody id="S2.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\bm{\mu}_{i},\bm{\Sigma}_{i}+t^{2}\bm{I})}." class="ltx_Math" display="inline" id="S2.E15.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>π</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><msub><mi>𝝁</mi><mi>i</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>i</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\bm{\mu}_{i},\bm{\Sigma}_{i}+t^{2}\bm{I})}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">On the other hand, the conditional expectation is as described before:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y=k]=\bm{\mu}_{k}+\bm{\Sigma}_{k}(\bm{\Sigma}_{k}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{k})." class="ltx_Math" display="block" id="S2.E16.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi></mrow></mrow><mo>=</mo><mi>k</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝝁</mi><mi>k</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><msub><mi>𝝁</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t},y=k]=\bm{\mu}_{k}+\bm{\Sigma}_{k}(\bm{\Sigma}_{k}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{k}).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y = italic_k ] = bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">So putting this all together, the true Bayes optimal denoiser is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\bm{\mu}_{i},\bm{\Sigma}_{i}+t^{2}\bm{I})}\cdot\left(\bm{\mu}_{k}+\bm{\Sigma}_{k}(\bm{\Sigma}_{k}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{k})\right)." class="ltx_Math" display="block" id="S2.E17.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>π</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><msub><mi>𝝁</mi><mi>i</mi></msub><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>i</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo>(</mo><mrow><msub><mi>𝝁</mi><mi>k</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><msub><mi>𝝁</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\pi_{k}\varphi(\bm{x}_{t};\bm{\mu}_{k},\bm{\Sigma}_{k}+t^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\bm{\mu}_{i},\bm{\Sigma}_{i}+t^{2}\bm{I})}\cdot\left(\bm{\mu}_{k}+\bm{\Sigma}_{k}(\bm{\Sigma}_{k}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu}_{k})\right).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG ⋅ ( bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This example is particularly important, and several special cases will give us great conceptual insight later. For now, let us attempt to extract some geometric intuition from the functional form of the optimal denoiser (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>).</p>
</div>
<div class="ltx_para" id="Thmexample2.p3">
<p class="ltx_p">To try to understand (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>) intuitively, let us first set <math alttext="K=1" class="ltx_Math" display="inline" id="Thmexample2.p3.m1"><semantics><mrow><mi>K</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">K=1</annotation><annotation encoding="application/x-llamapun">italic_K = 1</annotation></semantics></math> (i.e., one Gaussian) such that <math alttext="\bm{x}\sim\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample2.p3.m2"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ caligraphic_N ( bold_italic_μ , bold_Σ )</annotation></semantics></math>. Let us then diagonalize <math alttext="\bm{\Sigma}=\bm{V}\bm{\Lambda}\bm{V}^{\top}" class="ltx_Math" display="inline" id="Thmexample2.p3.m3"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{\Sigma}=\bm{V}\bm{\Lambda}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_Σ = bold_italic_V bold_Λ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>. Then the Bayes optimal denoiser is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\bm{\mu}+\bm{\Sigma}(\bm{\Sigma}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu})=\bm{\mu}+\bm{V}\begin{bmatrix}\lambda_{1}/(\lambda_{1}+t^{2})&amp;&amp;\\
&amp;\ddots&amp;\\
&amp;&amp;\lambda_{D}/(\lambda_{D}+t^{2})\end{bmatrix}\bm{V}^{\top}(\bm{x}_{t}-\bm{\mu})," class="ltx_Math" display="block" id="S2.E18.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝝁</mi><mo>+</mo><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>𝝁</mi><mo>+</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd><mi mathvariant="normal">⋱</mi></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd></mtd><mtd><mrow><msub><mi>λ</mi><mi>D</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>D</mi></msub><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\bm{\mu}+\bm{\Sigma}(\bm{\Sigma}+t^{2}\bm{I})^{-1}(\bm{x}_{t}-\bm{\mu})=\bm{\mu}+\bm{V}\begin{bmatrix}\lambda_{1}/(\lambda_{1}+t^{2})&amp;&amp;\\
&amp;\ddots&amp;\\
&amp;&amp;\lambda_{D}/(\lambda_{D}+t^{2})\end{bmatrix}\bm{V}^{\top}(\bm{x}_{t}-\bm{\mu}),</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = bold_italic_μ + bold_Σ ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ ) = bold_italic_μ + bold_italic_V [ start_ARG start_ROW start_CELL italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / ( italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL ⋱ end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL italic_λ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT / ( italic_λ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_CELL end_ROW end_ARG ] bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda_{1},\dots,\lambda_{D}" class="ltx_Math" display="inline" id="Thmexample2.p3.m4"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>λ</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{1},\dots,\lambda_{D}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_λ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math> are the eigenvalues of <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p3.m5"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math>. We can observe that this denoiser has three steps:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p">Translate the input <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> by <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m2"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p">Contract the (translated) input <math alttext="\bm{x}_{t}-\bm{\mu}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m1"><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mi>𝝁</mi></mrow><annotation encoding="application/x-tex">\bm{x}_{t}-\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ</annotation></semantics></math> in each eigenvector direction by a quantity <math alttext="\lambda_{i}/(\lambda_{i}+t^{2})" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m2"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda_{i}/(\lambda_{i}+t^{2})</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>. If the translated input is low-rank and some eigenvalues of <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m3"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math> are zero, these directions get immediately contracted to <math alttext="0" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m4"><mn>0</mn></math> by the denoiser, ensuring that the output of the contraction is similarly low-rank.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p">Translate the output back by <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m1"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p">It is easy to show that it contracts the current <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample2.p3.m6"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> towards the mean <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="Thmexample2.p3.m7"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\|\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})-\bm{\mu}\|_{2}\leq\|\bm{x}_{t}-\bm{\mu}\|_{2}." class="ltx_Math" display="block" id="S2.E19.m1"><semantics><mrow><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\|\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})-\bm{\mu}\|_{2}\leq\|\bm{x}_{t}-\bm{\mu}\|_{2}.</annotation><annotation encoding="application/x-llamapun">∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_μ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_μ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.19)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="Thmexample2.p4">
<p class="ltx_p">This is the geometric interpretation of the denoiser of a <span class="ltx_text ltx_font_italic">single</span> Gaussian. The overall denoiser of the Gaussian mixture model (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>) uses <math alttext="K" class="ltx_Math" display="inline" id="Thmexample2.p4.m1"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> such denoisers, weighting their output by the posterior probabilities <math alttext="\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="Thmexample2.p4.m2"><semantics><mrow><mi>ℙ</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>y</mi><mo>=</mo><mrow><mi>k</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{P}}[y=k\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_P [ italic_y = italic_k ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math>. If the means of the Gaussians are well-separated, these posterior probabilities are very close to <math alttext="0" class="ltx_Math" display="inline" id="Thmexample2.p4.m3"><mn>0</mn></math> or <math alttext="1" class="ltx_Math" display="inline" id="Thmexample2.p4.m4"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> near each mean or cluster. In this regime, the overall denoiser (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>) has the same geometric interpretation as the above single Gaussian denoiser.</p>
</div>
<div class="ltx_para" id="Thmexample2.p5">
<p class="ltx_p">At first glance, such a contraction mapping (<a class="ltx_ref" href="#S2.E19" title="Equation 3.2.19 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.19</span></a>) may appear similar to power iterations (see <a class="ltx_ref" href="Ch2.html#S1.SS2" title="2.1.2 Pursuing Low-rank Structure via Power Iteration ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1.2</span></a>). However, the two are fundamentally different. Power iteration implements a contraction mapping towards a subspace—namely the subspace spanned by the first principal component. In contrast, the iterates in (<a class="ltx_ref" href="#S2.E19" title="Equation 3.2.19 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.19</span></a>) converge to the mean <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="Thmexample2.p5.m1"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> of the underlying distribution, which is a single point.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample2.p5.m2"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<figure class="ltx_figure" id="F4"><img alt="Figure 3.4 : Bayes optimal denoiser and score of a Gaussian mixture model. In the same setting as Figure 3.3 , we demonstrate the effect of the Bayes optimal denoiser 𝒙 ¯ ∗ \bar{\bm{x}}^{\ast} over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT by plotting 𝒙 t \bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (red) and 𝒙 ¯ ∗ ​ ( t , 𝒙 t ) \bar{\bm{x}}^{\ast}(t,\bm{x}_{t}) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (green) for some choice t t italic_t and 𝒙 t \bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . By Tweedie’s formula Theorem 3.3 , the residual between them is proportional to the so-called (Hyvärinen) score ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) \nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t}) ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . We can see that the score points towards the modes of the distribution of 𝒙 t \bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ." class="ltx_graphics ltx_img_landscape" height="104" id="F4.g1" src="chapters/chapter3/figs/ve_forward_diffusion_denoising.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Bayes optimal denoiser and score of a Gaussian mixture model.<span class="ltx_text ltx_font_medium"> In the same setting as <a class="ltx_ref" href="#F3" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, we demonstrate the effect of the Bayes optimal denoiser <math alttext="\bar{\bm{x}}^{\ast}" class="ltx_Math" display="inline" id="F4.m8"><semantics><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> by plotting <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="F4.m9"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> (red) and <math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})" class="ltx_Math" display="inline" id="F4.m10"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> (green) for some choice <math alttext="t" class="ltx_Math" display="inline" id="F4.m11"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="F4.m12"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. By Tweedie’s formula <a class="ltx_ref" href="#Thmtheorem3" title="Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, the residual between them is proportional to the so-called (Hyvärinen) score <math alttext="\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})" class="ltx_Math" display="inline" id="F4.m13"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>. We can see that the score points towards the modes of the distribution of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="F4.m14"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p">Intuitively, and as we can see from <a class="ltx_ref" href="#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the Bayes optimal denoiser <math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p6.m1"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> should move its input <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p6.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> towards the modes of the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p6.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. It turns out that, actually, we can quantify this by showing that the Bayes optimal denoiser <span class="ltx_text ltx_font_italic">takes a gradient ascent step</span> on the (log-)density of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p6.m4"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, which (recall) we denoted <math alttext="p_{t}" class="ltx_Math" display="inline" id="S2.SS1.p6.m5"><semantics><msub><mi>p</mi><mi>t</mi></msub><annotation encoding="application/x-tex">p_{t}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. That is, following the denoiser means moving from the input iterate to a region of higher probability within this (perturbed) distribution. For small <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p6.m6"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, the perturbation is small so our initial intutition is therefore (almost) exactly right. The picture is visualized in <a class="ltx_ref" href="#F4" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.4</span></a> and rigorously formulated as Tweedie’s formula <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx230" title="">Rob56</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.3</span></span><span class="ltx_text ltx_font_bold"> </span>(Tweedie’s Formula)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="(\bm{x}_{t})_{t\in[0,T]}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m1"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></msub><annotation encoding="application/x-tex">(\bm{x}_{t})_{t\in[0,T]}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT</annotation></semantics></math> obeys (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). Let <math alttext="p_{t}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m2"><semantics><msub><mi>p</mi><mi>t</mi></msub><annotation encoding="application/x-tex">p_{t}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> be the density of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m3"><semantics><msub><mi>𝐱</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> (as previously declared). Then</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]=\bm{x}_{t}+t^{2}\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})." class="ltx_Math" display="block" id="S2.E20.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]=\bm{x}_{t}+t^{2}\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t}).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.20)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p">For the proof let us suppose that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p7.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has a density (even though the
theorem is true without this assumption), and call this density <math alttext="p" class="ltx_Math" display="inline" id="S2.SS1.p7.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>. Let
<math alttext="p_{0\mid t}" class="ltx_Math" display="inline" id="S2.SS1.p7.m3"><semantics><msub><mi>p</mi><mrow><mn>0</mn><mo>∣</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">p_{0\mid t}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT 0 ∣ italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p_{t\mid 0}" class="ltx_Math" display="inline" id="S2.SS1.p7.m4"><semantics><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mn>0</mn></mrow></msub><annotation encoding="application/x-tex">p_{t\mid 0}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t ∣ 0 end_POSTSUBSCRIPT</annotation></semantics></math> be the conditional densities of <math alttext="\bm{x}=\bm{x}_{0}" class="ltx_Math" display="inline" id="S2.SS1.p7.m5"><semantics><mrow><mi>𝒙</mi><mo>=</mo><msub><mi>𝒙</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{x}_{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p7.m6"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p7.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p7.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> respectively.
Let <math alttext="\varphi(\bm{x};\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="S2.SS1.p7.m9"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\bm{x};\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">italic_φ ( bold_italic_x ; bold_italic_μ , bold_Σ )</annotation></semantics></math> be the density of <math alttext="\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="S2.SS1.p7.m10"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_italic_μ , bold_Σ )</annotation></semantics></math> evaluated at <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p7.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, so that <math alttext="p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})=\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS1.p7.m12"><semantics><mrow><mrow><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mn>0</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})=\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t ∣ 0 end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x ) = italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>. Then a simple calculation gives</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx17">
<tbody id="S2.E21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})" class="ltx_Math" display="inline" id="S2.E21.m1"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\nabla_{\bm{x}_{t}}p_{t}(\bm{x}_{t})}{p_{t}(\bm{x}_{t})}" class="ltx_Math" display="inline" id="S2.E21.m2"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\nabla_{\bm{x}_{t}}p_{t}(\bm{x}_{t})}{p_{t}(\bm{x}_{t})}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.21)</span></td>
</tr></tbody>
<tbody id="S2.E22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\nabla_{\bm{x}_{t}}\int_{\mathbb{R}^{D}}p(\bm{x})p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E22.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mn>0</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\nabla_{\bm{x}_{t}}\int_{\mathbb{R}^{D}}p(\bm{x})p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_p start_POSTSUBSCRIPT italic_t ∣ 0 end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x ) roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.22)</span></td>
</tr></tbody>
<tbody id="S2.E23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\nabla_{\bm{x}_{t}}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E23.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\nabla_{\bm{x}_{t}}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.23)</span></td>
</tr></tbody>
<tbody id="S2.E24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})[\nabla_{\bm{x}_{t}}\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})]\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E24.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mi>φ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})[\nabla_{\bm{x}_{t}}\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})]\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) [ ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) ] roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.24)</span></td>
</tr></tbody>
<tbody id="S2.E25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\left[-\frac{\bm{x}_{t}-\bm{x}}{t^{2}}\right]\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E25.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mi>𝒙</mi></mrow><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\left[-\frac{\bm{x}_{t}-\bm{x}}{t^{2}}\right]\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) [ - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_x end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ] roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.25)</span></td>
</tr></tbody>
<tbody id="S2.E26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})[\bm{x}-\bm{x}_{t}]\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E26.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})[\bm{x}-\bm{x}_{t}]\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) [ bold_italic_x - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.26)</span></td>
</tr></tbody>
<tbody id="S2.E27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\mathrm{d}\bm{x}" class="ltx_Math" display="inline" id="S2.E27.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><msub><mi>𝒙</mi><mi>t</mi></msub><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mi>𝒙</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})\varphi(\bm{x}_{t};\bm{x},t^{2}\bm{I})\mathrm{d}\bm{x}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) bold_italic_x roman_d bold_italic_x - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_x , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) roman_d bold_italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.27)</span></td>
</tr></tbody>
<tbody id="S2.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}p_{t}(\bm{x}_{t})" class="ltx_Math" display="inline" id="S2.E28.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mrow><mi>t</mi><mo>∣</mo><mn>0</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><msub><mi>𝒙</mi><mi>t</mi></msub><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p(\bm{x})p_{t\mid 0}(\bm{x}_{t}\mid\bm{x})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}p_{t}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ) italic_p start_POSTSUBSCRIPT italic_t ∣ 0 end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x ) bold_italic_x roman_d bold_italic_x - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.28)</span></td>
</tr></tbody>
<tbody id="S2.E29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p_{t}(\bm{x}_{t})p_{0\mid t}(\bm{x}\mid\bm{x}_{t})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}p_{t}(\bm{x}_{t})" class="ltx_Math" display="inline" id="S2.E29.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mrow><mn>0</mn><mo>∣</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><msub><mi>𝒙</mi><mi>t</mi></msub><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}p_{t}(\bm{x}_{t})}\int_{\mathbb{R}^{D}}p_{t}(\bm{x}_{t})p_{0\mid t}(\bm{x}\mid\bm{x}_{t})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}p_{t}(\bm{x}_{t})}p_{t}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_p start_POSTSUBSCRIPT 0 ∣ italic_t end_POSTSUBSCRIPT ( bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_x roman_d bold_italic_x - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.29)</span></td>
</tr></tbody>
<tbody id="S2.E30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\int_{\mathbb{R}^{D}}p_{0\mid t}(\bm{x}\mid\bm{x}_{t})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}}" class="ltx_Math" display="inline" id="S2.E30.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>D</mi></msup></msub></mstyle><mrow><msub><mi>p</mi><mrow><mn>0</mn><mo>∣</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><msub><mi>𝒙</mi><mi>t</mi></msub><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\int_{\mathbb{R}^{D}}p_{0\mid t}(\bm{x}\mid\bm{x}_{t})\bm{x}\mathrm{d}\bm{x}-\frac{\bm{x}_{t}}{t^{2}}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT 0 ∣ italic_t end_POSTSUBSCRIPT ( bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_x roman_d bold_italic_x - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.30)</span></td>
</tr></tbody>
<tbody id="S2.E31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]-\frac{\bm{x}_{t}}{t^{2}}" class="ltx_Math" display="inline" id="S2.E31.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>−</mo><mstyle displaystyle="true"><mfrac><msub><mi>𝒙</mi><mi>t</mi></msub><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]-\frac{\bm{x}_{t}}{t^{2}}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] - divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.31)</span></td>
</tr></tbody>
<tbody id="S2.E32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]-\bm{x}_{t}}{t^{2}}." class="ltx_Math" display="inline" id="S2.E32.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]-\bm{x}_{t}}{t^{2}}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.32)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Simple rearranging of the above equality proves the theorem.
∎</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p">This result develops a connection between denoising and optimization: the Bayes-optimal denoiser takes a single step of gradient ascent on the perturbed data density <math alttext="p_{t}" class="ltx_Math" display="inline" id="S2.SS1.p8.m1"><semantics><msub><mi>p</mi><mi>t</mi></msub><annotation encoding="application/x-tex">p_{t}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and the step size adaptively becomes smaller (i.e., taking more precise steps) as the perturbation to the data distribution grows smaller. The quantity <math alttext="\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})" class="ltx_Math" display="inline" id="S2.SS1.p8.m2"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> is called the <span class="ltx_text ltx_font_italic">(Hyvärinen) score</span> and frequently appears in discussions about denoising, etc.; it first appeared in a paper of Aapo Hyvärinen in the context of ICA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx117" title="">Hyv05</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p9">
<p class="ltx_p">Similar to how one step of gradient descent is almost never sufficient to
minimize an objective in practice when initializing far from the optimum, the
output of the Bayes-optimal denoiser <math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p9.m1"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> is almost never contained in a high-probability region of the data distribution when <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p9.m2"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> is large, <span class="ltx_text ltx_font_italic">especially</span> when the data have low-dimensional structures. We illustrate this point explicitly in the following example.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.3</span></span><span class="ltx_text ltx_font_italic"> </span>(Denoising a Two-Point Mixture)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample3.p1">
<p class="ltx_p">Let <math alttext="x" class="ltx_Math" display="inline" id="Thmexample3.p1.m1"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math> be uniform on the two-point set <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="Thmexample3.p1.m2"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation><annotation encoding="application/x-llamapun">{ - 1 , + 1 }</annotation></semantics></math> and let <math alttext="(\bm{x}_{t})_{t\in[0,T]}" class="ltx_Math" display="inline" id="Thmexample3.p1.m3"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></msub><annotation encoding="application/x-tex">(\bm{x}_{t})_{t\in[0,T]}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT</annotation></semantics></math> follow (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). This is precisely a degenerate Gaussian mixture model with priors equal to <math alttext="\frac{1}{2}" class="ltx_Math" display="inline" id="Thmexample3.p1.m4"><semantics><mfrac><mn>1</mn><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{1}{2}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG 2 end_ARG</annotation></semantics></math>, means <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="Thmexample3.p1.m5"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation><annotation encoding="application/x-llamapun">{ - 1 , + 1 }</annotation></semantics></math>, and covariances both equal to <math alttext="0" class="ltx_Math" display="inline" id="Thmexample3.p1.m6"><mn>0</mn></math>. For a fixed <math alttext="t&gt;0" class="ltx_Math" display="inline" id="Thmexample3.p1.m7"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t&gt;0</annotation><annotation encoding="application/x-llamapun">italic_t &gt; 0</annotation></semantics></math> we can use the calculation of the Bayes-optimal denoiser in (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>) to obtain (proof as exercise)</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{x}^{\ast}(t,x_{t})=\frac{\varphi(x_{t};+1,t^{2})-\varphi(x_{t};-1,t^{2})}{\varphi(x_{t};1,t^{2})+\varphi(x_{t};-1,t^{2})}=\tanh\left(-\frac{x_{t}}{t^{2}}\right)." class="ltx_Math" display="block" id="S2.E33.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>x</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo>,</mo><msup><mi>t</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><msup><mi>t</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><mn>1</mn><mo>,</mo><msup><mi>t</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><msup><mi>t</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mfrac><msub><mi>x</mi><mi>t</mi></msub><msup><mi>t</mi><mn>2</mn></msup></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{x}^{\ast}(t,x_{t})=\frac{\varphi(x_{t};+1,t^{2})-\varphi(x_{t};-1,t^{2})}{\varphi(x_{t};1,t^{2})+\varphi(x_{t};-1,t^{2})}=\tanh\left(-\frac{x_{t}}{t^{2}}\right).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG italic_φ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; + 1 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) - italic_φ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; - 1 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_φ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; 1 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + italic_φ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; - 1 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG = roman_tanh ( - divide start_ARG italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For <math alttext="t" class="ltx_Math" display="inline" id="Thmexample3.p1.m8"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> near <math alttext="0" class="ltx_Math" display="inline" id="Thmexample3.p1.m9"><mn>0</mn></math>, this quantity is near <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="Thmexample3.p1.m10"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation><annotation encoding="application/x-llamapun">{ - 1 , + 1 }</annotation></semantics></math> for almost all inputs <math alttext="\bar{x}^{\ast}(t,x_{t})" class="ltx_Math" display="inline" id="Thmexample3.p1.m11"><semantics><mrow><msup><mover accent="true"><mi>x</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{x}^{\ast}(t,x_{t})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>. However, for <math alttext="t" class="ltx_Math" display="inline" id="Thmexample3.p1.m12"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> large, this quantity is not necessarily even approximately in the original support of <math alttext="x" class="ltx_Math" display="inline" id="Thmexample3.p1.m13"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math>, which, remember, is <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="Thmexample3.p1.m14"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation><annotation encoding="application/x-llamapun">{ - 1 , + 1 }</annotation></semantics></math>. In particular, for <math alttext="x_{t}\approx 0" class="ltx_Math" display="inline" id="Thmexample3.p1.m15"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x_{t}\approx 0</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≈ 0</annotation></semantics></math> it holds <math alttext="\bar{x}^{\ast}(t,x_{t})\approx 0" class="ltx_Math" display="inline" id="Thmexample3.p1.m16"><semantics><mrow><mrow><msup><mover accent="true"><mi>x</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\bar{x}^{\ast}(t,x_{t})\approx 0</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ≈ 0</annotation></semantics></math> which lies completely in between the two possible points. Thus <math alttext="\bar{x}^{\ast}" class="ltx_Math" display="inline" id="Thmexample3.p1.m17"><semantics><msup><mover accent="true"><mi>x</mi><mo>¯</mo></mover><mo>∗</mo></msup><annotation encoding="application/x-tex">\bar{x}^{\ast}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_italic">will not output “realistic” <math alttext="x" class="ltx_Math" display="inline" id="Thmexample3.p1.m18"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math></span>. Or more mathematically, the distribution of <math alttext="\bar{x}(t,x_{t})" class="ltx_Math" display="inline" id="Thmexample3.p1.m19"><semantics><mrow><mover accent="true"><mi>x</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{x}(t,x_{t})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG italic_x end_ARG ( italic_t , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> is very different from the distribution of <math alttext="x" class="ltx_Math" display="inline" id="Thmexample3.p1.m20"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math>.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample3.p1.m21"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p10">
<p class="ltx_p">Therefore, if we want to denoise the very noisy sample <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="S2.SS1.p10.m1"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> (where—recall—<math alttext="T" class="ltx_Math" display="inline" id="S2.SS1.p10.m2"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation><annotation encoding="application/x-llamapun">italic_T</annotation></semantics></math> is the maximum time), we cannot just use the denoiser <span class="ltx_text ltx_font_italic">once</span>. Instead, we must use the denoiser many times, analogously to gradient descent with <span class="ltx_text ltx_font_italic">decaying step sizes</span>, to converge to a stationary point <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS1.p10.m3"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>. Namely, we shall use the denoiser to go from <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="S2.SS1.p10.m4"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="\hat{\bm{x}}_{T-\delta}" class="ltx_Math" display="inline" id="S2.SS1.p10.m5"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>T</mi><mo>−</mo><mi>δ</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{T-\delta}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T - italic_δ end_POSTSUBSCRIPT</annotation></semantics></math> which approximates <math alttext="\bm{x}_{T-\delta}" class="ltx_Math" display="inline" id="S2.SS1.p10.m6"><semantics><msub><mi>𝒙</mi><mrow><mi>T</mi><mo>−</mo><mi>δ</mi></mrow></msub><annotation encoding="application/x-tex">\bm{x}_{T-\delta}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T - italic_δ end_POSTSUBSCRIPT</annotation></semantics></math>, then from <math alttext="\hat{\bm{x}}_{T-\delta}" class="ltx_Math" display="inline" id="S2.SS1.p10.m7"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>T</mi><mo>−</mo><mi>δ</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{T-\delta}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T - italic_δ end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="\hat{\bm{x}}_{T-2\delta}" class="ltx_Math" display="inline" id="S2.SS1.p10.m8"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>T</mi><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi></mrow></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{T-2\delta}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T - 2 italic_δ end_POSTSUBSCRIPT</annotation></semantics></math>, etc., etc., all the way from <math alttext="\hat{\bm{x}}_{\delta}" class="ltx_Math" display="inline" id="S2.SS1.p10.m9"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>δ</mi></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{\delta}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="\hat{\bm{x}}=\hat{\bm{x}}_{0}" class="ltx_Math" display="inline" id="S2.SS1.p10.m10"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\hat{\bm{x}}_{0}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. Each time we take a denoising step, the action of the denoiser becomes more like a gradient step on the original (log-)density.</p>
</div>
<div class="ltx_para" id="S2.SS1.p11">
<p class="ltx_p">More formally, we uniformly discretize <math alttext="[0,T]" class="ltx_Math" display="inline" id="S2.SS1.p11.m1"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,T]</annotation><annotation encoding="application/x-llamapun">[ 0 , italic_T ]</annotation></semantics></math> into <math alttext="L+1" class="ltx_Math" display="inline" id="S2.SS1.p11.m2"><semantics><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L+1</annotation><annotation encoding="application/x-llamapun">italic_L + 1</annotation></semantics></math> timesteps <math alttext="0=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}=T" class="ltx_Math" display="inline" id="S2.SS1.p11.m3"><semantics><mrow><mn>0</mn><mo>=</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><msub><mi>t</mi><mn>1</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}=T</annotation><annotation encoding="application/x-llamapun">0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = italic_T</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="t_{\ell}=\frac{\ell}{L}T,\qquad\ell\in\{0,1,\dots,L\}." class="ltx_Math" display="block" id="S2.E34.m1"><semantics><mrow><mrow><mrow><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>=</mo><mrow><mfrac><mi mathvariant="normal">ℓ</mi><mi>L</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">t_{\ell}=\frac{\ell}{L}T,\qquad\ell\in\{0,1,\dots,L\}.</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = divide start_ARG roman_ℓ end_ARG start_ARG italic_L end_ARG italic_T , roman_ℓ ∈ { 0 , 1 , … , italic_L } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.34)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then for each <math alttext="\ell\in[L]=\{1,2,\dots,L\}" class="ltx_Math" display="inline" id="S2.SS1.p11.m4"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\ell\in[L]=\{1,2,\dots,L\}</annotation><annotation encoding="application/x-llamapun">roman_ℓ ∈ [ italic_L ] = { 1 , 2 , … , italic_L }</annotation></semantics></math>, going from <math alttext="\ell=L" class="ltx_Math" display="inline" id="S2.SS1.p11.m5"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">\ell=L</annotation><annotation encoding="application/x-llamapun">roman_ℓ = italic_L</annotation></semantics></math> to <math alttext="\ell=1" class="ltx_Math" display="inline" id="S2.SS1.p11.m6"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\ell=1</annotation><annotation encoding="application/x-llamapun">roman_ℓ = 1</annotation></semantics></math>, we can run the iteration</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx18">
<tbody id="S2.E35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{\bm{x}}_{t_{\ell-1}}" class="ltx_Math" display="inline" id="S2.E35.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><annotation encoding="application/x-tex">\displaystyle\hat{\bm{x}}_{t_{\ell-1}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{\mathbb{E}}[\bm{x}_{t_{\ell-1}}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]" class="ltx_Math" display="inline" id="S2.E35.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>𝒙</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>∣</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{\mathbb{E}}[\bm{x}_{t_{\ell-1}}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.35)</span></td>
</tr></tbody>
<tbody id="S2.E36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{\mathbb{E}}[\bm{x}+t_{\ell-1}\bm{g}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]" class="ltx_Math" display="inline" id="S2.E36.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>+</mo><mrow><mrow><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>∣</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></mrow><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{\mathbb{E}}[\bm{x}+t_{\ell-1}\bm{g}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ bold_italic_x + italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT bold_italic_g ∣ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.36)</span></td>
</tr></tbody>
<tbody id="S2.E37"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{\mathbb{E}}\left[\bm{x}+t_{\ell-1}\cdot\frac{\bm{x}_{t_{\ell}}-\bm{x}}{t_{\ell}}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}\right]" class="ltx_Math" display="inline" id="S2.E37.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mi>𝒙</mi><mo>+</mo><mrow><mrow><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>−</mo><mi>𝒙</mi></mrow><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></mfrac></mstyle></mrow><mo>∣</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></mrow><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{\mathbb{E}}\left[\bm{x}+t_{\ell-1}\cdot\frac{\bm{x}_{t_{\ell}}-\bm{x}}{t_{\ell}}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}\right]</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ bold_italic_x + italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT ⋅ divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT - bold_italic_x end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_ARG ∣ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.37)</span></td>
</tr></tbody>
<tbody id="S2.E38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{t_{\ell-1}}{t_{\ell}}\hat{\bm{x}}_{t_{\ell}}+\left(1-\frac{t_{\ell-1}}{t_{\ell}}\right)\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]" class="ltx_Math" display="inline" id="S2.E38.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></mfrac></mstyle></mrow><mo>)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{t_{\ell-1}}{t_{\ell}}\hat{\bm{x}}_{t_{\ell}}+\left(1-\frac{t_{\ell-1}}{t_{\ell}}\right)\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t_{\ell}}=\hat{\bm{x}}_{t_{\ell}}]</annotation><annotation encoding="application/x-llamapun">= divide start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( 1 - divide start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_ARG ) blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.38)</span></td>
</tr></tbody>
<tbody id="S2.E39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\left(1-\frac{1}{\ell}\right)\cdot\hat{\bm{x}}_{t_{\ell}}+\frac{1}{\ell}\cdot\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})." class="ltx_Math" display="inline" id="S2.E39.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi mathvariant="normal">ℓ</mi></mfrac></mstyle></mrow><mo rspace="0.055em">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi mathvariant="normal">ℓ</mi></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\left(1-\frac{1}{\ell}\right)\cdot\hat{\bm{x}}_{t_{\ell}}+\frac{1}{\ell}\cdot\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}}).</annotation><annotation encoding="application/x-llamapun">= ( 1 - divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG ) ⋅ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG ⋅ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.39)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The effect of this iteration is as follows. At the beginning of the iteration where <math alttext="\ell" class="ltx_Math" display="inline" id="S2.SS1.p11.m7"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> is large, we barely trust the output of the denoiser, and mostly keep the current iterate. This makes sense, as the denoiser can have huge variance (cf <a class="ltx_ref" href="#Thmexample3" title="Example 3.3 (Denoising a Two-Point Mixture). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). When <math alttext="\ell" class="ltx_Math" display="inline" id="S2.SS1.p11.m8"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> is small, the denoiser will “lock on” to the modes of the data distribution as a denoising step basically takes a gradient step on the true distribution’s log-density, and we can trust it to not produce unreasonable samples, so the denoising step mostly involves the output of the denoiser. At <math alttext="\ell=1" class="ltx_Math" display="inline" id="S2.SS1.p11.m9"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\ell=1</annotation><annotation encoding="application/x-llamapun">roman_ℓ = 1</annotation></semantics></math> we even throw away the current iterate and just keep the output of the denoiser.</p>
</div>
<figure class="ltx_figure" id="F5"><img alt="Figure 3.5 : Denoising a low-rank mixture of Gaussians. Each figure represents samples from the true data distribution (gray, orange, red) and samples undergoing the denoising process ( 3.2.66 ) (light blue). At top left, the process has just started, and the noise is very large. As the process continues, the noise is pushed further towards the support of the low-rank data distribution. Finally, in the bottom right, the generated samples are perfectly aligned with the support of the data and look very much like samples drawn from the low-rank Gaussian mixture model." class="ltx_graphics ltx_img_landscape" height="393" id="F5.g1" src="chapters/chapter3/figs/ve_gmm_denoising.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Denoising a low-rank mixture of Gaussians.<span class="ltx_text ltx_font_medium"> Each figure represents samples from the true data distribution (gray, orange, red) and samples undergoing the denoising process (<a class="ltx_ref" href="#S2.E66" title="Equation 3.2.66 ‣ 2nd item ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.66</span></a>) (light blue). At top left, the process has just started, and the noise is very large. As the process continues, the noise is pushed further towards the support of the low-rank data distribution. Finally, in the bottom right, the generated samples are perfectly aligned with the support of the data and look very much like samples drawn from the low-rank Gaussian mixture model.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p12">
<p class="ltx_p">The above is intuition for why we expect the denoising process to converge. We visualize the convergence process in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S2.SS1.p12.m1"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> in <a class="ltx_ref" href="#F5" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>. We will develop some rigorous results about convergence later. For now, recall that we wanted to build a process to reduce the entropy. While we did do this in a roundabout way by inverting a process which adds entropy, it is now time to pay the piper and confirm that our iterative denoising process reduces the entropy.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.4</span></span><span class="ltx_text ltx_font_bold"> </span>(Simplified Version of <a class="ltx_ref" href="A2.html#Thmtheorem3" title="Theorem B.3. ‣ B.2.2 Denoising Process Reduces Entropy Over Time ‣ B.2 Diffusion and Denoising Processes ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">B.3</span></a>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="(\bm{x}_{t})_{t\in[0,T]}" class="ltx_Math" display="inline" id="Thmtheorem4.p1.m1"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></msub><annotation encoding="application/x-tex">(\bm{x}_{t})_{t\in[0,T]}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT</annotation></semantics></math> obeys (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). Then, under certain technical conditions on <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem4.p1.m2"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, for every <math alttext="s&lt;t" class="ltx_Math" display="inline" id="Thmtheorem4.p1.m3"><semantics><mrow><mi>s</mi><mo>&lt;</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">s&lt;t</annotation><annotation encoding="application/x-llamapun">italic_s &lt; italic_t</annotation></semantics></math> with <math alttext="s,t\in(0,T]" class="ltx_Math" display="inline" id="Thmtheorem4.p1.m4"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">s,t\in(0,T]</annotation><annotation encoding="application/x-llamapun">italic_s , italic_t ∈ ( 0 , italic_T ]</annotation></semantics></math>,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E40">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\operatorname{\mathbb{E}}[\bm{x}_{s}\mid\bm{x}_{t}])&lt;h(\bm{x}_{t})." class="ltx_Math" display="block" id="S2.E40.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝒙</mi><mi>s</mi></msub><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\operatorname{\mathbb{E}}[\bm{x}_{s}\mid\bm{x}_{t}])&lt;h(\bm{x}_{t}).</annotation><annotation encoding="application/x-llamapun">italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) &lt; italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.40)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p13">
<p class="ltx_p">The full statement of the theorem, and the proof itself, requires some technicality, so it is postponed to <a class="ltx_ref" href="A2.html#S2.SS2" title="B.2.2 Denoising Process Reduces Entropy Over Time ‣ B.2 Diffusion and Denoising Processes ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.2.2</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p14">
<p class="ltx_p">The last thing we discuss here is that many times, we will <span class="ltx_text ltx_font_italic">not be able to compute</span> <math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p14.m1"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> for any <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p14.m2"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, since we do not have the distribution <math alttext="p_{t}" class="ltx_Math" display="inline" id="S2.SS1.p14.m3"><semantics><msub><mi>p</mi><mi>t</mi></msub><annotation encoding="application/x-tex">p_{t}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. But we can try to <span class="ltx_text ltx_font_italic">learn one from data</span>. Recall that the denoiser <math alttext="\bar{\bm{x}}^{\ast}" class="ltx_Math" display="inline" id="S2.SS1.p14.m4"><semantics><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is defined in (<a class="ltx_ref" href="#S2.E3" title="Equation 3.2.3 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>) as minimizing the mean-squared error <math alttext="\operatorname{\mathbb{E}}\|\bar{\bm{x}}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}" class="ltx_Math" display="inline" id="S2.SS1.p14.m5"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}\|\bar{\bm{x}}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_E ∥ over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. We can use this mean-squared error as a loss or objective function to learn the denoiser. For example, we can parameterize <math alttext="\bar{\bm{x}}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p14.m6"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG ( italic_t , ⋅ )</annotation></semantics></math> by a neural network, writing it as <math alttext="\bar{\bm{x}}_{\theta}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p14.m7"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , ⋅ )</annotation></semantics></math>, and optimize the loss over the parameter space <math alttext="\Theta" class="ltx_Math" display="inline" id="S2.SS1.p14.m8"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation><annotation encoding="application/x-llamapun">roman_Θ</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E41">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta\in\Theta}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}." class="ltx_Math" display="block" id="S2.E41.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant="normal">Θ</mi></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\theta\in\Theta}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.41)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The solution to this optimization problem, implemented via gradient descent or a similar algorithm, will give us a <math alttext="\bar{\bm{x}}_{\theta^{\ast}}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p14.m9"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msup><mi>θ</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta^{\ast}}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> which is a good approximation to <math alttext="\bar{\bm{x}}^{\ast}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p14.m10"><semantics><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , ⋅ )</annotation></semantics></math> (at least if the training works) and which we will use as our denoiser.</p>
</div>
<div class="ltx_para" id="S2.SS1.p15">
<p class="ltx_p">What is a good architecture for this neural network <math alttext="\bar{\bm{x}}_{\theta^{\ast}}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p15.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msup><mi>θ</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta^{\ast}}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t , ⋅ )</annotation></semantics></math>? To answer this question, we will examine the ubiquitous case of a <span class="ltx_text ltx_font_italic">Gaussian mixture model</span>, whose denoiser we computed in <a class="ltx_ref" href="#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. This model is relevant because it can approximate many types of distributions: in particular, given a distribution for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p15.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, there is a Gaussian mixture model that can approximate it arbitrarily well. So optimizing among the class of denoisers for Gaussian mixture models can give us something close to the optimal denoiser for the real data distribution.</p>
</div>
<div class="ltx_para" id="S2.SS1.p16">
<p class="ltx_p">In our case, we assume that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p16.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is low-dimensional, which loosely translates into the requirement that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p16.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is <span class="ltx_text ltx_font_italic">approximately</span> distributed according to a <span class="ltx_text ltx_font_italic">mixture of low-rank Gaussians</span>. Formally, we write</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E42">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\sim\frac{1}{K}\sum_{k=1}^{K}\operatorname{\mathcal{N}}(\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top})" class="ltx_Math" display="block" id="S2.E42.m1"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mfrac><mn>1</mn><mi>K</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim\frac{1}{K}\sum_{k=1}^{K}\operatorname{\mathcal{N}}(\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.42)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{U}_{k}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}" class="ltx_Math" display="inline" id="S2.SS1.p16.m3"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>P</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ sansserif_O ( italic_D , italic_P ) ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> is an orthogonal matrix. Then the optimal denoiser under (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) is (from <a class="ltx_ref" href="#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>)</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E43">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\varphi(\bm{x}_{t};\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})}{\sum_{i=1}^{K}\varphi(\bm{x}_{t};\bm{0},\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})}\cdot\left(\bm{U}_{k}\bm{U}_{k}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right)." class="ltx_Math" display="block" id="S2.E43.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo>(</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\varphi(\bm{x}_{t};\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})}{\sum_{i=1}^{K}\varphi(\bm{x}_{t};\bm{0},\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})}\cdot\left(\bm{U}_{k}\bm{U}_{k}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG ⋅ ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.43)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that within the computation <math alttext="\varphi" class="ltx_Math" display="inline" id="S2.SS1.p16.m4"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation><annotation encoding="application/x-llamapun">italic_φ</annotation></semantics></math> and outside of it, we compute the inverse <math alttext="(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}" class="ltx_Math" display="inline" id="S2.SS1.p16.m5"><semantics><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>. This is a low-rank perturbation of the full-rank matrix <math alttext="t^{2}\bm{I}" class="ltx_Math" display="inline" id="S2.SS1.p16.m6"><semantics><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">t^{2}\bm{I}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I</annotation></semantics></math>, and thus ripe for simplification via the <span class="ltx_text ltx_font_italic">Sherman-Morrison-Woodbury identity</span>, i.e., for matrices <math alttext="\bm{A},\bm{C},\bm{U},\bm{V}" class="ltx_Math" display="inline" id="S2.SS1.p16.m7"><semantics><mrow><mi>𝑨</mi><mo>,</mo><mi>𝑪</mi><mo>,</mo><mi>𝑼</mi><mo>,</mo><mi>𝑽</mi></mrow><annotation encoding="application/x-tex">\bm{A},\bm{C},\bm{U},\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_A , bold_italic_C , bold_italic_U , bold_italic_V</annotation></semantics></math> such that <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S2.SS1.p16.m8"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> and <math alttext="\bm{C}" class="ltx_Math" display="inline" id="S2.SS1.p16.m9"><semantics><mi>𝑪</mi><annotation encoding="application/x-tex">\bm{C}</annotation><annotation encoding="application/x-llamapun">bold_italic_C</annotation></semantics></math> are invertible,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E44">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{A}+\bm{U}\bm{C}\bm{V})^{-1}=\bm{A}^{-1}-\bm{A}^{-1}\bm{U}(\bm{C}^{-1}+\bm{V}\bm{A}^{-1}\bm{U})^{-1}\bm{V}\bm{A}^{-1}." class="ltx_Math" display="block" id="S2.E44.m1"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo>+</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑪</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mrow><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑪</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\bm{A}+\bm{U}\bm{C}\bm{V})^{-1}=\bm{A}^{-1}-\bm{A}^{-1}\bm{U}(\bm{C}^{-1}+\bm{V}\bm{A}^{-1}\bm{U})^{-1}\bm{V}\bm{A}^{-1}.</annotation><annotation encoding="application/x-llamapun">( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ( bold_italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.44)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We prove this identity in <a class="ltx_ref" href="#Thmexercise3" title="Exercise 3.3. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>. For now we apply this identity with <math alttext="\bm{A}=t^{2}\bm{I}" class="ltx_Math" display="inline" id="S2.SS1.p16.m10"><semantics><mrow><mi>𝑨</mi><mo>=</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{A}=t^{2}\bm{I}</annotation><annotation encoding="application/x-llamapun">bold_italic_A = italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I</annotation></semantics></math>, <math alttext="\bm{U}=\bm{U}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p16.m11"><semantics><mrow><mi>𝑼</mi><mo>=</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\bm{U}=\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\bm{V}=\bm{U}_{k}^{\top}" class="ltx_Math" display="inline" id="S2.SS1.p16.m12"><semantics><mrow><mi>𝑽</mi><mo>=</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><annotation encoding="application/x-tex">\bm{V}=\bm{U}_{k}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_V = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\bm{C}=\bm{I}" class="ltx_Math" display="inline" id="S2.SS1.p16.m13"><semantics><mrow><mi>𝑪</mi><mo>=</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">\bm{C}=\bm{I}</annotation><annotation encoding="application/x-llamapun">bold_italic_C = bold_italic_I</annotation></semantics></math>, obtaining</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx19">
<tbody id="S2.E45"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}" class="ltx_Math" display="inline" id="S2.E45.m1"><semantics><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\displaystyle(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\bm{I}-\frac{1}{t^{4}}\bm{U}_{k}\left(\bm{I}+\frac{1}{t^{2}}\bm{U}_{k}^{\top}\bm{U}_{k}\right)^{-1}\bm{U}_{k}^{\top}" class="ltx_Math" display="inline" id="S2.E45.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>4</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\bm{I}-\frac{1}{t^{4}}\bm{U}_{k}\left(\bm{I}+\frac{1}{t^{2}}\bm{U}_{k}^{\top}\bm{U}_{k}\right)^{-1}\bm{U}_{k}^{\top}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_I - divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_I + divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.45)</span></td>
</tr></tbody>
<tbody id="S2.E46"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\bm{I}-\frac{1}{t^{4}\left(1+\frac{1}{t^{2}}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}" class="ltx_Math" display="inline" id="S2.E46.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msup><mi>t</mi><mn>4</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mrow><mo>)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\bm{I}-\frac{1}{t^{4}\left(1+\frac{1}{t^{2}}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_I - divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ( 1 + divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.46)</span></td>
</tr></tbody>
<tbody id="S2.E47"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right)." class="ltx_Math" display="inline" id="S2.E47.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( bold_italic_I - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.47)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then we can compute the posterior probabilities as follows. Note that since <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p16.m14"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>’s are all orthogonal, <math alttext="\det(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS1.p16.m15"><semantics><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\det(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">roman_det ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> are all the same for each <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.p16.m16"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>. So</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx20">
<tbody id="S2.E48"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\frac{\varphi(\bm{x}_{t};\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})}{\sum_{i=1}^{K}\varphi(\bm{x}_{t};\bm{0},\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})}" class="ltx_Math" display="inline" id="S2.E48.m1"><semantics><mstyle displaystyle="true"><mfrac><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mstyle><annotation encoding="application/x-tex">\displaystyle\frac{\varphi(\bm{x}_{t};\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})}{\sum_{i=1}^{K}\varphi(\bm{x}_{t};\bm{0},\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})}</annotation><annotation encoding="application/x-llamapun">divide start_ARG italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp\left(-\frac{1}{2}\bm{x}_{t}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2}\bm{x}_{t}^{\top}(\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right)}" class="ltx_Math" display="inline" id="S2.E48.m2"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\exp\left(-\frac{1}{2}\bm{x}_{t}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2}\bm{x}_{t}^{\top}(\bm{U}_{i}\bm{U}_{i}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}\right)}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.48)</span></td>
</tr></tbody>
<tbody id="S2.E49"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\bm{x}_{t}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{x}_{t}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2t^{2}}\bm{x}_{t}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{i}\bm{U}_{i}^{\top}\right)\bm{x}_{t}\right)}" class="ltx_Math" display="inline" id="S2.E49.m1"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\bm{x}_{t}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{x}_{t}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2t^{2}}\bm{x}_{t}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{i}\bm{U}_{i}^{\top}\right)\bm{x}_{t}\right)}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_I - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_I - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.49)</span></td>
</tr></tbody>
<tbody id="S2.E50"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}+\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}+\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}" class="ltx_Math" display="inline" id="S2.E50.m1"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}+\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}+\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.50)</span></td>
</tr></tbody>
<tbody id="S2.E51"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}\right)\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}\right)\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}" class="ltx_Math" display="inline" id="S2.E51.m1"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}\right)\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\exp\left(-\frac{1}{2t^{2}}\|\bm{x}_{t}\|_{2}^{2}\right)\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.51)</span></td>
</tr></tbody>
<tbody id="S2.E52"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}." class="ltx_Math" display="inline" id="S2.E52.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.52)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is a softmax operation weighted by the projection of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p16.m17"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> onto each subspace measured by <math alttext="\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}" class="ltx_Math" display="inline" id="S2.SS1.p16.m18"><semantics><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> (tempered by a temperature <math alttext="2t^{2}(1+t^{2})" class="ltx_Math" display="inline" id="S2.SS1.p16.m19"><semantics><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">2t^{2}(1+t^{2})</annotation><annotation encoding="application/x-llamapun">2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>). Meanwhile, the component denoisers can be written as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx21">
<tbody id="S2.E53"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{U}_{k}\bm{U}_{k}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.E53.m1"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\displaystyle\bm{U}_{k}\bm{U}_{k}^{\top}(\bm{U}_{k}\bm{U}_{k}^{\top}+t^{2}\bm{I})^{-1}\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.E53.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\left(\bm{I}-\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_I - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.53)</span></td>
</tr></tbody>
<tbody id="S2.E54"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{t^{2}}\left(1-\frac{1}{1+t^{2}}\right)\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.E54.m1"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mi>t</mi><mn>2</mn></msup></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{t^{2}}\left(1-\frac{1}{1+t^{2}}\right)\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( 1 - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.54)</span></td>
</tr></tbody>
<tbody id="S2.E55"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}." class="ltx_Math" display="inline" id="S2.E55.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{1+t^{2}}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.55)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Putting these together, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E56">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}," class="ltx_Math" display="block" id="S2.E56.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t},</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.56)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">i.e., a projection of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p16.m20"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> onto each of <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p16.m21"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> subspaces, weighted by a soft-max operation of a quadratic function of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS1.p16.m22"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. This functional form is similar to an <span class="ltx_text ltx_font_italic">attention mechanism</span> in a transformer architecture! As we will see in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, this is no coincidence at all; the deep link between denoising and lossy compression (to be covered in <a class="ltx_ref" href="#S3" title="3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) makes transformer denoisers so effective in practice. And so overall, our Gaussian mixture model theory motivates the use of transformer-like neural networks for denoising.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Connections between denoising a distribution and probabilistic PCA.</span> Here, we would like to connect denoising a low-dimensional distribution to probabilistic PCA (see <a class="ltx_ref" href="Ch2.html#S1.SS3" title="2.1.3 Probabilistic PCA ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1.3</span></a> for more details about probabilistic PCA). Suppose that we consider <math alttext="K=1" class="ltx_Math" display="inline" id="Thmremark1.p1.m1"><semantics><mrow><mi>K</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">K=1</annotation><annotation encoding="application/x-llamapun">italic_K = 1</annotation></semantics></math> in (<a class="ltx_ref" href="#S2.E42" title="Equation 3.2.42 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.42</span></a>), i.e., <math alttext="\bm{x}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{U}\bm{U}^{\top})" class="ltx_Math" display="inline" id="Thmremark1.p1.m2"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{U}\bm{U}^{\top})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="\bm{U}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}" class="ltx_Math" display="inline" id="Thmremark1.p1.m3"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>P</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_P ) ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> is an orthogonal matrix. According to (<a class="ltx_ref" href="#S2.E56" title="Equation 3.2.56 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.56</span></a>), the Bayes optimal denoiser is</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx22">
<tbody id="S2.E57"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\bm{U}\bm{U}^{\top}\bm{x}_{t}." class="ltx_Math" display="inline" id="S2.E57.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\bm{U}\bm{U}^{\top}\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.57)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">To learn this Bayes optimal denoiser, we can accordingly parameterize the denoising operator <math alttext="\bar{\bm{x}}(t,\bm{x}_{t})" class="ltx_Math" display="inline" id="Thmremark1.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}(t,\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx23">
<tbody id="S2.E58"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bar{\bm{x}}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}_{t}," class="ltx_Math" display="inline" id="S2.E58.m1"><semantics><mrow><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bar{\bm{x}}(t,\bm{x}_{t})=\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}_{t},</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.58)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{V}\in\mathsf{O}(D,P)" class="ltx_Math" display="inline" id="Thmremark1.p1.m5"><semantics><mrow><mi>𝑽</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{V}\in\mathsf{O}(D,P)</annotation><annotation encoding="application/x-llamapun">bold_italic_V ∈ sansserif_O ( italic_D , italic_P )</annotation></semantics></math> are learnable parameters. Substituting this into the training loss (<a class="ltx_ref" href="#S2.E3" title="Equation 3.2.3 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>) yields</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E59">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}_{t}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{x},\bm{g}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}(\bm{x}+t\bm{g})\right\|_{2}^{2}," class="ltx_Math" display="block" id="S2.E59.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>𝑽</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒈</mi></mrow></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}_{t}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{x},\bm{g}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}(\bm{x}+t\bm{g})\right\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_V ∈ sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_x + italic_t bold_italic_g ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.59)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the equality is due to (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). Conditioned on <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark1.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we compute</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx24">
<tbody id="S2.E60"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}(\bm{x}+t\bm{g})\right\|_{2}^{2}" class="ltx_Math" display="inline" id="S2.E60.m1"><semantics><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}(\bm{x}+t\bm{g})\right\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_x + italic_t bold_italic_g ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.60)</span></td>
</tr></tbody>
<tbody id="S2.E61"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S2.E61.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}-\frac{t}{1+t^{2}}\operatorname{\mathbb{E}}_{\bm{g}}\left\langle\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x},\bm{V}\bm{V}^{\top}\bm{g}\right\rangle+\frac{t^{2}}{(1+t^{2})^{2}}\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}\bm{V}^{\top}\bm{g}\right\|_{2}^{2}" class="ltx_Math" display="inline" id="S2.E61.m2"><semantics><mrow><mrow><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mi>t</mi><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo>⁡</mo><mrow><mo>⟨</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo>,</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>⟩</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><msup><mi>t</mi><mn>2</mn></msup><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}-\frac{t}{1+t^{2}}\operatorname{\mathbb{E}}_{\bm{g}}\left\langle\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x},\bm{V}\bm{V}^{\top}\bm{g}\right\rangle+\frac{t^{2}}{(1+t^{2})^{2}}\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}\bm{V}^{\top}\bm{g}\right\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG italic_t end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ⟨ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x , bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_g ⟩ + divide start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_g ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.61)</span></td>
</tr></tbody>
<tbody id="S2.E62"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S2.E62.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}+\frac{t^{2}P}{(1+t^{2})^{2}}" class="ltx_Math" display="inline" id="S2.E62.m2"><semantics><mrow><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mstyle displaystyle="true"><mfrac><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>P</mi></mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}+\frac{t^{2}P}{(1+t^{2})^{2}}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_P end_ARG start_ARG ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.62)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the second equality follows from <math alttext="\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="Thmremark1.p1.m7"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> and <math alttext="\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}\bm{V}^{\top}\bm{g}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}^{\top}\bm{g}\right\|_{2}^{2}=P" class="ltx_Math" display="inline" id="Thmremark1.p1.m8"><semantics><mrow><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>=</mo><mi>P</mi></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}\bm{V}^{\top}\bm{g}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{g}}\left\|\bm{V}^{\top}\bm{g}\right\|_{2}^{2}=P</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_g ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_g ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_P</annotation></semantics></math> due to <math alttext="\bm{V}\in\mathsf{O}(D,P)" class="ltx_Math" display="inline" id="Thmremark1.p1.m9"><semantics><mrow><mi>𝑽</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{V}\in\mathsf{O}(D,P)</annotation><annotation encoding="application/x-llamapun">bold_italic_V ∈ sansserif_O ( italic_D , italic_P )</annotation></semantics></math>. Therefore, Problem (<a class="ltx_ref" href="#S2.E59" title="Equation 3.2.59 ‣ Remark 3.1. ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.59</span></a>) in equivalent to</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx25">
<tbody id="S2.E63"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{x}\|_{2}^{2}+\left(\frac{1}{(1+t^{2})^{2}}-\frac{2}{1+t^{2}}\right)\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{V}^{\top}\bm{x}\|_{2}^{2}." class="ltx_Math" display="inline" id="S2.E63.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>𝑽</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo>⁡</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒙</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mfrac></mstyle><mo>−</mo><mstyle displaystyle="true"><mfrac><mn>2</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\min_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x}}\left\|\bm{x}-\frac{1}{1+t^{2}}\bm{V}\bm{V}^{\top}\bm{x}\right\|_{2}^{2}=\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{x}\|_{2}^{2}+\left(\frac{1}{(1+t^{2})^{2}}-\frac{2}{1+t^{2}}\right)\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{V}^{\top}\bm{x}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_V ∈ sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ∥ bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( divide start_ARG 1 end_ARG start_ARG ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG - divide start_ARG 2 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ∥ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.63)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is further equivalent to</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx26">
<tbody id="S2.E64"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{V}^{\top}\bm{x}\|_{2}^{2}," class="ltx_Math" display="inline" id="S2.E64.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><mi>𝑽</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{V}\in\mathsf{O}(D,P)}\operatorname{\mathbb{E}}_{\bm{x}}\|\bm{V}^{\top}\bm{x}\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_V ∈ sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ∥ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.64)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is essentially Problem (<a class="ltx_ref" href="Ch2.html#S1.E27" title="Equation 2.1.27 ‣ 2.1.3 Probabilistic PCA ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.27</span></a>).</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p17">
<p class="ltx_p">Overall, the learned denoiser forms an (implicit parametric) encoding scheme of the given data, since it can be used to denoise/project onto the data distribution. Training a denoiser is equivalent to finding a better coding scheme, and this partially fulfills one of the desiderata (the <span class="ltx_text ltx_font_italic">second</span>) at the end of <a class="ltx_ref" href="#S1.SS3" title="3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.3</span></a>. In the sequel, we will discuss how to fulfill the other (the <span class="ltx_text ltx_font_italic">first</span>).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2.2 </span>Learning and Sampling a Distribution via Iterative Denoising</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">Remember that at the end of <a class="ltx_ref" href="#S1.SS3" title="3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.3</span></a>, we discussed a pair of desiderata for pursuing a distribution with low-dimensional structure. The first such desideratum is to start with a normal distribution, say with high entropy, and gradually reduce its entropy until it reaches the distribution of the data. We will call this procedure <span class="ltx_text ltx_font_italic">sampling</span> since we are generating new samples. It is now time for us to discuss how to do this with the toolkit we have built up.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">We know how to denoise very noisy samples <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="S2.SS2.p2.m1"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> to attain approximations <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS2.p2.m2"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> which have similar distributions to the target random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. But the desideratum says that, to sample, we want to start with a template distribution with <span class="ltx_text ltx_font_italic">no</span> influence from the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.p2.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, and use the denoiser to guide the iterates towards the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. How can we do this? One way is motivated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E65">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\bm{x}_{T}}{T}=\frac{\bm{x}+T\bm{g}}{T}=\frac{\bm{x}}{T}+\bm{g}\to\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})." class="ltx_Math" display="block" id="S2.E65.m1"><semantics><mrow><mrow><mfrac><msub><mi>𝒙</mi><mi>T</mi></msub><mi>T</mi></mfrac><mo>=</mo><mfrac><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mi>T</mi></mfrac><mo>=</mo><mrow><mfrac><mi>𝒙</mi><mi>T</mi></mfrac><mo>+</mo><mi>𝒈</mi></mrow><mo stretchy="false">→</mo><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\frac{\bm{x}_{T}}{T}=\frac{\bm{x}+T\bm{g}}{T}=\frac{\bm{x}}{T}+\bm{g}\to\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I}).</annotation><annotation encoding="application/x-llamapun">divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG italic_T end_ARG = divide start_ARG bold_italic_x + italic_T bold_italic_g end_ARG start_ARG italic_T end_ARG = divide start_ARG bold_italic_x end_ARG start_ARG italic_T end_ARG + bold_italic_g → bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.65)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, <math alttext="\bm{x}_{T}\approx\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS2.p2.m6"><semantics><mrow><msub><mi>𝒙</mi><mi>T</mi></msub><mo>≈</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{T}\approx\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ≈ caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>. This approximation is quite good for almost all practical distributions, and visualized in <a class="ltx_ref" href="#F6" title="In 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.6</span></a>.</p>
</div>
<figure class="ltx_figure" id="F6"><img alt="Figure 3.6 : Visualizing x T \bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT versus 𝒩 ⁡ ( 𝟎 , T 2 ​ I ) \operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I}) caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) . Left: A plot of Gaussian mixture model data 𝒙 \bm{x} bold_italic_x . Right: A plot of 𝒙 \bm{x} bold_italic_x as well as 𝒙 T \bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and an independent sample of 𝒩 ⁡ ( 𝟎 , T 2 ​ 𝑰 ) \operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I}) caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) , for T = 10 T=10 italic_T = 10 . On the right plot, 𝒙 \bm{x} bold_italic_x is plotted in the same colors as the left: however, samples from 𝒙 T \bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and 𝒩 ⁡ ( 𝟎 , T 2 ​ 𝑰 ) \operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I}) caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) are both much larger, on average, than samples from 𝒙 \bm{x} bold_italic_x , and so it appears much smaller because of the scaling. Despite this large blow-up, we clearly observe the similarities in the distributions of 𝒙 T \bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and 𝒩 ⁡ ( 𝟎 , T 2 ​ 𝑰 ) \operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I}) caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) ." class="ltx_graphics ltx_img_landscape" height="197" id="F6.g1" src="chapters/chapter3/figs/xT_vs_noise.png" width="449"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Visualizing <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="F6.m14"><semantics><msub><mi>x</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> versus <math alttext="\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="F6.m15"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.<span class="ltx_text ltx_font_medium"> <span class="ltx_text ltx_font_italic">Left:</span> A plot of Gaussian mixture model data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m16"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. <span class="ltx_text ltx_font_italic">Right:</span> A plot of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m17"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> as well as <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="F6.m18"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> and an independent sample of <math alttext="\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="F6.m19"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>, for <math alttext="T=10" class="ltx_Math" display="inline" id="F6.m20"><semantics><mrow><mi>T</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">T=10</annotation><annotation encoding="application/x-llamapun">italic_T = 10</annotation></semantics></math>. On the right plot, <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m21"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is plotted in the same colors as the left: however, samples from <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="F6.m22"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="F6.m23"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> are both much larger, on average, than samples from <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m24"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, and so it appears much smaller because of the scaling. Despite this large blow-up, we clearly observe the similarities in the distributions of <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="F6.m25"><semantics><msub><mi>𝒙</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="F6.m26"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">So, discretizing <math alttext="[0,T]" class="ltx_Math" display="inline" id="S2.SS2.p3.m1"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,T]</annotation><annotation encoding="application/x-llamapun">[ 0 , italic_T ]</annotation></semantics></math> into <math alttext="0=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}=T" class="ltx_Math" display="inline" id="S2.SS2.p3.m2"><semantics><mrow><mn>0</mn><mo>=</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><msub><mi>t</mi><mn>1</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}=T</annotation><annotation encoding="application/x-llamapun">0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = italic_T</annotation></semantics></math> uniformly using <math alttext="t_{\ell}=T\ell/L" class="ltx_Math" display="inline" id="S2.SS2.p3.m3"><semantics><mrow><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>=</mo><mrow><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">ℓ</mi></mrow><mo>/</mo><mi>L</mi></mrow></mrow><annotation encoding="application/x-tex">t_{\ell}=T\ell/L</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = italic_T roman_ℓ / italic_L</annotation></semantics></math> (as in the previous section), one possible way to sample from pure noise is:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p">Sample <math alttext="\hat{\bm{x}}_{T}\sim\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>T</mi></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{T}\sim\operatorname{\mathcal{N}}(\bm{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> (i.i.d. of everything else)</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p">Run the denoising iteration as in <a class="ltx_ref" href="#S2.SS1" title="3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E66">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}=\left(1-\frac{1}{\ell}\right)\cdot\hat{\bm{x}}_{t_{\ell}}+\frac{1}{\ell}\cdot\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})." class="ltx_Math" display="block" id="S2.E66.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>=</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><mn>1</mn><mi mathvariant="normal">ℓ</mi></mfrac></mrow><mo rspace="0.055em">)</mo></mrow><mo rspace="0.222em">⋅</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mfrac><mn>1</mn><mi mathvariant="normal">ℓ</mi></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}=\left(1-\frac{1}{\ell}\right)\cdot\hat{\bm{x}}_{t_{\ell}}+\frac{1}{\ell}\cdot\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( 1 - divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG ) ⋅ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG ⋅ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.66)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p">Output <math alttext="\hat{\bm{x}}=\hat{\bm{x}}_{0}" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\hat{\bm{x}}_{0}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p">This conceptually is all there is behind <span class="ltx_text ltx_font_italic">diffusion models</span>, which transform noise into data samples in accordance with the first desideratum. However, there are a few steps left to take before we get models which can actually sample from real data distributions like images given practical resource constraints. In the sequel, we will introduce and motivate several such steps.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Step 1: different discretizations.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p">The first step we do is motivated by the following point: <span class="ltx_text ltx_font_italic">we do not need to spend so many denoising iterations</span> at large <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m1"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>. If we look at <a class="ltx_ref" href="#F5" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>, we observe that the first <math alttext="200" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m2"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation><annotation encoding="application/x-llamapun">200</annotation></semantics></math> or <math alttext="300" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m3"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation><annotation encoding="application/x-llamapun">300</annotation></semantics></math> iterations, out of the <math alttext="500" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m4"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation><annotation encoding="application/x-llamapun">500</annotation></semantics></math> iterations of the sampling process, are just spent contracting the noise towards the data distribution as a whole, before the remaining iterations push the samples towards a subspace. Given a fixed iteration count <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m5"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>, this signals that we should spend more timesteps <math alttext="t_{\ell}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m6"><semantics><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">t_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math> near <math alttext="t=0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m7"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation><annotation encoding="application/x-llamapun">italic_t = 0</annotation></semantics></math> compared to <math alttext="t=T" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m8"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation><annotation encoding="application/x-llamapun">italic_t = italic_T</annotation></semantics></math>. During sampling (and training), we can therefore use another discretization of <math alttext="[0,T]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m9"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,T]</annotation><annotation encoding="application/x-llamapun">[ 0 , italic_T ]</annotation></semantics></math> into <math alttext="0\leq t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}\leq T" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m10"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><msub><mi>t</mi><mn>1</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0\leq t_{0}&lt;t_{1}&lt;\cdots&lt;t_{L}\leq T</annotation><annotation encoding="application/x-llamapun">0 ≤ italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ≤ italic_T</annotation></semantics></math>, such as an <span class="ltx_text ltx_font_italic">exponential discretization</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E67">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="t_{\ell}=C_{1}(e^{C_{2}\ell}-1),\qquad\forall\ell\in\{0,1,\dots,L\}" class="ltx_Math" display="block" id="S2.E67.m1"><semantics><mrow><mrow><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>=</mo><mrow><msub><mi>C</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>e</mi><mrow><msub><mi>C</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi mathvariant="normal">ℓ</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t_{\ell}=C_{1}(e^{C_{2}\ell}-1),\qquad\forall\ell\in\{0,1,\dots,L\}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_e start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_ℓ end_POSTSUPERSCRIPT - 1 ) , ∀ roman_ℓ ∈ { 0 , 1 , … , italic_L }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.67)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="C_{1},C_{2}&gt;0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m11"><semantics><mrow><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><msub><mi>C</mi><mn>2</mn></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">C_{1},C_{2}&gt;0</annotation><annotation encoding="application/x-llamapun">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT &gt; 0</annotation></semantics></math> are constants which can be tuned for optimal performance in practice; theoretical analysis will often specify such optimal constants as well. Then the denoising/sampling iteration becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E68">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{t_{\ell-1}}{t_{\ell}}\hat{\bm{x}}_{t_{\ell}}+\left(1-\frac{t_{\ell-1}}{t_{\ell}}\right)\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})," class="ltx_Math" display="block" id="S2.E68.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>≐</mo><mrow><mrow><mfrac><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></mfrac></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{t_{\ell-1}}{t_{\ell}}\hat{\bm{x}}_{t_{\ell}}+\left(1-\frac{t_{\ell-1}}{t_{\ell}}\right)\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}}),</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≐ divide start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( 1 - divide start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_ARG ) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.68)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with, again, <math alttext="\hat{\bm{x}}_{t_{L}}\sim\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m12"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi>L</mi></msub></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{L}}\sim\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Step 2: different noise models.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p">The second step is to consider slightly different models compared to (<a class="ltx_ref" href="#S2.E1" title="Equation 3.2.1 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). The basic motivation for this is as follows. In practice, the noise distribution <math alttext="\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> becomes an increasingly poor estimate of the true covariance in high dimensions, i.e., (<a class="ltx_ref" href="#S2.E65" title="Equation 3.2.65 ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.65</span></a>) becomes an increasingly worse approximation, especially with anisotropic high-dimensional data. The increased distance between <math alttext="\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m2"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> and the true distribution of <math alttext="\bm{x}_{t_{L}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m3"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>L</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{L}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> may cause the denoiser to perform worse in such circumstances. Theoretically, <math alttext="\bm{x}_{t_{L}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m4"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>L</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{L}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> never converges to any distribution as <math alttext="t_{L}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m5"><semantics><msub><mi>t</mi><mi>L</mi></msub><annotation encoding="application/x-tex">t_{L}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> increases, so this setup is difficult to analyze end-to-end. In this case, our remedy is to <span class="ltx_text ltx_font_italic">simultaneously add noise and shrink the contribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m6"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, such that <math alttext="\bm{x}_{T}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m7"><semantics><msub><mi>𝐱</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\bm{x}_{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> converges as <math alttext="T\to\infty" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m8"><semantics><mrow><mi>T</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">T\to\infty</annotation><annotation encoding="application/x-llamapun">italic_T → ∞</annotation></semantics></math></span>. The rate of added noise is denoted <math alttext="\sigma\colon[0,T]\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m9"><semantics><mrow><mi>σ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\sigma\colon[0,T]\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_σ : [ 0 , italic_T ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>, and the rate of shrinkage is denoted <math alttext="\alpha\colon[0,T]\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m10"><semantics><mrow><mi>α</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha\colon[0,T]\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_α : [ 0 , italic_T ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>, such that <math alttext="\sigma" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m11"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation><annotation encoding="application/x-llamapun">italic_σ</annotation></semantics></math> is <span class="ltx_text ltx_font_italic">increasing</span> and <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m12"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> is (not strictly) <span class="ltx_text ltx_font_italic">decreasing</span>, and</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E69">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{t}\doteq\alpha_{t}\bm{x}+\sigma_{t}\bm{g},\qquad\forall t\in[0,T]." class="ltx_Math" display="block" id="S2.E69.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>≐</mo><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>+</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{t}\doteq\alpha_{t}\bm{x}+\sigma_{t}\bm{g},\qquad\forall t\in[0,T].</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_g , ∀ italic_t ∈ [ 0 , italic_T ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.69)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The previous setup has <math alttext="\alpha_{t}=1" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m13"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_{t}=1</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1</annotation></semantics></math> and <math alttext="\sigma_{t}=t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m14"><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\sigma_{t}=t</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t</annotation></semantics></math>, and this is called the <span class="ltx_text ltx_font_italic">variance-exploding (VE) process</span>. A popular choice which decreases the contribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m15"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, as we described originally, has <math alttext="T=1" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m16"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=1</annotation><annotation encoding="application/x-llamapun">italic_T = 1</annotation></semantics></math> (so that <math alttext="t\in[0,1]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m17"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_t ∈ [ 0 , 1 ]</annotation></semantics></math>), <math alttext="\alpha_{t}=\sqrt{1-t^{2}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m18"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\alpha_{t}=\sqrt{1-t^{2}}</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG 1 - italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math> and <math alttext="\sigma_{t}=t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m19"><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\sigma_{t}=t</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t</annotation></semantics></math>; this is the <span class="ltx_text ltx_font_italic">variance-preserving (VP) process</span>. Note that under the VP process, <math alttext="\bm{x}_{1}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m20"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{1}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> exactly, so we can just sample from this standard distribution and iteratively denoise. As a result, the VP process is much easier to analyze theoretically and more stable empirically.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Why use the whole <math alttext="\alpha,\sigma" class="ltx_Math" display="inline" id="footnote6.m1"><semantics><mrow><mi>α</mi><mo>,</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex">\alpha,\sigma</annotation><annotation encoding="application/x-llamapun">italic_α , italic_σ</annotation></semantics></math> setup? As we will see in <a class="ltx_ref" href="#Thmexercise5" title="Exercise 3.5. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>, it encapsulates and unifies many proposed processes, including the recently popular so-called <span class="ltx_text ltx_font_italic">flow matching</span> process. Despite this, the VE and VP processes are still the most popular empirically and theoretically (so far), and so we will consider them in this Section.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p">With this more general setup, Tweedie’s formula (<a class="ltx_ref" href="#S2.E20" title="Equation 3.2.20 ‣ Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.20</span></a>) becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E70">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]=\frac{1}{\alpha_{t}}\left(\bm{x}_{t}+\sigma_{t}^{2}\nabla\log p_{t}(\bm{x})\right)." class="ltx_Math" display="block" id="S2.E70.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>α</mi><mi>t</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>+</mo><mrow><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]=\frac{1}{\alpha_{t}}\left(\bm{x}_{t}+\sigma_{t}^{2}\nabla\log p_{t}(\bm{x})\right).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide start_ARG 1 end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.70)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The denoising iteration (<a class="ltx_ref" href="#S2.E68" title="Equation 3.2.68 ‣ Step 1: different discretizations. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.68</span></a>) becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E71">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}=\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})." class="ltx_Math" display="block" id="S2.E71.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>=</mo><mrow><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>α</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>−</mo><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}=\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bar{\bm{x}}^{\ast}(t_{\ell},\hat{\bm{x}}_{t_{\ell}}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT - divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.71)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, the Gaussian mixture model denoiser (<a class="ltx_ref" href="#S2.E17" title="Equation 3.2.17 ‣ Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.17</span></a>) becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E72">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\pi_{k}\varphi(\bm{x}_{t};\alpha_{t}\bm{\mu}_{k},\alpha_{t}^{2}\bm{\Sigma}_{k}+\sigma_{t}^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\alpha_{t}\bm{\mu}_{i},\alpha_{t}^{2}\bm{\Sigma}_{i}+\sigma_{t}^{2}\bm{I})}\cdot\left(\bm{\mu}_{k}+\alpha_{t}\bm{\Sigma}_{k}(\alpha_{t}^{2}\bm{\Sigma}_{k}+\sigma_{t}^{2}\bm{I})^{-1}(\bm{x}_{t}-\alpha_{t}\bm{\mu}_{k})\right)." class="ltx_Math" display="block" id="S2.E72.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝁</mi><mi>k</mi></msub></mrow><mo>,</mo><mrow><mrow><msubsup><mi>α</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚺</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>π</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>;</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝁</mi><mi>i</mi></msub></mrow><mo>,</mo><mrow><mrow><msubsup><mi>α</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚺</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo>(</mo><mrow><msub><mi>𝝁</mi><mi>k</mi></msub><mo>+</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚺</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>α</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚺</mi><mi>k</mi></msub></mrow><mo>+</mo><mrow><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝁</mi><mi>k</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{x}_{t})=\sum_{k=1}^{K}\frac{\pi_{k}\varphi(\bm{x}_{t};\alpha_{t}\bm{\mu}_{k},\alpha_{t}^{2}\bm{\Sigma}_{k}+\sigma_{t}^{2}\bm{I})}{\sum_{i=1}^{K}\pi_{i}\varphi(\bm{x}_{t};\alpha_{t}\bm{\mu}_{i},\alpha_{t}^{2}\bm{\Sigma}_{i}+\sigma_{t}^{2}\bm{I})}\cdot\left(\bm{\mu}_{k}+\alpha_{t}\bm{\Sigma}_{k}(\alpha_{t}^{2}\bm{\Sigma}_{k}+\sigma_{t}^{2}\bm{I})^{-1}(\bm{x}_{t}-\alpha_{t}\bm{\mu}_{k})\right).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_φ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_ARG ⋅ ( bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.72)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><a class="ltx_ref" href="#F7" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.7</span></a> demonstrates iterations of the sampling procedure. Note that the denoising iteration (<a class="ltx_ref" href="#S2.E71" title="Equation 3.2.71 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.71</span></a>) gives a sampling algorithm called the DDIM (“Denoising Diffusion Implicit Model”) sampler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx248" title="">SME20</a>]</cite>, and is one of the most popular sampling algorithms used today in diffusion models. We summarize it here in <a class="ltx_ref" href="#alg1" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="F7"><img alt="Figure 3.7 : Denoising a mixture of Gaussians using the VP diffusion process. We use the same figure setup and data distribution as Figure 3.5 . Note that compared to Figure 3.5 , the noise distribution is much more concentrated around the origin." class="ltx_graphics ltx_img_landscape" height="392" id="F7.g1" src="chapters/chapter3/figs/vp_gmm_denoising.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Denoising a mixture of Gaussians using the VP diffusion process.<span class="ltx_text ltx_font_medium"> We use the same figure setup and data distribution as <a class="ltx_ref" href="#F5" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>. Note that compared to <a class="ltx_ref" href="#F5" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>, the noise distribution is much more concentrated around the origin.</span></span></figcaption>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 3.1</span> </span> Sampling using a denoiser.</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span>An ordered list of timesteps <math alttext="0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T" class="ltx_Math" display="inline" id="alg1.l1.m1"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T</annotation><annotation encoding="application/x-llamapun">0 ≤ italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ≤ italic_T</annotation></semantics></math> to use for sampling.
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>A denoiser <math alttext="\bar{\bm{x}}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\to\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg1.l2.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo rspace="0.055em" stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo rspace="0.222em">×</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\to\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span>Scale and noise level functions <math alttext="\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="alg1.l3.m1"><semantics><mrow><mrow><mi>α</mi><mo>,</mo><mi>σ</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_α , italic_σ : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>A sample <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="alg1.l4.m1"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, approximately from the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="alg1.l4.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span><span class="ltx_text ltx_font_bold">function</span> <span class="ltx_text ltx_font_smallcaps">DDIMSampler</span>(<math alttext="\bar{\bm{x}},(t_{\ell})_{\ell=0}^{L}" class="ltx_Math" display="inline" id="alg1.l5.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>,</mo><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup></mrow><annotation encoding="application/x-tex">\bar{\bm{x}},(t_{\ell})_{\ell=0}^{L}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG , ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>     Initialize <math alttext="\tilde{\bm{x}}_{t_{L}}\sim" class="ltx_Math" display="inline" id="alg1.l6.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>~</mo></mover><msub><mi>t</mi><mi>L</mi></msub></msub><mo>∼</mo><mi></mi></mrow><annotation encoding="application/x-tex">\tilde{\bm{x}}_{t_{L}}\sim</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∼</annotation></semantics></math> approximate distribution of <math alttext="\bm{x}_{t_{L}}" class="ltx_Math" display="inline" id="alg1.l6.m2"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>L</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{L}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l6.m3"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> VP <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="alg1.l6.m4"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>, VE <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="alg1.l6.m5"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.
</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="\ell=L,L-1,\dots,1" class="ltx_Math" display="inline" id="alg1.l7.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mrow><mi>L</mi><mo>,</mo><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\ell=L,L-1,\dots,1</annotation><annotation encoding="application/x-llamapun">roman_ℓ = italic_L , italic_L - 1 , … , 1</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>         Compute
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bar{\bm{x}}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})" class="ltx_Math" display="block" id="S2.Ex1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>≐</mo><mrow><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>α</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>−</mo><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bar{\bm{x}}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≐ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT - divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) over¯ start_ARG bold_italic_x end_ARG ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\hat{\bm{x}}_{t_{0}}" class="ltx_Math" display="inline" id="alg1.l10.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mn>0</mn></msub></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{0}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Step 3: optimizing training pipelines.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p">If we use the procedure dictated by <a class="ltx_ref" href="#S2.SS1" title="3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a> to learn a separate denoiser <math alttext="\bar{\bm{x}}(t,\cdot)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}(t,\cdot)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG ( italic_t , ⋅ )</annotation></semantics></math> for each time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m2"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> to be used in the sampling algorithm, <span class="ltx_text ltx_font_italic">we would have to learn <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m3"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> separate denoisers!</span> This is highly inefficient—the usual case is that we have to train <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m4"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> separate neural networks, taking up <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m5"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> times the training time and storage memory, and then be locked into using these timesteps for sampling forever. Instead, we can <span class="ltx_text ltx_font_italic">train a single neural network</span> to denoise across all times <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m6"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, taking as input the continuous variables <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m8"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> (instead of just <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m9"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> before). Mechanically, our training loss averages over <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m10"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, i.e., solves the following problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E73">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\operatorname{\mathbb{E}}_{t,\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}." class="ltx_Math" display="block" id="S2.E73.m1"><semantics><mrow><mrow><munder><mi>min</mi><mi>θ</mi></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mrow><mi>t</mi><mo>,</mo><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\theta}\operatorname{\mathbb{E}}_{t,\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t , bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.73)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Similar to Step 1, where we used more timesteps closer to <math alttext="t=0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m11"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation><annotation encoding="application/x-llamapun">italic_t = 0</annotation></semantics></math> to ensure a better sampling process, we may want to ensure that the denoiser is higher quality closer to <math alttext="t=0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m12"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation><annotation encoding="application/x-llamapun">italic_t = 0</annotation></semantics></math>, and thereby <span class="ltx_text ltx_font_italic">weight the loss</span> so that <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m13"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> near <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m14"><mn>0</mn></math> has higher weight. Letting <math alttext="w_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m15"><semantics><msub><mi>w</mi><mi>t</mi></msub><annotation encoding="application/x-tex">w_{t}</annotation><annotation encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> be the weight at time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m16"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, the weighted loss would look like</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E74">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\operatorname{\mathbb{E}}_{t}w_{t}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}." class="ltx_Math" display="block" id="S2.E74.m1"><semantics><mrow><mrow><munder><mi>min</mi><mi>θ</mi></munder><mo lspace="0.167em">⁡</mo><mrow><msub><mi>𝔼</mi><mi>t</mi></msub><mo lspace="0.167em">⁡</mo><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\theta}\operatorname{\mathbb{E}}_{t}w_{t}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bar{\bm{x}}_{\theta}(t,\bm{x}_{t})-\bm{x}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.74)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">One reasonable choice of weight in practice is <math alttext="w_{t}=\alpha_{t}/\sigma_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m17"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>/</mo><msub><mi>σ</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">w_{t}=\alpha_{t}/\sigma_{t}</annotation><annotation encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. The precise reason will be covered in the next paragraph, but generally it serves to up-weight the losses corresponding to <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m18"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> near <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.m19"><mn>0</mn></math> while still remaining reasonably numerically stable. Also, of course, we cannot compute the expectation in practice, so we use the most straightforward Monte-Carlo average to estimate it. The series of changes made here have several conceptual and computational benefits: we do not need to train multiple denoisers, we can train on one set of timesteps and sample using a subset (or others entirely), etc. The full pipeline is discussed in <a class="ltx_ref" href="#alg2" title="In Step 3: optimizing training pipelines. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span>Dataset <math alttext="\mathcal{D}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg1.l1a.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l2a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>An ordered list of timesteps <math alttext="0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T" class="ltx_Math" display="inline" id="alg1.l2a.m1"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T</annotation><annotation encoding="application/x-llamapun">0 ≤ italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ≤ italic_T</annotation></semantics></math> to use for sampling.
</div>
<div class="ltx_listingline" id="alg1.l3a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span>A weighting function <math alttext="w\colon\{t_{\ell}\}_{\ell=1}^{L}\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="alg1.l3a.m1"><semantics><mrow><mi>w</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">w\colon\{t_{\ell}\}_{\ell=1}^{L}\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_w : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l4a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>Scale and noise level functions <math alttext="\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="alg1.l4a.m1"><semantics><mrow><mrow><mi>α</mi><mo>,</mo><mi>σ</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_α , italic_σ : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l5a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>A parameter space <math alttext="\Theta" class="ltx_Math" display="inline" id="alg1.l5a.m1"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation><annotation encoding="application/x-llamapun">roman_Θ</annotation></semantics></math> and a denoiser architecture <math alttext="\bar{\bm{x}}_{\theta}" class="ltx_Math" display="inline" id="alg1.l5a.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l6a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>An optimization algorithm for the parameters.
</div>
<div class="ltx_listingline" id="alg1.l7a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span>The number of optimization iterations <math alttext="M" class="ltx_Math" display="inline" id="alg1.l7a.m1"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation><annotation encoding="application/x-llamapun">italic_M</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l8a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>The number of Monte-Carlo draws <math alttext="N" class="ltx_Math" display="inline" id="alg1.l8a.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> per iteration (to approximate the expectation in (<a class="ltx_ref" href="#S2.E74" title="Equation 3.2.74 ‣ Step 3: optimizing training pipelines. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.74</span></a>))
</div>
<div class="ltx_listingline" id="alg1.l9a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>A trained denoiser <math alttext="\bar{\bm{x}}_{\theta^{\ast}}" class="ltx_Math" display="inline" id="alg1.l9a.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msup><mi>θ</mi><mo>∗</mo></msup></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta^{\ast}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l10a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span><span class="ltx_text ltx_font_bold">function</span> <span class="ltx_text ltx_font_smallcaps">TrainDenoiser</span>(<math alttext="\mathcal{D},\Theta" class="ltx_Math" display="inline" id="alg1.l10a.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>,</mo><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\mathcal{D},\Theta</annotation><annotation encoding="application/x-llamapun">caligraphic_D , roman_Θ</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg1.l11a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span>     Initialize <math alttext="\theta^{(1)}\in\Theta" class="ltx_Math" display="inline" id="alg1.l11a.m1"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\theta^{(1)}\in\Theta</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT ∈ roman_Θ</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="i\in[M]" class="ltx_Math" display="inline" id="alg1.l12.m1"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>M</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">i\in[M]</annotation><annotation encoding="application/x-llamapun">italic_i ∈ [ italic_M ]</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span>         <span class="ltx_text ltx_font_bold">for</span> <math alttext="n\in[N]" class="ltx_Math" display="inline" id="alg1.l13.m1"><semantics><mrow><mi>n</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">n\in[N]</annotation><annotation encoding="application/x-llamapun">italic_n ∈ [ italic_N ]</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">14:</span></span>              <math alttext="\bm{x}_{n}^{(i)}\sim\mathcal{D}" class="ltx_Math" display="inline" id="alg1.l14.m1"><semantics><mrow><msubsup><mi>𝒙</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow><annotation encoding="application/x-tex">\bm{x}_{n}^{(i)}\sim\mathcal{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∼ caligraphic_D</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l14.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Draw a sample from the dataset.
</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">15:</span></span>              <math alttext="t_{n}^{(i)}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{U}}(\{t_{\ell}\}_{\ell=1}^{L})" class="ltx_math_unparsed" display="inline" id="alg1.l15.m1"><semantics><mrow><msubsup><mi>t</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mover><mo>∼</mo><mrow><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">d</mi><mo lspace="0em">.</mo></mrow></mover><mrow><mi class="ltx_font_mathcaligraphic">𝒰</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t_{n}^{(i)}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{U}}(\{t_{\ell}\}_{\ell=1}^{L})</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG ∼ end_ARG start_ARG roman_i . roman_i . roman_d . end_ARG end_RELOP caligraphic_U ( { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT )</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l15.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Sample a timestep.
</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">16:</span></span>              <math alttext="\bm{g}_{n}^{(i)}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_math_unparsed" display="inline" id="alg1.l16.m1"><semantics><mrow><msubsup><mi>𝒈</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mover><mo>∼</mo><mrow><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">d</mi><mo lspace="0em">.</mo></mrow></mover><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}_{n}^{(i)}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_RELOP SUPERSCRIPTOP start_ARG ∼ end_ARG start_ARG roman_i . roman_i . roman_d . end_ARG end_RELOP caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l16.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Sample a noise vector.
</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">17:</span></span>              <math alttext="\bm{x}_{t,n}^{(i)}\doteq\alpha_{t_{n}^{(i)}}\bm{x}_{n}^{(i)}+\sigma_{t_{n}^{(i)}}\bm{g}_{n}^{(i)}" class="ltx_Math" display="inline" id="alg1.l17.m1"><semantics><mrow><msubsup><mi>𝒙</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><mrow><mrow><msub><mi>α</mi><msubsup><mi>t</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msub><mi>σ</mi><msubsup><mi>t</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒈</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t,n}^{(i)}\doteq\alpha_{t_{n}^{(i)}}\bm{x}_{n}^{(i)}+\sigma_{t_{n}^{(i)}}\bm{g}_{n}^{(i)}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t , italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ≐ italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_g start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l17.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Compute the noised sample.
</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">18:</span></span>              <math alttext="w_{n}^{(i)}\doteq w_{t_{n}^{(i)}}" class="ltx_Math" display="inline" id="alg1.l18.m1"><semantics><mrow><msubsup><mi>w</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><msub><mi>w</mi><msubsup><mi>t</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></msub></mrow><annotation encoding="application/x-tex">w_{n}^{(i)}\doteq w_{t_{n}^{(i)}}</annotation><annotation encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ≐ italic_w start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l18.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Compute the loss weight.
</span>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">19:</span></span>         <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">20:</span></span>         <math alttext="\hat{\mathcal{L}}^{(i)}\doteq\displaystyle\frac{1}{N}\sum_{n=1}^{N}w_{n}^{(i)}\|\bm{x}_{n}^{(i)}-\bar{\bm{x}}_{\theta^{(i)}}(t_{n}^{(i)},\bm{x}_{t,n}^{(i)})\|_{2}^{2}" class="ltx_Math" display="inline" id="alg1.l20.m1"><semantics><mrow><msup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>≐</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msubsup><mi>w</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝒙</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>t</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝒙</mi><mrow><mi>t</mi><mo>,</mo><mi>n</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}^{(i)}\doteq\displaystyle\frac{1}{N}\sum_{n=1}^{N}w_{n}^{(i)}\|\bm{x}_{n}^{(i)}-\bar{\bm{x}}_{\theta^{(i)}}(t_{n}^{(i)},\bm{x}_{t,n}^{(i)})\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ≐ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT - over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_t , italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l20.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Compute the loss estimate.
</span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">21:</span></span>         <math alttext="\theta^{(i+1)}\doteq\texttt{OptimizationUpdate}^{(i)}(\theta^{(i)},\nabla_{\theta^{(i)}}\hat{\mathcal{L}}^{(i)})" class="ltx_Math" display="inline" id="alg1.l21.m1"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>≐</mo><mrow><msup><mtext class="ltx_mathvariant_monospace">OptimizationUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mrow><msub><mo rspace="0.167em">∇</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></msub><msup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{(i+1)}\doteq\texttt{OptimizationUpdate}^{(i)}(\theta^{(i)},\nabla_{\theta^{(i)}}\hat{\mathcal{L}}^{(i)})</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT ≐ OptimizationUpdate start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l21.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Update parameters.
</span>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">22:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">23:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\bar{\bm{x}}_{\theta^{(K+1)}}" class="ltx_Math" display="inline" id="alg1.l23.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>K</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta^{(K+1)}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUPERSCRIPT ( italic_K + 1 ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l24">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">24:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">function</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 3.2</span> </span> Learning a denoiser from data.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">(Optional) Step 4: changing the estimation target.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px4.p1">
<p class="ltx_p">Note that it is common to instead reorient the whole denoising pipeline around <span class="ltx_text ltx_font_italic">noise predictors</span>, i.e., estimates of <math alttext="\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m1"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒈</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_g ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math>. In practice, noise predictors are slightly easier to train because their output is (almost) always of comparable size to a Gaussian random variable, so training is more numerically stable. Note that by (<a class="ltx_ref" href="#S2.E69" title="Equation 3.2.69 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.69</span></a>) we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E75">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{t}=\alpha_{t}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]+\sigma_{t}\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]\implies\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]=\frac{1}{\sigma_{t}}\left(\bm{x}_{t}-\alpha_{t}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]\right)," class="ltx_Math" display="block" id="S2.E75.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒈</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo stretchy="false">⟹</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒈</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>σ</mi><mi>t</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\alpha_{t}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]+\sigma_{t}\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]\implies\operatorname{\mathbb{E}}[\bm{g}\mid\bm{x}_{t}]=\frac{1}{\sigma_{t}}\left(\bm{x}_{t}-\alpha_{t}\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]\right),</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E [ bold_italic_g ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ⟹ blackboard_E [ bold_italic_g ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide start_ARG 1 end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.75)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore any predictor for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be turned into a predictor for <math alttext="\bm{g}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m3"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_g</annotation></semantics></math> using the above relation, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E76">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{g}}(t,\bm{x}_{t})=\frac{1}{\sigma_{t}}\bm{x}_{t}-\frac{\alpha_{t}}{\sigma_{t}}\bar{\bm{x}}(t,\bm{x}_{t})," class="ltx_Math" display="block" id="S2.E76.m1"><semantics><mrow><mrow><mrow><mover accent="true"><mi>𝒈</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><msub><mi>σ</mi><mi>t</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>−</mo><mrow><mfrac><msub><mi>α</mi><mi>t</mi></msub><msub><mi>σ</mi><mi>t</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{g}}(t,\bm{x}_{t})=\frac{1}{\sigma_{t}}\bm{x}_{t}-\frac{\alpha_{t}}{\sigma_{t}}\bar{\bm{x}}(t,\bm{x}_{t}),</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_g end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.76)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and vice-versa. Thus a good network for estimating <math alttext="\bar{\bm{g}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m4"><semantics><mover accent="true"><mi>𝒈</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{g}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_g end_ARG</annotation></semantics></math> is the same as a good network for estimating <math alttext="\bar{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m5"><semantics><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG</annotation></semantics></math> <span class="ltx_text ltx_font_italic">plus a residual connection</span> (as seen in, e.g., transformers). Their losses are also the same as the denoiser, up to the factor of <math alttext="\alpha_{t}/\sigma_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m6"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>/</mo><msub><mi>σ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_{t}/\sigma_{t}</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E77">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}_{t}w_{t}\operatorname{\mathbb{E}}_{\bm{g},\bm{x}_{t}}\|\bm{g}-\bar{\bm{g}}(t,\bm{x}_{t})\|_{2}^{2}=\operatorname{\mathbb{E}}_{t}w_{t}\frac{\alpha_{t}^{2}}{\sigma_{t}^{2}}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bm{x}-\bar{\bm{x}}(t,\bm{x}_{t})\|_{2}^{2}." class="ltx_Math" display="block" id="S2.E77.m1"><semantics><mrow><mrow><mrow><msub><mi>𝔼</mi><mi>t</mi></msub><mo lspace="0.167em">⁡</mo><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒈</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒈</mi><mo>−</mo><mrow><mover accent="true"><mi>𝒈</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mi>t</mi></msub><mo lspace="0.167em">⁡</mo><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mfrac><msubsup><mi>α</mi><mi>t</mi><mn>2</mn></msubsup><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}_{t}w_{t}\operatorname{\mathbb{E}}_{\bm{g},\bm{x}_{t}}\|\bm{g}-\bar{\bm{g}}(t,\bm{x}_{t})\|_{2}^{2}=\operatorname{\mathbb{E}}_{t}w_{t}\frac{\alpha_{t}^{2}}{\sigma_{t}^{2}}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t}}\|\bm{x}-\bar{\bm{x}}(t,\bm{x}_{t})\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_g , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_g - over¯ start_ARG bold_italic_g end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT divide start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x - over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.77)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For the sake of completeness we will mention that other targets have been proposed for different tasks, e.g., <math alttext="\operatorname{\mathbb{E}}[\frac{\mathrm{d}}{\mathrm{d}t}\bm{x}_{t}\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m7"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mfrac><mi mathvariant="normal">d</mi><mrow><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\frac{\mathrm{d}}{\mathrm{d}t}\bm{x}_{t}\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math> (called <math alttext="v" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p1.m8"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation><annotation encoding="application/x-llamapun">italic_v</annotation></semantics></math>-prediction or velocity prediction), etc., but denoising and noise prediction remain commonly used. Throughout the rest of this book we will only consider denoising.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px4.p2">
<p class="ltx_p">We have made lots of changes to our original platonic noising/denoising process. To assure ourselves that the new process still works in practice, we can compute numerical examples (such as <a class="ltx_ref" href="#F7" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.7</span></a>). To assure ourselves that it is theoretically sound, we can prove a <span class="ltx_text ltx_font_italic">bound on the error rate</span> for the sampling algorithm, which shows that the error rate is small. We will now furnish such a rate from the literature, which shows that the output distribution of the sampler converges in the so-called <span class="ltx_text ltx_font_italic">total variation (TV) distance</span> to the true distribution. The TV distance is defined between two random variables <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p2.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E78">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathsf{TV}}(\bm{x},\bm{y})\doteq\sup_{A\subseteq\mathbb{R}^{d}}\left\lvert\operatorname{\mathbb{P}}[\bm{x}\in A]-\operatorname{\mathbb{P}}[\bm{y}\in A]\right\rvert." class="ltx_Math" display="block" id="S2.E78.m1"><semantics><mrow><mrow><mrow><mi>𝖳𝖵</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">≐</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0em">sup</mo><mrow><mi>A</mi><mo>⊆</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></munder><mrow><mo>|</mo><mrow><mrow><mi>ℙ</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∈</mo><mi>A</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mrow><mi>ℙ</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒚</mi><mo>∈</mo><mi>A</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>|</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{TV}}(\bm{x},\bm{y})\doteq\sup_{A\subseteq\mathbb{R}^{d}}\left\lvert\operatorname{\mathbb{P}}[\bm{x}\in A]-\operatorname{\mathbb{P}}[\bm{y}\in A]\right\rvert.</annotation><annotation encoding="application/x-llamapun">sansserif_TV ( bold_italic_x , bold_italic_y ) ≐ roman_sup start_POSTSUBSCRIPT italic_A ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | blackboard_P [ bold_italic_x ∈ italic_A ] - blackboard_P [ bold_italic_y ∈ italic_A ] | .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.78)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p2.m4"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> are very close (uniformly), then the supremum will be small. So the TV distance measures the closeness of random variables. (It is indeed a metric, as the name suggests: the proof is an exercise.)</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.5</span></span><span class="ltx_text ltx_font_bold"> </span>(<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx156" title="">LY24</a>]</cite> Theorem 1, Simplified)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="\operatorname{\mathbb{E}}\|\bm{x}\|_{2}&lt;\infty" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m1"><semantics><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝐱</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><mo>&lt;</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}\|\bm{x}\|_{2}&lt;\infty</annotation><annotation encoding="application/x-llamapun">blackboard_E ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT &lt; ∞</annotation></semantics></math>. If <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m2"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is denoised according to the VP process with an exponential discretization<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">7</span></span><span class="ltx_text ltx_font_upright">The precise definition is rather lengthy in our notation and only defined up to various absolute constants, so we omit it here for brevity. Of course it is in the original paper </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright">[</span><a class="ltx_ref" href="bib.html#bibx156" title="">LY24</a><span class="ltx_text ltx_font_upright">]</span></cite><span class="ltx_text ltx_font_upright">.</span></span></span></span> as in (<a class="ltx_ref" href="#S2.E67" title="Equation 3.2.67 ‣ Step 1: different discretizations. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.67</span></a>), the output <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m3"><semantics><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> of <a class="ltx_ref" href="#alg1" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> satisfies the total variation bound</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E79">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathsf{TV}}(\bm{x},\hat{\bm{x}})=\tilde{\mathcal{O}}\left(\underbrace{\frac{D}{L}}_{\text{discretization error}}+\underbrace{\sqrt{\frac{1}{L}\sum_{\ell=1}^{L}\frac{\alpha_{t_{\ell}}}{\sigma_{t_{\ell}}^{2}}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t_{\ell}}}\|\bar{\bm{x}}^{\ast}(t_{\ell},\bm{x}_{t_{\ell}})-\bar{\bm{x}}(t_{\ell},\bm{x}_{t_{\ell}})\|_{2}^{2}}}_{\text{average excess error of the denoiser}}\right)" class="ltx_Math" display="block" id="S2.E79.m1"><semantics><mrow><mrow><mi>𝖳𝖵</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">𝒪</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><munder><munder accentunder="true"><mfrac><mi>D</mi><mi>L</mi></mfrac><mo>⏟</mo></munder><mtext class="ltx_mathvariant_italic">discretization error</mtext></munder><mo>+</mo><munder><munder accentunder="true"><msqrt><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mfrac><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><msubsup><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mn>2</mn></msubsup></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></msub><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow></msqrt><mo>⏟</mo></munder><mtext class="ltx_mathvariant_italic">average excess error of the denoiser</mtext></munder></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{TV}}(\bm{x},\hat{\bm{x}})=\tilde{\mathcal{O}}\left(\underbrace{\frac{D}{L}}_{\text{discretization error}}+\underbrace{\sqrt{\frac{1}{L}\sum_{\ell=1}^{L}\frac{\alpha_{t_{\ell}}}{\sigma_{t_{\ell}}^{2}}\operatorname{\mathbb{E}}_{\bm{x},\bm{x}_{t_{\ell}}}\|\bar{\bm{x}}^{\ast}(t_{\ell},\bm{x}_{t_{\ell}})-\bar{\bm{x}}(t_{\ell},\bm{x}_{t_{\ell}})\|_{2}^{2}}}_{\text{average excess error of the denoiser}}\right)</annotation><annotation encoding="application/x-llamapun">sansserif_TV ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) = over~ start_ARG caligraphic_O end_ARG ( under⏟ start_ARG divide start_ARG italic_D end_ARG start_ARG italic_L end_ARG end_ARG start_POSTSUBSCRIPT discretization error end_POSTSUBSCRIPT + under⏟ start_ARG square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT divide start_ARG italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) - over¯ start_ARG bold_italic_x end_ARG ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_POSTSUBSCRIPT average excess error of the denoiser end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.79)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\bar{\bm{x}}^{\ast}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m4"><semantics><msup><mover accent="true"><mi>𝐱</mi><mo>¯</mo></mover><mo>∗</mo></msup><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is the Bayes optimal denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m5"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, and <math alttext="\tilde{\mathcal{O}}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m6"><semantics><mover accent="true"><mi class="ltx_font_mathcaligraphic">𝒪</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\mathcal{O}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG caligraphic_O end_ARG</annotation></semantics></math> is a version of the big-<math alttext="\mathcal{O}" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m7"><semantics><mi class="ltx_font_mathcaligraphic">𝒪</mi><annotation encoding="application/x-tex">\mathcal{O}</annotation><annotation encoding="application/x-llamapun">caligraphic_O</annotation></semantics></math> notation which ignores logarithmic factors in <math alttext="L" class="ltx_Math" display="inline" id="Thmtheorem5.p1.m8"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px4.p3">
<p class="ltx_p">The very high-level proof technique is, as discussed earlier, to bound the error at each step, distinguish the error sources (between discretization and denoiser error) and carefully ensure that the errors do not accumulate too much (or even cancel out).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px4.p4">
<p class="ltx_p">Note that, if <math alttext="L\to\infty" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p4.m1"><semantics><mrow><mi>L</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">L\to\infty</annotation><annotation encoding="application/x-llamapun">italic_L → ∞</annotation></semantics></math> and we correctly learn the Bayes optimal denoiser <math alttext="\bar{\bm{x}}=\bar{\bm{x}}^{\ast}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p4.m2"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>=</mo><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}=\bar{\bm{x}}^{\ast}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG = over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> (such that the excess error is <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p4.m3"><mn>0</mn></math>), then the sampling process in <a class="ltx_ref" href="#alg1" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> yields a <span class="ltx_text ltx_font_italic">perfect (in distribution) inverse</span> of the noising process, since the error rate in <a class="ltx_ref" href="#Thmtheorem5" title="Theorem 3.5 ([LY24] Theorem 1, Simplified). ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.5</span></a> goes to <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px4.p4.m4"><mn>0</mn></math>,<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>There are similar results for VE processes, though none are as sharp as this to our knowledge.</span></span></span> as heuristically argued previously.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark2.p1">
<p class="ltx_p">What if the data is low-dimensional, say supported on a <span class="ltx_text ltx_font_italic">low-rank subspace</span> of the high dimensional space <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmremark2.p1.m1"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>? If the data distribution is compactly supported—say if the data is normalized to the unit hypercube, which is often ensured as a pre-processing step for real data such as images—it is possible to do better. Namely, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx156" title="">LY24</a>]</cite> also define a measure of <span class="ltx_text ltx_font_italic">approximate intrinsic dimension</span> using the asymptotics of the so-called covering number, which is extremely similar in intuition (if not in implementation) to the rate distortion function presented in the next Section. Then they show that using a particular small modification of the DDIM sampler in <a class="ltx_ref" href="#alg1" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> (i.e., slightly perturbing the update coefficients), the discretization error becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E80">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{\mathcal{O}}\left(\frac{\text{approximate intrinsic dimension}}{L}\right)" class="ltx_Math" display="block" id="S2.E80.m1"><semantics><mrow><mover accent="true"><mi class="ltx_font_mathcaligraphic">𝒪</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mtext>approximate intrinsic dimension</mtext><mi>L</mi></mfrac><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\tilde{\mathcal{O}}\left(\frac{\text{approximate intrinsic dimension}}{L}\right)</annotation><annotation encoding="application/x-llamapun">over~ start_ARG caligraphic_O end_ARG ( divide start_ARG approximate intrinsic dimension end_ARG start_ARG italic_L end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.80)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">instead of <math alttext="\frac{D}{L}" class="ltx_Math" display="inline" id="Thmremark2.p1.m2"><semantics><mfrac><mi>D</mi><mi>L</mi></mfrac><annotation encoding="application/x-tex">\frac{D}{L}</annotation><annotation encoding="application/x-llamapun">divide start_ARG italic_D end_ARG start_ARG italic_L end_ARG</annotation></semantics></math> like it was in <a class="ltx_ref" href="#Thmtheorem5" title="Theorem 3.5 ([LY24] Theorem 1, Simplified). ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>. Therefore, using this modified algorithm, <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m3"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> does not have to be too large even as <math alttext="D" class="ltx_Math" display="inline" id="Thmremark2.p1.m4"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> reaches the thousands or millions, since real data has low-dimensional structure. However in practice we use the DDIM sampler instead, so <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m5"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> should have a mild dependence on <math alttext="D" class="ltx_Math" display="inline" id="Thmremark2.p1.m6"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> to achieve consistent error rates. The exact choice of <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m7"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> trades off between the computational complexity (e.g., runtime or memory consumption) of sampling and the statistical complexity of learning a denoiser for low-dimensional structures. The value of <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m8"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> is often different at training time (where a larger <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m9"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> allows better coverage of the interval <math alttext="[0,T]" class="ltx_Math" display="inline" id="Thmremark2.p1.m10"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,T]</annotation><annotation encoding="application/x-llamapun">[ 0 , italic_T ]</annotation></semantics></math>, which helps the network learn a relationship which generalizes over <math alttext="t" class="ltx_Math" display="inline" id="Thmremark2.p1.m11"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>) and sampling time (where <math alttext="L" class="ltx_Math" display="inline" id="Thmremark2.p1.m12"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> being smaller means more efficient sampling). One can even pick the timesteps adaptively at sampling time in order to optimize this tradeoffs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx15" title="">BLZ+22</a>]</cite>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark3.p1">
<p class="ltx_p">Various other works define the reverse process as moving backward in the time index <math alttext="t" class="ltx_Math" display="inline" id="Thmremark3.p1.m1"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> using an explicit difference equation, or differential equation in the limit <math alttext="L\to\infty" class="ltx_Math" display="inline" id="Thmremark3.p1.m2"><semantics><mrow><mi>L</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">L\to\infty</annotation><annotation encoding="application/x-llamapun">italic_L → ∞</annotation></semantics></math>, or forward in time using the transformation <math alttext="\bm{y}_{t}=\bm{x}_{T-t}" class="ltx_Math" display="inline" id="Thmremark3.p1.m3"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><msub><mi>𝒙</mi><mrow><mi>T</mi><mo>−</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\bm{y}_{t}=\bm{x}_{T-t}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_T - italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, such that if <math alttext="t" class="ltx_Math" display="inline" id="Thmremark3.p1.m4"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> increases then <math alttext="\bm{y}_{t}" class="ltx_Math" display="inline" id="Thmremark3.p1.m5"><semantics><msub><mi>𝒚</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{y}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> becomes closer to <math alttext="\bm{x}_{0}" class="ltx_Math" display="inline" id="Thmremark3.p1.m6"><semantics><msub><mi>𝒙</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{x}_{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. In this work we strive to keep consistency: we move forward in time to noise, and backward in time to denoise. If you are reading another work which is not clear on the time index, or trying to implement an algorithm which is similarly unclear, there is one way to do it right every time: the sampling process should always have a <span class="ltx_text ltx_font_italic">positive</span> coefficient on both the denoiser term and the current iterate when moving from step to step. But in general many papers define their own notation and it is not user-friendly.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark4.p1">
<p class="ltx_p">The theory presented at the end of the last <a class="ltx_ref" href="#S2.SS1" title="3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a> seems to suggest (loosely speaking) that in practice, using a transformer-like network is a good choice for learning or approximating a denoiser. This is reasonable, but what is the problem with using any old neural network (such as a multi-layer perceptron (MLP)) and just trying to scale it up to infinity? To observe the problem with this, let us look at another special case of the Gaussian mixture model studied in <a class="ltx_ref" href="#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. Namely, the <span class="ltx_text ltx_font_italic">empirical distribution</span> is an instance of a degenerate Gaussian mixture model, with <math alttext="K=N" class="ltx_Math" display="inline" id="Thmremark4.p1.m1"><semantics><mrow><mi>K</mi><mo>=</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">K=N</annotation><annotation encoding="application/x-llamapun">italic_K = italic_N</annotation></semantics></math> components <math alttext="\operatorname{\mathcal{N}}(\bm{x}_{i},\bm{0})" class="ltx_Math" display="inline" id="Thmremark4.p1.m2"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mn>𝟎</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathcal{N}}(\bm{x}_{i},\bm{0})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_0 )</annotation></semantics></math> sampled with equal probability <math alttext="\pi_{i}=\frac{1}{N}" class="ltx_Math" display="inline" id="Thmremark4.p1.m3"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\pi_{i}=\frac{1}{N}</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG</annotation></semantics></math>. In this case the Bayes optimal denoiser is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E81">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\star}(t,\bm{x}_{t})=\sum_{i=1}^{N}\frac{e^{-\|\bm{x}_{t}-\alpha_{t}\bm{x}_{i}\|_{2}^{2}/(2\sigma_{t}^{2})}}{\sum_{j=1}^{N}e^{-\|\bm{x}_{t}-\alpha_{t}\bm{x}_{j}\|_{2}^{2}/(2\sigma_{t}^{2})}}\bm{x}_{i}." class="ltx_Math" display="block" id="S2.E81.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></msup><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>j</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\star}(t,\bm{x}_{t})=\sum_{i=1}^{N}\frac{e^{-\|\bm{x}_{t}-\alpha_{t}\bm{x}_{i}\|_{2}^{2}/(2\sigma_{t}^{2})}}{\sum_{j=1}^{N}e^{-\|\bm{x}_{t}-\alpha_{t}\bm{x}_{j}\|_{2}^{2}/(2\sigma_{t}^{2})}}\bm{x}_{i}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG italic_e start_POSTSUPERSCRIPT - ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( 2 italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( 2 italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT end_ARG bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.2.81)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is a convex combination of the data <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="Thmremark4.p1.m4"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and the coefficients get “sharper” (i.e., closer to <math alttext="0" class="ltx_Math" display="inline" id="Thmremark4.p1.m5"><mn>0</mn></math> or <math alttext="1" class="ltx_Math" display="inline" id="Thmremark4.p1.m6"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>) as <math alttext="t\to 0" class="ltx_Math" display="inline" id="Thmremark4.p1.m7"><semantics><mrow><mi>t</mi><mo stretchy="false">→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\to 0</annotation><annotation encoding="application/x-llamapun">italic_t → 0</annotation></semantics></math>. Notice that this denoiser <span class="ltx_text ltx_font_italic">optimally solves</span> the denoising optimization problem (<a class="ltx_ref" href="#S2.E74" title="Equation 3.2.74 ‣ Step 3: optimizing training pipelines. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.74</span></a>) when we compute the loss based on drawing <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark4.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> uniformly at random from a fixed finite dataset <math alttext="\bm{X}=\{\bm{x}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="Thmremark4.p1.m9"><semantics><mrow><mi>𝑿</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\bm{X}=\{\bm{x}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, which is a very realistic setting. Thus, if our network architecture <math alttext="\bar{\bm{x}}_{\theta}" class="ltx_Math" display="inline" id="Thmremark4.p1.m10"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is expressive enough such that optimal denoisers of the above form (<a class="ltx_ref" href="#S2.E81" title="Equation 3.2.81 ‣ Remark 3.4. ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.81</span></a>) may be well-approximated, then the learned denoiser may do just that. Then, our iterative denoising <a class="ltx_ref" href="#alg1" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.1</span></a> will sample exactly from the empirical distribution, re-generating samples in the training data, as certified by <a class="ltx_ref" href="#Thmtheorem5" title="Theorem 3.5 ([LY24] Theorem 1, Simplified). ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>. This is a bad sampler, not really more interesting than a database of all samples, and so it is important to understand how to avoid this in practice. The key is to come up with a network architecture which can well-approximate the true denoiser (say corresponding to a low-rank distribution as in (<a class="ltx_ref" href="#S2.E56" title="Equation 3.2.56 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.56</span></a>)) but not the empirical Bayesian denoiser as in (<a class="ltx_ref" href="#S2.E81" title="Equation 3.2.81 ‣ Remark 3.4. ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.81</span></a>). Some work has explored this fine line and why modern diffusion models, which use transformer- and convolutional-based network architectures, can memorize and generalize in different regimes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx131" title="">KG24</a>, <a class="ltx_ref" href="bib.html#bibx197" title="">NZM+24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Thmremark4.p2">
<p class="ltx_p">At a high level, a denoiser which memorizes all the training points, as in (<a class="ltx_ref" href="#S2.E81" title="Equation 3.2.81 ‣ Remark 3.4. ‣ (Optional) Step 4: changing the estimation target. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.81</span></a>), corresponds to a parametric model of the distribution which has minimal coding rate, and achieves this by just coding every sample separately. We will discuss this problem (and seeming paradox with our initial desiderata at the end of <a class="ltx_ref" href="#S1.SS3" title="3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.3</span></a>) from the perspective of information theory in the next section.</p>
</div>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.3 </span>Compression via Lossy Coding</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">Let us recap what we have covered so far. We have discussed how to fit a <span class="ltx_text ltx_font_italic">denoiser</span> <math alttext="\bar{\bm{x}}_{\theta}" class="ltx_Math" display="inline" id="S3.p1.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> using finite samples. We showed that this denoiser encodes a distribution in that it is directly connected to its log-density via Tweedie’s formula (<a class="ltx_ref" href="#S2.E20" title="Equation 3.2.20 ‣ Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.20</span></a>). Then, we used it to gradually transform a pure noise (high-entropy) distribution towards the learned distribution via <span class="ltx_text ltx_font_italic">iterative denoising</span>. Thus, we have developed the first way of learning or pursuing a distribution laid out at the end of <a class="ltx_ref" href="#S1.SS3" title="3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">Nevertheless, in this methodology the encoding of the distribution is implicit in the denoiser’s functional form and parameters, if any. In fact, acute readers might have noticed that, for a general distribution, we have never explicitly specified what the functional form for the denoiser is. In practice, people typically model it by some deep neural network with an empirically designed architecture. In addition, although we know the above denoising process reduces the entropy, we do not know by how much, nor do we know the entropy of the intermediate and resulting distributions.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p">Recall that our general goal is to model data from a (continuous) distribution
with a low-dimensional support. If our goal is to identify the “simplest”
model that generates the data, one could consider three typical measures of
parsimony: the dimension, the volume, or the (differential) entropy. Well, if
one uses the dimension, then obviously the best model for a given dataset is the
empirical distribution itself which is zero-dimensional. For all distributions with low-dimensional supports, the differential entropy is always negative infinity; the volume of their supports are always zero. So, among all distributions of low-dimensional supports that could have generated the same data samples, how can we decide which one is better based on these measures of parsimony that cannot distinguish among low-dimensional models at all? This section aims to address this seemingly baffling situation.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p">In the remainder of this chapter, we discuss a framework that allows us to alleviate the above technical difficulty by associating the learned distribution with an explicit computable encoding and decoding scheme, following the second approach suggested at the end of <a class="ltx_ref" href="#S1.SS3" title="3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.3</span></a>. As we will see, such an approach essentially allows us to accurately approximate the entropy of the learned distributions in terms of a (lossy) coding length or coding rate associated with the coding scheme. With such a measure, not only can we accurately measure how much the entropy is reduced, hence information gained, by any processing (including denoising) of the distribution, but we can also derive an explicit form of the optimal operator that can conduct such operations in the most efficient way. As we will see in the next <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, this will lead to a principled explanation for the architecture of deep networks, as well as to more efficient deep-architecture designs.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.1 </span>Necessity of Lossy Coding</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">We have previously, multiple times, discussed a difficulty: if we learn the distribution from finite samples in the end, and our function class of denoisers contains enough functions, how do we ensure that we sample from the <span class="ltx_text ltx_font_italic">true</span> distribution (with low-dimensional supports), instead of any other distribution that may produce those finite samples with high probability? Let us reveal some of the conceptual and technical difficulties with some concrete examples.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.4</span></span><span class="ltx_text ltx_font_italic"> </span>(Volume, Dimension, and Entropy)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample4.p1">
<p class="ltx_p">For the example shown on the top of Figure <a class="ltx_ref" href="#F8" title="Figure 3.8 ‣ Example 3.5 (Density). ‣ 3.3.1 Necessity of Lossy Coding ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.8</span></a>, suppose we have taken some samples from a uniform distribution on a line (say in a 2D plane). The volume of the line or the sample sets is zero.
Geometrically, the empirical distribution on the produced finite sample set is <span class="ltx_text ltx_font_italic">the minimum-dimension</span> one which can produce the finite sample set.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>A set of discrete samples are all of zero dimension whereas the supporting line is one dimension.</span></span></span> But this is in seemingly contrast with yet another measure of complex: entropy. The (differential) entropy of the line is negative infinity but the (discrete) entropy of this sample set is finite and positive. So we seems to have a paradoxical situation according to these common measures of parsimony or complexity: they cannot properly differentiate among (models for) distributions of low-dimensional supports at all, and some seem to differentiate them even in exactly opposite manners.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Of course, strictly speaking, differential entropy and discrete entropy are not directly comparable.</span></span></span>
<math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample4.p1.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.5</span></span><span class="ltx_text ltx_font_italic"> </span>(Density)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample5.p1">
<p class="ltx_p">Consider the two sets of sampled data points shown in Figure <a class="ltx_ref" href="#F8" title="Figure 3.8 ‣ Example 3.5 (Density). ‣ 3.3.1 Necessity of Lossy Coding ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.8</span></a>. Geometrically, they are essentially the same: each set consists of eight points and each point has occurred with equal frequency <math alttext="1/8" class="ltx_Math" display="inline" id="Thmexample5.p1.m1"><semantics><mrow><mn>1</mn><mo>/</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">1/8</annotation><annotation encoding="application/x-llamapun">1 / 8</annotation></semantics></math>th. The only difference is that for the second data set, some points are “close” enough to be viewed as having a higher density around their respective “cluster.” Which one is more relevant to the true distribution that may have generated the samples? How can we reconcile such ambiguity in interpreting this kind of (empirical) distributions?</p>
</div>
<figure class="ltx_figure" id="F8"><img alt="Figure 3.8 : Eight points observed on a line." class="ltx_graphics ltx_img_landscape" height="138" id="F8.g1" src="chapters/chapter3/figs/one-dim-distribution.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.8</span>: </span><span class="ltx_text" style="font-size:90%;">Eight points observed on a line.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample5.p2">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample5.p2.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">There is yet another technical difficulty associated with constructing an explicit encoding and decoding scheme for a data set. Given a sampled data set in <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S3.SS1.p2.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>, how to design a coding scheme that is implementable on machines with finite memory and computing resources? Note that even representing a general real number requires an infinite number of digits or bits. Therefore, one may wonder whether the entropy of a distribution is a direct measure for the complexity of its (optimal) coding scheme. We examine this matter with another simple example.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.6</span></span><span class="ltx_text ltx_font_italic"> </span>(Precision)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample6.p1">
<p class="ltx_p">Consider a discrete distribution <math alttext="\bm{X}=[e,\pi]" class="ltx_Math" display="inline" id="Thmexample6.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><mi>e</mi><mo>,</mo><mi>π</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[e,\pi]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ italic_e , italic_π ]</annotation></semantics></math> with equal probability <math alttext="1/2" class="ltx_Math" display="inline" id="Thmexample6.p1.m2"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1/2</annotation><annotation encoding="application/x-llamapun">1 / 2</annotation></semantics></math> taking the values of the Euler number <math alttext="e\approx 2.71828" class="ltx_Math" display="inline" id="Thmexample6.p1.m3"><semantics><mrow><mi>e</mi><mo>≈</mo><mn>2.71828</mn></mrow><annotation encoding="application/x-tex">e\approx 2.71828</annotation><annotation encoding="application/x-llamapun">italic_e ≈ 2.71828</annotation></semantics></math> or the number <math alttext="\pi\approx 3.14159" class="ltx_Math" display="inline" id="Thmexample6.p1.m4"><semantics><mrow><mi>π</mi><mo>≈</mo><mn>3.14159</mn></mrow><annotation encoding="application/x-tex">\pi\approx 3.14159</annotation><annotation encoding="application/x-llamapun">italic_π ≈ 3.14159</annotation></semantics></math>. The entropy of this distribution is <math alttext="H=1" class="ltx_Math" display="inline" id="Thmexample6.p1.m5"><semantics><mrow><mi>H</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">H=1</annotation><annotation encoding="application/x-llamapun">italic_H = 1</annotation></semantics></math>, which suggests that one may encode the two numbers by a one-bit digit <math alttext="0" class="ltx_Math" display="inline" id="Thmexample6.p1.m6"><mn>0</mn></math> or <math alttext="1" class="ltx_Math" display="inline" id="Thmexample6.p1.m7"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>, respectively. But can you realize a decoding scheme for this code on a finite-state machine? The answer is actually no, as it takes infinitely many bits to describe either number precisely.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample6.p1.m8"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">Hence, it is generally impossible to have an encoding and decoding scheme that can precisely reproduce samples from an arbitrary real-valued distribution.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>That is, if one wants to encode such samples precisely, the only way is to memorize every single sample. </span></span></span> But there would be little practical value to encode a distribution without being able to decode for samples drawn from the same distribution.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p">So to ensure that any encoding/decoding scheme is computable and implementable
with finite memory and computational resources, we need to quantify the sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.p4.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and encode it only up to a certain precision, say <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S3.SS1.p4.m2"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>. <span class="ltx_text ltx_font_italic">By doing so, in essence, we treat any two data points equivalent if their distance is less than <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p4.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>.</span> More precisely, we would like to consider coding schemes</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\mapsto\hat{\bm{x}}" class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{x}\mapsto\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ↦ over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">such that the expected error caused by the quantization is bounded by <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p4.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>. It is mathematically more convenient, and conceptually almost identical, to bound the expected <span class="ltx_text ltx_font_italic">squared</span> error by <math alttext="\epsilon^{2}" class="ltx_Math" display="inline" id="S3.SS1.p4.m5"><semantics><msup><mi>ϵ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\epsilon^{2}</annotation><annotation encoding="application/x-llamapun">italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[d(\bm{x},\hat{\bm{x}})^{2}]\leq\epsilon^{2}." class="ltx_Math" display="block" id="S3.E2.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[d(\bm{x},\hat{\bm{x}})^{2}]\leq\epsilon^{2}.</annotation><annotation encoding="application/x-llamapun">blackboard_E [ italic_d ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Typically, the distance <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p4.m6"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> is chosen to be the Euclidean distance, or the
2-norm.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>More generally, we can replace <math alttext="d^{2}" class="ltx_Math" display="inline" id="footnote12.m1"><semantics><msup><mi>d</mi><mn>2</mn></msup><annotation encoding="application/x-tex">d^{2}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> with any so-called
<span class="ltx_text ltx_font_italic">divergence</span>.</span></span></span> We will adopt this choice in the sequel.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.2 </span>Rate Distortion and Data Geometry</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">Of course, among all encoding schemes that satisfy the above constraint, we
would like to choose the one that minimizes the resulting coding rate. For
a given random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and a precision <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, this rate is known as
the <span class="ltx_text ltx_font_italic">rate distortion</span>, denoted as <math alttext="\mathcal{R}_{\epsilon}(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math>.
A deep theorem in information theory, originally proved by
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx243" title="">Sha59</a>]</cite>, establishes that this rate can be expressed
equivalently in purely probabilistic terms as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{x})=\min_{p(\hat{\bm{x}}\mid\bm{x}):\operatorname{\mathbb{E}}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}}I(\bm{x};\hat{\bm{x}})," class="ltx_Math" display="block" id="S3.E3.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>min</mi><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∣</mo><mi>𝒙</mi></mrow><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><mi>I</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})=\min_{p(\hat{\bm{x}}\mid\bm{x}):\operatorname{\mathbb{E}}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}}I(\bm{x};\hat{\bm{x}}),</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT italic_p ( over^ start_ARG bold_italic_x end_ARG ∣ bold_italic_x ) : blackboard_E [ ∥ bold_italic_x - over^ start_ARG bold_italic_x end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the quantity <math alttext="I(\bm{x};\hat{\bm{x}})" class="ltx_Math" display="inline" id="S3.SS2.p1.m4"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\bm{x};\hat{\bm{x}})</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG )</annotation></semantics></math> is known as the <span class="ltx_text ltx_font_italic">mutual
information</span>, defined by</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(\bm{x};\hat{\bm{x}})=\operatorname{\mathsf{KL}}(p(\bm{x},\hat{\bm{x}})\;\|\;p(\bm{x})p(\hat{\bm{x}}))." class="ltx_Math" display="block" id="S3.E4.m1"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">∥</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(\bm{x};\hat{\bm{x}})=\operatorname{\mathsf{KL}}(p(\bm{x},\hat{\bm{x}})\;\|\;p(\bm{x})p(\hat{\bm{x}})).</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = sansserif_KL ( italic_p ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) ∥ italic_p ( bold_italic_x ) italic_p ( over^ start_ARG bold_italic_x end_ARG ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that the minimization
in (<a class="ltx_ref" href="#S3.E3" title="Equation 3.3.3 ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>) is over all conditional distributions <math alttext="p(\hat{\bm{x}}\mid\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p1.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\hat{\bm{x}}\mid\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( over^ start_ARG bold_italic_x end_ARG ∣ bold_italic_x )</annotation></semantics></math>
that satisfy the distortion constraint <math alttext="\operatorname{\mathbb{E}}_{\bm{x},\hat{\bm{x}}}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.m6"><semantics><mrow><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>,</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}_{\bm{x},\hat{\bm{x}}}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_x , over^ start_ARG bold_italic_x end_ARG end_POSTSUBSCRIPT [ ∥ bold_italic_x - over^ start_ARG bold_italic_x end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Each such conditional distribution induces a joint distribution <math alttext="p(\bm{x},\hat{\bm{x}})=p(\hat{\bm{x}}\mid\bm{x})p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p1.m7"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x},\hat{\bm{x}})=p(\hat{\bm{x}}\mid\bm{x})p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) = italic_p ( over^ start_ARG bold_italic_x end_ARG ∣ bold_italic_x ) italic_p ( bold_italic_x )</annotation></semantics></math>, which determines the mutual
information (<a class="ltx_ref" href="#S3.E4" title="Equation 3.3.4 ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.4</span></a>).
Many convenient properties of the mutual information (and hence the rate
distortion) are implied by corresponding properties of the KL divergence (recall
<a class="ltx_ref" href="#Thmtheorem1" title="Theorem 3.1 (Information Inequality). ‣ 3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>).
From the definition, we know that <math alttext="\mathcal{R}_{\epsilon}(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p1.m8"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic">non-increasing</span>
function in <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.p1.m9"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark5.p1">
<p class="ltx_p">As it turns out, the rate distortion is an implementable
approximation to the entropy of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in the following sense.
Assume that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmremark5.p1.m3"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> are continuous random vectors.
Then the mutual information can be written as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(\bm{x};\hat{\bm{x}})=h(\bm{x})-h(\bm{x}\mid\hat{\bm{x}})," class="ltx_Math" display="block" id="S3.E5.m1"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">I(\bm{x};\hat{\bm{x}})=h(\bm{x})-h(\bm{x}\mid\hat{\bm{x}}),</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_x ) - italic_h ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="h(\bm{x}\mid\hat{\bm{x}})=\mathbb{E}[\log_{2}p(\bm{x}\mid\hat{\bm{x}})]" class="ltx_Math" display="inline" id="Thmremark5.p1.m4"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x}\mid\hat{\bm{x}})=\mathbb{E}[\log_{2}p(\bm{x}\mid\hat{\bm{x}})]</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) = blackboard_E [ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) ]</annotation></semantics></math> is the
<span class="ltx_text ltx_font_italic">conditional entropy</span> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmremark5.p1.m6"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>.
Hence, the minimal coding rate is achieved when the difference between the
entropy of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the conditional entropy of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmremark5.p1.m9"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>
is minimized among all distributions that satisfy
the constraint: <math alttext="\mathbb{E}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}" class="ltx_Math" display="inline" id="Thmremark5.p1.m10"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathbb{E}[\|\bm{x}-\hat{\bm{x}}\|_{2}^{2}]\leq\epsilon^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_E [ ∥ bold_italic_x - over^ start_ARG bold_italic_x end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Thmremark5.p2">
<p class="ltx_p">In fact, it is not necessary to assume that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmremark5.p2.m2"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> are continuous
to obtain the above type of conclusion. For example, if both random vectors
are instead discrete, we have after a suitable interpretation of the KL
divergence for discrete-valued random vectors that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(\bm{x};\hat{\bm{x}})=H(\bm{x})-H(\bm{x}\mid\hat{\bm{x}})." class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">I(\bm{x};\hat{\bm{x}})=H(\bm{x})-H(\bm{x}\mid\hat{\bm{x}}).</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">More generally, advanced mathematical notions from measure theory can be
used to define the mutual information (and hence the rate distortion) for
arbitrary random variables <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark5.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmremark5.p2.m4"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, including those with rather
exotic low-dimensional distributions; see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>, §8.5]</cite>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.6</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark6.p1">
<p class="ltx_p">Given a set of data points in <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="Thmremark6.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>, one can always
interpret them as samples from a uniform discrete distribution with equal
probability <math alttext="1/N" class="ltx_Math" display="inline" id="Thmremark6.p1.m2"><semantics><mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1/N</annotation><annotation encoding="application/x-llamapun">1 / italic_N</annotation></semantics></math> on these <math alttext="N" class="ltx_Math" display="inline" id="Thmremark6.p1.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> vectors. The entropy for such a distribution
is <math alttext="H(\bm{X})=\frac{1}{N}\log_{2}N" class="ltx_Math" display="inline" id="Thmremark6.p1.m4"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{X})=\frac{1}{N}\log_{2}N</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_N</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Note again, even if we can encode
these vectors with this coding rate, we cannot decode them with an arbitrary
precision.</span></span></span> However, even if <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmremark6.p1.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is a uniform distribution on its samples,
the coding rate <math alttext="\mathcal{R}_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="Thmremark6.p1.m6"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> achievable with a lossy coding scheme
could be significantly lower than <math alttext="H(\bm{X})" class="ltx_Math" display="inline" id="Thmremark6.p1.m7"><semantics><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_X )</annotation></semantics></math> if these samples are not so evenly
distributed and many are clustered closely to each other. Therefore, for the
second distribution shown in Figure <a class="ltx_ref" href="#F8" title="Figure 3.8 ‣ Example 3.5 (Density). ‣ 3.3.1 Necessity of Lossy Coding ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.8</span></a>, for a properly chosen
quantization error <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark6.p1.m8"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, the achievable lossy coding rate can be
significantly lower than coding it as a uniform
distribution.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Nevertheless, for this discrete uniform distribution,
when <math alttext="\epsilon" class="ltx_Math" display="inline" id="footnote14.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small enough, we always have <math alttext="H(\bm{X})\approx\mathcal{R}_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="footnote14.m2"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{X})\approx\mathcal{R}_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_X ) ≈ caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>.</span></span></span> Also notice that, with the notion of rate distortion, the difficulty discussed in Example <a class="ltx_ref" href="#Thmexample6" title="Example 3.6 (Precision). ‣ 3.3.1 Necessity of Lossy Coding ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.6</span></a> also disappears: We can choose two rational numbers that are close enough to each of the two irrational numbers. The resulting coding scheme will have a finite complexity.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.7</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample7.p1">
<p class="ltx_p">Sometimes, one may face an opposite situation when we want to fix the coding
rate first and try to find a coding scheme that minimizes the distortion.
For example, suppose that we only want to use a fixed number of codes for
points sampled from a distribution, and we want to know how to design the
codes such that the average or maximum distortion is minimized during the
encoding/decoding scheme. For example, given a uniform distribution on
a unit square, we wonder how precisely we can encode points drawn from this
distribution, with say <math alttext="n" class="ltx_Math" display="inline" id="Thmexample7.p1.m1"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> bits. This problem is equivalent to asking what
is the minimum radius (i.e., distortion) such that we can cover the unit
square with <math alttext="2^{n}" class="ltx_Math" display="inline" id="Thmexample7.p1.m2"><semantics><msup><mn>2</mn><mi>n</mi></msup><annotation encoding="application/x-tex">2^{n}</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> discs of this radius. Figure
<a class="ltx_ref" href="#F9" title="Figure 3.9 ‣ Example 3.7. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.9</span></a> shows approximately optimal coverings of
a square with <math alttext="n=4,6,8" class="ltx_Math" display="inline" id="Thmexample7.p1.m3"><semantics><mrow><mi>n</mi><mo>=</mo><mrow><mn>4</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>8</mn></mrow></mrow><annotation encoding="application/x-tex">n=4,6,8</annotation><annotation encoding="application/x-llamapun">italic_n = 4 , 6 , 8</annotation></semantics></math>, so that <math alttext="2^{n}=16,64,256" class="ltx_Math" display="inline" id="Thmexample7.p1.m4"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup><mo>=</mo><mrow><mn>16</mn><mo>,</mo><mn>64</mn><mo>,</mo><mn>256</mn></mrow></mrow><annotation encoding="application/x-tex">2^{n}=16,64,256</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = 16 , 64 , 256</annotation></semantics></math> discs, respectively. Notice that the optimal radii of the discs decreases as the number of discs <math alttext="2^{n}" class="ltx_Math" display="inline" id="Thmexample7.p1.m5"><semantics><msup><mn>2</mn><mi>n</mi></msup><annotation encoding="application/x-tex">2^{n}</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> increases.</p>
</div>
<figure class="ltx_figure" id="F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 3.9 : Approximations to the optimal solutions for 2 4 2^{4} 2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , 2 6 2^{6} 2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT , and 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT discs covering a square, along with the corresponding radii, calculated using a heuristic optimization algorithm." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F9.g1" src="chapters/chapter3/figs/16.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 3.9 : Approximations to the optimal solutions for 2 4 2^{4} 2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , 2 6 2^{6} 2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT , and 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT discs covering a square, along with the corresponding radii, calculated using a heuristic optimization algorithm." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F9.g2" src="chapters/chapter3/figs/64.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 3.9 : Approximations to the optimal solutions for 2 4 2^{4} 2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , 2 6 2^{6} 2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT , and 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT discs covering a square, along with the corresponding radii, calculated using a heuristic optimization algorithm." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F9.g3" src="chapters/chapter3/figs/256.png" width="180"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.9</span>: </span><span class="ltx_text" style="font-size:90%;">Approximations to the optimal solutions for <math alttext="2^{4}" class="ltx_Math" display="inline" id="F9.m4"><semantics><msup><mn>2</mn><mn>4</mn></msup><annotation encoding="application/x-tex">2^{4}</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math>,
<math alttext="2^{6}" class="ltx_Math" display="inline" id="F9.m5"><semantics><msup><mn>2</mn><mn>6</mn></msup><annotation encoding="application/x-tex">2^{6}</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="2^{8}" class="ltx_Math" display="inline" id="F9.m6"><semantics><msup><mn>2</mn><mn>8</mn></msup><annotation encoding="application/x-tex">2^{8}</annotation><annotation encoding="application/x-llamapun">2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT</annotation></semantics></math> discs covering a square, along with the
corresponding radii, calculated using a heuristic optimization algorithm.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample7.p2">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample7.p2.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<figure class="ltx_figure" id="F10"><img alt="Figure 3.10 : The approximation of a low-dimensional distribution by ϵ \epsilon italic_ϵ balls. We can see that as the ϵ \epsilon italic_ϵ parameter shrinks, the union of ϵ \epsilon italic_ϵ -balls approximates the support of the true distribution (black) increasingly well. Furthermore, the associated denoisers (whose input-output mapping is given by the provided arrows) obtained by approximating the true distribution by a mixture of Gaussians, each with covariance ( ϵ 2 / D ) ​ 𝑰 (\epsilon^{2}/D)\bm{I} ( italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_D ) bold_italic_I , increasingly well-approximate the true denoisers. At large ϵ \epsilon italic_ϵ , such denoisers do not point near the true distribution at all, whereas at small ϵ \epsilon italic_ϵ they closely approximate the true denoisers. Theorem 3.6 establishes that this approximation characterizes the rate distortion function at small distortions ϵ \epsilon italic_ϵ , unifying the parallel approaches of coding rate minimization and denoising for learning low-dimensional distributions without pathologies." class="ltx_graphics ltx_img_landscape" height="270" id="F10.g1" src="chapters/chapter3/figs/continuation.png" width="359"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.10</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The approximation of a low-dimensional distribution
by <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m8"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> balls.<span class="ltx_text ltx_font_medium"> We can see that as the <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m9"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> parameter
shrinks, the union of <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m10"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls approximates the support of the
true distribution (black) increasingly well. Furthermore, the associated
denoisers (whose input-output mapping is given by the provided arrows)
obtained by approximating the true distribution by a mixture of Gaussians,
each with covariance <math alttext="(\epsilon^{2}/D)\bm{I}" class="ltx_Math" display="inline" id="F10.m11"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>/</mo><mi>D</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">(\epsilon^{2}/D)\bm{I}</annotation><annotation encoding="application/x-llamapun">( italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_D ) bold_italic_I</annotation></semantics></math>, increasingly well-approximate
the true denoisers. At large <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m12"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, such denoisers do not point near
the true distribution at all, whereas at small <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m13"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> they closely
approximate the true denoisers. <a class="ltx_ref" href="#Thmtheorem6" title="Theorem 3.6. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.6</span></a>
establishes that this approximation characterizes the rate distortion
function at small distortions <math alttext="\epsilon" class="ltx_Math" display="inline" id="F10.m14"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, unifying the parallel approaches
of coding rate minimization and denoising for learning low-dimensional
distributions without pathologies.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">It turns out to be a notoriously hard problem to obtain closed-form expressions
for the rate distortion function (<a class="ltx_ref" href="#S3.E3" title="Equation 3.3.3 ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>) for general
distributions <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>. However, as
<a class="ltx_ref" href="#Thmexample7" title="Example 3.7. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.7</span></a> suggests, there are important
special cases where the <span class="ltx_text ltx_font_italic">geometry</span> of the support of the distribution
<math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> can be linked to the rate distortion function, and hence to the optimal
coding rate at distortion level <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.p2.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>.
In fact, this example can be generalized to any setting where the
support of <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p2.m4"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> is a sufficiently regular compact set—including
low-dimensional distributions—and <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.p2.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> is uniformly distributed on its
support.
This covers a vast number of cases of practical interest.
We formalize this notion in the following result, which establishes this
property for a special case.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.6</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m1"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is a random variable such that its support <math alttext="K\doteq\operatorname{Supp}(\bm{x})" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m2"><semantics><mrow><mi>K</mi><mo>≐</mo><mrow><mi>Supp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">K\doteq\operatorname{Supp}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_K ≐ roman_Supp ( bold_italic_x )</annotation></semantics></math> is a compact set. Define the covering number <math alttext="\mathcal{N}_{\epsilon}(K)" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m3"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒩</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}_{\epsilon}(K)</annotation><annotation encoding="application/x-llamapun">caligraphic_N start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_K )</annotation></semantics></math> as the minimum number of balls of radius <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> that can cover <math alttext="K" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m5"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math>, i.e.,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{N}_{\epsilon}(K)\doteq\min\left\{n\in\mathbb{N}\colon\exists\bm{p}_{1},\dots,\bm{p}_{n}\in K\ \text{s.t.}\ K\subseteq\bigcup_{i=1}^{n}B_{\epsilon}(\bm{p}_{i})\right\}," class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒩</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mi>n</mi><mo>∈</mo><mi>ℕ</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mrow><mo rspace="0.167em">∃</mo><msub><mi>𝒑</mi><mn>1</mn></msub></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒑</mi><mi>n</mi></msub></mrow><mo>∈</mo><mrow><mi>K</mi><mo lspace="0.500em" rspace="0em">​</mo><mtext class="ltx_mathvariant_italic">s.t.</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>K</mi></mrow><mo rspace="0.111em">⊆</mo><mrow><munderover><mo movablelimits="false">⋃</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>B</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒑</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}_{\epsilon}(K)\doteq\min\left\{n\in\mathbb{N}\colon\exists\bm{p}_{1},\dots,\bm{p}_{n}\in K\ \text{s.t.}\ K\subseteq\bigcup_{i=1}^{n}B_{\epsilon}(\bm{p}_{i})\right\},</annotation><annotation encoding="application/x-llamapun">caligraphic_N start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_K ) ≐ roman_min { italic_n ∈ blackboard_N : ∃ bold_italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ italic_K s.t. italic_K ⊆ ⋃ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="B_{\epsilon}(\bm{p})=\{\bm{\xi}\in\mathbb{R}^{D}\mid\|\bm{\xi}-\bm{p}\|_{2}\leq\epsilon\}" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m6"><semantics><mrow><mrow><msub><mi>B</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐩</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>𝛏</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝛏</mi><mo>−</mo><mi>𝐩</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><mi>ϵ</mi></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">B_{\epsilon}(\bm{p})=\{\bm{\xi}\in\mathbb{R}^{D}\mid\|\bm{\xi}-\bm{p}\|_{2}\leq\epsilon\}</annotation><annotation encoding="application/x-llamapun">italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_p ) = { bold_italic_ξ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ∣ ∥ bold_italic_ξ - bold_italic_p ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_ϵ }</annotation></semantics></math> is the Euclidean ball of radius <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m7"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> centered at <math alttext="\bm{p}" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m8"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\bm{p}</annotation><annotation encoding="application/x-llamapun">bold_italic_p</annotation></semantics></math>.
Then it holds</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{x})\leq\log_{2}\mathcal{N}_{\epsilon}(K)." class="ltx_Math" display="block" id="S3.E8.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><msub><mi class="ltx_font_mathcaligraphic">𝒩</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})\leq\log_{2}\mathcal{N}_{\epsilon}(K).</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≤ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_K ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">If, in addition, <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m9"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is uniformly distributed on <math alttext="K" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m10"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> and <math alttext="K" class="ltx_Math" display="inline" id="Thmtheorem6.p1.m11"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> is
a mixture of mutually orthogonal low-rank subspaces,<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">15</span></span><span class="ltx_text ltx_font_upright">In fact, it is
possible to treat highly irregular </span><math alttext="K" class="ltx_Math" display="inline" id="footnote15.m1"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math><span class="ltx_text ltx_font_upright">, such as fractals, with a parallel
result, but its statement
becomes far more technical: c.f. Riegler et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright">[</span><a class="ltx_ref" href="bib.html#bibx227" title="">RBK18</a>, <a class="ltx_ref" href="bib.html#bibx228" title="">RKB23</a><span class="ltx_text ltx_font_upright">]</span></cite><span class="ltx_text ltx_font_upright">. We give a simple proof in
</span><a class="ltx_ref ltx_font_upright" href="A2.html#S3" title="B.3 Lossy Coding and Sphere Packing ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.3</span></a><span class="ltx_text ltx_font_upright"> which shows the result for mixtures of
subspaces.</span></span></span></span>
then a matching lower bound holds:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{x})\geq\log_{2}\mathcal{N}_{\epsilon}(K)-O(D)." class="ltx_Math" display="block" id="S3.E9.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><msub><mi class="ltx_font_mathcaligraphic">𝒩</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})\geq\log_{2}\mathcal{N}_{\epsilon}(K)-O(D).</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_K ) - italic_O ( italic_D ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.9)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">A proof of this theorem is beyond the scope of this book and we defer it to <a class="ltx_ref" href="A2.html#S3" title="B.3 Lossy Coding and Sphere Packing ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.3</span></a>.
∎</p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p">The implication of <a class="ltx_ref" href="#Thmtheorem6" title="Theorem 3.6. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.6</span></a> can be summarized
as follows: for sufficiently accurate coding of the distribution of
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.p4.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, the minimum rate distortion coding framework is completely characterized
by the sphere packing problem on the support of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.p4.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
The core of the proof of <a class="ltx_ref" href="#Thmtheorem6" title="Theorem 3.6. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.6</span></a> can indeed
be generalized to more complex distributions such as sufficiently incoherent
mixtures of manifolds, but we leave this for a future study.
So the rate distortion can be thought of as a “probability-aware” way to
approximate the support of the distribution of
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.p4.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> by a mixture of many small balls.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p">We now discuss another connection between this and the
denoising-diffusion-entropy complexity hierarchy we discussed earlier in the
Chapter.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.7</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark7.p1">
<p class="ltx_p">The key ingredient in the proof of the lower bound in
<a class="ltx_ref" href="#Thmtheorem6" title="Theorem 3.6. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.6</span></a> is an important result from
information theory known as the <span class="ltx_text ltx_font_italic">Shannon lower bound</span> for the rate
distortion, named after Claude Shannon, who first derived it in a special case
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx243" title="">Sha59</a>]</cite>. It
asserts the following estimate for the rate distortion function, for any random
variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark7.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with
a density <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="Thmremark7.p1.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> and finite expected squared norm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx164" title="">LZ94</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{x})\geq h(\bm{x})-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{D}," class="ltx_Math" display="block" id="S3.E10.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><msub><mi>C</mi><mi>D</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{x})\geq h(\bm{x})-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{D},</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ italic_h ( bold_italic_x ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="C_{D}&gt;0" class="ltx_Math" display="inline" id="Thmremark7.p1.m3"><semantics><mrow><msub><mi>C</mi><mi>D</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">C_{D}&gt;0</annotation><annotation encoding="application/x-llamapun">italic_C start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT &gt; 0</annotation></semantics></math> is a constant depending only on <math alttext="D" class="ltx_Math" display="inline" id="Thmremark7.p1.m4"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>. Moreover, this lower bound
is actually sharp as <math alttext="\epsilon\to 0" class="ltx_Math" display="inline" id="Thmremark7.p1.m5"><semantics><mrow><mi>ϵ</mi><mo stretchy="false">→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon\to 0</annotation><annotation encoding="application/x-llamapun">italic_ϵ → 0</annotation></semantics></math>: that is,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lim_{\epsilon\to 0}\mathcal{R}_{\epsilon}(\bm{x})-\left[h(\bm{x})-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{D}\right]=0." class="ltx_Math" display="block" id="S3.E11.m1"><semantics><mrow><mrow><mrow><mrow><munder><mo movablelimits="false">lim</mo><mrow><mi>ϵ</mi><mo stretchy="false">→</mo><mn>0</mn></mrow></munder><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mo>[</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><msub><mi>C</mi><mi>D</mi></msub></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\lim_{\epsilon\to 0}\mathcal{R}_{\epsilon}(\bm{x})-\left[h(\bm{x})-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{D}\right]=0.</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_ϵ → 0 end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) - [ italic_h ( bold_italic_x ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ] = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">So when the distortion <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p1.m6"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small, we can think solely in terms of
the Shannon lower bound, rather than the (generally intractable)
optimization problem defining the rate distortion
(<a class="ltx_ref" href="#S3.E3" title="Equation 3.3.3 ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>).</p>
</div>
<div class="ltx_para" id="Thmremark7.p2">
<p class="ltx_p">The Shannon lower bound is the bridge between the coding rate, entropy
minimization/denoising, and geometric sphere packing approaches for learning
low-dimensional distributions. Notice that in the special case of a uniform
density <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="Thmremark7.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>, (<a class="ltx_ref" href="#S3.E10" title="Equation 3.3.10 ‣ Remark 3.7. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.10</span></a>) becomes</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx27">
<tbody id="S3.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{R}_{\epsilon}(\bm{x})" class="ltx_Math" display="inline" id="S3.E12.m1"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{R}_{\epsilon}(\bm{x})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\geq-\int_{K}\frac{1}{\operatorname{vol}(K)}\log_{2}\frac{1}{\operatorname{vol}(K)}\mathrm{d}\bm{\xi}-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{d}" class="ltx_Math" display="inline" id="S3.E12.m2"><semantics><mrow><mi></mi><mo>≥</mo><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><msub><mo>∫</mo><mi>K</mi></msub></mstyle><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><msub><mi>C</mi><mi>d</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\geq-\int_{K}\frac{1}{\operatorname{vol}(K)}\log_{2}\frac{1}{\operatorname{vol}(K)}\mathrm{d}\bm{\xi}-\log_{2}\operatorname{vol}(B_{\epsilon})-C_{d}</annotation><annotation encoding="application/x-llamapun">≥ - ∫ start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_vol ( italic_K ) end_ARG roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_vol ( italic_K ) end_ARG roman_d bold_italic_ξ - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.12)</span></td>
</tr></tbody>
<tbody id="S3.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\log_{2}\operatorname{vol}(K)/\operatorname{vol}(B_{\epsilon})-C_{d}." class="ltx_Math" display="inline" id="S3.E13.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>−</mo><msub><mi>C</mi><mi>d</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\log_{2}\operatorname{vol}(K)/\operatorname{vol}(B_{\epsilon})-C_{d}.</annotation><annotation encoding="application/x-llamapun">= roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_K ) / roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The ratio <math alttext="\operatorname{vol}(K)/\operatorname{vol}(B_{\epsilon})" class="ltx_Math" display="inline" id="Thmremark7.p2.m2"><semantics><mrow><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vol}(K)/\operatorname{vol}(B_{\epsilon})</annotation><annotation encoding="application/x-llamapun">roman_vol ( italic_K ) / roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT )</annotation></semantics></math> approximates the number of
<math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p2.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls needed to cover <math alttext="K" class="ltx_Math" display="inline" id="Thmremark7.p2.m4"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> by a worst-case argument, which is
accurate for sufficiently regular sets <math alttext="K" class="ltx_Math" display="inline" id="Thmremark7.p2.m5"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> when <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p2.m6"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small (see
<a class="ltx_ref" href="A2.html#S3" title="B.3 Lossy Coding and Sphere Packing ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.3</span></a> for details).
Meanwhile, recall the Gaussian denoising model <math alttext="\bm{x}_{\epsilon}=\bm{x}+\epsilon\bm{g}" class="ltx_Math" display="inline" id="Thmremark7.p2.m7"><semantics><mrow><msub><mi>𝒙</mi><mi>ϵ</mi></msub><mo>=</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{\epsilon}=\bm{x}+\epsilon\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT = bold_italic_x + italic_ϵ bold_italic_g</annotation></semantics></math> from
earlier in the Chapter, where <math alttext="\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="Thmremark7.p2.m8"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> is independent of
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark7.p2.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
Interestingly, the differential entropy of the joint distribution <math alttext="(\bm{x},\bm{g})" class="ltx_Math" display="inline" id="Thmremark7.p2.m10"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{g})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_g )</annotation></semantics></math>
can be calculated as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx28">
<tbody id="S3.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle h(\bm{x},\bm{g})" class="ltx_Math" display="inline" id="S3.E14.m1"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle h(\bm{x},\bm{g})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x , bold_italic_g )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\int p(\bm{\xi})p(\bm{\gamma})\log_{2}p(\bm{\xi})p(\bm{\gamma})\mathrm{d}\bm{\xi}\mathrm{d}\bm{\gamma}" class="ltx_Math" display="inline" id="S3.E14.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mo>−</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜸</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜸</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝜸</mi></mrow></mrow></mrow></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=-\int p(\bm{\xi})p(\bm{\gamma})\log_{2}p(\bm{\xi})p(\bm{\gamma})\mathrm{d}\bm{\xi}\mathrm{d}\bm{\gamma}</annotation><annotation encoding="application/x-llamapun">= - ∫ italic_p ( bold_italic_ξ ) italic_p ( bold_italic_γ ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( bold_italic_ξ ) italic_p ( bold_italic_γ ) roman_d bold_italic_ξ roman_d bold_italic_γ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.14)</span></td>
</tr></tbody>
<tbody id="S3.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=h(\bm{x})+h(\epsilon\bm{g})." class="ltx_Math" display="inline" id="S3.E15.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=h(\bm{x})+h(\epsilon\bm{g}).</annotation><annotation encoding="application/x-llamapun">= italic_h ( bold_italic_x ) + italic_h ( italic_ϵ bold_italic_g ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We have seen the Gaussian entropy calculated in
<a class="ltx_ref" href="#S1.E4" title="In Example 3.1 (Entropy of Gaussian Distributions). ‣ 3.1.2 Differential Entropy ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.1.4</span></a>: when <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p2.m11"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small, it is equal, up
to additive constants, to the volumetric quantity <math alttext="-\log_{2}\operatorname{vol}(B_{\epsilon})" class="ltx_Math" display="inline" id="Thmremark7.p2.m12"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo lspace="0.167em">⁡</mo><mrow><mi>vol</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>ϵ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">-\log_{2}\operatorname{vol}(B_{\epsilon})</annotation><annotation encoding="application/x-llamapun">- roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT )</annotation></semantics></math> we have seen in the Shannon lower bound.
In certain special cases (e.g., data supported on incoherent low-rank subspaces),
when <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p2.m13"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small and the support of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark7.p2.m14"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is sufficiently regular,
the distribution of <math alttext="\bm{x}_{\epsilon}" class="ltx_Math" display="inline" id="Thmremark7.p2.m15"><semantics><msub><mi>𝒙</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">\bm{x}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> can even be well-approximated locally by the
product of the distributions <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="Thmremark7.p2.m16"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> and <math alttext="p(\bm{g})" class="ltx_Math" display="inline" id="Thmremark7.p2.m17"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{g})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_g )</annotation></semantics></math>, justifying the above
computation.
Hence the Gaussian denoising process yields yet another interpretation of
the Shannon lower bound, as arising from the entropy of a noisy version of
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark7.p2.m18"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, with noise level proportional to the distortion level <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmremark7.p2.m19"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Thmremark7.p3">
<p class="ltx_p">Thus, this finite rate distortion approach via sphere covering re-enables or
generalizes all previous measures of complexity of the distribution, allowing us
to differentiate between and rank different distributions in a unified way.
These interrelated viewpoints are visualized in <a class="ltx_ref" href="#F10" title="In 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.10</span></a>.</p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p">For a general distribution at finite distortion levels, it is typically impossible to find its rate
distortion function in an analytical form. One must often resort to numerical computation<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>Interested readers may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx21" title="">Bla72</a>]</cite> for a classic algorithm that computes rate distortion function numerically for a discrete distribution.</span></span></span>. Nevertheless, as we will see, in our context, we often need to know the rate distortion as an explicit function of a set of data points or their representations. This is because we want to use the coding rate as a measure of the goodness of the representations. An explicit analytical form makes it easy to determine how to transform the data distribution to improve the representation. So, we should work with distributions whose rate distortion functions take explicit analytical forms. To this end, we start with the simplest, and also the most important, family of distributions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.3 </span>Lossy Coding Rate for a Low-Dimensional Gaussian</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">Now suppose we are given a set of data samples in <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S3.SS3.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> from any distribution.<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>Or these data points could be viewed as an (empirical) distribution themselves.</span></span></span> We would like to come up with a constructive scheme that can encode the data up to certain precision, say</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}\mapsto\hat{\bm{x}}_{i},\quad\mbox{subject to}\quad\|\bm{x}_{i}-\hat{\bm{x}}_{i}\|_{2}\leq\epsilon." class="ltx_Math" display="block" id="S3.E16.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">↦</mo><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo rspace="1.167em">,</mo><mtext>subject to</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><mi>ϵ</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\mapsto\hat{\bm{x}}_{i},\quad\mbox{subject to}\quad\|\bm{x}_{i}-\hat{\bm{x}}_{i}\|_{2}\leq\epsilon.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ↦ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , subject to ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_ϵ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that this is a sufficient, explicit, and interpretable condition which
ensures that the data are encoded such that <math alttext="\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\hat{\bm{x}}_{i}\|_{2}^{2}\leq\epsilon^{2}" class="ltx_Math" display="inline" id="S3.SS3.p1.m2"><semantics><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>≤</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\hat{\bm{x}}_{i}\|_{2}^{2}\leq\epsilon^{2}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. This latter inequality is exactly the rate distortion constraint for the provided empirical distribution and its encoding. For example, in <a class="ltx_ref" href="#Thmexample7" title="Example 3.7. ‣ 3.3.2 Rate Distortion and Data Geometry ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.7</span></a>, we used this simplified criterion to explicitly find the minimum distortion and explicit coding scheme for a given coding rate.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">Without loss of generality, let us assume the mean of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is zero, i.e., <math alttext="\frac{1}{N}\sum_{i=1}^{N}\bm{x}_{i}=\bm{0}" class="ltx_Math" display="inline" id="S3.SS3.p2.m2"><semantics><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\frac{1}{N}\sum_{i=1}^{N}\bm{x}_{i}=\bm{0}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0</annotation></semantics></math>. Without any prior knowledge about the nature of the distribution behind <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we may view <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.p2.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> as sampled from a Gaussian distribution <math alttext="\mathcal{N}(\bm{0},{\bm{\Sigma}})" class="ltx_Math" display="inline" id="S3.SS3.p2.m5"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\bm{0},{\bm{\Sigma}})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , bold_Σ )</annotation></semantics></math> with the covariance<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>It is known that given a fixed variance, the Gaussian achieves the maximal entropy. That is, it gives an upper bound for what the worst case could be in terms of possible coding rate.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\bm{\Sigma}}=\frac{1}{N}\bm{X}\bm{X}^{\top}." class="ltx_Math" display="block" id="S3.E17.m1"><semantics><mrow><mrow><mi>𝚺</mi><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">{\bm{\Sigma}}=\frac{1}{N}\bm{X}\bm{X}^{\top}.</annotation><annotation encoding="application/x-llamapun">bold_Σ = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that geometrically <math alttext="{\bm{\Sigma}}" class="ltx_Math" display="inline" id="S3.SS3.p2.m6"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">{\bm{\Sigma}}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math> characterizes an ellipsoidal region where most of the samples <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.m7"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> reside.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p">We may view <math alttext="\hat{\bm{X}}=[\hat{\bm{x}}_{1},\ldots,\hat{\bm{x}}_{N}]" class="ltx_Math" display="inline" id="S3.SS3.p3.m1"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}=[\hat{\bm{x}}_{1},\ldots,\hat{\bm{x}}_{N}]</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG = [ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> as a noisy version of <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S3.SS3.p3.m2"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{i}=\bm{x}_{i}+\bm{w}_{i}," class="ltx_Math" display="block" id="S3.E18.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>+</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{i}=\bm{x}_{i}+\bm{w}_{i},</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{w}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m3"><semantics><msub><mi>𝒘</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a Gaussian noise <math alttext="\bm{w}_{i}\sim\mathcal{N}(\bm{0},{\epsilon^{2}}\bm{I}/{D})" class="ltx_Math" display="inline" id="S3.SS3.p3.m4"><semantics><mrow><msub><mi>𝒘</mi><mi>i</mi></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>/</mo><mi>D</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{w}_{i}\sim\mathcal{N}(\bm{0},{\epsilon^{2}}\bm{I}/{D})</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I / italic_D )</annotation></semantics></math> independent of <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m5"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Then the covariance of <math alttext="\hat{\bm{x}}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m6"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{i}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{\Sigma}}=\mathbb{E}\left[\hat{\bm{x}}_{i}\hat{\bm{x}}_{i}^{\top}\right]=\frac{\epsilon^{2}}{D}\bm{I}+\frac{1}{N}\bm{X}\bm{X}^{\top}." class="ltx_Math" display="block" id="S3.E19.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝚺</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><msup><mi>ϵ</mi><mn>2</mn></msup><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{\Sigma}}=\mathbb{E}\left[\hat{\bm{x}}_{i}\hat{\bm{x}}_{i}^{\top}\right]=\frac{\epsilon^{2}}{D}\bm{I}+\frac{1}{N}\bm{X}\bm{X}^{\top}.</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_Σ end_ARG = blackboard_E [ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] = divide start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I + divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that the volume of the region spanned by the vectors <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m7"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is proportional to the square root of the determinant of the
covariance matrix</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{volume}(\hat{\bm{x}}_{i})\propto\sqrt{\det\big{(}\hat{\bm{\Sigma}}\big{)}}=\sqrt{\det\left(\frac{\epsilon^{2}}{D}\bm{I}+\frac{1}{N}\bm{X}\bm{X}^{\top}\right)}." class="ltx_Math" display="block" id="S3.E20.m1"><semantics><mrow><mrow><mrow><mtext>volume</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msqrt><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mover accent="true"><mi>𝚺</mi><mo>^</mo></mover><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></msqrt><mo>=</mo><msqrt><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mrow><mfrac><msup><mi>ϵ</mi><mn>2</mn></msup><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></msqrt></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{volume}(\hat{\bm{x}}_{i})\propto\sqrt{\det\big{(}\hat{\bm{\Sigma}}\big{)}}=\sqrt{\det\left(\frac{\epsilon^{2}}{D}\bm{I}+\frac{1}{N}\bm{X}\bm{X}^{\top}\right)}.</annotation><annotation encoding="application/x-llamapun">volume ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∝ square-root start_ARG roman_det ( over^ start_ARG bold_Σ end_ARG ) end_ARG = square-root start_ARG roman_det ( divide start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I + divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The volume spanned by each random vector <math alttext="\bm{w}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m8"><semantics><msub><mi>𝒘</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is
proportional to</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{volume}(\bm{w}_{i})\propto\sqrt{\det\left(\frac{\epsilon^{2}}{D}\bm{I}\right)}." class="ltx_Math" display="block" id="S3.E21.m1"><semantics><mrow><mrow><mrow><mtext>volume</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒘</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msqrt><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mfrac><msup><mi>ϵ</mi><mn>2</mn></msup><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>)</mo></mrow></mrow></msqrt></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{volume}(\bm{w}_{i})\propto\sqrt{\det\left(\frac{\epsilon^{2}}{D}\bm{I}\right)}.</annotation><annotation encoding="application/x-llamapun">volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∝ square-root start_ARG roman_det ( divide start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.21)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="F11"><img alt="Figure 3.11 : Covering the region spanned by the data vectors using ϵ \epsilon italic_ϵ -balls. The larger the volume of the space, the more balls are needed, hence the more bits are needed to encode and enumerate the balls. Each real-valued vector 𝒙 \bm{x} bold_italic_x can be encoded as the number of the ball which it falls into." class="ltx_graphics ltx_img_landscape" height="175" id="F11.g1" src="chapters/chapter3/figs/Gaussian-compression.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.11</span>: </span><span class="ltx_text" style="font-size:90%;">Covering the region spanned by the data vectors using <math alttext="\epsilon" class="ltx_Math" display="inline" id="F11.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls. The larger the volume of the space, the more balls are needed, hence the more bits are needed to encode and enumerate the balls. Each real-valued vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F11.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be encoded as the number of the ball which it falls into.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p">To encode vectors that fall into the region spanned by <math alttext="\hat{\bm{x}}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p4.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{i}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we can cover the region with non-overlapping
balls of radius <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS3.p4.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, as illustrated in Figure <a class="ltx_ref" href="#F11" title="Figure 3.11 ‣ 3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.11</span></a>. When the volume of the region spanned by <math alttext="\hat{\bm{x}}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p4.m3"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{i}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is significantly larger than the volume of the <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS3.p4.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-ball, the total number of balls that we need to cover the region is
approximately equal to the ratio of the two volumes:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\#\,\epsilon\mbox{-balls}\approx\frac{\mbox{volume}(\hat{\bm{x}}_{i})}{\mbox{volume}(\bm{w}_{i})}=\sqrt{\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right)}." class="ltx_Math" display="block" id="S3.E22.m1"><semantics><mrow><mrow><mrow><mi mathvariant="normal">#</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-balls</mtext></mrow><mo>≈</mo><mfrac><mrow><mtext>volume</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><mtext>volume</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒘</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>=</mo><msqrt><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>D</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></msqrt></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\#\,\epsilon\mbox{-balls}\approx\frac{\mbox{volume}(\hat{\bm{x}}_{i})}{\mbox{volume}(\bm{w}_{i})}=\sqrt{\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right)}.</annotation><annotation encoding="application/x-llamapun"># italic_ϵ -balls ≈ divide start_ARG volume ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG = square-root start_ARG roman_det ( bold_italic_I + divide start_ARG italic_D end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we use binary numbers to label all the <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS3.p4.m5"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls in the region of
interest, the total number of binary bits needed is thus</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{X})\approx\log_{2}(\#\,\epsilon\mbox{-balls})\approx R_{\epsilon}(\bm{X})\doteq\frac{1}{2}\log\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right)." class="ltx_Math" display="block" id="S3.E23.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">#</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-balls</mtext></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>D</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{X})\approx\log_{2}(\#\,\epsilon\mbox{-balls})\approx R_{\epsilon}(\bm{X})\doteq\frac{1}{2}\log\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) ≈ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( # italic_ϵ -balls ) ≈ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_D end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.23)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.8</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample8.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F11" title="Figure 3.11 ‣ 3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.11</span></a> shows an example of a 2D distribution with an ellipsoidal support – approximating the support of a 2D Gaussian distribution. The region is covered by small balls of size <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample8.p1.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>. All the balls are numbered from <math alttext="1" class="ltx_Math" display="inline" id="Thmexample8.p1.m2"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> to say <math alttext="n" class="ltx_Math" display="inline" id="Thmexample8.p1.m3"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>. Then given any vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample8.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in this region, we
only need to determine to which <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample8.p1.m5"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-ball center it is the closest, denoted as <math alttext="\operatorname{ball}_{\epsilon}(\bm{x})" class="ltx_Math" display="inline" id="Thmexample8.p1.m6"><semantics><mrow><msub><mi>ball</mi><mi>ϵ</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{ball}_{\epsilon}(\bm{x})</annotation><annotation encoding="application/x-llamapun">roman_ball start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math>. To remember <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample8.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we only need to remember the number of this ball, which takes <math alttext="\log(n)" class="ltx_Math" display="inline" id="Thmexample8.p1.m8"><semantics><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(n)</annotation><annotation encoding="application/x-llamapun">roman_log ( italic_n )</annotation></semantics></math> bits to store. If we need to decode <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample8.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from this number, we simply take <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="Thmexample8.p1.m10"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> as the center of the ball. This leads to an explicit encoding and decoding scheme:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\longrightarrow\operatorname{ball}_{\epsilon}(\bm{x})\longrightarrow\hat{\bm{x}}=\mbox{center of}\operatorname{ball}_{\epsilon}(\bm{x})." class="ltx_Math" display="block" id="S3.E24.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo stretchy="false">⟶</mo><mrow><msub><mi>ball</mi><mi>ϵ</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟶</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mtext>center of</mtext><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mi>ball</mi><mi>ϵ</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\longrightarrow\operatorname{ball}_{\epsilon}(\bm{x})\longrightarrow\hat{\bm{x}}=\mbox{center of}\operatorname{ball}_{\epsilon}(\bm{x}).</annotation><annotation encoding="application/x-llamapun">bold_italic_x ⟶ roman_ball start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ⟶ over^ start_ARG bold_italic_x end_ARG = center of roman_ball start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">One may refer to these ball centers as “codes” of a code book or a dictionary for the encoding scheme. It is easy to see that the accuracy of this (lossy) encoding-decoding scheme is about the radius of the ball <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample8.p1.m11"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>.
Clearly <math alttext="\mathcal{R}_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="Thmexample8.p1.m12"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math> is the average
number of bits required to encode the ball number of each vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmexample8.p1.m13"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> with
this coding scheme, and hence can be called the <span class="ltx_text ltx_font_italic">coding rate</span> associated with this scheme.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample8.p1.m14"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p">From the above derivation, we know that the coding rate <math alttext="\mathcal{R}_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="S3.SS3.p5.m1"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> is (approximately) achievable with an explicit encoding (and decoding) scheme. It has two interesting properties:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">First, one may notice that <math alttext="R_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m1"><semantics><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> closely resembles the rate distortion function of a Gaussian source <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>. Indeed, when <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is small, the above expression is a close approximation to the rate distortion of a Gaussian source, as pointed out by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx171" title="">MDH+07</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">Second, the same closed-form coding rate <math alttext="R_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m1"><semantics><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> can be derived as an approximation of <math alttext="\mathcal{R}_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m2"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> if the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> are assumed to be from a linear subspace. This can be shown by properly quantifying the singular value decomposition (SVD) of <math alttext="\bm{X}=\bm{U}\bm{\Sigma}\bm{V}^{\top}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m4"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{\Sigma}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_Σ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> and constructing a lossy coding scheme for vectors in the subspace spanned by <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m5"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx171" title="">MDH+07</a>]</cite>.</p>
</div>
</li>
</ul>
<p class="ltx_p">In our context, the closed-form expression <math alttext="R_{\epsilon}(\bm{X})" class="ltx_Math" display="inline" id="S3.SS3.p5.m2"><semantics><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> is rather fundamental: it is the coding rate associated with an explicit and natural lossy coding scheme for data drawn from either a Gaussian distribution or a linear subspace. As we will see in the next chapter, this formula plays an important role in understanding the architecture of deep neural networks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.4 </span>Clustering a Mixture of Low-Dimensional Gaussians</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">As we have discussed before, the given dataset <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> often has low-dimensional intrinsic structures. Hence, encoding it as a general Gaussian would be very redundant. If we can identify those intrinsic structures in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we could design much better coding schemes that give much lower coding rates. Or equivalently, the codes used to encode such <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> can be compressed. We will see that compression gives a unifying computable way to identify such structures. In this section, we demonstrate this important idea with the most basic family of low-dimensional structures: a mixture of (low-dimensional) Gaussians or subspaces.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.9</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample9.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F12" title="Figure 3.12 ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.12</span></a> shows an example in which the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmexample9.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> are distributed around two subspaces (or low-dimensional Gaussians). If they are viewed and coded together as one single Gaussian, the associated discrete (lossy) code book, represented by all the blue balls, is obviously very redundant. We can try to identify the locations of the two subspaces, denoted by <math alttext="S_{1}" class="ltx_Math" display="inline" id="Thmexample9.p1.m2"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_{1}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="S_{2}" class="ltx_Math" display="inline" id="Thmexample9.p1.m3"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_{2}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, and design a code book that only covers the two subspaces, i.e., the green balls. If we can correctly partition samples in the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmexample9.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into the two subspaces: <math alttext="\bm{X}=[\bm{X}_{1},\bm{X}_{2}]\bm{\Pi}" class="ltx_Math" display="inline" id="Thmexample9.p1.m5"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mrow><mo stretchy="false">[</mo><msub><mi>𝑿</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝑿</mi><mn>2</mn></msub><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝚷</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{X}_{1},\bm{X}_{2}]\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] bold_Π</annotation></semantics></math> with <math alttext="\bm{X}_{1}\in S_{1}" class="ltx_Math" display="inline" id="Thmexample9.p1.m6"><semantics><mrow><msub><mi>𝑿</mi><mn>1</mn></msub><mo>∈</mo><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\bm{X}_{1}\in S_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{2}\in S_{2}" class="ltx_Math" display="inline" id="Thmexample9.p1.m7"><semantics><mrow><msub><mi>𝑿</mi><mn>2</mn></msub><mo>∈</mo><msub><mi>S</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\bm{X}_{2}\in S_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="Thmexample9.p1.m8"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> denotes a permutation matrix, then the resulting coding rate for the data will be much lower. This gives a more parsimonious, hence more desirable, representation of the data.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample9.p1.m9"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<figure class="ltx_figure" id="F12"><img alt="Figure 3.12 : Comparison of two lossy coding schemes for data that are distributed around two subspaces. One is to pack (blue) ϵ \epsilon italic_ϵ -balls for the entire space spanned by the two subspaces; the other is to pack balls only in a tabular neighborhood around the two subspaces. The latter obviously has a much smaller code book and results in a much lower coding rate for samples on the subspaces." class="ltx_graphics ltx_img_landscape" height="181" id="F12.g1" src="chapters/chapter3/figs/Two-subspaces.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.12</span>: </span><span class="ltx_text" style="font-size:90%;">Comparison of two lossy coding schemes for data that are distributed around two subspaces. One is to pack (blue) <math alttext="\epsilon" class="ltx_Math" display="inline" id="F12.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls for the entire space spanned by the two subspaces; the other is to pack balls only in a tabular neighborhood around the two subspaces. The latter obviously has a much smaller code book and results in a much lower coding rate for samples on the subspaces.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p">So, more generally speaking, if the data are drawn from any mixture of subspaces or low-dimensional Gaussians, it would be desirable to identify those components and encode the data based on the intrinsic dimensions of those components. It turns out that we do not lose much generality by assuming that the data are drawn from a mixture of low-dimensional Gaussians. This is because a mixture of Gaussians can closely approximate most general distributions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx24" title="">BDS16</a>]</cite>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">The clustering problem.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p">Now for this specific family of distributions, how can we effectively and efficiently identify those low-dimensional components from a set of samples</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}=\left[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N}\right]," class="ltx_Math" display="block" id="S3.E25.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo>[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}=\left[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N}\right],</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">drawn from them? In other words, given the whole data set <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we want to partition, or cluster, it into multiple, say <math alttext="K" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.m2"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math>, subsets:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\bm{\Pi}=[\bm{X}_{1},\bm{X}_{2},\dots,\bm{X}_{K}]," class="ltx_Math" display="block" id="S3.E26.m1"><semantics><mrow><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚷</mi></mrow><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝑿</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝑿</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑿</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\bm{\Pi}=[\bm{X}_{1},\bm{X}_{2},\dots,\bm{X}_{K}],</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_Π = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_X start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where each subset consists of samples drawn from only one low-dimensional Gaussian or subspace and <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.m3"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> is a permutation matrix to indicate membership of the partition. Note that, depending the situation, the partition could be either deterministic or probabilistic. As shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>, for mixture of Gaussians, probabilistic partition does not lead to a lower coding rate. So for simplicity, we here consider a deterministic partition only.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Clustering via lossy compression.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p">The main difficulty in solving the above clustering problem is that we normally do not know the number of clusters <math alttext="K" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p1.m1"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math>, nor do we know the dimension of each component. There has been a long history for the study of this clustering problem. The textbook <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx276" title="">VMS16</a>]</cite> gives a systematic and comprehensive coverage of different approaches to this problem. To find an effective approach to this problem, we first need to understand and clarify why we want to cluster. In other words, what exactly do we gain from clustering the data, compared with not to? How do we measure the gain? From the perspective of data compression, a correct clustering should lead to a more efficient encoding (and decoding) scheme.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p2">
<p class="ltx_p">For any given data set <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, there are already two obvious encoding schemes as the baseline. They represent two extreme ways to encode the data:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p">Simply view all the samples together drawn as from one single Gaussian. The associated coding rate is, as derived before, given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{\epsilon}(\bm{X})\approx R_{\epsilon}(\bm{X})=\frac{1}{2}\log\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right)." class="ltx_Math" display="block" id="S3.E27.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>D</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{\epsilon}(\bm{X})\approx R_{\epsilon}(\bm{X})=\frac{1}{2}\log\det\left(\bm{I}+\frac{D}{N\epsilon^{2}}\bm{X}\bm{X}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) ≈ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_D end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.27)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p">Simply memorize all the samples separately by assigning a different number to each sample. The coding rate would be:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E28">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}_{0}(\bm{X})=\log(N)." class="ltx_Math" display="block" id="S3.E28.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{R}_{0}(\bm{X})=\log(N).</annotation><annotation encoding="application/x-llamapun">caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.28)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p3">
<p class="ltx_p">Note that either coding scheme can become the “optimal” solution for certain (extreme) choice of the quantization error <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p3.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>:</p>
<ol class="ltx_enumerate" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Lazy Regime</span>: If we choose <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.I3.i1.p1.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> to be extremely large, all samples in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.I3.i1.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> can be covered by a single ball. The rate is <math alttext="\lim_{\epsilon\rightarrow\infty}\mathcal{R}_{\epsilon}\rightarrow\frac{1}{2}\log\det(\bm{I})=0" class="ltx_Math" display="inline" id="S3.I3.i1.p1.m3"><semantics><mrow><mrow><msub><mo>lim</mo><mrow><mi>ϵ</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></msub><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub></mrow><mo stretchy="false">→</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lim_{\epsilon\rightarrow\infty}\mathcal{R}_{\epsilon}\rightarrow\frac{1}{2}\log\det(\bm{I})=0</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_ϵ → ∞ end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT → divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I ) = 0</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Memorization Regime</span>: If <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.I3.i2.p1.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is extremely small, every sample in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.I3.i2.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is covered by a different <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.I3.i2.p1.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-ball, hence the total is <math alttext="N" class="ltx_Math" display="inline" id="S3.I3.i2.p1.m4"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math>. The rate is <math alttext="\lim_{\epsilon\rightarrow 0}\mathcal{R}_{\epsilon}\rightarrow\log(N)" class="ltx_Math" display="inline" id="S3.I3.i2.p1.m5"><semantics><mrow><mrow><msub><mo>lim</mo><mrow><mi>ϵ</mi><mo stretchy="false">→</mo><mn>0</mn></mrow></msub><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>ϵ</mi></msub></mrow><mo stretchy="false">→</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lim_{\epsilon\rightarrow 0}\mathcal{R}_{\epsilon}\rightarrow\log(N)</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_ϵ → 0 end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT → roman_log ( italic_N )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
<p class="ltx_p">Note that the first scheme corresponds to the scenario when one does not care about anything interesting about the distribution at all. One does not want to spare any bit for anything informative. We call this the “lazy regime.” The second scheme corresponds to the scenario when one wants to decode every sample with an extremely high precision. So one would better “memorize” every sample. We call this the “memorization regime.”</p>
</div>
<figure class="ltx_figure" id="F13"><img alt="Figure 3.13 : A number of random samples on a 2D plane. Consider an ϵ \epsilon italic_ϵ -disc assigned to each sample with the sample as its center. The density of the samples increases from left to right." class="ltx_graphics ltx_img_landscape" height="183" id="F13.g1" src="chapters/chapter3/figs/circle-packing.png" width="538"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.13</span>: </span><span class="ltx_text" style="font-size:90%;">A number of random samples on a 2D plane. Consider an <math alttext="\epsilon" class="ltx_Math" display="inline" id="F13.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-disc assigned to each sample with the sample as its center. The density of the samples increases from left to right.</span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample10">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.10</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample10.p1">
<p class="ltx_p">To see when the memorization regime is preferred or not, let us consider a number, say <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math>, of samples randomly distributed in a unit area on a 2D plane.<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>Say the points are drawn by a Poisson process with density <math alttext="N" class="ltx_Math" display="inline" id="footnote19.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> points per unit area.</span></span></span> Imagine we try to design a lossy coding scheme with a fixed quantization error <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample10.p1.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>. This is equivalent to putting an <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample10.p1.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-disc around each sample, as shown in Figure <a class="ltx_ref" href="#F13" title="Figure 3.13 ‣ Clustering via lossy compression. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.13</span></a>. When <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m4"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is small, the chance that all the discs overlap with each other is zero. A codebook of size <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m5"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is necessary and optimal in this case. When <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m6"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> or the density reaches a certain critical value <math alttext="N_{c}" class="ltx_Math" display="inline" id="Thmexample10.p1.m7"><semantics><msub><mi>N</mi><mi>c</mi></msub><annotation encoding="application/x-tex">N_{c}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, with high probability all the discs start to overlap and connect into one cluster that covers the whole plane—this phenomenon is known as continuum “percolation” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx91" title="">Gil61</a>, <a class="ltx_ref" href="bib.html#bibx184" title="">MM12</a>]</cite>. When <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m8"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> becomes larger than this value, the discs overlap heavily. The number <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m9"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> of discs becomes very redundant because we only want to encode points on the plane up to the given precision <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample10.p1.m10"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>. The number of discs needed to cover all the samples is much less than <math alttext="N" class="ltx_Math" display="inline" id="Thmexample10.p1.m11"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>In fact, there are efficient algorithms to find such a covering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx22" title="">BBF+01</a>]</cite>.</span></span></span>
<math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample10.p1.m12"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p4">
<p class="ltx_p">Both the lazy and memorization regimes are somewhat trivial and perhaps are of little theoretical or practical interest. Either scheme would be far from optimal when used to encode a large number of samples drawn from a distribution that has a <span class="ltx_text ltx_font_italic">compact and low-dimensional support</span>. The interesting regime exists in between these two.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample11">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.11</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<figure class="ltx_figure" id="F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Figure 3.14 : Top: 358 noisy samples drawn from two lines and one plane in ℝ 3 \mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . Bottom: the effect of varying ϵ \epsilon italic_ϵ on the clustering result and the coding rate. The red line marks the variance ϵ 0 \epsilon_{0} italic_ϵ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the Gaussian noise added to the samples." class="ltx_graphics ltx_figure_panel ltx_img_square" height="236" id="F14.g1" src="chapters/chapter3/figs/Two-lines-and-plane.png" width="240"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Figure 3.14 : Top: 358 noisy samples drawn from two lines and one plane in ℝ 3 \mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . Bottom: the effect of varying ϵ \epsilon italic_ϵ on the clustering result and the coding rate. The red line marks the variance ϵ 0 \epsilon_{0} italic_ϵ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the Gaussian noise added to the samples." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="167" id="F14.g2" src="chapters/chapter3/figs/Coding-Rate.jpg" width="538"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.14</span>: </span><span class="ltx_text" style="font-size:90%;">Top: 358 noisy samples drawn from two lines and one plane in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="F14.m4"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. Bottom: the effect of varying <math alttext="\epsilon" class="ltx_Math" display="inline" id="F14.m5"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> on the clustering result and the coding rate. The red line marks the variance <math alttext="\epsilon_{0}" class="ltx_Math" display="inline" id="F14.m6"><semantics><msub><mi>ϵ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\epsilon_{0}</annotation><annotation encoding="application/x-llamapun">italic_ϵ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> of the Gaussian noise added to the samples.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample11.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F14" title="Figure 3.14 ‣ Example 3.11. ‣ Clustering via lossy compression. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.14</span></a> shows an example with noisy samples
drawn from two lines and one plane in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="Thmexample11.p1.m1"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. As we notice from the
plot (c) on the right, the optimal coding rate decreases monotonically as we
increase <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample11.p1.m2"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>, as anticipated from the property of the rate distortion
function. The plots (a) and (b) show, when varying <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample11.p1.m3"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> from very
small (near zero) to very large (towards infinite), the optimal number of
clusters when the coding rate is minimal. We can clearly see the lazy regime
and the memorization regime on the two ends of the plots. But one can also
notice in plot (b), when the quantization error <math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmexample11.p1.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is chosen to be
around the level of the true noise variance <math alttext="\epsilon_{0}" class="ltx_Math" display="inline" id="Thmexample11.p1.m5"><semantics><msub><mi>ϵ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\epsilon_{0}</annotation><annotation encoding="application/x-llamapun">italic_ϵ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, the optimal number
of clusters is the “correct” number three that represents two planes and
one subspace. We informally refer to this middle regime as the
“generalization regime”. Notice that a sharp phase transition takes place
between these regimes.<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span>So far, to our best knowledge, there is no rigorous theoretical justification for these phase transition behaviors.</span></span></span>
<math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample11.p1.m6"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p5">
<p class="ltx_p">From the above discussion and examples, we see that, when the quantization error relative to the sample density<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span>or the sample density relative to the quantization error</span></span></span> is in a proper range, minimizing the lossy coding rate would allow us to uncover the underlying (low-dimensional) distribution of the sampled data. <span class="ltx_text ltx_font_italic">Hence, quantization, started as a choice of practicality, seems to be becoming necessary for learning a continuous distribution from its empirical distribution with finite samples.</span> Although a rigorous theory for explaining this phenomenon remains elusive, here, for learning purposes, we care about how to exploit the phenomenon to design algorithms that can find the correct distribution.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p6">
<p class="ltx_p">Let us use the simple example shown in Figure <a class="ltx_ref" href="#F12" title="Figure 3.12 ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.12</span></a> to illustrate the basic ideas. If one can partition all samples in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into two clusters in <math alttext="\bm{X}_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m2"><semantics><msub><mi>𝑿</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{X}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m3"><semantics><msub><mi>𝑿</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\bm{X}_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="N_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m4"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m5"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> samples respectively, then the associated coding rate would be<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>We here ignore some overhead bits needed to encode the membership for each sample, say via the Huffman coding.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})=\frac{N_{1}}{N}R_{\epsilon}(\bm{X}_{1})+\frac{N_{2}}{N}R_{\epsilon}(\bm{X}_{2})," class="ltx_Math" display="block" id="S3.E29.m1"><semantics><mrow><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><msub><mi>N</mi><mn>1</mn></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msub><mi>N</mi><mn>2</mn></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})=\frac{N_{1}}{N}R_{\epsilon}(\bm{X}_{1})+\frac{N_{2}}{N}R_{\epsilon}(\bm{X}_{2}),</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ∣ bold_Π ) = divide start_ARG italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + divide start_ARG italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where we use <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m6"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> to indicate membership of the partition. If the partition respects the low-dimensional structures of the distribution, in this case <math alttext="\bm{X}_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m7"><semantics><msub><mi>𝑿</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{X}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.p6.m8"><semantics><msub><mi>𝑿</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\bm{X}_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> belonging to the two subspaces respectively, then the resulting coding rate should be significantly smaller than the above two basic schemes:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\ll R_{\epsilon}(\bm{X}),\quad R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\ll R_{0}(\bm{X})." class="ltx_Math" display="block" id="S3.E30.m1"><semantics><mrow><mrow><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≪</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≪</mo><mrow><msub><mi>R</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\ll R_{\epsilon}(\bm{X}),\quad R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\ll R_{0}(\bm{X}).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ∣ bold_Π ) ≪ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) , italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ∣ bold_Π ) ≪ italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In general, we can cast the clustering problem into an optimization problem that minimizes the coding rate:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{\Pi}}\left\{R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\doteq\sum_{k=1}^{K}\frac{N_{k}}{N}R_{\epsilon}(\bm{X}_{k})\right\}." class="ltx_Math" display="block" id="S3.E31.m1"><semantics><mrow><mrow><munder><mi>min</mi><mi>𝚷</mi></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><msub><mi>N</mi><mi>k</mi></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{\Pi}}\left\{R_{\epsilon}^{c}(\bm{X}\mid\bm{\Pi})\doteq\sum_{k=1}^{K}\frac{N_{k}}{N}R_{\epsilon}(\bm{X}_{k})\right\}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_Π end_POSTSUBSCRIPT { italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ∣ bold_Π ) ≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.31)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Optimization strategies to cluster.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p">The remaining question is how we optimize the above coding rate objective to find the optimal clusters. There are three natural approaches to this objective:</p>
<ol class="ltx_enumerate" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p">We may start with the whole set <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.I4.i1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> as a single cluster (i.e. the lazy regime) and then search (say randomly) to partition it so that it would lead to a smaller coding rate.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p">Inversely, we may start with each sample <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.I4.i2.p1.m1"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as its own cluster
(i.e. the memorization regime) and search to merge clusters that would result in a smaller coding rate.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I4.i3.p1">
<p class="ltx_p">Alternatively, if we could represent (or approximate) the membership <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.I4.i3.p1.m1"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> as some continuous parameters, we may use optimization methods such as gradient descent (GD).</p>
</div>
</li>
</ol>
<p class="ltx_p">The first approach is not so appealing computationally as the number of possible partitions that one needs to try is exponential in the number of samples. For example, the number of partitions of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into two subsets of equal size is <math alttext="N\choose N/2" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p1.m2"><semantics><mrow><mo>(</mo><mfrac linethickness="0pt"><mi>N</mi><mrow><mi>N</mi><mo>/</mo><mn>2</mn></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">N\choose N/2</annotation><annotation encoding="application/x-llamapun">( binomial start_ARG italic_N end_ARG start_ARG italic_N / 2 end_ARG )</annotation></semantics></math> which explodes as <math alttext="N" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p1.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> becomes large. We will explore the third approach in the next Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>. There, we will see how the role of deep neural networks, transformers in particular, are connected with the coding rate objective.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p2">
<p class="ltx_p">The second approach was originally suggested in the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>. It demonstrates the benefit of being able to evaluate the coding rate efficiently (say with an analytical form). With it, the (low-dimensional) clusters of the data can be found rather efficiently and effectively via the principle of minimizing coding length (MCL). Note that for a cluster <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p2.m1"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="N_{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p2.m2"><semantics><msub><mi>N</mi><mi>k</mi></msub><annotation encoding="application/x-tex">N_{k}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> samples, the length of binary bits needed to encode all the samples in <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p2.m3"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is given by:<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>In fact, a more accurate estimate of the coding length is <math alttext="L(\bm{X}_{k})=(N_{k}+D)R_{\epsilon}(\bm{X}_{k})" class="ltx_Math" display="inline" id="footnote24.m1"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>+</mo><mi>D</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{X}_{k})=(N_{k}+D)R_{\epsilon}(\bm{X}_{k})</annotation><annotation encoding="application/x-llamapun">italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = ( italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_D ) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> where the extra bits are used to encode the basis of the subspace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>. Here we omit this overhead for simplicity.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(\bm{X}_{k})=N_{k}R_{\epsilon}(\bm{X}_{k})." class="ltx_Math" display="block" id="S3.E32.m1"><semantics><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">L(\bm{X}_{k})=N_{k}R_{\epsilon}(\bm{X}_{k}).</annotation><annotation encoding="application/x-llamapun">italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.32)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we have two clusters <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p2.m4"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{l}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p2.m5"><semantics><msub><mi>𝑿</mi><mi>l</mi></msub><annotation encoding="application/x-tex">\bm{X}_{l}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, if we want to code the samples as two separate clusters, the length of binary bits needed is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L^{c}(\bm{X}_{k},\bm{X}_{l})=N_{k}R_{\epsilon}(\bm{X}_{k})+N_{l}R_{\epsilon}(\bm{X}_{l})-N_{k}\log\frac{N_{k}}{N_{k}+N_{l}}-N_{l}\log\frac{N_{l}}{N_{k}+N_{l}}." class="ltx_Math" display="block" id="S3.Ex1.m1"><semantics><mrow><mrow><mrow><msup><mi>L</mi><mi>c</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>N</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><msub><mi>N</mi><mi>k</mi></msub><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>+</mo><msub><mi>N</mi><mi>l</mi></msub></mrow></mfrac></mrow></mrow><mo>−</mo><mrow><msub><mi>N</mi><mi>l</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><msub><mi>N</mi><mi>l</mi></msub><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>+</mo><msub><mi>N</mi><mi>l</mi></msub></mrow></mfrac></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">L^{c}(\bm{X}_{k},\bm{X}_{l})=N_{k}R_{\epsilon}(\bm{X}_{k})+N_{l}R_{\epsilon}(\bm{X}_{l})-N_{k}\log\frac{N_{k}}{N_{k}+N_{l}}-N_{l}\log\frac{N_{l}}{N_{k}+N_{l}}.</annotation><annotation encoding="application/x-llamapun">italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG - italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT roman_log divide start_ARG italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The last two terms are the number of bits needed to encode the memberships of samples according to the Huffman code.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p3">
<p class="ltx_p">Then, given any two separate clusters <math alttext="\bm{X}_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p3.m1"><semantics><msub><mi>𝑿</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{X}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p3.m2"><semantics><msub><mi>𝑿</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\bm{X}_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, we can decide whether to merge them or not based on the difference between the two coding lengths:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})" class="ltx_Math" display="block" id="S3.E33.m1"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>∪</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>L</mi><mi>c</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})</annotation><annotation encoding="application/x-llamapun">italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.3.33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is positive or negative and <math alttext="\bm{X}_{k}\cup\bm{X}_{l}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p3.m3"><semantics><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>∪</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\bm{X}_{k}\cup\bm{X}_{l}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> denotes the union of the sets of samples in <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p3.m4"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{X}_{l}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p3.m5"><semantics><msub><mi>𝑿</mi><mi>l</mi></msub><annotation encoding="application/x-tex">\bm{X}_{l}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. If it is negative, it means the coding length would become smaller if we merge the two clusters into one. This simple fact leads to the following clustering algorithm proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>:</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 3.3</span> </span> Pairwise Steepest Descent of Coding Length</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg3.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span><math alttext="N" class="ltx_Math" display="inline" id="alg3.l1.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> data points <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="alg3.l1.m2"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg3.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>A set <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg3.l2.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation><annotation encoding="application/x-llamapun">caligraphic_C</annotation></semantics></math> of clusters
</div>
<div class="ltx_listingline" id="alg3.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span><span class="ltx_text ltx_font_bold">procedure</span> <span class="ltx_text ltx_font_smallcaps">PairwiseSteepestDescentOfCodingLength</span>(<math alttext="\{\bm{x}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="alg3.l3.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg3.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>     <math alttext="\mathcal{C}\leftarrow\{\{\bm{x}_{i}\}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="alg3.l4.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">←</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{C}\leftarrow\{\{\bm{x}_{i}\}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">caligraphic_C ← { { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l4.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Initialize <math alttext="N" class="ltx_Math" display="inline" id="alg3.l4.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> clusters <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="alg3.l4.m4"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> with one element each
</span>
</div>
<div class="ltx_listingline" id="alg3.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>     <span class="ltx_text ltx_font_bold">while</span> <math alttext="\lvert\mathcal{C}\rvert&gt;1" class="ltx_Math" display="inline" id="alg3.l5.m1"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lvert\mathcal{C}\rvert&gt;1</annotation><annotation encoding="application/x-llamapun">| caligraphic_C | &gt; 1</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg3.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>         <span class="ltx_text ltx_font_bold">if</span> <math alttext="\displaystyle\min_{\bm{X}_{k},\bm{X}_{l}\in\mathcal{C}}[L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})]\geq 0" class="ltx_Math" display="inline" id="alg3.l6.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow></munder><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>∪</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>L</mi><mi>c</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\displaystyle\min_{\bm{X}_{k},\bm{X}_{l}\in\mathcal{C}}[L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})]\geq 0</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ caligraphic_C end_POSTSUBSCRIPT [ italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ] ≥ 0</annotation></semantics></math> <span class="ltx_text ltx_font_bold">then</span> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l6.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> If no bits are saved by any merging
</span>
</div>
<div class="ltx_listingline" id="alg3.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span>              <span class="ltx_text ltx_font_bold">return</span> <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg3.l7.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation><annotation encoding="application/x-llamapun">caligraphic_C</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l7.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Early return <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg3.l7.m3"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation><annotation encoding="application/x-llamapun">caligraphic_C</annotation></semantics></math> and exit
</span>
</div>
<div class="ltx_listingline" id="alg3.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>         <span class="ltx_text ltx_font_bold">else</span>
</div>
<div class="ltx_listingline" id="alg3.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>              <math alttext="\displaystyle\bm{X}_{k^{\ast}},\bm{X}_{l^{\ast}}\leftarrow\operatorname*{arg\ min}_{\bm{X}_{k},\bm{X}_{l}\in\mathcal{C}}[L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})]" class="ltx_Math" display="inline" id="alg3.l9.m1"><semantics><mrow><mrow><msub><mi>𝑿</mi><msup><mi>k</mi><mo>∗</mo></msup></msub><mo>,</mo><msub><mi>𝑿</mi><msup><mi>l</mi><mo>∗</mo></msup></msub></mrow><mo stretchy="false">←</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow></munder><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>∪</mo><msub><mi>𝑿</mi><mi>l</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>L</mi><mi>c</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{X}_{k^{\ast}},\bm{X}_{l^{\ast}}\leftarrow\operatorname*{arg\ min}_{\bm{X}_{k},\bm{X}_{l}\in\mathcal{C}}[L(\bm{X}_{k}\cup\bm{X}_{l})-L^{c}(\bm{X}_{k},\bm{X}_{l})]</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ← start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ caligraphic_C end_POSTSUBSCRIPT [ italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ]</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l9.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Merge clusters which save the most bits
</span>
</div>
<div class="ltx_listingline" id="alg3.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span>              <math alttext="\displaystyle\mathcal{C}\leftarrow[\mathcal{C}\setminus\{\bm{X}_{k^{\ast}},\bm{X}_{l^{\ast}}\}]\cup\{\bm{X}_{k^{\ast}}\cup\bm{X}_{l^{\ast}}\}" class="ltx_Math" display="inline" id="alg3.l10.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">←</mo><mrow><mrow><mo stretchy="false">[</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>∖</mo><mrow><mo stretchy="false">{</mo><msub><mi>𝑿</mi><msup><mi>k</mi><mo>∗</mo></msup></msub><mo>,</mo><msub><mi>𝑿</mi><msup><mi>l</mi><mo>∗</mo></msup></msub><mo stretchy="false">}</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo>∪</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>𝑿</mi><msup><mi>k</mi><mo>∗</mo></msup></msub><mo>∪</mo><msub><mi>𝑿</mi><msup><mi>l</mi><mo>∗</mo></msup></msub></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{C}\leftarrow[\mathcal{C}\setminus\{\bm{X}_{k^{\ast}},\bm{X}_{l^{\ast}}\}]\cup\{\bm{X}_{k^{\ast}}\cup\bm{X}_{l^{\ast}}\}</annotation><annotation encoding="application/x-llamapun">caligraphic_C ← [ caligraphic_C ∖ { bold_italic_X start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } ] ∪ { bold_italic_X start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT }</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l10.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Remove unmerged clusters and add back the merged one
</span>
</div>
<div class="ltx_listingline" id="alg3.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span>         <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">if</span>
</div>
<div class="ltx_listingline" id="alg3.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">while</span>
</div>
<div class="ltx_listingline" id="alg3.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg3.l13.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation><annotation encoding="application/x-llamapun">caligraphic_C</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg3.l13.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> If all merges yield savings, return one cluster
</span>
</div>
<div class="ltx_listingline" id="alg3.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">14:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p4">
<p class="ltx_p">Note that this algorithm is tractable as the total number of (pairwise) comparisons and merges is about <math alttext="O(N^{2}\log N)" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.p4.m1"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N^{2}\log N)</annotation><annotation encoding="application/x-llamapun">italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N )</annotation></semantics></math>. However, due to its greedy nature, there is no theoretical guarantee that the process will converge to the globally optimal clustering solution. Nevertheless, as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>, in practice, this seemingly simple algorithm works extremely well. The clustering results plotted in Figure <a class="ltx_ref" href="#F14" title="Figure 3.14 ‣ Example 3.11. ‣ Clustering via lossy compression. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.14</span></a> were actually computed by this algorithm.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample12">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.12</span></span><span class="ltx_text ltx_font_italic"> </span>(Image Segmentation)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample12.p1">
<p class="ltx_p">The above measure of coding length and the associated clustering algorithm assume the data distribution is a mixture of (low-dimensional) Gaussians. Although this seems somewhat idealistic, the measure and algorithm can already be very useful and even powerful in scenarios when the model is (approximately) valid.</p>
</div>
<div class="ltx_para" id="Thmexample12.p2">
<p class="ltx_p">For example, a natural image typically consists of multiple regions with nearly homogeneous textures. If we take many small windows from each region, they should resemble samples drawn from a (low-dimensional) Gaussian, as illustrated in Figure <a class="ltx_ref" href="#F15" title="Figure 3.15 ‣ Optimization strategies to cluster. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.15</span></a>. Figure <a class="ltx_ref" href="#F16" title="Figure 3.16 ‣ Optimization strategies to cluster. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.16</span></a> shows the results of image segmentation based on applying the above clustering algorithm to the image patches directly. More technical details regarding customizing the algorithm to the image segmentation problem can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx190" title="">MRY+11</a>]</cite>.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample12.p2.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<figure class="ltx_figure" id="F15"><img alt="Figure 3.15 : Image patches with a size of w × w w\times w italic_w × italic_w pixels." class="ltx_graphics ltx_img_landscape" height="153" id="F15.g1" src="chapters/chapter3/figs/image-segmentation-tiles.png" width="240"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.15</span>: </span><span class="ltx_text" style="font-size:90%;">Image patches with a size of <math alttext="w\times w" class="ltx_Math" display="inline" id="F15.m2"><semantics><mrow><mi>w</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">w\times w</annotation><annotation encoding="application/x-llamapun">italic_w × italic_w</annotation></semantics></math> pixels.</span></figcaption>
</figure>
<figure class="ltx_figure" id="F16"><img alt="Figure 3.16 : Segmentation results based on the clustering algorithm applied to the image patches." class="ltx_graphics ltx_img_landscape" height="377" id="F16.g1" src="chapters/chapter3/figs/image-segmentation.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.16</span>: </span><span class="ltx_text" style="font-size:90%;">Segmentation results based on the clustering algorithm applied to the image patches.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.4 </span>Maximizing Information Gain</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">So far in this chapter, we have discussed how to identify a distribution with low-dimensional structures through the principle of compression. As we have seen from the previous two sections, computational compression can be realized through either the denoising operation or through clustering. Figure <a class="ltx_ref" href="#F17" title="Figure 3.17 ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.17</span></a> illustrates this concept with our favorite example.</p>
</div>
<figure class="ltx_figure" id="F17"><img alt="Figure 3.17 : Identify a low-dimensional distribution with two subspaces (left) via denoising or clustering, starting from a generic random Gaussian distribution (right)." class="ltx_graphics ltx_img_landscape" height="131" id="F17.g1" src="chapters/chapter3/figs/Gaussian-Subspaces.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.17</span>: </span><span class="ltx_text" style="font-size:90%;">Identify a low-dimensional distribution with two subspaces (left) via denoising or clustering, starting from a generic random Gaussian distribution (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">Of course, the ultimate goal for identifying a data distribution is to use it to
facilitate certain subsequent tasks such as segmentation, classification, or
generation (of images). Hence, how the resulting distribution is “represented”
matters tremendously with respect to how information related to these subsequent tasks can be efficiently and effectively retrieved and utilized. This naturally raises a fundamental question: <span class="ltx_text ltx_font_italic">what makes a representation truly “good” for downstream use?</span> In the following, we will explore the essential properties that a meaningful and useful representation should possess, and how these properties can be explicitly characterized and pursued via maximizing information gain.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">How to measure the goodness of representations.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p">One may view a given dataset as samples of a random vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with a certain distribution in a high-dimensional space, say <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m2"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Typically, the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has a much lower intrinsic dimension than the ambient space. Generally speaking, <span class="ltx_text ltx_font_italic">learning a representation</span> refers to learning a continuous mapping, say <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m4"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math>, that transforms <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to a so-called <span class="ltx_text ltx_font_italic">feature vector</span> <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> in another (typically lower-dimensional) space, say <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m7"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="d&lt;D" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m8"><semantics><mrow><mi>d</mi><mo>&lt;</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d&lt;D</annotation><annotation encoding="application/x-llamapun">italic_d &lt; italic_D</annotation></semantics></math>. It is hopeful that through such a mapping</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\in\mathbb{R}^{D}\xrightarrow{\hskip 5.69054ptf(\bm{x})\hskip 5.69054pt}\bm{z}\in\mathbb{R}^{d}," class="ltx_Math" display="block" id="S4.E1.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}\xrightarrow{\hskip 5.69054ptf(\bm{x})\hskip 5.69054pt}\bm{z}\in\mathbb{R}^{d},</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT → end_ARROW bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">the low-dimensional intrinsic structures of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> are identified and represented by <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m10"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> in a more compact and structured way so as to facilitate subsequent tasks such as classification or generation. The feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m11"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> can be viewed as a (learned) compact code for the original data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m12"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, so the mapping <math alttext="f" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.m13"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is also called an <span class="ltx_text ltx_font_italic">encoder</span>.
The fundamental question of representation learning is</p>
<div class="ltx_logical-block">
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.SS0.SSS0.Px1.p1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">What is a principled and effective measure for the goodness of representations?</span></p>
</div>
</div>
</div>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p">Conceptually, the quality of a representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p2.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> depends on how well it identifies the most relevant and sufficient information of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> for subsequent tasks and how efficiently it represents this information.
For a long time, it was believed and argued that the “sufficiency” or “goodness” of a learned feature representation should be defined in terms of a specific task. For example, <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p2.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> just needs to be sufficient for predicting the class label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p2.m4"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> in a classification problem. Below, let us start with the classic problem of image classification and argue why such a notion of a task-specific “representation” is limited and needs to be generalized.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.1 </span>Linear Discriminative Representations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">Suppose that <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS1.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is a random vector drawn from a mixture of <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p1.m2"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> (component) distributions <math alttext="\mathcal{D}=\{\mathcal{D}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S4.SS1.p1.m3"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{D}=\{\mathcal{D}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">caligraphic_D = { caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>. Give a finite set of i.i.d. samples <math alttext="\bm{X}=[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S4.SS1.p1.m4"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> of the random vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we <span class="ltx_text ltx_font_italic">seek a good representation</span> through a continuous mapping <math alttext="f(\bm{x}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS1.p1.m6"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x ) : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> that captures intrinsic structures of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and best facilitates the subsequent classification task.<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span>Classification is the domain where deep learning demonstrated the initial success, sparking the explosive interest in deep networks. Although our study focuses on classification, we believe the ideas and principles can be naturally generalized to other settings, such as regression.</span></span></span> To ease the task of learning distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS1.p1.m8"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math>, in the popular supervised classification setting, a true class label (or a code word for each class), usually represented by a one-hot vector <math alttext="\bm{y}_{i}\in\mathbb{R}^{K}" class="ltx_Math" display="inline" id="S4.SS1.p1.m9"><semantics><mrow><msub><mi>𝒚</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\bm{y}_{i}\in\mathbb{R}^{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>, is given for each sample <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S4.SS1.p1.m10"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Encoding class information via cross entropy.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p">Extensive studies have shown that for many practical datasets (e.g., images, audio, and natural languages), the (encoding) mapping from the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to its class label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> can be effectively modeled by training a deep network,<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span>Here let us not worry about yet which network we should use here and why. The purpose here is to consider any empirically tested deep network. We will leave the justification of the network architectures to the next chapter.</span></span></span> here denoted as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f(\bm{x},\theta):\bm{x}\mapsto\bm{y}" class="ltx_Math" display="block" id="S4.Ex1.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒚</mi></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta):\bm{x}\mapsto\bm{y}</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ ) : bold_italic_x ↦ bold_italic_y</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">with network parameters <math alttext="\theta\in\Theta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m3"><semantics><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\theta\in\Theta</annotation><annotation encoding="application/x-llamapun">italic_θ ∈ roman_Θ</annotation></semantics></math>, where <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m4"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation><annotation encoding="application/x-llamapun">roman_Θ</annotation></semantics></math> denotes the parameter space. For the output <math alttext="f(\bm{x},\theta)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m5"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math> to match well with the label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m6"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, we like to minimize the <span class="ltx_text ltx_font_italic">cross-entropy loss</span> over a training set <math alttext="\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m7"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta\in\Theta}\;-\mathbb{E}[\langle\bm{y},\log(f(\bm{x},\theta))\rangle]\,\approx-\frac{1}{N}\sum_{i=1}^{N}\langle\bm{y}_{i},\log\left(f(\bm{x}_{i},\theta)\right)\rangle." class="ltx_Math" display="block" id="S4.E2.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant="normal">Θ</mi></mrow></munder><mo lspace="0.280em">−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mo stretchy="false">⟨</mo><mi>𝒚</mi><mo>,</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟩</mo></mrow><mo rspace="0.170em" stretchy="false">]</mo></mrow></mrow></mrow><mo>≈</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo stretchy="false">⟨</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo stretchy="false">⟩</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\theta\in\Theta}\;-\mathbb{E}[\langle\bm{y},\log(f(\bm{x},\theta))\rangle]\,\approx-\frac{1}{N}\sum_{i=1}^{N}\langle\bm{y}_{i},\log\left(f(\bm{x}_{i},\theta)\right)\rangle.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT - blackboard_E [ ⟨ bold_italic_y , roman_log ( italic_f ( bold_italic_x , italic_θ ) ) ⟩ ] ≈ - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⟨ bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_log ( italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_θ ) ) ⟩ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The optimal network parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m8"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> is typically found by optimizing the above objective through an efficient gradient descent scheme, with gradients computed via back propagation (BP), as described in Section <a class="ltx_ref" href="A1.html#S2.SS3" title="A.2.3 Back Propagation ‣ A.2 Computing Gradients via Automatic Differentiation ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">A.2.3</span></a> of Appendix <a class="ltx_ref" href="A1.html" title="Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p">Despite its effectiveness and enormous popularity, there are two serious limitations with this approach: 1) It aims only to predict the labels <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> even if they might be mislabeled. Empirical studies show that deep networks, used as a “black box,” can even fit random labels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx319" title="">ZBH+17</a>]</cite>.
2) With such an end-to-end data fitting,
despite plenty of empirical efforts in trying to interpret the so-learned
features, it is not clear to what extent the intermediate features learned by the network capture the intrinsic structures of the data that make meaningful classification possible in the first place. The precise geometric and statistical properties of the learned features are also often obscured, which leads to the lack of interpretability and subsequent performance guarantees (e.g., generalizability, transferability, and robustness, etc.) in deep learning.
Therefore, <span class="ltx_text ltx_font_italic">one of the goals of this section is to address such limitations by reformulating the objective towards learning explicitly meaningful and useful representations for the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.m2"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, not limited to classification.</span></p>
</div>
<figure class="ltx_figure" id="F18"><img alt="Figure 3.18 : Evolution of penultimate layer outputs of a VGG13 neural network when trained on the CIFAR10 dataset with 3 randomly selected classes. Figure from [ PHD20 ] ." class="ltx_graphics" id="F18.g1" src="chapters/chapter3/figs/neural_collapse.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.18</span>: </span><span class="ltx_text" style="font-size:90%;">Evolution of penultimate layer outputs of a VGG13 neural network when trained on the CIFAR10 dataset with 3 randomly selected classes. Figure from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx208" title="">PHD20</a>]</cite>.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Minimal discriminative features via information bottleneck.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p">One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features <math alttext="\bm{z}=f(\bm{x},\theta)\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m1"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}=f(\bm{x},\theta)\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z = italic_f ( bold_italic_x , italic_θ ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> of the data that are discriminative among multiple classes. Learned representations <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> then facilitate the subsequent classification task for predicting the class label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> by optimizing a classifier <math alttext="g(\bm{z})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m4"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z})</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054ptf(\bm{x},\theta)\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054ptg(\bm{z})\hskip 5.69054pt}\bm{y}." class="ltx_Math" display="block" id="S4.E3.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒚</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054ptf(\bm{x},\theta)\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054ptg(\bm{z})\hskip 5.69054pt}\bm{y}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_θ ) end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT italic_g ( bold_italic_z ) end_OVERACCENT → end_ARROW bold_italic_y .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We know from information theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite> that <span class="ltx_text ltx_font_italic">the mutual information</span> between two random variables, say <math alttext="\bm{x},\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m5"><semantics><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">\bm{x},\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_x , bold_italic_z</annotation></semantics></math>, is defined to be</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I(\bm{x};\bm{z})=H(\bm{x})-H(\bm{x}\mid\bm{z})," class="ltx_Math" display="block" id="S4.E4.m1"><semantics><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒛</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">I(\bm{x};\bm{z})=H(\bm{x})-H(\bm{x}\mid\bm{z}),</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x ; bold_italic_z ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ∣ bold_italic_z ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="H(\bm{x}|\bm{z})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m6"><semantics><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo fence="false">|</mo><mi>𝒛</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(\bm{x}|\bm{z})</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_x | bold_italic_z )</annotation></semantics></math> is the conditional entropy of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m8"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. The mutual information is also known as <span class="ltx_text ltx_font_italic">the information gain</span>: It measures how much the entropy of the random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be reduced once <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m10"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is given. Or equivalently, it measures how much information <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m11"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> contains about <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m12"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. The <span class="ltx_text ltx_font_italic">information bottleneck</span> (IB) formulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx262" title="">TZ15</a>]</cite> further hypothesizes that the role of the network is to learn <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m13"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> as the minimal sufficient statistics for predicting <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m14"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>. Formally, it seeks to maximize the mutual information <math alttext="I(\bm{z},\bm{y})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m15"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\bm{z},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_z , bold_italic_y )</annotation></semantics></math>
between <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m16"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m17"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> while minimizing the mutual information <math alttext="I(\bm{x},\bm{z})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m18"><semantics><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(\bm{x},\bm{z})</annotation><annotation encoding="application/x-llamapun">italic_I ( bold_italic_x , bold_italic_z )</annotation></semantics></math> between <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m19"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m20"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\theta\in\Theta}\;\mbox{IB}(\bm{x},\bm{y},\bm{z})\doteq I(\bm{z};\bm{y})-\beta I(\bm{x};\bm{z})\quad\ \mathrm{s.t.}\ \bm{z}=f(\bm{x},\theta)," class="ltx_Math" display="block" id="S4.E5.m1"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>θ</mi><mo>∈</mo><mi mathvariant="normal">Θ</mi></mrow></munder><mo lspace="0.447em">⁡</mo><mtext>IB</mtext></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>;</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1.5em"></mspace><mi mathvariant="normal">s</mi></mrow></mrow><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">t</mi><mo lspace="0em" rspace="0.667em">.</mo><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{\theta\in\Theta}\;\mbox{IB}(\bm{x},\bm{y},\bm{z})\doteq I(\bm{z};\bm{y})-\beta I(\bm{x};\bm{z})\quad\ \mathrm{s.t.}\ \bm{z}=f(\bm{x},\theta),</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT IB ( bold_italic_x , bold_italic_y , bold_italic_z ) ≐ italic_I ( bold_italic_z ; bold_italic_y ) - italic_β italic_I ( bold_italic_x ; bold_italic_z ) roman_s . roman_t . bold_italic_z = italic_f ( bold_italic_x , italic_θ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\beta&gt;0" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m21"><semantics><mrow><mi>β</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta&gt;0</annotation><annotation encoding="application/x-llamapun">italic_β &gt; 0</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p">Given one can overcome some caveats associated with this framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx140" title="">KTV18</a>]</cite>, such as how to accurately evaluate mutual information with finite samples of degenerate distributions, this framework can be helpful in explaining certain behaviors of deep networks.
For example, recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx208" title="">PHD20</a>]</cite> indeed shows that the representations learned via the cross-entropy loss (<a class="ltx_ref" href="#S4.E2" title="Equation 3.4.2 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) exhibit a <em class="ltx_emph ltx_font_italic">neural collapse</em> phenomenon.
That is, features of each class are mapped to a one-dimensional vector whereas all other information of the class is suppressed, as illustrated in Figure <a class="ltx_ref" href="#F18" title="Figure 3.18 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.18</span></a>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.8</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark8.p1">
<p class="ltx_p">Neural collapse refers to a phenomenon observed in deep neural networks trained for classification, where the learned feature representations and classifier weights exhibit highly symmetric and structured behavior during the terminal phase of training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx208" title="">PHD20</a>, <a class="ltx_ref" href="bib.html#bibx327" title="">ZDZ+21</a>]</cite>. Specifically, within each class, features collapse to their class mean, and across classes, these means become maximally separated, forming a simplex equiangular configuration. The linear classifier aligns with the class mean up to rescaling. Additionally, the last-layer classifier converges to choosing whichever class has the nearest train class mean. Neural collapse reveals deep connections between optimization dynamics, generalization, and geometric structures arising in supervised learning.</p>
</div>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p3">
<p class="ltx_p">From the above example of classification, we see that the so-learned representation gives a very simple encoder that essentially maps each class of data to only one code word: the one-hot vector representing each class. From the lossy compression perspective, such an encoder is too lossy to preserve information in the data distribution. Other information, such as that useful for tasks such as image generation, is severely lost in such a supervised learning process. To remedy this situation, we want to learn a different encoding scheme such that the resulting feature representation can capture much richer information about the data distribution, not limited to that useful for classification alone.</p>
</div>
<figure class="ltx_figure" id="F19"><img alt="Figure 3.19 : After identifying the low-dimensional data distribution, we would like to further transform the data distribution to a more informative structure representation: R R italic_R is the number of ϵ \epsilon italic_ϵ -balls covering the whole space and R c R^{c} italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT is the sum of the numbers for all the subspaces (the green balls). Δ ​ R \Delta R roman_Δ italic_R is their difference (the number of blue balls)." class="ltx_graphics ltx_img_landscape" height="133" id="F19.g1" src="chapters/chapter3/figs/Expansion2.png" width="592"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.19</span>: </span><span class="ltx_text" style="font-size:90%;">After identifying the low-dimensional data distribution, we would like to further transform the data distribution to a more informative structure representation: <math alttext="R" class="ltx_Math" display="inline" id="F19.m5"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation><annotation encoding="application/x-llamapun">italic_R</annotation></semantics></math> is the number of <math alttext="\epsilon" class="ltx_Math" display="inline" id="F19.m6"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls covering the whole space and <math alttext="R^{c}" class="ltx_Math" display="inline" id="F19.m7"><semantics><msup><mi>R</mi><mi>c</mi></msup><annotation encoding="application/x-tex">R^{c}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> is the sum of the numbers for all the subspaces (the green balls). <math alttext="\Delta R" class="ltx_Math" display="inline" id="F19.m8"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\Delta R</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R</annotation></semantics></math> is their difference (the number of blue balls). </span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Linear discriminative representations.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p">Whether the given data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> of a mixed distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> can be effectively classified or clustered depends on how separable (or discriminative) the component distributions <math alttext="\mathcal{D}_{k}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m3"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are (or can be made). One popular working assumption is that the distribution of each class has relatively <span class="ltx_text ltx_font_italic">low-dimensional</span> intrinsic structures. Hence we may assume that the distribution <math alttext="\mathcal{D}_{k}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m4"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> of each class has a support on a low-dimensional submanifold, say <math alttext="\mathcal{M}_{k}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m5"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{M}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> with dimension <math alttext="d_{k}\ll D" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m6"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>≪</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d_{k}\ll D</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≪ italic_D</annotation></semantics></math>, and the distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m7"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is supported on the mixture of those submanifolds, <math alttext="\mathcal{M}=\cup_{k=1}^{K}\mathcal{M}_{k}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m9"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo rspace="0em">=</mo><mrow><msubsup><mo lspace="0em">∪</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{M}=\cup_{k=1}^{K}\mathcal{M}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_M = ∪ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, in the high-dimensional ambient space <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m10"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p2">
<p class="ltx_p">Not only do we need to identify the low-dimensional distribution, but we also want to represent the distribution in a form that best facilitates subsequent tasks such as classification, clustering, and conditioned generation (as we will see in the future). To do so, we require our learned feature representations to have the following properties:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Within-Class Compressible:</span> Features of samples from the same class should be strongly <span class="ltx_text ltx_font_italic">correlated</span> in the sense that they belong to a low-dimensional linear subspace.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Between-Class Discriminative:</span> Features of samples from different classes should be highly <span class="ltx_text ltx_font_italic">uncorrelated</span> and belong to different low-dimensional linear subspaces.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Maximally Diverse Representation:</span> Dimension (or variance) of the features of each class should be <span class="ltx_text ltx_font_italic">as large as possible</span> as long as they are incoherent to the other classes.</p>
</div>
</li>
</ol>
<p class="ltx_p">We refer to such a representation the <span class="ltx_text ltx_font_italic">linear discriminative representation</span> (LDR). Notice that the first property aligns well with the objective of the classic <span class="ltx_text ltx_font_italic">principal component analysis</span> (PCA) that we have discussed in Chapter <a class="ltx_ref" href="Ch2.html#S1.SS1" title="2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>. The second property resembles that of the classic <span class="ltx_text ltx_font_italic">linear discriminant analysis</span> (LDA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx100" title="">HTF09</a>]</cite>. Figure <a class="ltx_ref" href="#F19" title="Figure 3.19 ‣ Minimal discriminative features via information bottleneck. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.19</span></a> illustrates these properties with a simple example when the data distribution is actually a mixture of two subspaces. Through compression (denoising or clustering), we first identify that the true data distribution is a mixture of two low-dimensional subspaces (middle) instead of a generic Gaussian distribution (left). We then would like to transform the distribution so that the two subspaces eventually become mutually incoherent/independent (right).</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 3.9</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark9.p1">
<p class="ltx_p">Linear discriminant analysis (LDA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx100" title="">HTF09</a>]</cite> is a supervised dimensionality reduction technique that aims to find a linear projection of data that maximizes class separability. Specifically, given labeled data, LDA seeks a linear transformation that projects high-dimensional inputs onto a lower-dimensional space where the classes are maximally separated. Note that PCA is an unsupervised method that projects data onto directions of maximum variance without considering class labels. While PCA focuses purely on preserving global variance structure, LDA explicitly exploits label information to enhance discriminative power; see the comparison in <a class="ltx_ref" href="#F20" title="In Linear discriminative representations. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.20</span></a>.</p>
</div>
</div>
<figure class="ltx_figure" id="F20"><img alt="Figure 3.20 : Comparison between PCA and LDA. Figures adpoted from https://sebastianraschka.com/Articles/2014_python_lda.html ." class="ltx_graphics ltx_img_landscape" height="214" id="F20.g1" src="chapters/chapter3/figs/LDA.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.20</span>: </span><span class="ltx_text" style="font-size:90%;">Comparison between PCA and LDA. Figures adpoted from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sebastianraschka.com/Articles/2014_python_lda.html" title="">https://sebastianraschka.com/Articles/2014_python_lda.html</a>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p3">
<p class="ltx_p">The third property is also important because we want the learned features to reveal all possible causes of why one class is different from all other classes. For example, to tell “apple” from “orange”, we care not only about color but also shape and the leaves. Ideally, the dimension of each subspace <math alttext="\{\mathcal{S}_{k}\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.m1"><semantics><mrow><mo stretchy="false">{</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\mathcal{S}_{k}\}</annotation><annotation encoding="application/x-llamapun">{ caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> should be equal to that of the corresponding submanifold <math alttext="\mathcal{M}_{k}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{M}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. This property will be important if we would like the map <math alttext="f(\bm{x},\theta)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math> to be <span class="ltx_text ltx_font_italic">invertible</span> for tasks such as image generation. For example, if we draw different sample points from the feature subspace for “apple”, we should be able to decode them to generate diverse images of apples. The feature learned from minimizing the cross entropy (<a class="ltx_ref" href="#S4.E2" title="Equation 3.4.2 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) clearly does not have this property.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p4">
<p class="ltx_p">In general, although the intrinsic structures of each class/cluster may be low-dimensional, they are by no means simply linear (or Gaussian) in their original representation <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and they need to be made linear first, through some nonlinear transformation.<span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span>We will discuss how this can be done explicitly in Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>.</span></span></span> Therefore, overall, we use the nonlinear transformation <math alttext="f(\bm{x},\theta)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m2"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math> to seek a representation of the data such that the subspaces that represent all the classes are maximally incoherent linear subspaces. To be more precise, we want to learn a mapping <math alttext="\bm{z}=f(\bm{x},\theta)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m3"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}=f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">bold_italic_z = italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math> that maps each of the submanifolds <math alttext="\mathcal{M}_{k}\subset\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m4"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{M}_{k}\subset\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> (Figure <a class="ltx_ref" href="#F21" title="Figure 3.21 ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.21</span></a> left) to a <span class="ltx_text ltx_font_italic">linear</span> subspace <math alttext="\mathcal{S}_{k}\subset\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m5"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo>⊂</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}_{k}\subset\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> (Figure <a class="ltx_ref" href="#F21" title="Figure 3.21 ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.21</span></a> right). To some extent, the resulting multiple subspaces <math alttext="\{\mathcal{S}_{k}\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m6"><semantics><mrow><mo stretchy="false">{</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\mathcal{S}_{k}\}</annotation><annotation encoding="application/x-llamapun">{ caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> can be viewed as discriminative <span class="ltx_text ltx_font_italic">generalized principal components</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx276" title="">VMS16</a>]</cite> or, if orthogonal, <span class="ltx_text ltx_font_italic">independent components</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx119" title="">HO00</a>]</cite> of the resulting features <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> for the original data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
As we will see in the next Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, deep networks precisely play the role of modeling and realizing this nonlinear transformation from the data distribution to linear discriminative representations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.2 </span>The Principle of Maximal Coding Rate Reduction</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">Although the three properties—<span class="ltx_text ltx_font_italic">between-class discriminative</span>, <span class="ltx_text ltx_font_italic">within-class compressible</span>, and <span class="ltx_text ltx_font_italic">maximally diverse representation</span>—for linear discriminative representations (LDRs) are all highly desired properties of the learned representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS2.p1.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, they are by no means easy to obtain: Are these properties compatible so that we can expect to achieve them all at once? If so, is there a <span class="ltx_text ltx_font_italic">simple but principled</span> objective that can measure the goodness of the resulting representations in terms of all these properties? The key to these questions is to find a principled “measure of compactness” or “information gain” for the distribution of a random variable <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS2.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> or from its finite samples <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S4.SS2.p1.m3"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. Such a measure should directly and accurately characterize intrinsic geometric or statistical properties of the distribution, in terms of its intrinsic dimension or volume. Unlike the cross entropy (<a class="ltx_ref" href="#S4.E2" title="Equation 3.4.2 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) or information bottleneck (<a class="ltx_ref" href="#S4.E5" title="Equation 3.4.5 ‣ Minimal discriminative features via information bottleneck. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.5</span></a>), such a measure should not depend exclusively on class labels so that it can work in more general settings such as supervised, self-supervised, semi-supervised, and unsupervised settings.</p>
</div>
<figure class="ltx_figure" id="F21"><img alt="Figure 3.21 : The distribution 𝒟 \mathcal{D} caligraphic_D of high-dimensional data 𝒙 ∈ ℝ D \bm{x}\in\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is supported on a manifold ℳ \mathcal{M} caligraphic_M and its classes on low-dimensional submanifolds ℳ k \mathcal{M}_{k} caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . We aim to learn a mapping f ​ ( 𝒙 , θ ) f(\bm{x},\theta) italic_f ( bold_italic_x , italic_θ ) parameterized by θ \theta italic_θ such that 𝒛 i = f ​ ( 𝒙 i , θ ) \bm{z}_{i}=f(\bm{x}_{i},\theta) bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_θ ) lie on a union of maximally uncorrelated subspaces { 𝒮 k } \{\mathcal{S}_{k}\} { caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } ." class="ltx_graphics ltx_img_landscape" height="141" id="F21.g1" src="chapters/chapter3/figs/mcr_diagram.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.21</span>: </span><span class="ltx_text" style="font-size:90%;">The distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="F21.m9"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> of high-dimensional data <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="F21.m10"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is supported on a manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="F21.m11"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> and its classes on low-dimensional submanifolds <math alttext="\mathcal{M}_{k}" class="ltx_Math" display="inline" id="F21.m12"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{M}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. We aim to learn a mapping <math alttext="f(\bm{x},\theta)" class="ltx_Math" display="inline" id="F21.m13"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math> parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="F21.m14"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> such that <math alttext="\bm{z}_{i}=f(\bm{x}_{i},\theta)" class="ltx_Math" display="inline" id="F21.m15"><semantics><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{i}=f(\bm{x}_{i},\theta)</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_θ )</annotation></semantics></math> lie on a union of maximally uncorrelated subspaces <math alttext="\{\mathcal{S}_{k}\}" class="ltx_Math" display="inline" id="F21.m16"><semantics><mrow><mo stretchy="false">{</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\mathcal{S}_{k}\}</annotation><annotation encoding="application/x-llamapun">{ caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">Without loss of generality, assume that the distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS2.p2.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> of the random vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS2.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is supported on a mixture of distributions, i.e., <math alttext="\mathcal{D}=\cup_{k=1}^{K}\mathcal{D}_{k}" class="ltx_Math" display="inline" id="S4.SS2.p2.m3"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo rspace="0em">=</mo><mrow><msubsup><mo lspace="0em">∪</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}=\cup_{k=1}^{K}\mathcal{D}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_D = ∪ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, where each <math alttext="\mathcal{D}_{k}\subset\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS2.p2.m4"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}_{k}\subset\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> has a low intrinsic dimension in the high-dimensional ambient space <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS2.p2.m5"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Let <math alttext="\bm{X}_{k}\in\mathbb{R}^{D\times N_{k}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m6"><semantics><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}_{k}\in\mathbb{R}^{D\times N_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> denote the data matrix whose columns are samples drawn from the distribution <math alttext="\mathcal{D}_{k}" class="ltx_Math" display="inline" id="S4.SS2.p2.m7"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{k}</annotation><annotation encoding="application/x-llamapun">caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="N_{k}" class="ltx_Math" display="inline" id="S4.SS2.p2.m8"><semantics><msub><mi>N</mi><mi>k</mi></msub><annotation encoding="application/x-tex">N_{k}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denotes the number of samples for each <math alttext="k=1,\dots,K" class="ltx_Math" display="inline" id="S4.SS2.p2.m9"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">k=1,\dots,K</annotation><annotation encoding="application/x-llamapun">italic_k = 1 , … , italic_K</annotation></semantics></math>. Then, we use <math alttext="\bm{X}=[\bm{X}_{1},\dots,\bm{X}_{K}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S4.SS2.p2.m10"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝑿</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑿</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{X}_{1},\dots,\bm{X}_{K}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_X start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> to denote all the samples, where <math alttext="N=\sum_{k=1}^{K}N_{k}" class="ltx_Math" display="inline" id="S4.SS2.p2.m11"><semantics><mrow><mi>N</mi><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>N</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">N=\sum_{k=1}^{K}N_{k}</annotation><annotation encoding="application/x-llamapun">italic_N = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.
Recall that we also use <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S4.SS2.p2.m12"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to denote the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p2.m13"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>-th sample of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS2.p2.m14"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, i.e., <math alttext="\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S4.SS2.p2.m15"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>. Under an encoding mapping:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054ptf(\bm{x})\hskip 5.69054pt}\bm{z}," class="ltx_Math" display="block" id="S4.E6.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒛</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054ptf(\bm{x})\hskip 5.69054pt}\bm{z},</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT → end_ARROW bold_italic_z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">the input samples are mapped to <math alttext="\bm{z}_{i}=f(\bm{x}_{i})" class="ltx_Math" display="inline" id="S4.SS2.p2.m16"><semantics><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{i}=f(\bm{x}_{i})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> for each <math alttext="i=1,\dots,N" class="ltx_Math" display="inline" id="S4.SS2.p2.m17"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">i=1,\dots,N</annotation><annotation encoding="application/x-llamapun">italic_i = 1 , … , italic_N</annotation></semantics></math>. With an abuse of notation, we also write <math alttext="\bm{Z}_{k}=f(\bm{X}_{k})" class="ltx_Math" display="inline" id="S4.SS2.p2.m18"><semantics><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{k}=f(\bm{X}_{k})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> and <math alttext="\bm{Z}=f(\bm{X})" class="ltx_Math" display="inline" id="S4.SS2.p2.m19"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X )</annotation></semantics></math>. Therefore, we have <math alttext="\bm{Z}=[\bm{Z}_{1},\dots,\bm{Z}_{K}]" class="ltx_Math" display="inline" id="S4.SS2.p2.m20"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒁</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒁</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{Z}_{1},\dots,\bm{Z}_{K}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="\bm{Z}=[\bm{z}_{1},\dots\bm{z}_{N}]" class="ltx_Math" display="inline" id="S4.SS2.p2.m21"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>N</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\dots\bm{z}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p">On one hand, for learned features to be discriminative, features of different classes/clusters are preferred to be <span class="ltx_text ltx_font_italic">maximally incoherent</span> to each other. Hence, they together should span a space of the largest possible volume (or dimension) and the coding rate of the whole set <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.p3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> should be as large as possible. On the other hand, learned features of the same class/cluster should be highly correlated and coherent. Hence, each class/cluster should only span a space (or subspace) of a very small volume and the coding rate should be as small as possible. Now, we will introduce how to measure the coding rate of the learned features.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Coding rate of features.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p">Notably, a practical challenge in evaluating the coding rate is that the underlying distribution of the feature representations <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is typically unknown. To address this, we may approximate the features <math alttext="\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m2"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> as samples drawn from a multivariate Gaussian distribution. Under this assumption, as discussed in Chapter <a class="ltx_ref" href="#S3.SS3" title="3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>, the compactness of the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> <span class="ltx_text ltx_font_italic">as a whole</span> can be measured in terms of the average coding length per sample, referred to as the <span class="ltx_text ltx_font_italic">coding rate</span>, subject to a precision level <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.m4"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math> (see (<a class="ltx_ref" href="#S3.E23" title="Equation 3.3.23 ‣ 3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.3.23</span></a>)) defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}(\bm{Z})=\frac{1}{2}\log\det\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}\bm{Z}^{\top}\right)." class="ltx_Math" display="block" id="S4.E7.m1"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>d</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{Z})=\frac{1}{2}\log\det\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}\bm{Z}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p2">
<p class="ltx_p">On the other hand, we hope that a nonlinear transformation <math alttext="f(\bm{x})" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x )</annotation></semantics></math> maps each class-specific submanifold <math alttext="\mathcal{M}_{k}\subset\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m2"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{M}_{k}\subset\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> to a maximally incoherent linear subspace <math alttext="\mathcal{S}_{k}\subset\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m3"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo>⊂</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}_{k}\subset\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> such that the learned features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> lie in a union of low-dimensional subspaces. This structure allows for a more accurate evaluation of the coding rate by analyzing each subspace separately. Recall that the columns of <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m5"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denotes the features of the samples in <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m6"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for each <math alttext="k=1,\dots,K" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m7"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">k=1,\dots,K</annotation><annotation encoding="application/x-llamapun">italic_k = 1 , … , italic_K</annotation></semantics></math>. The coding rate for the features in <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p2.m8"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> can be computed as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx29">
<tbody id="S4.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R_{\epsilon}(\bm{Z}_{k})=\frac{N_{k}}{2N}\log\det\left(\bm{I}+\frac{d}{N_{k}\epsilon^{2}}\bm{Z}_{k}\bm{Z}_{k}^{\top}\right)" class="ltx_Math" display="inline" id="S4.E8.m1"><semantics><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><msub><mi>N</mi><mi>k</mi></msub><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mi>d</mi><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle R_{\epsilon}(\bm{Z}_{k})=\frac{N_{k}}{2N}\log\det\left(\bm{I}+\frac{d}{N_{k}\epsilon^{2}}\bm{Z}_{k}\bm{Z}_{k}^{\top}\right)</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG 2 italic_N end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then, the sum of the average coding rates of features in each class is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}^{c}(\bm{Z})\doteq\sum_{k=1}^{K}R_{\epsilon}(\bm{Z}_{k})," class="ltx_Math" display="block" id="S4.E9.m1"><semantics><mrow><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}^{c}(\bm{Z})\doteq\sum_{k=1}^{K}R_{\epsilon}(\bm{Z}_{k}),</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.9)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p3">
<p class="ltx_p">Therefore, a good representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is the one that achieves a large difference between the coding rate for the whole and that for all the classes:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta R_{\epsilon}(\bm{Z})\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})." class="ltx_Math" display="block" id="S4.E10.m1"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z})\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z}).</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that, as per our discussions earlier in this chapter, this difference can be interpreted as the amount of “information gained” by identifying the correct low-dimensional clusters <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p3.m3"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> within the overall set <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p3.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p4">
<p class="ltx_p">If we choose our feature mapping <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math> to be a deep neural network <math alttext="f(\cdot,\theta)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m2"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , italic_θ )</annotation></semantics></math> with network parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m3"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, the overall process of the feature representation and the resulting rate reduction can be illustrated by the following diagram:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\theta)\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\epsilon\hskip 5.69054pt}\Delta R_{\epsilon}(\bm{Z})." class="ltx_Math" display="block" id="S4.E11.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_mathvariant_italic" mathvariant="italic">ϵ</mo></mover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\theta)\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\epsilon\hskip 5.69054pt}\Delta R_{\epsilon}(\bm{Z}).</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_θ ) end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_ϵ end_OVERACCENT → end_ARROW roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m4"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> is <span class="ltx_text ltx_font_italic">monotonic</span> in the scale of the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>. To ensure fair comparison across different representations, it is essential to <span class="ltx_text ltx_font_italic">normalize the scale</span> of the learned features. This can be achieved by either imposing the Frobenius norm of each class <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m6"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> to scale with the number of features in <math alttext="\bm{Z}_{k}\in\mathbb{R}^{d\times N_{k}}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m7"><semantics><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{k}\in\mathbb{R}^{d\times N_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="\|\bm{Z}_{k}\|_{F}^{2}=N_{k}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m8"><semantics><mrow><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>N</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\|\bm{Z}_{k}\|_{F}^{2}=N_{k}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, or by normalizing each feature to be on the unit sphere, i.e., <math alttext="\bm{z}_{i}\in\mathbb{S}^{d-1}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m9"><semantics><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{i}\in\mathbb{S}^{d-1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="N_{k}=\mathrm{tr}(\bm{\Pi}_{k})" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m10"><semantics><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">N_{k}=\mathrm{tr}(\bm{\Pi}_{k})</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> denotes the number of samples in the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p4.m11"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th class. This formulation offers a natural justification for the need for “batch normalization” in the practice of
training deep neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx121" title="">IS15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p5">
<p class="ltx_p">Once the representations are comparable, the goal becomes to learn a set of features <math alttext="\bm{Z}=f(\bm{X},\theta)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p5.m1"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X},\theta)</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X , italic_θ )</annotation></semantics></math> such that they maximize the reduction between the coding rate of all features and that of the sum of features w.r.t. their classes:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E12">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E12X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\theta}" class="ltx_Math" display="inline" id="S4.E12X.m2"><semantics><munder><mi>max</mi><mi>θ</mi></munder><annotation encoding="application/x-tex">\displaystyle\max_{\theta}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\;\Delta R_{\epsilon}\big{(}\bm{Z}\big{)}\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})," class="ltx_Math" display="inline" id="S4.E12X.m3"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mi>𝒁</mi><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\;\Delta R_{\epsilon}\big{(}\bm{Z}\big{)}\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z}),</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3.4.12)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E12Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic">s.t.</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\ \ \,\bm{Z}=f(\bm{X},\theta),\ \|\bm{Z}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K." class="ltx_Math" display="inline" id="S4.E12Xa.m3"><semantics><mrow><mrow><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>N</mi><mi>k</mi></msub></mrow><mo rspace="0.667em">,</mo><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\ \ \,\bm{Z}=f(\bm{X},\theta),\ \|\bm{Z}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X , italic_θ ) , ∥ bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k = 1 , … , italic_K .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">We refer to this as the principle of <span class="ltx_text ltx_font_italic">maximal coding rate reduction</span> (MCR<sup class="ltx_sup">2</sup>),
a true embodiment of Aristotle’s famous quote:</p>
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">The whole is greater than the sum of its parts.</span>”</p>
</blockquote>
<p class="ltx_p">To learn the best representation, we require that <span class="ltx_text ltx_font_italic">the whole is maximally greater than the sum of its parts</span>. Let us examine the example shown in Figure <a class="ltx_ref" href="#F19" title="Figure 3.19 ‣ Minimal discriminative features via information bottleneck. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.19</span></a> again. From a compression perspective, the representation on the right is <span class="ltx_text ltx_font_italic">the most compact one</span> in the sense that the difference between the coding rate when all features are encoded as a single Gaussian (blue) and that when the features are properly clustered and encoded as two separate subspaces (green) is maximal.<span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span>Intuitively, the ratio between the “volume” of the whole space spanned by all features and that actually occupied by the features is maximal.</span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p6">
<p class="ltx_p">Note that the above MCR<sup class="ltx_sup">2</sup> principle is designed for supervised learning problems, where the group memberships (or class labels) are known. However, this principle can be naturally extended to unsupervised learning problems by introducing a membership matrix, which encodes the (potentially soft) assignment of each data point to latent groups or clusters. Specifically, let <math alttext="\bm{\Pi}=\{\bm{\Pi}_{k}\}_{k=1}^{K}\subset\mathbb{R}^{N\times N}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m2"><semantics><mrow><mi>𝚷</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo>⊂</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Pi}=\{\bm{\Pi}_{k}\}_{k=1}^{K}\subset\mathbb{R}^{N\times N}</annotation><annotation encoding="application/x-llamapun">bold_Π = { bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> be a set of diagonal matrices whose diagonal entries encode the membership of the <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> samples into <math alttext="K" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m4"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> classes. That is, <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m5"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> lies in a simplex <math alttext="\Omega\doteq\{\bm{\Pi}:\bm{\Pi}_{k}\geq\bm{0}:\sum_{k=1}^{K}\bm{\Pi}_{k}=\bm{I}_{N}\}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m6"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo>≐</mo><mrow><mo stretchy="false">{</mo><mi>𝚷</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msub><mi>𝚷</mi><mi>k</mi></msub><mo>≥</mo><mn>𝟎</mn></mrow><mo lspace="0.278em" rspace="0.111em">:</mo><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>𝚷</mi><mi>k</mi></msub></mrow><mo>=</mo><msub><mi>𝑰</mi><mi>N</mi></msub></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega\doteq\{\bm{\Pi}:\bm{\Pi}_{k}\geq\bm{0}:\sum_{k=1}^{K}\bm{\Pi}_{k}=\bm{I}_{N}\}</annotation><annotation encoding="application/x-llamapun">roman_Ω ≐ { bold_Π : bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≥ bold_0 : ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math>. Then, we can define the average coding rate with respect to the partition <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m7"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx30">
<tbody id="S4.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})\doteq\sum_{k=1}^{K}\frac{\mathrm{tr}(\bm{\Pi}_{k})}{2N}\log\det\left(\bm{I}+\frac{d}{\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top}\right)." class="ltx_Math" display="inline" id="S4.E13.m1"><semantics><mrow><mrow><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mi>d</mi><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})\doteq\sum_{k=1}^{K}\frac{\mathrm{tr}(\bm{\Pi}_{k})}{2N}\log\det\left(\bm{I}+\frac{d}{\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_Π ) ≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_N end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">When <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m8"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is given, <math alttext="R_{\epsilon}^{c}(\bm{Z}|\bm{\Pi})" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m9"><semantics><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo fence="false">|</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}^{c}(\bm{Z}|\bm{\Pi})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z | bold_Π )</annotation></semantics></math> is a concave function of <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m10"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math>. Then the MCR<sup class="ltx_sup">2</sup> principle for unsupervised learning problems becomes as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx31">
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\Pi},\theta}" class="ltx_Math" display="inline" id="S4.Ex2.m1"><semantics><munder><mi>max</mi><mrow><mi>𝚷</mi><mo>,</mo><mi>θ</mi></mrow></munder><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\Pi},\theta}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_Π , italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\ \Delta R_{\epsilon}\big{(}\bm{Z}\mid\bm{\Pi})\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})" class="ltx_Math" display="inline" id="S4.Ex2.m2"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\ \Delta R_{\epsilon}\big{(}\bm{Z}\mid\bm{\Pi})\doteq R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_Π )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathrm{s.t.}" class="ltx_Math" display="inline" id="S4.E14.m1"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">t</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{s.t.}</annotation><annotation encoding="application/x-llamapun">roman_s . roman_t .</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\ \ \ \bm{Z}=f(\bm{X},\theta),\ \|\bm{Z}\bm{\Pi}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K,\ \bm{\Pi}\in\Omega." class="ltx_Math" display="inline" id="S4.E14.m2"><semantics><mrow><mrow><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>N</mi><mi>k</mi></msub></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mi>𝚷</mi><mo>∈</mo><mi mathvariant="normal">Ω</mi></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\ \ \ \bm{Z}=f(\bm{X},\theta),\ \|\bm{Z}\bm{\Pi}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K,\ \bm{\Pi}\in\Omega.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X , italic_θ ) , ∥ bold_italic_Z bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k = 1 , … , italic_K , bold_Π ∈ roman_Ω .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Compared to (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>), the formulation here allows for the joint optimization of both the group memberships and the network parameters. In particular, when <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m12"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> is fixed to a group membership matrix that assigns <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m13"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> data points into <math alttext="K" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p6.m14"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> groups, Problem (<a class="ltx_ref" href="#S4.Ex2" title="Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) can recover Problem (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>).</p>
</div>
<figure class="ltx_figure" id="F22"><img alt="Figure 3.22 : Local optimization landscape: According to Theorem • ‣ 3.7 , the global maximum of the rate reduction objective corresponds to a solution with mutually incoherent subspaces." class="ltx_graphics ltx_img_landscape" height="173" id="F22.g1" src="chapters/chapter3/figs/mcr2-global.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.22</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Local optimization landscape:<span class="ltx_text ltx_font_medium"> According to Theorem <a class="ltx_ref" href="#S4.I2.i2" title="2nd item ‣ Theorem 3.7 (Characterization of Global Optimal Solutions). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">•</span> ‣ <span class="ltx_text ltx_ref_tag">3.7</span></a>, the global maximum of the rate reduction objective corresponds to a solution with mutually incoherent subspaces.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.3 </span>Optimization Properties of Coding Rate Reduction</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">In this subsection, we study the optimization properties of the MCR<sup class="ltx_sup">2</sup> function by analyzing its optimal solutions and the structure of its optimization landscape. To get around the technical difficulty introduced by the neural networks, we consider a simplified version of Problem (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx32">
<tbody id="S4.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{Z}}\ R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})\qquad\mathrm{s.t.}\quad\|\bm{Z}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K." class="ltx_math_unparsed" display="inline" id="S4.E15.m1"><semantics><mrow><munder><mi>max</mi><mi>𝒁</mi></munder><msub><mi>R</mi><mi>ϵ</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><mo>−</mo><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><mspace width="2em"></mspace><mi mathvariant="normal">s</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">t</mi><mo lspace="0em">.</mo><mspace width="1.167em"></mspace><mo rspace="0.167em">∥</mo><msub><mi>𝒁</mi><mi>k</mi></msub><msubsup><mo lspace="0em" rspace="0.0835em">∥</mo><mi>F</mi><mn>2</mn></msubsup><mo lspace="0.0835em">=</mo><msub><mi>N</mi><mi>k</mi></msub><mo rspace="0.667em">,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{Z}}\ R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})\qquad\mathrm{s.t.}\quad\|\bm{Z}_{k}\|_{F}^{2}=N_{k},\ k=1,\dots,K.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) roman_s . roman_t . ∥ bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k = 1 , … , italic_K .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In theory, the MCR<sup class="ltx_sup">2</sup> principle (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>) benefits from great generalizability and can be applied to representations <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.p1.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> of <span class="ltx_text ltx_font_italic">any</span> distributions as long as the rates <math alttext="R_{\epsilon}" class="ltx_Math" display="inline" id="S4.SS3.p1.m4"><semantics><msub><mi>R</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="S4.SS3.p1.m5"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> for the distributions can be accurately and efficiently evaluated. The optimal representation <math alttext="\bm{Z}^{\ast}" class="ltx_Math" display="inline" id="S4.SS3.p1.m6"><semantics><msup><mi>𝒁</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{\ast}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> should have some interesting geometric and statistical properties. We here reveal nice properties of the optimal representation with the special case of subspaces, which have many important use cases in machine learning. When the desired representation for <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.p1.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is multiple subspaces, the rates <math alttext="R_{\epsilon}" class="ltx_Math" display="inline" id="S4.SS3.p1.m8"><semantics><msub><mi>R</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="S4.SS3.p1.m9"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> in (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>) are given by (<a class="ltx_ref" href="#S4.E7" title="Equation 3.4.7 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.7</span></a>)
and (<a class="ltx_ref" href="#S4.E9" title="Equation 3.4.9 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.9</span></a>), respectively. At the maximal rate reduction, MCR<sup class="ltx_sup">2</sup> achieves its optimal representations, denoted as <math alttext="\bm{Z}^{\ast}=[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}]" class="ltx_Math" display="inline" id="S4.SS3.p1.m11"><semantics><mrow><msup><mi>𝒁</mi><mo>∗</mo></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒁</mi><mn>1</mn><mo>∗</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒁</mi><mi>K</mi><mo>∗</mo></msubsup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ast}=[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ]</annotation></semantics></math> with <math alttext="\operatorname{rank}{(\bm{Z}_{k}^{*})}\leq d_{k}" class="ltx_Math" display="inline" id="S4.SS3.p1.m12"><semantics><mrow><mrow><mi>rank</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\operatorname{rank}{(\bm{Z}_{k}^{*})}\leq d_{k}</annotation><annotation encoding="application/x-llamapun">roman_rank ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ≤ italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. One can show that <math alttext="\bm{Z}^{\ast}" class="ltx_Math" display="inline" id="S4.SS3.p1.m13"><semantics><msup><mi>𝒁</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{\ast}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> has the following desired properties (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx313" title="">YCY+20</a>]</cite> for a formal statement and detailed proofs).</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.7</span></span><span class="ltx_text ltx_font_bold"> </span>(<span class="ltx_text ltx_font_bold">Characterization of Global Optimal Solutions).</span>
</h6>
<div class="ltx_para" id="Thmtheorem7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose <math alttext="\bm{Z}^{\ast}=[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}]" class="ltx_Math" display="inline" id="Thmtheorem7.p1.m1"><semantics><mrow><msup><mi>𝐙</mi><mo>∗</mo></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>𝐙</mi><mn>1</mn><mo>∗</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝐙</mi><mi>K</mi><mo>∗</mo></msubsup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ast}=[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ]</annotation></semantics></math> is a global optimal solution of Problem (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>). The following statements hold:</span></p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_normal">Between-Class Discriminative</span><span class="ltx_text ltx_font_italic">: As long as the ambient space is adequately large (</span><math alttext="d\geq\sum_{k=1}^{K}d_{k}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m1"><semantics><mrow><mi>d</mi><mo rspace="0.111em">≥</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>d</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">d\geq\sum_{k=1}^{K}d_{k}</annotation><annotation encoding="application/x-llamapun">italic_d ≥ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic">), the subspaces are all orthogonal to each other, </span><span class="ltx_text ltx_font_normal">i.e.</span><span class="ltx_text ltx_font_italic">, </span><math alttext="(\bm{Z}_{k}^{*})^{\top}\bm{Z}_{l}^{*}=\bm{0}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m2"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>l</mi><mo>∗</mo></msubsup></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">(\bm{Z}_{k}^{*})^{\top}\bm{Z}_{l}^{*}=\bm{0}</annotation><annotation encoding="application/x-llamapun">( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = bold_0</annotation></semantics></math><span class="ltx_text ltx_font_italic"> for </span><math alttext="k\not=l" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m3"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">k\not=l</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_l</annotation></semantics></math><span class="ltx_text ltx_font_italic">.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_normal">Maximally Diverse Representation</span><span class="ltx_text ltx_font_italic">:
As long as the coding precision is adequately high, i.e., </span><math alttext="\epsilon^{4}&lt;c\cdot\min_{k}\left\{\frac{N_{k}}{N}\frac{d^{2}}{d_{k}^{2}}\right\}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m1"><semantics><mrow><msup><mi>ϵ</mi><mn>4</mn></msup><mo>&lt;</mo><mrow><mi>c</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><msub><mi>min</mi><mi>k</mi></msub><mo>⁡</mo><mrow><mo>{</mo><mrow><mfrac><msub><mi>N</mi><mi>k</mi></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><msup><mi>d</mi><mn>2</mn></msup><msubsup><mi>d</mi><mi>k</mi><mn>2</mn></msubsup></mfrac></mrow><mo>}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\epsilon^{4}&lt;c\cdot\min_{k}\left\{\frac{N_{k}}{N}\frac{d^{2}}{d_{k}^{2}}\right\}</annotation><annotation encoding="application/x-llamapun">italic_ϵ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT &lt; italic_c ⋅ roman_min start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT { divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG }</annotation></semantics></math><span class="ltx_text ltx_font_italic">, where </span><math alttext="c&gt;0" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m2"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c&gt;0</annotation><annotation encoding="application/x-llamapun">italic_c &gt; 0</annotation></semantics></math><span class="ltx_text ltx_font_italic"> is a constant. Each subspace achieves its maximal dimension, i.e. </span><math alttext="\mathrm{rank}{(\bm{Z}_{k}^{*})}=d_{k}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m3"><semantics><mrow><mrow><mi>rank</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathrm{rank}{(\bm{Z}_{k}^{*})}=d_{k}</annotation><annotation encoding="application/x-llamapun">roman_rank ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) = italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic">. In addition, the largest </span><math alttext="d_{k}-1" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m4"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d_{k}-1</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1</annotation></semantics></math><span class="ltx_text ltx_font_italic"> singular values of </span><math alttext="\bm{Z}_{k}^{*}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m5"><semantics><msubsup><mi>𝒁</mi><mi>k</mi><mo>∗</mo></msubsup><annotation encoding="application/x-tex">\bm{Z}_{k}^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> are equal.</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p">This theorem indicates that the MCR<sup class="ltx_sup">2</sup> principle promotes embedding of data into multiple independent subspaces (as illustrated in Figure <a class="ltx_ref" href="#F22" title="Figure 3.22 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.22</span></a>), with features distributed <span class="ltx_text ltx_font_italic">isotropically</span> in each subspace (except for possibly one dimension). Notably, this theorem also confirms that the features learned by the MCR<sup class="ltx_sup">2</sup> principle exhibit the desired low-dimensional discriminative properties discussed in <a class="ltx_ref" href="#S4.SS1" title="3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4.1</span></a>. In addition, among all such discriminative representations, it prefers the one with the highest dimensions in the ambient space. This is substantially different from the objective of information bottleneck (<a class="ltx_ref" href="#S4.E5" title="Equation 3.4.5 ‣ Minimal discriminative features via information bottleneck. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.5</span></a>).</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample13">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 3.13</span></span><span class="ltx_text ltx_font_italic"> </span>(Classification of Images on CIFAR-10)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample13.p1">
<p class="ltx_p">We here present how the MCR<sup class="ltx_sup">2</sup> objective helps learn better representations than the cross entropy (<a class="ltx_ref" href="#S4.E2" title="Equation 3.4.2 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) for image classification. Here we adopt the popular neural network architecture, the ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx103" title="">HZR+16a</a>]</cite>, to model the feature mapping <math alttext="\bm{z}=f(\bm{x},\theta)" class="ltx_Math" display="inline" id="Thmexample13.p1.m2"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}=f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">bold_italic_z = italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math>. We optimize the neural network parameters <math alttext="\theta" class="ltx_Math" display="inline" id="Thmexample13.p1.m3"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> to maximize the coding rate reduction. We evaluate the performance with the CIFAR10 image classification dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx144" title="">KH+09</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F23">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F23.sf1"><img alt="(a) Evolution of R ϵ R_{\epsilon} italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT , R ϵ c R^{c}_{\epsilon} italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT , Δ ​ R ϵ \Delta R_{\epsilon} roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT during the training process." class="ltx_graphics" id="F23.sf1.g1" src="chapters/chapter3/figs/loss_log.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Evolution of <math alttext="R_{\epsilon}" class="ltx_Math" display="inline" id="F23.sf1.m4"><semantics><msub><mi>R</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="F23.sf1.m5"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="F23.sf1.m6"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> during the training process.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F23.sf2"><img alt="(a) Evolution of R ϵ R_{\epsilon} italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT , R ϵ c R^{c}_{\epsilon} italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT , Δ ​ R ϵ \Delta R_{\epsilon} roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT during the training process." class="ltx_graphics" id="F23.sf2.g1" src="chapters/chapter3/figs/pca_mainline.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">PCA: (<span class="ltx_text ltx_font_bold">red</span>) overall data; (<span class="ltx_text ltx_font_bold">blue</span>) individual classes.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.23</span>: </span><span class="ltx_text" style="font-size:90%;">Evolution of the rates of MCR<sup class="ltx_sup">2</sup> in the training process, the principal components of learned features.</span></figcaption>
</figure>
<figure class="ltx_figure" id="F24">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 3.24 : Cosine similarity between learned features by using the MCR 2 objective ( left ) and CE loss ( right )." class="ltx_graphics ltx_centering ltx_figure_panel" id="F24.g1" src="chapters/chapter3/figs/heatmap_mcr2.png"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 3.24 : Cosine similarity between learned features by using the MCR 2 objective ( left ) and CE loss ( right )." class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="210" id="F24.g2" src="chapters/chapter3/figs/heatmap_ce.png" width="252"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.24</span>: </span><span class="ltx_text" style="font-size:90%;">Cosine similarity between learned features by using the MCR<sup class="ltx_sup">2</sup> objective (<span class="ltx_text ltx_font_bold">left</span>) and CE loss (<span class="ltx_text ltx_font_bold">right</span>).</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample13.p2">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F23.sf1" title="Figure 3.23(a) ‣ Figure 3.23 ‣ Example 3.13 (Classification of Images on CIFAR-10). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.23(a)</span></a> illustrates how the two rates and their difference (for both training and test data) evolves over epochs of training: After an initial phase, <math alttext="R_{\epsilon}" class="ltx_Math" display="inline" id="Thmexample13.p2.m1"><semantics><msub><mi>R</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> gradually increases while <math alttext="R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="Thmexample13.p2.m2"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> decreases, indicating that features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="Thmexample13.p2.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> are expanding as a whole while each class <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="Thmexample13.p2.m4"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is being compressed.
Figure <a class="ltx_ref" href="#F23.sf2" title="Figure 3.23(b) ‣ Figure 3.23 ‣ Example 3.13 (Classification of Images on CIFAR-10). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.23(b)</span></a> shows the distribution of singular values per <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="Thmexample13.p2.m5"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. Figure <a class="ltx_ref" href="#F24" title="Figure 3.24 ‣ Example 3.13 (Classification of Images on CIFAR-10). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.24</span></a> shows the cosine similarities between the learned features sorted by class. We compare the similarities of the learned features by using the cross-entropy (<a class="ltx_ref" href="#S4.E2" title="Equation 3.4.2 ‣ Encoding class information via cross entropy. ‣ 3.4.1 Linear Discriminative Representations ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) and the MCR<sup class="ltx_sup">2</sup> objective (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>). From the plots, one can clearly see that the representations learned by using MCR<sup class="ltx_sup">2</sup> loss are much more diverse than the ones learned by using cross-entropy loss. More details of this experiment can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx41" title="">CYY+22</a>]</cite>.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample13.p2.m8"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p">However, there has been an apparent lack of justification of the network architectures used in the above experiments. It is yet unclear why the network adopted here (the ResNet-18) is suitable for representing the map <math alttext="f(\bm{x},\theta)" class="ltx_Math" display="inline" id="S4.SS3.p3.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , italic_θ )</annotation></semantics></math>, let alone for interpreting the layer operators and parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S4.SS3.p3.m2"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> learned inside. In the next chapter, <span class="ltx_text ltx_font_italic">we will show how to derive network architectures and components entirely as a “white box” from the desired objective (say the rate reduction)</span>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Regularized MCR<sup class="ltx_sup">2</sup>.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p">The above theorem characterizes properties of the global optima of the rate reduction objectives. What about other optima, such as local ones? Due to the constraints of the Frobenius norm, it is a difficult task to analyze Problem (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>) from an optimization-theoretic perspective. Therefore, we consider the Lagrangian formulation of (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>). This can be viewed as a tight relaxation or even an equivalent problem of (<a class="ltx_ref" href="#S4.E15" title="Equation 3.4.15 ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.15</span></a>) whose optimal solutions agree under specific settings of the regularization parameter; see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx283" title="">WLP+24</a>, Proposition 1]</cite>.
Specifically, the formulation we study, referred to henceforth as the <span class="ltx_text ltx_font_italic">regularized MCR<sup class="ltx_sup"><span class="ltx_text ltx_font_upright">2</span></sup> problem</span>, is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx33">
<tbody id="S4.E16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{Z}}\ R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})-\frac{\lambda}{2}\|\bm{Z}\|_{F}^{2}," class="ltx_Math" display="inline" id="S4.E16.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>𝒁</mi></munder><mo lspace="0.667em">⁡</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mi>λ</mi><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{Z}}\ R_{\epsilon}(\bm{Z})-R_{\epsilon}^{c}(\bm{Z})-\frac{\lambda}{2}\|\bm{Z}\|_{F}^{2},</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) - divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda&gt;0" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m2"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation><annotation encoding="application/x-llamapun">italic_λ &gt; 0</annotation></semantics></math> is the regularization parameter. Although the program (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) is highly nonconcave and involves matrix inverses in its gradient computation, we can still explicitly characterize its local and global optima as follows.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.8</span></span><span class="ltx_text ltx_font_bold"> </span>(<span class="ltx_text ltx_font_bold">Local and Global Optima).</span>
</h6>
<div class="ltx_para" id="Thmtheorem8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="N_{k}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m1"><semantics><msub><mi>N</mi><mi>k</mi></msub><annotation encoding="application/x-tex">N_{k}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denote the number of training samples in the <math alttext="k" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m2"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th class for each <math alttext="k\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m3"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_k ∈ { 1 , … , italic_K }</annotation></semantics></math>, <math alttext="N_{\max}\doteq\max\{N_{1},\dots,N_{K}\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m4"><semantics><mrow><msub><mi>N</mi><mi>max</mi></msub><mo>≐</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><msub><mi>N</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>N</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">N_{\max}\doteq\max\{N_{1},\dots,N_{K}\}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ≐ roman_max { italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_N start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT }</annotation></semantics></math>, <math alttext="\alpha=d/(N\epsilon^{2})" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m5"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha=d/(N\epsilon^{2})</annotation><annotation encoding="application/x-llamapun">italic_α = italic_d / ( italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, and <math alttext="\alpha_{k}=d/(N_{k}\epsilon^{2})" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m6"><semantics><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha_{k}=d/(N_{k}\epsilon^{2})</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d / ( italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> for each <math alttext="k\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m7"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_k ∈ { 1 , … , italic_K }</annotation></semantics></math>. Given a coding precision <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m8"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>, if the regularization parameter satisfies</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx34">
<tbody id="S4.E17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda\in\left(0,\frac{d(\sqrt{N/N_{\max}}-1)}{N(\sqrt{N/N_{\max}}+1)\epsilon^{2}}\right]," class="ltx_Math" display="inline" id="S4.E17.m1"><semantics><mrow><mrow><mi>λ</mi><mo>∈</mo><mrow><mo>(</mo><mn>0</mn><mo>,</mo><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msqrt><mrow><mi>N</mi><mo>/</mo><msub><mi>N</mi><mi>max</mi></msub></mrow></msqrt><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msqrt><mrow><mi>N</mi><mo>/</mo><msub><mi>N</mi><mi>max</mi></msub></mrow></msqrt><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda\in\left(0,\frac{d(\sqrt{N/N_{\max}}-1)}{N(\sqrt{N/N_{\max}}+1)\epsilon^{2}}\right],</annotation><annotation encoding="application/x-llamapun">italic_λ ∈ ( 0 , divide start_ARG italic_d ( square-root start_ARG italic_N / italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_ARG - 1 ) end_ARG start_ARG italic_N ( square-root start_ARG italic_N / italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_ARG + 1 ) italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">then the following statements hold: 
<br class="ltx_break"/>(i) (</span><span class="ltx_text ltx_font_bold">Local maximizers</span><span class="ltx_text ltx_font_italic">) <math alttext="\bm{Z}^{*}=\left[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}\right]" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m9"><semantics><mrow><msup><mi>𝐙</mi><mo>∗</mo></msup><mo>=</mo><mrow><mo>[</mo><msubsup><mi>𝐙</mi><mn>1</mn><mo>∗</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝐙</mi><mi>K</mi><mo>∗</mo></msubsup><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{*}=\left[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}\right]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ]</annotation></semantics></math> is a local maximizer of Problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) if and only if the <math alttext="k" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m10"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th block admits the following decomposition</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx35">
<tbody id="S4.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{k}^{*}=\left(\frac{\eta_{k}+\sqrt{\eta_{k}^{2}-4\lambda^{2}N/N_{k}}}{2\lambda\alpha_{k}}\right)^{1/2}\bm{U}_{k}\bm{V}_{k}^{\top}," class="ltx_Math" display="inline" id="S4.E18.m1"><semantics><mrow><mrow><msubsup><mi>𝒁</mi><mi>k</mi><mo>∗</mo></msubsup><mo>=</mo><mrow><msup><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>η</mi><mi>k</mi></msub><mo>+</mo><msqrt><mrow><msubsup><mi>η</mi><mi>k</mi><mn>2</mn></msubsup><mo>−</mo><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>λ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow><mo>/</mo><msub><mi>N</mi><mi>k</mi></msub></mrow></mrow></msqrt></mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><mi>k</mi></msub></mrow></mfrac></mstyle><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑽</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{k}^{*}=\left(\frac{\eta_{k}+\sqrt{\eta_{k}^{2}-4\lambda^{2}N/N_{k}}}{2\lambda\alpha_{k}}\right)^{1/2}\bm{U}_{k}\bm{V}_{k}^{\top},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = ( divide start_ARG italic_η start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + square-root start_ARG italic_η start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 4 italic_λ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_N / italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG start_ARG 2 italic_λ italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.4.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where (a) <math alttext="r_{k}=\mathrm{rank}(\bm{Z}_{k}^{*})" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m11"><semantics><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>rank</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝐙</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{k}=\mathrm{rank}(\bm{Z}_{k}^{*})</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_rank ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT )</annotation></semantics></math> satisfies <math alttext="r_{k}\in[0,\min\{N_{k},d\})" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m12"><semantics><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><msub><mi>N</mi><mi>k</mi></msub><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">r_{k}\in[0,\min\{N_{k},d\})</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ [ 0 , roman_min { italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_d } )</annotation></semantics></math> and <math alttext="\sum_{k=1}^{K}r_{k}\leq\min\{N,d\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m13"><semantics><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>r</mi><mi>k</mi></msub></mrow><mo>≤</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mi>N</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}r_{k}\leq\min\{N,d\}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ roman_min { italic_N , italic_d }</annotation></semantics></math>, (b) <math alttext="\bm{U}_{k}\in\mathcal{O}^{d\times r_{k}}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m14"><semantics><mrow><msub><mi>𝐔</mi><mi>k</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>r</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathcal{O}^{d\times r_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_O start_POSTSUPERSCRIPT italic_d × italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> satisfies <math alttext="\bm{U}_{k}^{\top}\bm{U}_{l}=\bm{0}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m15"><semantics><mrow><mrow><msubsup><mi>𝐔</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐔</mi><mi>l</mi></msub></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{\top}\bm{U}_{l}=\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = bold_0</annotation></semantics></math> for all <math alttext="k\neq l" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m16"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">k\neq l</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_l</annotation></semantics></math>, <math alttext="\bm{V}_{k}\in\mathcal{O}^{N_{k}\times r_{k}}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m17"><semantics><mrow><msub><mi>𝐕</mi><mi>k</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><mrow><msub><mi>N</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>r</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{V}_{k}\in\mathcal{O}^{N_{k}\times r_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_O start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, and (c) <math alttext="\eta_{k}=(\alpha_{k}-\alpha)-\lambda(N/N_{k}+1)" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m18"><semantics><mrow><msub><mi>η</mi><mi>k</mi></msub><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>−</mo><mi>α</mi></mrow><mo stretchy="false">)</mo></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>N</mi><mo>/</mo><msub><mi>N</mi><mi>k</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\eta_{k}=(\alpha_{k}-\alpha)-\lambda(N/N_{k}+1)</annotation><annotation encoding="application/x-llamapun">italic_η start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ( italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_α ) - italic_λ ( italic_N / italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + 1 )</annotation></semantics></math> for each <math alttext="k\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m19"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_k ∈ { 1 , … , italic_K }</annotation></semantics></math>.
<br class="ltx_break"/>(ii) (</span><span class="ltx_text ltx_font_bold">Global maximizers</span><span class="ltx_text ltx_font_italic">) <math alttext="\bm{Z}^{*}=\left[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}\right]" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m20"><semantics><mrow><msup><mi>𝐙</mi><mo>∗</mo></msup><mo>=</mo><mrow><mo>[</mo><msubsup><mi>𝐙</mi><mn>1</mn><mo>∗</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝐙</mi><mi>K</mi><mo>∗</mo></msubsup><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{*}=\left[\bm{Z}_{1}^{*},\dots,\bm{Z}_{K}^{*}\right]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ]</annotation></semantics></math> is a global maximizer of Problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) if and only if (a) it satisfies the above all conditions and <math alttext="\sum_{k=1}^{K}r_{k}=\min\{m,d\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m21"><semantics><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>r</mi><mi>k</mi></msub></mrow><mo>=</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mi>m</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}r_{k}=\min\{m,d\}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_min { italic_m , italic_d }</annotation></semantics></math>, and (b) for all <math alttext="k\neq l\in[K]" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m22"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>l</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\neq l\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_l ∈ [ italic_K ]</annotation></semantics></math> satisfying <math alttext="N_{k}&lt;N_{l}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m23"><semantics><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>&lt;</mo><msub><mi>N</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">N_{k}&lt;N_{l}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT &lt; italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="r_{l}&gt;0" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m24"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">r_{l}&gt;0</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT &gt; 0</annotation></semantics></math>, we have <math alttext="r_{k}=\min\{N_{k},d\}" class="ltx_Math" display="inline" id="Thmtheorem8.p1.m25"><semantics><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><msub><mi>N</mi><mi>k</mi></msub><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{k}=\min\{N_{k},d\}</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_min { italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_d }</annotation></semantics></math>.</span></p>
</div>
</div>
<figure class="ltx_figure" id="F25"><img alt="Figure 3.25 : Global optimization landscape: According to [ SQW15 , LSJ+16 ] , Theorems 3.8 and 3.9 , both global and local maxima of the (regularized) rate reduction objective correspond to a solution with mutually incoherent subspaces. All other critical points are strict saddle points." class="ltx_graphics ltx_img_landscape" height="177" id="F25.g1" src="chapters/chapter3/figs/mcr2-global-local.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3.25</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Global optimization landscape:<span class="ltx_text ltx_font_medium"> According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx253" title="">SQW15</a>, <a class="ltx_ref" href="bib.html#bibx153" title="">LSJ+16</a>]</cite>, <a class="ltx_ref" href="#Thmtheorem8" title="Theorem 3.8 (Local and Global Optima). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorems</span> <span class="ltx_text ltx_ref_tag">3.8</span></a> and <a class="ltx_ref" href="#Thmtheorem9" title="Theorem 3.9 (Benign Global Optimization Landscape). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.9</span></a>, both global and local maxima of the (regularized) rate reduction objective correspond to a solution with mutually incoherent subspaces. All other critical points are strict saddle points.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p">This theorem explicitly characterizes the local and global optima of problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>). Intuitively, this shows that the features represented by each local maximizer of Problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) are low-dimensional and discriminative. Although we have characterized the local and global optimal solutions in Theorem <a class="ltx_ref" href="#Thmtheorem8" title="Theorem 3.8 (Local and Global Optima). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.8</span></a>, it remains unknown whether these solutions can be efficiently computed using GD to solve the problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>), since GD may get stuck at other critical points such as a saddle point.
Fortunately, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx253" title="">SQW15</a>, <a class="ltx_ref" href="bib.html#bibx153" title="">LSJ+16</a>]</cite> showed that if a function is twice continuously differentiable and satisfies the <span class="ltx_text ltx_font_italic">strict saddle property</span>, i.e., each critical point is either a local minimizer or a strict saddle point<span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span>We say that a critical point is a strict saddle point of Problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) if it has a direction with strictly positive curvature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx253" title="">SQW15</a>]</cite>. This includes classical saddle points with strictly positive curvature as well as local minimizers.</span></span></span>, GD converges to its local minimizer almost surely with random initialization. We investigate the global optimization landscape of the problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) by characterizing all its critical points as follows.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3.9</span></span><span class="ltx_text ltx_font_bold"> </span>(<span class="ltx_text ltx_font_bold">Benign Global Optimization Landscape).</span>
</h6>
<div class="ltx_para" id="Thmtheorem9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Given a coding precision <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="Thmtheorem9.p1.m1"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>, if the regularization parameter satisfies (<a class="ltx_ref" href="#S4.E17" title="Equation 3.4.17 ‣ Theorem 3.8 (Local and Global Optima). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.17</span></a>),
it holds that any critical point <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="Thmtheorem9.p1.m2"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> of the problem (<a class="ltx_ref" href="#S4.E16" title="Equation 3.4.16 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.16</span></a>) is either a local maximizer or a strict saddle point.</span></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p3">
<p class="ltx_p">Together, the above two theorems
show that the learned features associated with each local maximizer of the rate
reduction objective—not just global maximizers—are structured as incoherent low-dimensional subspaces. Furthermore, the (regularized) rate reduction objective (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) has a very benign landscape with only local maxima and strict saddles as critical points, as illustrated in Figure <a class="ltx_ref" href="#F25" title="Figure 3.25 ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.25</span></a>.
According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx253" title="">SQW15</a>, <a class="ltx_ref" href="bib.html#bibx153" title="">LSJ+16</a>]</cite>, <a class="ltx_ref" href="#Thmtheorem8" title="Theorem 3.8 (Local and Global Optima). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorems</span> <span class="ltx_text ltx_ref_tag">3.8</span></a> and <a class="ltx_ref" href="#Thmtheorem9" title="Theorem 3.9 (Benign Global Optimization Landscape). ‣ Regularized MCR2. ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.9</span></a> imply that low-dimensional and discriminative representations (LDRs) can be efficiently found by applying (stochastic) gradient descent to the rate reduction objective (<a class="ltx_ref" href="#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) from random initialization. These results also indirectly explain why in Example <a class="ltx_ref" href="#F24" title="Figure 3.24 ‣ Example 3.13 (Classification of Images on CIFAR-10). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.24</span></a>, if the chosen network is expressive enough and trained well, the resulting representation typically gives an incoherent linear representation which likely corresponds to the globally optimal solution.
Interested readers are referred to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx283" title="">WLP+24</a>]</cite> for proofs.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.5 </span>Summary and Notes</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">The use of denoising and diffusion for sampling has a rich history. The first work which is clearly about a diffusion model is probably <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx247" title="">SWM+15</a>]</cite>, but before this there are many works about denoising as a computational and statistical problem. The most relevant of these is probably <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx117" title="">Hyv05</a>]</cite>, which explicitly uses the score function to denoise (as well as perform independent component analysis). The most popular follow-ups are basically co-occurring: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx106" title="">HJA20</a>, <a class="ltx_ref" href="bib.html#bibx249" title="">SE19</a>]</cite>. Since then, thousands of papers have built on diffusion models; we will revisit this topic in <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p">Many of these works use a different stochastic process than the simple linear combination (<a class="ltx_ref" href="#S2.E69" title="Equation 3.2.69 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.69</span></a>). In fact, all works listed above emphasize the need to add <span class="ltx_text ltx_font_italic">independent</span> Gaussian noise at the beginning of each step of the forward process. Theoretically-minded work actually uses Brownian motion or stochastic differential equations to formulate the forward process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx250" title="">SSK+21</a>]</cite>. However, since linear combinations of Gaussians still result in Gaussians, the <span class="ltx_text ltx_font_italic">marginal distributions</span> of such processes still take the form of (<a class="ltx_ref" href="#S2.E69" title="Equation 3.2.69 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.69</span></a>). Most of our discussion requires only that the marginal distributions are what they are, and hence our overly simplistic model is actually quite enough for almost everything. In fact, the only time where marginal distributions are not enough is when we derive an expression for <math alttext="\operatorname{\mathbb{E}}[\bm{x}_{s}\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S5.p2.m1"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝒙</mi><mi>s</mi></msub><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}_{s}\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math> in terms of <math alttext="\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]" class="ltx_Math" display="inline" id="S5.p2.m2"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\mid\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math>. Different (noising) processes give different such expressions, which can be used for sampling (and of course there are other ways to derive efficient samplers, such as the ever-popular DDPM sampler). The process in (<a class="ltx_ref" href="#S2.E69" title="Equation 3.2.69 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.69</span></a>) is a bona fide stochastic process, however, whose “natural” denoising iteration takes the form of the popular DDIM algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx248" title="">SME20</a>]</cite>. (Even this equivalence is not trivial; we cite <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx67" title="">DGG+25</a>]</cite> as a justification.)</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p">On top of the theoretical work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx156" title="">LY24</a>]</cite> covered in <a class="ltx_ref" href="Ch1.html#S3.SS1.SSSx3" title="General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1.3.1</span></a>, and the lineage of work that it builds on, which studies the <span class="ltx_text ltx_font_italic">sampling</span> efficiency of diffusion models when the data has low-dimensional structure, there is a large body of work which studies the <span class="ltx_text ltx_font_italic">training</span> efficiency of diffusion models when the data has low-dimensional structure. Specifically, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx45" title="">CHZ+23</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx285" title="">WZZ+24</a>]</cite> characterized the approximation and estimation error of denoisers when the data belongs to a mixture of low-rank Gaussians, showing that the number of training samples required to accurately learn the distribution scales with the intrinsic dimension of the data rather than the ambient distribution. There is considerable <span class="ltx_text ltx_font_italic">methodological</span> work which attempts to utilize the low-dimensional structure of the data in order to do various things with diffusion models. We highlight three here: image editing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx48" title="">CZG+24</a>]</cite>, watermarking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx159" title="">LZQ24</a>]</cite>, and unlearning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx49" title="">CZL+25</a>]</cite>, though as always this is an inexhaustive list.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.6 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<p class="ltx_p">Please show that (<a class="ltx_ref" href="#S2.E4" title="Equation 3.2.4 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.4</span></a>) is the optimal solution of Problem (<a class="ltx_ref" href="#S2.E3" title="Equation 3.2.3 ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>).</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<p class="ltx_p">Consider random vectors <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m2"><semantics><mrow><mi>𝒚</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{y}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, such that the
pair <math alttext="(\bm{x},\bm{y})\in\mathbb{R}^{D+d}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m3"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo>+</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})\in\mathbb{R}^{D+d}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_D + italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is jointly Gaussian. This means that</p>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{x}\\
\bm{y}\end{bmatrix}\sim\mathcal{N}\left(\begin{bmatrix}\bm{\mu}_{\bm{x}}\\
\bm{\mu}_{\bm{y}}\end{bmatrix},\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}\right)," class="ltx_Math" display="block" id="S6.Ex1.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒙</mi></mtd></mtr><mtr><mtd><mi>𝒚</mi></mtd></mtr></mtable><mo>]</mo></mrow><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝝁</mi><mi>𝒙</mi></msub></mtd></mtr><mtr><mtd><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>,</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>𝒙</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{x}\\
\bm{y}\end{bmatrix}\sim\mathcal{N}\left(\begin{bmatrix}\bm{\mu}_{\bm{x}}\\
\bm{\mu}_{\bm{y}}\end{bmatrix},\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}\right),</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_y end_CELL end_ROW end_ARG ] ∼ caligraphic_N ( [ start_ARG start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] , [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the mean and covariance parameters are given by</p>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{\mu}_{\bm{x}}=\mathbb{E}[\bm{x}],\quad\bm{\mu}_{\bm{y}}=\mathbb{E}[\bm{y}],\quad\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}=\mathbb{E}\left[\begin{bmatrix}\bm{x}-\mathbb{E}[\bm{x}]\\
\bm{y}-\mathbb{E}[\bm{y}]\end{bmatrix}\begin{bmatrix}\bm{x}-\mathbb{E}[\bm{x}]\\
\bm{y}-\mathbb{E}[\bm{y}]\end{bmatrix}^{\top}\right]" class="ltx_Math" display="block" id="S6.Ex2.m1"><semantics><mrow><mrow><msub><mi>𝝁</mi><mi>𝒙</mi></msub><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>𝝁</mi><mi>𝒚</mi></msub><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒚</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>𝒙</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒚</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝒚</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>⊤</mo></msup></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\mu}_{\bm{x}}=\mathbb{E}[\bm{x}],\quad\bm{\mu}_{\bm{y}}=\mathbb{E}[\bm{y}],\quad\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}=\mathbb{E}\left[\begin{bmatrix}\bm{x}-\mathbb{E}[\bm{x}]\\
\bm{y}-\mathbb{E}[\bm{y}]\end{bmatrix}\begin{bmatrix}\bm{x}-\mathbb{E}[\bm{x}]\\
\bm{y}-\mathbb{E}[\bm{y}]\end{bmatrix}^{\top}\right]</annotation><annotation encoding="application/x-llamapun">bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT = blackboard_E [ bold_italic_x ] , bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT = blackboard_E [ bold_italic_y ] , [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] = blackboard_E [ [ start_ARG start_ROW start_CELL bold_italic_x - blackboard_E [ bold_italic_x ] end_CELL end_ROW start_ROW start_CELL bold_italic_y - blackboard_E [ bold_italic_y ] end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_x - blackboard_E [ bold_italic_x ] end_CELL end_ROW start_ROW start_CELL bold_italic_y - blackboard_E [ bold_italic_y ] end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Assume that <math alttext="\bm{\Sigma}_{\bm{y}}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m4"><semantics><msub><mi>𝚺</mi><mi>𝒚</mi></msub><annotation encoding="application/x-tex">\bm{\Sigma}_{\bm{y}}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> is positive definite (hence invertible); then
positive semidefiniteness of the covariance matrix is equivalent to the Schur
complement condition <math alttext="\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\succeq\mathbf{0}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m5"><semantics><mrow><mrow><msub><mi>𝚺</mi><mi>𝒙</mi></msub><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow><mo>⪰</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\succeq\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⪰ bold_0</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Thmexercise2.p2">
<p class="ltx_p">In this exercise, we will prove that the conditional distribution <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="Thmexercise2.p2.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> is Gaussian: namely,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{x}\mid\bm{y}}\sim\mathcal{N}\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}}),\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right)." class="ltx_Math" display="block" id="S6.E1.m1"><semantics><mrow><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>𝝁</mi><mi>𝒙</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><msub><mi>𝚺</mi><mi>𝒙</mi></msub><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}\sim\mathcal{N}\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}}),\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ) , bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">A direct path to prove this result manipulates the defining ratio of
densities <math alttext="p_{\bm{x},\bm{y}}/p_{\bm{y}}" class="ltx_Math" display="inline" id="Thmexercise2.p2.m2"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></msub><mo>/</mo><msub><mi>p</mi><mi>𝒚</mi></msub></mrow><annotation encoding="application/x-tex">p_{\bm{x},\bm{y}}/p_{\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT / italic_p start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>. We sketch an algebraically-concise
argument of this form below.</p>
</div>
<div class="ltx_para" id="Thmexercise2.p3">
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p">Verify the following matrix identity for the covariance:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}=\begin{bmatrix}\bm{I}_{D}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\\
\mathbf{0}&amp;\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}\begin{bmatrix}\bm{I}_{D}&amp;\mathbf{0}\\
\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{I}_{d}\end{bmatrix}." class="ltx_Math" display="block" id="S6.E2.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>𝒙</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝑰</mi><mi>D</mi></msub></mtd><mtd><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><msub><mi>𝑰</mi><mi>d</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><msub><mi>𝚺</mi><mi>𝒙</mi></msub><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝑰</mi><mi>D</mi></msub></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mtd><mtd><msub><mi>𝑰</mi><mi>d</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}=\begin{bmatrix}\bm{I}_{D}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\\
\mathbf{0}&amp;\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}\begin{bmatrix}\bm{I}_{D}&amp;\mathbf{0}\\
\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{I}_{d}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL bold_italic_I start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_I start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">One arrives at this identity by performing two rounds of (block) Gaussian
elimination on the covariance matrix.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p">Based on the previous identity, show that</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}^{-1}=\begin{bmatrix}\bm{I}_{D}&amp;\mathbf{0}\\
-\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\left(\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right)^{-1}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}^{-1}\end{bmatrix}\begin{bmatrix}\bm{I}_{D}&amp;-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\\
\mathbf{0}&amp;\bm{I}_{d}\end{bmatrix}" class="ltx_Math" display="block" id="S6.E3.m1"><semantics><mrow><msup><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>𝒙</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝑰</mi><mi>D</mi></msub></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mo>−</mo><mrow><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow></mtd><mtd><msub><mi>𝑰</mi><mi>d</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mrow><mo>(</mo><mrow><msub><mi>𝚺</mi><mi>𝒙</mi></msub><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝑰</mi><mi>D</mi></msub></mtd><mtd><mrow><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mrow></mrow></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><msub><mi>𝑰</mi><mi>d</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}^{-1}=\begin{bmatrix}\bm{I}_{D}&amp;\mathbf{0}\\
-\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{I}_{d}\end{bmatrix}\begin{bmatrix}\left(\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right)^{-1}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}^{-1}\end{bmatrix}\begin{bmatrix}\bm{I}_{D}&amp;-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\\
\mathbf{0}&amp;\bm{I}_{d}\end{bmatrix}</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_italic_I start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL - bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL ( bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_I start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT end_CELL start_CELL - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">whenever the relevant inverses are defined.<span class="ltx_note ltx_role_footnote" id="footnote30"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span>In cases where the
Schur complement term is not invertible, the same result holds with its
inverse replaced by the Moore-Penrose pseudoinverse. In particular, the
conditional distribution (<a class="ltx_ref" href="#S6.E1" title="Equation 3.6.1 ‣ Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.6.1</span></a>) becomes
a degenerate Gaussian distribution.</span></span></span>
Conclude that</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx36">
<tbody id="S6.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\begin{bmatrix}\bm{x}-\bm{\mu}_{\bm{x}}\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}^{\top}\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}^{-1}\begin{bmatrix}\bm{x}-\bm{\mu}_{\bm{x}}\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}" class="ltx_Math" display="inline" id="S6.E4.m1"><semantics><mrow><msup><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒙</mi></msub></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd><msub><mi>𝚺</mi><mi>𝒙</mi></msub></mtd><mtd><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub></mtd></mtr><mtr><mtd><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mtd><mtd><msub><mi>𝚺</mi><mi>𝒚</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒙</mi></msub></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\begin{bmatrix}\bm{x}-\bm{\mu}_{\bm{x}}\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}^{\top}\begin{bmatrix}\bm{\Sigma}_{\bm{x}}&amp;\bm{\Sigma}_{\bm{x}\bm{y}}\\
\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}&amp;\bm{\Sigma}_{\bm{y}}\end{bmatrix}^{-1}\begin{bmatrix}\bm{x}-\bm{\mu}_{\bm{x}}\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x - bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL bold_italic_x - bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.4)</span></td>
</tr></tbody>
<tbody id="S6.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad=\begin{bmatrix}\bm{x}-\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}})\right)\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}^{\top}\begin{bmatrix}\left(\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right)^{-1}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}^{-1}\end{bmatrix}\begin{bmatrix}\bm{x}-\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}})\right)\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}." class="ltx_Math" display="inline" id="S6.E5.m1"><semantics><mrow><mrow><mi></mi><mo lspace="2.278em">=</mo><mrow><msup><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><mrow><mo>(</mo><mrow><msub><mi>𝝁</mi><mi>𝒙</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd><msup><mrow><mo>(</mo><mrow><msub><mi>𝚺</mi><mi>𝒙</mi></msub><mo>−</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝒙</mi><mo>−</mo><mrow><mo>(</mo><mrow><msub><mi>𝝁</mi><mi>𝒙</mi></msub><mo>+</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mi>𝒚</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝒚</mi><mo>−</mo><msub><mi>𝝁</mi><mi>𝒚</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\qquad=\begin{bmatrix}\bm{x}-\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}})\right)\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}^{\top}\begin{bmatrix}\left(\bm{\Sigma}_{\bm{x}}-\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}\bm{\Sigma}_{\bm{x}\bm{y}}^{\top}\right)^{-1}&amp;\mathbf{0}\\
\mathbf{0}&amp;\bm{\Sigma}_{\bm{y}}^{-1}\end{bmatrix}\begin{bmatrix}\bm{x}-\left(\bm{\mu}_{\bm{x}}+\bm{\Sigma}_{\bm{x}\bm{y}}\bm{\Sigma}_{\bm{y}}^{-1}(\bm{y}-\bm{\mu}_{\bm{y}})\right)\\
\bm{y}-\bm{\mu}_{\bm{y}}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">= [ start_ARG start_ROW start_CELL bold_italic_x - ( bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL ( bold_Σ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_x - ( bold_italic_μ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT + bold_Σ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL bold_italic_y - bold_italic_μ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">(<span class="ltx_text ltx_font_italic">Hint: To economize algebraic manipulations, note that the first
and last matrices on the RHS of
<a class="ltx_ref" href="#S6.E2" title="In Item 1 ‣ Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.6.2</span></a> are transposes of one
another.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p">By dividing <math alttext="p_{\bm{x},\bm{y}}/p_{\bm{y}}" class="ltx_Math" display="inline" id="S6.I1.i3.p1.m1"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></msub><mo>/</mo><msub><mi>p</mi><mi>𝒚</mi></msub></mrow><annotation encoding="application/x-tex">p_{\bm{x},\bm{y}}/p_{\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT / italic_p start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, prove
<a class="ltx_ref" href="#S6.E1" title="In Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.6.1</span></a>. (<span class="ltx_text ltx_font_italic">Hint: Using the previous
identities, only minimal algebra should be necessary. For the normalizing
constant, use <a class="ltx_ref" href="#S6.E3" title="In Item 2 ‣ Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.6.3</span></a> to
factor the determinant similarly.</span>)</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise3.p1">
<p class="ltx_p">Show the Sherman-Morrison-Woodbury identity, i.e., for matrices <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, <math alttext="\bm{C}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m2"><semantics><mi>𝑪</mi><annotation encoding="application/x-tex">\bm{C}</annotation><annotation encoding="application/x-llamapun">bold_italic_C</annotation></semantics></math>, <math alttext="\bm{U}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>, <math alttext="\bm{V}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m4"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math> such that <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, <math alttext="\bm{C}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m6"><semantics><mi>𝑪</mi><annotation encoding="application/x-tex">\bm{C}</annotation><annotation encoding="application/x-llamapun">bold_italic_C</annotation></semantics></math>, and <math alttext="\bm{A}+\bm{U}\bm{C}\bm{V}" class="ltx_Math" display="inline" id="Thmexercise3.p1.m7"><semantics><mrow><mi>𝑨</mi><mo>+</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑪</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{A}+\bm{U}\bm{C}\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_A + bold_italic_U bold_italic_C bold_italic_V</annotation></semantics></math> are invertible,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{A}+\bm{U}\bm{C}\bm{V})^{-1}=\bm{A}^{-1}-\bm{A}^{-1}\bm{U}(\bm{C}^{-1}+\bm{V}\bm{A}^{-1}\bm{U})^{-1}\bm{V}\bm{A}^{-1}" class="ltx_Math" display="block" id="S6.E6.m1"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo>+</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑪</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mrow><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑪</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>+</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">(\bm{A}+\bm{U}\bm{C}\bm{V})^{-1}=\bm{A}^{-1}-\bm{A}^{-1}\bm{U}(\bm{C}^{-1}+\bm{V}\bm{A}^{-1}\bm{U})^{-1}\bm{V}\bm{A}^{-1}</annotation><annotation encoding="application/x-llamapun">( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ( bold_italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.6)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise4.p1">
<p class="ltx_p">Rederive the following, assuming <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexercise4.p1.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> follows the generalized noise model (<a class="ltx_ref" href="#S2.E69" title="Equation 3.2.69 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.69</span></a>).</p>
<ul class="ltx_itemize" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p">Tweedie’s formula: (<a class="ltx_ref" href="#S2.E70" title="Equation 3.2.70 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.70</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p">The DDIM iteration: (<a class="ltx_ref" href="#S2.E71" title="Equation 3.2.71 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.71</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I2.i3.p1">
<p class="ltx_p">The Bayes optimal denoiser for a Gaussian mixture model: (<a class="ltx_ref" href="#S2.E72" title="Equation 3.2.72 ‣ Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.72</span></a>).</p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise5.p1">
<ol class="ltx_enumerate" id="S6.I3">
<li class="ltx_item" id="S6.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I3.i1.p1">
<p class="ltx_p">Implement the formulae derived in <a class="ltx_ref" href="#Thmexercise4" title="Exercise 3.4. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, building a sampler for Gaussian mixtures.</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I3.i2.p1">
<p class="ltx_p">Reproduce <a class="ltx_ref" href="#F4" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.4</span></a> and <a class="ltx_ref" href="#F7" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3.7</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I3.i3.p1">
<p class="ltx_p">We now introduce a separate process called <span class="ltx_text ltx_font_italic">Flow Matching (FM)</span>, as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\alpha_{t}=1-t,\qquad\sigma_{t}=t." class="ltx_Math" display="block" id="S6.E7.m1"><semantics><mrow><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow></mrow><mo rspace="2.167em">,</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\alpha_{t}=1-t,\qquad\sigma_{t}=t.</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Implement this process using the same framework, and test it for sampling in high dimensions. Which process seems to give better or more stable results?</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 3.6</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise6.p1">
<p class="ltx_p">Please show the following properties of the <math alttext="\log\det(\cdot)" class="ltx_Math" display="inline" id="Thmexercise6.p1.m1"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\log\det(\cdot)</annotation><annotation encoding="application/x-llamapun">roman_log roman_det ( ⋅ )</annotation></semantics></math> function.</p>
<ol class="ltx_enumerate" id="S6.I4">
<li class="ltx_item" id="S6.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I4.i1.p1">
<p class="ltx_p">Show that</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx37">
<tbody id="S6.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f(\bm{X})=\log\det\left(\bm{X}\right)" class="ltx_Math" display="inline" id="S6.Ex3.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mi>𝑿</mi><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle f(\bm{X})=\log\det\left(\bm{X}\right)</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">is a concave function. (<span class="ltx_text ltx_font_bold">Hint:</span> The function <math alttext="f(\bm{x})" class="ltx_Math" display="inline" id="S6.I4.i1.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x )</annotation></semantics></math> is convex if and only if the function <math alttext="f(\bm{x}+t\bm{h})" class="ltx_Math" display="inline" id="S6.I4.i1.p1.m2"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒉</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x}+t\bm{h})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x + italic_t bold_italic_h )</annotation></semantics></math> for all <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{h}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.m4"><semantics><mi>𝒉</mi><annotation encoding="application/x-tex">\bm{h}</annotation><annotation encoding="application/x-llamapun">bold_italic_h</annotation></semantics></math>.)</p>
</div>
</li>
<li class="ltx_item" id="S6.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I4.i2.p1">
<p class="ltx_p">Show that:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx38">
<tbody id="S6.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\log\det(\bm{I}+\bm{X}^{\top}\bm{X})=\log\det(\bm{I}+\bm{X}\bm{X}^{\top})" class="ltx_Math" display="inline" id="S6.Ex4.m1"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log\det(\bm{I}+\bm{X}^{\top}\bm{X})=\log\det(\bm{I}+\bm{X}\bm{X}^{\top})</annotation><annotation encoding="application/x-llamapun">roman_log roman_det ( bold_italic_I + bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) = roman_log roman_det ( bold_italic_I + bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S6.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I4.i3.p1">
<p class="ltx_p">Let <math alttext="\bm{A}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="S6.I4.i3.p1.m1"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> be a positive definite matrix. Please show that:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx39">
<tbody id="S6.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\log\det\left(\bm{A}\right)=\sum_{i=1}^{n}\log(\lambda_{i})," class="ltx_Math" display="inline" id="S6.E8.m1"><semantics><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mi>𝑨</mi><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\log\det\left(\bm{A}\right)=\sum_{i=1}^{n}\log(\lambda_{i}),</annotation><annotation encoding="application/x-llamapun">roman_log roman_det ( bold_italic_A ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3.6.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda_{1},\lambda_{2},\dots,\lambda_{n}" class="ltx_Math" display="inline" id="S6.I4.i3.p1.m2"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>λ</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{1},\lambda_{2},\dots,\lambda_{n}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_λ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> are the eigenvalues of <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S6.I4.i3.p1.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 09:48:41 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
