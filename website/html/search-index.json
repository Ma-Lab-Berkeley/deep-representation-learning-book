{"generated": "2025-08-18T16:49:04.970561+00:00", "count": 1346, "entries": [{"page": "Appendix A Optimization Methods", "href": "A1.html#top", "title": "Appendix A Optimization Methods", "snippet": ""}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1", "title": "A.1 Steepest Descent", "snippet": "A.1 Steepest Descent Optimization is concerned with the question of how to find where a function, say L ​ ( θ ) L(\\theta) italic_L ( italic_θ ) , reaches its minimum value. Mathematically, this is stated as a problem: arg ​ min θ ∈ Θ ⁡ ℒ ​ ( θ ) , \\operatorname*{arg\\ min}_{\\theta"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2", "title": "A.2 Computing Gradients via Automatic Differentiation", "snippet": "A.2 Computing Gradients via Automatic Differentiation Above, we discussed several optimization algorithms for deep networks which assumed access to a first-order oracle , i.e., a device which would allow us to compute ℒ ​ ( θ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_θ ) and ∇"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3", "title": "A.3 Game Theory and Minimax Optimization", "snippet": "A.3 Game Theory and Minimax Optimization In certain cases, such as in Chapter 5 , a learning problem cannot be reduced to a single optimization problem but rather represents multiple potentially opposing components of the system try to each minimize their own objective. Examples "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S4", "title": "A.4 Exercises", "snippet": "A.4 Exercises Exercise A.1 . We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have convexity, let alone strong convexity. Fortunately, "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1", "title": "A.1.1 Vanilla Gradient Descent for Smooth Problems", "snippet": "A.1.1 Vanilla Gradient Descent for Smooth Problems The simplest and most widely used method for optimization is gradient descent (GD). It was first introduced by Cauchy in 1847. The idea is very simple: starting from an initial state, we iteratively take small steps such that eac"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2", "title": "A.1.2 Preconditioned Gradient Descent for Badly-Conditioned Problems", "snippet": "A.1.2 Preconditioned Gradient Descent for Badly-Conditioned Problems Figure A.3 : The negative gradient − ∇ ℒ λ -\\nabla\\mathcal{L}_{\\lambda} - ∇ caligraphic_L start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT and pre-conditioned (Newton’s method step) vector field − [ ∇ 2 ℒ λ ] − 1 "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS3", "title": "A.1.3 Proximal Gradient Descent for Non-Smooth Problems", "snippet": "A.1.3 Proximal Gradient Descent for Non-Smooth Problems Even in very toy problems, however, such as LASSO or dictionary learning, the problem is not strongly convex but rather just convex, and the objective is no longer just smooth but rather the sum of a smooth function and a no"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4", "title": "A.1.4 Stochastic Gradient Descent for Large-Scale Problems", "snippet": "A.1.4 Stochastic Gradient Descent for Large-Scale Problems In deep learning, the objective function ℒ \\mathcal{L} caligraphic_L usually cannot be computed exactly, and instead at each optimization step it is estimated using finite samples (say, using a mini-batch). A common way t"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS5", "title": "A.1.5 Putting Everything Together: Adam", "snippet": "A.1.5 Putting Everything Together: Adam The gradient descent scheme proposes an iteration of the form θ k + 1 = θ k + h ​ 𝒗 k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRI"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1", "title": "A.2.1 Differentials", "snippet": "A.2.1 Differentials A full accounting of this subsection is given in the excellent guide [ BEJ25 ] . To motivate differentials, let us first consider the simple example of a differentiable function ℒ : ℝ → ℝ \\mathcal{L}\\colon\\mathbb{R}\\to\\mathbb{R} caligraphic_L : blackboard_R → "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS2", "title": "A.2.2 Automatic Differentiation", "snippet": "A.2.2 Automatic Differentiation The main idea of AD is to compute the chain rule efficiently. The basic problem we need to cope with is the following. In the optimization section of the appendix, we considered that the parameter space Θ \\Theta roman_Θ was an abstract Euclidean sp"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3", "title": "A.2.3 Back Propagation", "snippet": "A.2.3 Back Propagation In this section, we will discuss algorithmic backpropagation using a simple yet completely practical example. Suppose that we fix an input-label pair ( 𝑿 , 𝒚 ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , and fix a network architecture f θ = f θ L ∘ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1", "title": "A.3.1 Learning Stackelberg Equilibria", "snippet": "A.3.1 Learning Stackelberg Equilibria How can we learn Stackelberg equilibria via GDA? In general this is clearly impossible, since learning Stackelberg equilibria via GDA is obviously at least as hard as computing a global minimizer of a loss function (say by setting the shared "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS2", "title": "A.3.2 Practical Considerations when Learning Stackelberg Equilibria", "snippet": "A.3.2 Practical Considerations when Learning Stackelberg Equilibria In practice, we do not know how to initialize parameters close to a (differential) Stackelberg equilibrium. Due to symmetries within the objective, including those induced by overparameterization of the neural ne"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmlemma1", "title": "Lemma A.1 (Majorization-Minimization) .", "snippet": "Lemma A.1 (Majorization-Minimization) . Suppose that u : Θ → ℝ u\\colon\\Theta\\to\\mathbb{R} italic_u : roman_Θ → blackboard_R is a global upper bound on ℒ \\mathcal{L} caligraphic_L , namely ℒ ​ ( θ ) ≤ u ​ ( θ ) \\mathcal{L}(\\theta)\\leq u(\\theta) caligraphic_L ( italic_θ ) ≤ italic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample1", "title": "Example A.1 .", "snippet": "Example A.1 . Let Γ ⊆ Θ \\Gamma\\subseteq\\Theta roman_Γ ⊆ roman_Θ be a set, and let χ Γ \\chi_{\\Gamma} italic_χ start_POSTSUBSCRIPT roman_Γ end_POSTSUBSCRIPT be the characteristic function on Γ \\Gamma roman_Γ , i.e., χ Γ ​ ( θ ) ≐ { 0 , if ​ θ ∈ Γ + ∞ , if ​ θ ∉ Γ . \\chi_{\\Gamma}(\\t"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample2", "title": "Example A.2 .", "snippet": "Example A.2 . The ℓ 1 \\ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm has a proximal operator which performs soft thresholding: S h ​ ( θ ) ≐ prox h , λ ∥ ⋅ ∥ 1 ⁡ ( θ ) = arg ​ min θ 1 ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + λ ​ ‖ θ ‖ 1 ] S_{h}(\\theta)\\doteq\\operatornam"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample3", "title": "Example A.3 .", "snippet": "Example A.3 . In Chapter 4 we use a proximal operator corresponding to the ℓ 1 \\ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm plus the characteristic function for the positive orthant ℝ + n ≐ { 𝒙 ∈ ℝ n : x i ≥ 0 ​ ∀ i } \\mathbb{R}_{+}^{n}\\doteq\\{\\bm{x}\\in\\mathb"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmtheorem1", "title": "Theorem A.1 (Differential Chain Rule) .", "snippet": "Theorem A.1 (Differential Chain Rule) . Suppose ℒ = f ∘ g \\mathcal{L}=f\\circ g caligraphic_L = italic_f ∘ italic_g where f f italic_f and g g italic_g are differentiable. Then d ​ ℒ = f ′ ​ ( g ​ ( θ ) ) ​ g ′ ​ ( θ ) ​ [ d ​ θ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\p"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample4", "title": "Example A.4 .", "snippet": "Example A.4 . Consider the function f ​ ( 𝑿 ) = 𝑾 ​ 𝑿 + 𝒃 ​ 𝟏 ⊤ f(\\bm{X})=\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top} italic_f ( bold_italic_X ) = bold_italic_W bold_italic_X + bold_italic_b bold_1 start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . Then d ​ f = f ​ ( 𝑿 + d ​ 𝑿 ) − f ​ ( 𝑿 ) = [ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample5", "title": "Example A.5 .", "snippet": "Example A.5 . Consider the function f = g ​ h f=gh italic_f = italic_g italic_h where g , h g,h italic_g , italic_h are differentiable functions whose outputs can multiply together. Then f = p ∘ v f=p\\circ v italic_f = italic_p ∘ italic_v where v = ( g , h ) v=(g,h) italic_v = ( "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample6", "title": "Example A.6 .", "snippet": "Example A.6 . Consider the function f ​ ( 𝑨 ) = 𝑨 ⊤ ​ 𝑨 ​ 𝑩 ​ 𝑨 f(\\bm{A})=\\bm{A}^{\\top}\\bm{A}\\bm{B}\\bm{A} italic_f ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_A bold_italic_B bold_italic_A where 𝑨 \\bm{A} bold_italic_A is a matrix and "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample7", "title": "Example A.7 .", "snippet": "Example A.7 . Consider the function f : ℝ m × n × k → ℝ m × n f\\colon\\mathbb{R}^{m\\times n\\times k}\\to\\mathbb{R}^{m\\times n} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n × italic_k end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n e"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample8", "title": "Example A.8 .", "snippet": "Example A.8 . Consider the “linear” (affine) layer f ℓ f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT f ℓ ​ ( 𝒁 , 𝑾 ℓ , 𝒃 ℓ ) ≐ 𝑾 ℓ ​ 𝒁 + 𝒃 ℓ ​ 𝟏 ⊤ = [ 𝑾 ℓ 𝒃 ℓ ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise1", "title": "Exercise A.1 .", "snippet": "Exercise A.1 . We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have convexity, let alone strong convexity. Fortunately, in some cases,"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise2", "title": "Exercise A.2 .", "snippet": "Exercise A.2 . Compute the differential and adjoint derivative of the softmax function, defined as follows. softmax ⁡ ( [ x 1 ⋮ x n ] ) = 1 ∑ i = 1 n e x i ​ [ x 1 ⋮ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\s"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise3", "title": "Exercise A.3 .", "snippet": "Exercise A.3 . Carry through the backpropagation computation for a L L italic_L -layer MLP, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise4", "title": "Exercise A.4 .", "snippet": "Exercise A.4 . Carry through the backpropagation computation for a L L italic_L -layer transformer, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise5", "title": "Exercise A.5 .", "snippet": "Exercise A.5 . Carry through the backpropagation computation for an autoencoder with L L italic_L encoder layers and L L italic_L decoder layers (without necessarily specifying an architecture)."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E1", "title": "arg ​ min θ ∈ Θ ⁡ ℒ ​ ( θ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT ca", "snippet": "arg ​ min θ ∈ Θ ⁡ ℒ ​ ( θ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT caligraphic_L ( italic_θ ) , (A.1.1)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E2", "title": "ℒ ​ ( θ + h ​ 𝒗 ) ≤ ℒ ​ ( θ ) . \\mathcal{L}(\\theta+h\\bm{v})\\leq\\mathcal{L}(\\theta). caligraphic_L ( italic_θ + italic_h bold_italic_v ) ≤ caligraphic_L ( italic_θ ) . (A.1.2)", "snippet": "ℒ ​ ( θ + h ​ 𝒗 ) ≤ ℒ ​ ( θ ) . \\mathcal{L}(\\theta+h\\bm{v})\\leq\\mathcal{L}(\\theta). caligraphic_L ( italic_θ + italic_h bold_italic_v ) ≤ caligraphic_L ( italic_θ ) . (A.1.2)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E3", "title": "ℒ ​ ( θ + h ​ 𝒗 ) = ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + o ​ ( h ) , \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+o(h), caligraphic_L ( italic_θ + i", "snippet": "ℒ ​ ( θ + h ​ 𝒗 ) = ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + o ​ ( h ) , \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+o(h), caligraphic_L ( italic_θ + italic_h bold_italic_v ) = caligraphic_L ( italic_θ ) + italic_h ⟨ ∇ caligraphic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E4", "title": "arg ​ min 𝒗 ∈ ℝ d ‖ 𝒗 ‖ 2 = 1 ⁡ [ ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ ] = arg ​ min 𝒗 ∈ ℝ d ‖ 𝒗 ‖ 2 = 1 ⁡ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ = − ∇ ℒ ​ ( θ ) ‖ ∇ ℒ ​ ( θ ) ‖ 2 , \\operatorname*{arg\\ min}_{\\begin{subar", "snippet": "arg ​ min 𝒗 ∈ ℝ d ‖ 𝒗 ‖ 2 = 1 ⁡ [ ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ ] = arg ​ min 𝒗 ∈ ℝ d ‖ 𝒗 ‖ 2 = 1 ⁡ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ = − ∇ ℒ ​ ( θ ) ‖ ∇ ℒ ​ ( θ ) ‖ 2 , \\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{d}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}[\\mathcal{L}(\\th"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E5", "title": "θ k + 1 = θ k − h ​ ∇ ℒ ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POST", "snippet": "θ k + 1 = θ k − h ​ ∇ ℒ ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ∇ caligraphic_L ( italic_θ start_POSTSUBSCRIPT italic_k end"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E6", "title": "h = arg ​ min h ≥ 0 ⁡ ℒ ​ ( θ k − h ​ ∇ ℒ ​ ( θ k ) ) . h=\\operatorname*{arg\\ min}_{h\\geq 0}\\mathcal{L}(\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k})). italic_h = start_OPERATOR roman_arg roman_min end_OP", "snippet": "h = arg ​ min h ≥ 0 ⁡ ℒ ​ ( θ k − h ​ ∇ ℒ ​ ( θ k ) ) . h=\\operatorname*{arg\\ min}_{h\\geq 0}\\mathcal{L}(\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k})). italic_h = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_h ≥ 0 end_POSTSUBSCRIPT caligraphic_L ( italic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E7", "title": "ℒ ​ ( θ ) ≥ l θ 0 , α ​ ( θ ) ≐ ℒ ​ ( θ 0 ) + ⟨ ∇ ℒ ​ ( θ 0 ) , θ − θ 0 ⟩ + α 2 ​ ‖ θ − θ 0 ‖ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathc", "snippet": "ℒ ​ ( θ ) ≥ l θ 0 , α ​ ( θ ) ≐ ℒ ​ ( θ 0 ) + ⟨ ∇ ℒ ​ ( θ 0 ) , θ − θ 0 ⟩ + α 2 ​ ‖ θ − θ 0 ‖ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\alpha}{2}\\|\\theta-\\theta_{0}\\|"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E8", "title": "‖ ∇ ℒ ​ ( θ ) − ∇ ℒ ​ ( θ 0 ) ‖ 2 ≤ β ​ ‖ θ − θ 0 ‖ 2 . \\|\\nabla\\mathcal{L}(\\theta)-\\nabla\\mathcal{L}(\\theta_{0})\\|_{2}\\leq\\beta\\|\\theta-\\theta_{0}\\|_{2}. ∥ ∇ caligraphic_L ( italic_θ ) - ∇ caligraphi", "snippet": "‖ ∇ ℒ ​ ( θ ) − ∇ ℒ ​ ( θ 0 ) ‖ 2 ≤ β ​ ‖ θ − θ 0 ‖ 2 . \\|\\nabla\\mathcal{L}(\\theta)-\\nabla\\mathcal{L}(\\theta_{0})\\|_{2}\\leq\\beta\\|\\theta-\\theta_{0}\\|_{2}. ∥ ∇ caligraphic_L ( italic_θ ) - ∇ caligraphic_L ( italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E9", "title": "ℒ ​ ( θ ) ≤ u θ 0 , β ​ ( θ ) ≐ ℒ ​ ( θ 0 ) + ⟨ ∇ ℒ ​ ( θ 0 ) , θ − θ 0 ⟩ + β 2 ​ ‖ θ − θ 0 ‖ 2 2 . \\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\math", "snippet": "ℒ ​ ( θ ) ≤ u θ 0 , β ​ ( θ ) ≐ ℒ ​ ( θ 0 ) + ⟨ ∇ ℒ ​ ( θ 0 ) , θ − θ 0 ⟩ + β 2 ​ ‖ θ − θ 0 ‖ 2 2 . \\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\beta}{2}\\|\\theta-\\theta_{0}\\|"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E10", "title": "θ 1 ∈ arg ​ min θ ∈ Θ ⁡ u ​ ( θ ) ⟹ ℒ ​ ( θ 1 ) ≤ u ​ ( θ 1 ) ≤ u ​ ( θ 0 ) = ℒ ​ ( θ 0 ) . \\theta_{1}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta)\\implies\\mathcal{L}(\\theta_{1})\\leq u(\\theta", "snippet": "θ 1 ∈ arg ​ min θ ∈ Θ ⁡ u ​ ( θ ) ⟹ ℒ ​ ( θ 1 ) ≤ u ​ ( θ 1 ) ≤ u ​ ( θ 0 ) = ℒ ​ ( θ 0 ) . \\theta_{1}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta)\\implies\\mathcal{L}(\\theta_{1})\\leq u(\\theta_{1})\\leq u(\\theta_{0})=\\mathcal{L}(\\theta_{0}). italic_θ start_POSTSUBSCRIPT 1 "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E11", "title": "if θ minimizes u θ 0 , β then ℒ ​ ( θ ) ≤ u θ 0 , β ​ ( θ ) ≤ u θ 0 , β ​ ( θ 0 ) = ℒ ​ ( θ 0 ) . \\text{if $\\theta$ minimizes $u_{\\theta_{0},\\beta}$ then}\\quad\\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\be", "snippet": "if θ minimizes u θ 0 , β then ℒ ​ ( θ ) ≤ u θ 0 , β ​ ( θ ) ≤ u θ 0 , β ​ ( θ 0 ) = ℒ ​ ( θ 0 ) . \\text{if $\\theta$ minimizes $u_{\\theta_{0},\\beta}$ then}\\quad\\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta_{0})=\\mathcal{L}(\\theta_{0}). if ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E12", "title": "θ k + 1 = θ k − 1 β ​ ∇ ℒ ​ ( θ k ) ⟹ ℒ ​ ( θ k + 1 ) ≤ ℒ ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-\\frac{1}{\\beta}\\nabla\\mathcal{L}(\\theta_{k})\\implies\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k}). ", "snippet": "θ k + 1 = θ k − 1 β ​ ∇ ℒ ​ ( θ k ) ⟹ ℒ ​ ( θ k + 1 ) ≤ ℒ ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-\\frac{1}{\\beta}\\nabla\\mathcal{L}(\\theta_{k})\\implies\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E21", "title": "‖ θ ⋆ − θ k + 1 ‖ 2 2 ≤ ( 1 − α / β ) k + 1 ​ ‖ θ ⋆ − θ 0 ‖ 2 2 , \\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2}\\leq(1-\\alpha/\\beta)^{k+1}\\|\\theta^{\\star}-\\theta_{0}\\|_{2}^{2}, ∥ italic_θ start_POSTSUPERSCRI", "snippet": "‖ θ ⋆ − θ k + 1 ‖ 2 2 ≤ ( 1 − α / β ) k + 1 ​ ‖ θ ⋆ − θ 0 ‖ 2 2 , \\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2}\\leq(1-\\alpha/\\beta)^{k+1}\\|\\theta^{\\star}-\\theta_{0}\\|_{2}^{2}, ∥ italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E22", "title": "ℒ λ ​ ( θ ) = ℒ λ ​ ( [ θ 1 θ 2 ] ) ≐ 1 2 ​ { ( 1 + λ ) ​ θ 1 2 + θ 2 2 } = 1 2 ​ θ ⊤ ​ [ 1 + λ 0 0 1 ] ​ θ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\\\ \\thet", "snippet": "ℒ λ ​ ( θ ) = ℒ λ ​ ( [ θ 1 θ 2 ] ) ≐ 1 2 ​ { ( 1 + λ ) ​ θ 1 2 + θ 2 2 } = 1 2 ​ θ ⊤ ​ [ 1 + λ 0 0 1 ] ​ θ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\\\ \\theta_{2}\\end{bmatrix}\\right)\\doteq\\frac{1}{2}\\left\\{(1+\\lambda)\\theta_{1}^{2}+\\thet"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E23", "title": "ℒ ​ ( θ + h ​ 𝒗 ) = ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h 2 ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ + o ​ ( h 2 ) . \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\b", "snippet": "ℒ ​ ( θ + h ​ 𝒗 ) = ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h 2 ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ + o ​ ( h 2 ) . \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h^{2}\\langle[\\nabla^{2}\\mathcal{L}(\\theta)]\\bm{v},\\bm{v}\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E25", "title": "arg ​ min 𝒗 ∈ ℝ n ⁡ [ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ ] = − 1 h ​ [ ∇ 2 ℒ ​ ( θ ) ] − 1 ​ [ ∇ ℒ ​ ( θ ) ] . \\operatorname*{arg\\ min}_{\\bm{v}\\in\\mathbb{R}^{n}}\\left[\\langl", "snippet": "arg ​ min 𝒗 ∈ ℝ n ⁡ [ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ ] = − 1 h ​ [ ∇ 2 ℒ ​ ( θ ) ] − 1 ​ [ ∇ ℒ ​ ( θ ) ] . \\operatorname*{arg\\ min}_{\\bm{v}\\in\\mathbb{R}^{n}}\\left[\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h\\langle[\\nabla^{2}\\mathcal{"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E26", "title": "θ k + 1 = θ k − [ ∇ 2 ℒ ​ ( θ k ) ] − 1 ​ [ ∇ ℒ ​ ( θ k ) ] , \\theta_{k+1}=\\theta_{k}-[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_θ start_POSTSUBSCRIPT italic_k + 1", "snippet": "θ k + 1 = θ k − [ ∇ 2 ℒ ​ ( θ k ) ] − 1 ​ [ ∇ ℒ ​ ( θ k ) ] , \\theta_{k+1}=\\theta_{k}-[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - ["}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E27", "title": "θ k + 1 = θ k − h ​ [ ∇ 2 ℒ ​ ( θ k ) ] − 1 ​ [ ∇ ℒ ​ ( θ k ) ] , \\theta_{k+1}=\\theta_{k}-h[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_θ start_POSTSUBSCRIPT italic_", "snippet": "θ k + 1 = θ k − h ​ [ ∇ 2 ℒ ​ ( θ k ) ] − 1 ​ [ ∇ ℒ ​ ( θ k ) ] , \\theta_{k+1}=\\theta_{k}-h[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E28", "title": "∇ L ​ ( θ + δ θ ) − ∇ ℒ ​ ( θ ) ⏟ ≐ δ 𝒈 = [ ∇ 2 ℒ ​ ( θ ) ] ​ δ θ + o ​ ( ‖ δ θ ‖ 2 ) . \\underbrace{\\nabla L(\\theta+\\delta_{\\theta})-\\nabla\\mathcal{L}(\\theta)}_{\\doteq\\delta_{\\bm{g}}}=[\\nabla^{2}\\math", "snippet": "∇ L ​ ( θ + δ θ ) − ∇ ℒ ​ ( θ ) ⏟ ≐ δ 𝒈 = [ ∇ 2 ℒ ​ ( θ ) ] ​ δ θ + o ​ ( ‖ δ θ ‖ 2 ) . \\underbrace{\\nabla L(\\theta+\\delta_{\\theta})-\\nabla\\mathcal{L}(\\theta)}_{\\doteq\\delta_{\\bm{g}}}=[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}+o(\\|\\delta_{\\theta}\\|_{2}). under⏟ start_ARG ∇ it"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E29", "title": "δ 𝒈 ≈ [ ∇ 2 ℒ ​ ( θ ) ] ​ δ θ ⟹ δ θ ≈ [ ∇ 2 ℒ ​ ( θ ) ] − 1 ​ δ 𝒈 \\delta_{\\bm{g}}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}\\implies\\delta_{\\theta}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]^{-1}", "snippet": "δ 𝒈 ≈ [ ∇ 2 ℒ ​ ( θ ) ] ​ δ θ ⟹ δ θ ≈ [ ∇ 2 ℒ ​ ( θ ) ] − 1 ​ δ 𝒈 \\delta_{\\bm{g}}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}\\implies\\delta_{\\theta}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]^{-1}\\delta_{\\bm{g}} italic_δ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ≈ ["}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E30", "title": "δ θ ≈ P ​ δ 𝒈 , \\delta_{\\theta}\\approx P\\delta_{\\bm{g}}, italic_δ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ≈ italic_P italic_δ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT , (A.1.30)", "snippet": "δ θ ≈ P ​ δ 𝒈 , \\delta_{\\theta}\\approx P\\delta_{\\bm{g}}, italic_δ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ≈ italic_P italic_δ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT , (A.1.30)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E33", "title": "ℒ ​ ( θ ) ≐ 𝒮 ​ ( θ ) + ℛ ​ ( θ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_θ ) ≐ caligraphic_S ( italic_θ ) + caligraphic_R ( italic_θ ) (A.1.33)", "snippet": "ℒ ​ ( θ ) ≐ 𝒮 ​ ( θ ) + ℛ ​ ( θ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_θ ) ≐ caligraphic_S ( italic_θ ) + caligraphic_R ( italic_θ ) (A.1.33)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E34", "title": "ℒ ​ ( θ 1 ) = 𝒮 ​ ( θ 1 ) + ℛ ​ ( θ 1 ) ≤ u θ 0 , β ​ ( θ 1 ) ≐ 𝒮 ​ ( θ 0 ) + ⟨ ∇ 𝒮 ​ ( θ 0 ) , θ 1 − θ 0 ⟩ + β 2 ​ ‖ θ 1 − θ 0 ‖ 2 2 + ℛ ​ ( θ 1 ) . \\mathcal{L}(\\theta_{1})=\\mathcal{S}(\\theta_{1})+\\m", "snippet": "ℒ ​ ( θ 1 ) = 𝒮 ​ ( θ 1 ) + ℛ ​ ( θ 1 ) ≤ u θ 0 , β ​ ( θ 1 ) ≐ 𝒮 ​ ( θ 0 ) + ⟨ ∇ 𝒮 ​ ( θ 0 ) , θ 1 − θ 0 ⟩ + β 2 ​ ‖ θ 1 − θ 0 ‖ 2 2 + ℛ ​ ( θ 1 ) . \\mathcal{L}(\\theta_{1})=\\mathcal{S}(\\theta_{1})+\\mathcal{R}(\\theta_{1})\\leq u_{\\theta_{0},\\beta}(\\theta_{1})\\doteq\\mathcal{S}(\\the"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E35", "title": "arg ​ min θ 1 ⁡ u θ 0 , β ​ ( θ 1 ) = arg ​ min θ 1 ⁡ [ β 2 ​ ‖ θ 1 − ( θ 0 − 1 β ​ ∇ 𝒮 ​ ( θ 0 ) ) ‖ 2 2 + ℛ ​ ( θ 1 ) ] . \\operatorname*{arg\\ min}_{\\theta_{1}}u_{\\theta_{0},\\beta}(\\theta_{1})=\\opera", "snippet": "arg ​ min θ 1 ⁡ u θ 0 , β ​ ( θ 1 ) = arg ​ min θ 1 ⁡ [ β 2 ​ ‖ θ 1 − ( θ 0 − 1 β ​ ∇ 𝒮 ​ ( θ 0 ) ) ‖ 2 2 + ℛ ​ ( θ 1 ) ] . \\operatorname*{arg\\ min}_{\\theta_{1}}u_{\\theta_{0},\\beta}(\\theta_{1})=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{\\beta}{2}\\left\\|\\theta_{1}-\\left(\\the"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E36", "title": "prox h , ℛ ⁡ ( θ ) ≐ arg ​ min θ 1 ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + ℛ ​ ( θ ) ] . \\operatorname{prox}_{h,\\mathcal{R}}(\\theta)\\doteq\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\t", "snippet": "prox h , ℛ ⁡ ( θ ) ≐ arg ​ min θ 1 ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + ℛ ​ ( θ ) ] . \\operatorname{prox}_{h,\\mathcal{R}}(\\theta)\\doteq\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\mathcal{R}(\\theta)\\right]. roman_prox start_POSTSUBSCRIPT italic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E37", "title": "θ k + 1 = prox h , ℛ ⁡ ( θ k − h ​ ∇ 𝒮 ​ ( θ k ) ) . \\theta_{k+1}=\\operatorname{prox}_{h,\\mathcal{R}}(\\theta_{k}-h\\nabla\\mathcal{S}(\\theta_{k})). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBS", "snippet": "θ k + 1 = prox h , ℛ ⁡ ( θ k − h ​ ∇ 𝒮 ​ ( θ k ) ) . \\theta_{k+1}=\\operatorname{prox}_{h,\\mathcal{R}}(\\theta_{k}-h\\nabla\\mathcal{S}(\\theta_{k})). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_h , caligraphic_R end_POSTSUBSCRIP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E38", "title": "χ Γ ​ ( θ ) ≐ { 0 , if ​ θ ∈ Γ + ∞ , if ​ θ ∉ Γ . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_χ start_POSTSUBSCRIP", "snippet": "χ Γ ​ ( θ ) ≐ { 0 , if ​ θ ∈ Γ + ∞ , if ​ θ ∉ Γ . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_χ start_POSTSUBSCRIPT roman_Γ end_POSTSUBSCRIPT ( italic_θ ) ≐ { start_ROW start_CELL 0 , end_CELL s"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E39", "title": "prox h , χ Γ ⁡ ( θ ) = arg ​ min θ 1 ∈ Γ ⁡ 1 2 ​ ‖ θ 1 − θ ‖ 2 2 = arg ​ min θ 1 ∈ Γ ⁡ ‖ θ 1 − θ ‖ 2 . \\operatorname{prox}_{h,\\chi_{\\Gamma}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamma}\\frac", "snippet": "prox h , χ Γ ⁡ ( θ ) = arg ​ min θ 1 ∈ Γ ⁡ 1 2 ​ ‖ θ 1 − θ ‖ 2 2 = arg ​ min θ 1 ∈ Γ ⁡ ‖ θ 1 − θ ‖ 2 . \\operatorname{prox}_{h,\\chi_{\\Gamma}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamma}\\frac{1}{2}\\|\\theta_{1}-\\theta\\|_{2}^{2}=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamm"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E40", "title": "S h ​ ( θ ) ≐ prox h , λ ∥ ⋅ ∥ 1 ⁡ ( θ ) = arg ​ min θ 1 ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + λ ​ ‖ θ ‖ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_", "snippet": "S h ​ ( θ ) ≐ prox h , λ ∥ ⋅ ∥ 1 ⁡ ( θ ) = arg ​ min θ 1 ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + λ ​ ‖ θ ‖ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{1"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E41", "title": "S h ​ ( θ ) i = { θ i − h ​ λ , if ​ θ i ≥ h ​ λ 0 , if ​ θ i ∈ [ − h ​ λ , h ​ λ ] θ i + h ​ λ , if ​ θ i ≤ − h ​ λ = { max ⁡ { | θ i | − h ​ λ , 0 } ​ sign ⁡ ( θ i ) , if ​ | θ i | ≥ h ​ λ 0 , if ​ ", "snippet": "S h ​ ( θ ) i = { θ i − h ​ λ , if ​ θ i ≥ h ​ λ 0 , if ​ θ i ∈ [ − h ​ λ , h ​ λ ] θ i + h ​ λ , if ​ θ i ≤ − h ​ λ = { max ⁡ { | θ i | − h ​ λ , 0 } ​ sign ⁡ ( θ i ) , if ​ | θ i | ≥ h ​ λ 0 , if ​ | θ i | < h ​ λ . S_{h}(\\theta)_{i}=\\begin{cases}\\theta_{i}-h\\lambda,&\\text{if}\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E42", "title": "T h ​ ( θ ) ≐ prox h , λ ∥ ⋅ ∥ 1 + χ ℝ + n ⁡ ( θ ) = arg ​ min θ 1 ∈ ℝ + n ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + λ ​ ‖ θ ‖ 1 ] , T_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}+\\chi_{\\mathbb{", "snippet": "T h ​ ( θ ) ≐ prox h , λ ∥ ⋅ ∥ 1 + χ ℝ + n ⁡ ( θ ) = arg ​ min θ 1 ∈ ℝ + n ⁡ [ 1 2 ​ h ​ ‖ θ 1 − θ ‖ 2 2 + λ ​ ‖ θ ‖ 1 ] , T_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}+\\chi_{\\mathbb{R}_{+}^{n}}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\mathbb{R}_{+}^{n}}\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E43", "title": "T h ​ ( θ ) i ≐ max ⁡ { θ i − h ​ λ , 0 } . T_{h}(\\theta)_{i}\\doteq\\max\\{\\theta_{i}-h\\lambda,0\\}. italic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_θ ) start_POSTSUBSCRIPT italic_i end_", "snippet": "T h ​ ( θ ) i ≐ max ⁡ { θ i − h ​ λ , 0 } . T_{h}(\\theta)_{i}\\doteq\\max\\{\\theta_{i}-h\\lambda,0\\}. italic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_θ ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≐ roman_max { italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E44", "title": "θ k + 1 = θ k − h ​ ∇ ℒ k ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}_{k}(\\theta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k en", "snippet": "θ k + 1 = θ k − h ​ ∇ ℒ k ​ ( θ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}_{k}(\\theta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ∇ caligraphic_L start_POSTSUBSCRIPT italic_k end_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E45", "title": "ℒ ω ​ ( θ ) ≐ 1 2 ​ ‖ θ − ξ ω ‖ 2 2 . \\mathcal{L}_{\\omega}(\\theta)\\doteq\\frac{1}{2}\\|\\theta-\\xi_{\\omega}\\|_{2}^{2}. caligraphic_L start_POSTSUBSCRIPT italic_ω end_POSTSUBSCRIPT ( italic_θ ) ≐ divide s", "snippet": "ℒ ω ​ ( θ ) ≐ 1 2 ​ ‖ θ − ξ ω ‖ 2 2 . \\mathcal{L}_{\\omega}(\\theta)\\doteq\\frac{1}{2}\\|\\theta-\\xi_{\\omega}\\|_{2}^{2}. caligraphic_L start_POSTSUBSCRIPT italic_ω end_POSTSUBSCRIPT ( italic_θ ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_θ - italic_ξ start_POSTSUBSCRIPT"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E48", "title": "θ k + 1 = θ k + h ​ 𝒗 k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_ita", "snippet": "θ k + 1 = θ k + h ​ 𝒗 k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , (A.1.48)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E49", "title": "𝒗 k = − ∇ ℒ ​ ( θ k ) ‖ ∇ ℒ ​ ( θ k ) ‖ 2 ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ 2 = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ . \\bm{v}_{k}=-\\frac{\\nabla\\mathcal{L}(\\theta_{k})}{\\|\\nabla\\mathcal{L}(\\theta_{k})\\|_{2}}\\in\\operatorn", "snippet": "𝒗 k = − ∇ ℒ ​ ( θ k ) ‖ ∇ ℒ ​ ( θ k ) ‖ 2 ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ 2 = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ . \\bm{v}_{k}=-\\frac{\\nabla\\mathcal{L}(\\theta_{k})}{\\|\\nabla\\mathcal{L}(\\theta_{k})\\|_{2}}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{2}=1\\en"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E50", "title": "𝒗 k ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ . \\bm{v}_{k}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\the", "snippet": "𝒗 k ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ . \\bm{v}_{k}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\bm{v}\\rangle. bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E51", "title": "𝒗 k = − sign ⁡ ( ∇ ℒ ​ ( θ k ) ) ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ ∞ = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ , \\bm{v}_{k}=-\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k}))\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c", "snippet": "𝒗 k = − sign ⁡ ( ∇ ℒ ​ ( θ k ) ) ∈ arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ ∞ = 1 ⁡ ⟨ ∇ ℒ ​ ( θ k ) , 𝒗 ⟩ , \\bm{v}_{k}=-\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k}))\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{\\infty}=1\\end{subarray}}\\langle\\nabla\\mat"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E52", "title": "θ k + 1 = θ k − h ​ sign ⁡ ( ∇ ℒ ​ ( θ k ) ) . \\theta_{k+1}=\\theta_{k}-h\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k})). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start", "snippet": "θ k + 1 = θ k − h ​ sign ⁡ ( ∇ ℒ ​ ( θ k ) ) . \\theta_{k+1}=\\theta_{k}-h\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k})). italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h roman_sign ( ∇ caligraphic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E53", "title": "sign ⁡ ( x ) = x | x | = x x 2 . \\operatorname{sign}(x)=\\frac{x}{\\lvert x\\rvert}=\\frac{x}{\\sqrt{x^{2}}}. roman_sign ( italic_x ) = divide start_ARG italic_x end_ARG start_ARG | italic_x | end_ARG = di", "snippet": "sign ⁡ ( x ) = x | x | = x x 2 . \\operatorname{sign}(x)=\\frac{x}{\\lvert x\\rvert}=\\frac{x}{\\sqrt{x^{2}}}. roman_sign ( italic_x ) = divide start_ARG italic_x end_ARG start_ARG | italic_x | end_ARG = divide start_ARG italic_x end_ARG start_ARG square-root start_ARG italic_x start_P"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E54", "title": "sign ⁡ ( 𝒙 ) = 𝒙 ⊘ [ 𝒙 ⊙ 2 ] ⊙ ( 1 / 2 ) . \\operatorname{sign}(\\bm{x})=\\bm{x}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\te", "snippet": "sign ⁡ ( 𝒙 ) = 𝒙 ⊘ [ 𝒙 ⊙ 2 ] ⊙ ( 1 / 2 ) . \\operatorname{sign}(\\bm{x})=\\bm{x}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptstyle\\oslash$}}{\\scalebox{0.8}{$\\script"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E55", "title": "θ k + 1 = θ k − h ​ ( [ ∇ ℒ ​ ( θ k ) ] ⊘ [ ∇ ℒ ​ ( θ k ) ⊙ 2 ] ⊙ 1 2 ) . \\theta_{k+1}=\\theta_{k}-h([\\nabla\\mathcal{L}(\\theta_{k})]\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scal", "snippet": "θ k + 1 = θ k − h ​ ( [ ∇ ℒ ​ ( θ k ) ] ⊘ [ ∇ ℒ ​ ( θ k ) ⊙ 2 ] ⊙ 1 2 ) . \\theta_{k+1}=\\theta_{k}-h([\\nabla\\mathcal{L}(\\theta_{k})]\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebo"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E59", "title": "θ k + 1 = θ k − η k ⊙ 𝒈 k where η k = h ​ 𝒔 k ⊙ ( − 1 2 ) \\theta_{k+1}=\\theta_{k}-\\eta_{k}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebo", "snippet": "θ k + 1 = θ k − η k ⊙ 𝒈 k where η k = h ​ 𝒔 k ⊙ ( − 1 2 ) \\theta_{k+1}=\\theta_{k}-\\eta_{k}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E1", "title": "ℒ ​ ( θ ) − ℒ ​ ( θ 0 ) = ℒ ′ ​ ( θ 0 ) ⋅ ( θ − θ 0 ) + o ​ ( | θ − θ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta", "snippet": "ℒ ​ ( θ ) − ℒ ​ ( θ 0 ) = ℒ ′ ​ ( θ 0 ) ⋅ ( θ − θ 0 ) + o ​ ( | θ − θ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta_{0}\\rvert). caligraphic_L ( italic_θ ) - caligraphic_L ( italic_θ start_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E2", "title": "δ ​ ℒ = ℒ ′ ​ ( θ 0 ) ⋅ δ ​ θ + o ​ ( | δ ​ θ | ) . \\delta\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot\\delta\\theta+o(\\lvert\\delta\\theta\\rvert). italic_δ caligraphic_L = caligraphic_L start_POSTSU", "snippet": "δ ​ ℒ = ℒ ′ ​ ( θ 0 ) ⋅ δ ​ θ + o ​ ( | δ ​ θ | ) . \\delta\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot\\delta\\theta+o(\\lvert\\delta\\theta\\rvert). italic_δ caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRI"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E3", "title": "d ​ ℒ = ℒ ′ ​ ( θ ) ⋅ d ​ θ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) ⋅ ", "snippet": "d ​ ℒ = ℒ ′ ​ ( θ ) ⋅ d ​ θ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) ⋅ roman_d italic_θ , (A.2.3)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E4", "title": "d ​ ℒ = ℒ ′ ​ ( θ ) ⋅ d ​ θ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) ⋅ rom", "snippet": "d ​ ℒ = ℒ ′ ​ ( θ ) ⋅ d ​ θ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) ⋅ roman_d italic_θ (A.2.4)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E5", "title": "d ​ ℒ = ℒ ′ ​ ( θ ) ​ [ d ​ θ ] . \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]. roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) [", "snippet": "d ​ ℒ = ℒ ′ ​ ( θ ) ​ [ d ​ θ ] . \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]. roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) [ roman_d italic_θ ] . (A.2.5)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E6", "title": "d ​ ℒ = f ′ ​ ( g ​ ( θ ) ) ​ g ′ ​ ( θ ) ​ [ d ​ θ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ′ end_PO", "snippet": "d ​ ℒ = f ′ ​ ( g ​ ( θ ) ) ​ g ′ ​ ( θ ) ​ [ d ​ θ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_g ( italic_θ ) ) italic_g start_POSTSUPERSCRIPT ′ end_POS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E7", "title": "ℒ ′ ​ ( θ ) = f ′ ​ ( g ​ ( θ ) ) ​ g ′ ​ ( θ ) \\mathcal{L}^{\\prime}(\\theta)=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) = italic_f s", "snippet": "ℒ ′ ​ ( θ ) = f ′ ​ ( g ​ ( θ ) ) ​ g ′ ​ ( θ ) \\mathcal{L}^{\\prime}(\\theta)=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) = italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_g ( italic_θ ) ) italic_g st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E8", "title": "d ​ f = f ​ ( 𝑿 + d ​ 𝑿 ) − f ​ ( 𝑿 ) = [ 𝑾 ​ ( 𝑿 + d ​ 𝑿 ) + 𝒃 ​ 𝟏 ⊤ ] − [ 𝑾 ​ 𝑿 + 𝒃 ​ 𝟏 ⊤ ] = 𝑾 ​ d ​ 𝑿 . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{", "snippet": "d ​ f = f ​ ( 𝑿 + d ​ 𝑿 ) − f ​ ( 𝑿 ) = [ 𝑾 ​ ( 𝑿 + d ​ 𝑿 ) + 𝒃 ​ 𝟏 ⊤ ] − [ 𝑾 ​ 𝑿 + 𝒃 ​ 𝟏 ⊤ ] = 𝑾 ​ d ​ 𝑿 . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{1}^{\\top}]-[\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top}]=\\bm{W}\\mathrm{d}\\bm{X}. roman_d it"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E9", "title": "f ′ ​ ( 𝑿 ) ​ [ d ​ 𝑿 ] = 𝑾 ​ d ​ 𝑿 ⟹ f ′ ​ ( 𝑿 ) = 𝑾 . f^{\\prime}(\\bm{X})[\\mathrm{d}\\bm{X}]=\\bm{W}\\mathrm{d}\\bm{X}\\implies f^{\\prime}(\\bm{X})=\\bm{W}. italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCR", "snippet": "f ′ ​ ( 𝑿 ) ​ [ d ​ 𝑿 ] = 𝑾 ​ d ​ 𝑿 ⟹ f ′ ​ ( 𝑿 ) = 𝑾 . f^{\\prime}(\\bm{X})[\\mathrm{d}\\bm{X}]=\\bm{W}\\mathrm{d}\\bm{X}\\implies f^{\\prime}(\\bm{X})=\\bm{W}. italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_X ) [ roman_d bold_italic_X ] = bold_italic_W roman_d bold_ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E12", "title": "d ​ f = p ′ ​ ( v ​ ( x ) ) ​ v ′ ​ ( x ) ​ [ d ​ x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_v ( ita", "snippet": "d ​ f = p ′ ​ ( v ​ ( x ) ) ​ v ′ ​ ( x ) ​ [ d ​ x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_v ( italic_x ) ) italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x ) [ ro"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E13", "title": "d ​ v = v ′ ​ ( x ) ​ [ d ​ x ] = v ​ ( x + d ​ x ) − v ​ ( x ) = [ g ​ ( x + d ​ x ) − g ​ ( x ) h ​ ( x + d ​ x ) − h ​ ( x ) ] = [ g ′ ​ ( x ) ​ [ d ​ x ] h ′ ​ ( x ) ​ [ d ​ x ] ] . \\mathrm{d}v=v^", "snippet": "d ​ v = v ′ ​ ( x ) ​ [ d ​ x ] = v ​ ( x + d ​ x ) − v ​ ( x ) = [ g ​ ( x + d ​ x ) − g ​ ( x ) h ​ ( x + d ​ x ) − h ​ ( x ) ] = [ g ′ ​ ( x ) ​ [ d ​ x ] h ′ ​ ( x ) ​ [ d ​ x ] ] . \\mathrm{d}v=v^{\\prime}(x)[\\mathrm{d}x]=v(x+\\mathrm{d}x)-v(x)=\\begin{bmatrix}g(x+\\mathrm{d}x)-g"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E16", "title": "p ′ ​ ( a , b ) ​ [ d ​ a , d ​ b ] = ( d ​ a ) ​ b + ( d ​ b ) ​ a . p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=(\\mathrm{d}a)b+(\\mathrm{d}b)a. italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( ita", "snippet": "p ′ ​ ( a , b ) ​ [ d ​ a , d ​ b ] = ( d ​ a ) ​ b + ( d ​ b ) ​ a . p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=(\\mathrm{d}a)b+(\\mathrm{d}b)a. italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_a , italic_b ) [ roman_d italic_a , roman_d italic_b ] = ( roman_d italic_a "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E19", "title": "f ′ ​ ( x ) ​ [ d ​ x ] = ( g ′ ​ ( x ) ​ [ d ​ x ] ) ​ h ​ ( x ) + g ​ ( x ) ​ ( h ′ ​ ( x ) ​ [ d ​ x ] ) . f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)[\\mathrm{d}x])h(x)+g(x)(h^{\\prime}(x)[\\mathrm{d}x", "snippet": "f ′ ​ ( x ) ​ [ d ​ x ] = ( g ′ ​ ( x ) ​ [ d ​ x ] ) ​ h ​ ( x ) + g ​ ( x ) ​ ( h ′ ​ ( x ) ​ [ d ​ x ] ) . f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)[\\mathrm{d}x])h(x)+g(x)(h^{\\prime}(x)[\\mathrm{d}x]). italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E20", "title": "f ′ ​ ( x ) ​ [ d ​ x ] = ( g ′ ​ ( x ) ​ h ​ ( x ) + g ​ ( x ) ​ h ′ ​ ( x ) ) ​ [ d ​ x ] ⟹ f ′ ​ ( x ) = g ′ ​ ( x ) ​ h ​ ( x ) + g ​ ( x ) ​ h ′ ​ ( x ) f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)h", "snippet": "f ′ ​ ( x ) ​ [ d ​ x ] = ( g ′ ​ ( x ) ​ h ​ ( x ) + g ​ ( x ) ​ h ′ ​ ( x ) ) ​ [ d ​ x ] ⟹ f ′ ​ ( x ) = g ′ ​ ( x ) ​ h ​ ( x ) + g ​ ( x ) ​ h ′ ​ ( x ) f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)h(x)+g(x)h^{\\prime}(x))[\\mathrm{d}x]\\implies f^{\\prime}(x)=g^{\\prime}(x)h(x)+g(x)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E23", "title": "f ​ ( 𝑨 ) i ​ j = ∑ t = 1 k A i ​ j ​ t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_t = 1 ", "snippet": "f ​ ( 𝑨 ) i ​ j = ∑ t = 1 k A i ​ j ​ t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_A st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E24", "title": "d ​ f i ​ j = [ f ​ ( 𝑨 + d ​ 𝑨 ) − f ​ ( 𝑨 ) ] i ​ j = ∑ t = 1 k d ​ 𝑨 i ​ j ​ t = 𝟏 k ⊤ ​ ( d ​ 𝑨 ) i ​ j . \\mathrm{d}f_{ij}=[f(\\bm{A}+\\mathrm{d}\\bm{A})-f(\\bm{A})]_{ij}=\\sum_{t=1}^{k}\\mathrm{d}\\bm{A", "snippet": "d ​ f i ​ j = [ f ​ ( 𝑨 + d ​ 𝑨 ) − f ​ ( 𝑨 ) ] i ​ j = ∑ t = 1 k d ​ 𝑨 i ​ j ​ t = 𝟏 k ⊤ ​ ( d ​ 𝑨 ) i ​ j . \\mathrm{d}f_{ij}=[f(\\bm{A}+\\mathrm{d}\\bm{A})-f(\\bm{A})]_{ij}=\\sum_{t=1}^{k}\\mathrm{d}\\bm{A}_{ijt}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}. roman_d italic_f start_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E25", "title": "( f ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] ) i ​ j = 𝟏 k ⊤ ​ ( d ​ 𝑨 ) i ​ j , (f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}])_{ij}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}, ( italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRI", "snippet": "( f ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] ) i ​ j = 𝟏 k ⊤ ​ ( d ​ 𝑨 ) i ​ j , (f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}])_{ij}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}, ( italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] ) start_POSTSUBSCRIPT italic_i it"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E26", "title": "d ​ ℒ = ℒ ′ ​ ( θ ) ​ [ d ​ θ ] = ⟨ ∇ ℒ ​ ( θ ) , d ​ θ ⟩ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]=\\langle\\nabla\\mathcal{L}(\\theta),\\mathrm{d}\\theta\\rangle, roman_d calig", "snippet": "d ​ ℒ = ℒ ′ ​ ( θ ) ​ [ d ​ θ ] = ⟨ ∇ ℒ ​ ( θ ) , d ​ θ ⟩ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]=\\langle\\nabla\\mathcal{L}(\\theta),\\mathrm{d}\\theta\\rangle, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E27", "title": "ℒ ′ ​ ( θ ) = a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) . \\mathcal{L}^{\\prime}(\\theta)=a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta). caligraphic_L start_POSTSU", "snippet": "ℒ ′ ​ ( θ ) = a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) . \\mathcal{L}^{\\prime}(\\theta)=a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta). caligraphic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_θ ) = italic_a start_POSTSUPERSCRIPT ′ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E28", "title": "c ′ ​ ( θ ) ⟹ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) c^{\\prime}(\\theta)\\implies b^{\\prime}(c(\\theta))c^{\\prime}(\\theta)\\implies a^{\\prime}(", "snippet": "c ′ ​ ( θ ) ⟹ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) c^{\\prime}(\\theta)\\implies b^{\\prime}(c(\\theta))c^{\\prime}(\\theta)\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta) italic_c start_POSTSUPERSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E29", "title": "a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) , a^{\\prime}(b(c(\\theta)))\\implies a^{\\prime}(b(c(\\the", "snippet": "a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ⟹ a ′ ​ ( b ​ ( c ​ ( θ ) ) ) ​ b ′ ​ ( c ​ ( θ ) ) ​ c ′ ​ ( θ ) , a^{\\prime}(b(c(\\theta)))\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E30", "title": "f ′ ​ ( 𝒙 ) = a ′ ​ ( b ​ ( c ​ ( 𝒙 ) ) ) ​ b ′ ​ ( c ​ ( 𝒙 ) ) ​ c ′ ​ ( 𝒙 ) f^{\\prime}(\\bm{x})=a^{\\prime}(b(c(\\bm{x})))b^{\\prime}(c(\\bm{x}))c^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ′ end_PO", "snippet": "f ′ ​ ( 𝒙 ) = a ′ ​ ( b ​ ( c ​ ( 𝒙 ) ) ) ​ b ′ ​ ( c ​ ( 𝒙 ) ) ​ c ′ ​ ( 𝒙 ) f^{\\prime}(\\bm{x})=a^{\\prime}(b(c(\\bm{x})))b^{\\prime}(c(\\bm{x}))c^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_x ) = italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPER"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E40", "title": "ℒ ​ ( θ ) ≐ 𝖫 ​ ( 𝒚 , 𝒚 ^ θ ​ ( 𝑿 ) ) , \\mathcal{L}(\\theta)\\doteq\\mathsf{L}(\\bm{y},\\hat{\\bm{y}}_{\\theta}(\\bm{X})), caligraphic_L ( italic_θ ) ≐ sansserif_L ( bold_italic_y , over^ start_ARG bold_itali", "snippet": "ℒ ​ ( θ ) ≐ 𝖫 ​ ( 𝒚 , 𝒚 ^ θ ​ ( 𝑿 ) ) , \\mathcal{L}(\\theta)\\doteq\\mathsf{L}(\\bm{y},\\hat{\\bm{y}}_{\\theta}(\\bm{X})), caligraphic_L ( italic_θ ) ≐ sansserif_L ( bold_italic_y , over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ) ,"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E82", "title": "f ℓ ​ ( 𝒁 , 𝑾 ℓ , 𝒃 ℓ ) ≐ 𝑾 ℓ ​ 𝒁 + 𝒃 ℓ ​ 𝟏 ⊤ = [ 𝑾 ℓ 𝒃 ℓ ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell", "snippet": "f ℓ ​ ( 𝒁 , 𝑾 ℓ , 𝒃 ℓ ) ≐ 𝑾 ℓ ​ 𝒁 + 𝒃 ℓ ​ 𝟏 ⊤ = [ 𝑾 ℓ 𝒃 ℓ ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell}\\end{bmatrix}. italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bol"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E85", "title": "d ​ f ℓ d ​ ( 𝑾 ℓ , 𝒃 ℓ ) ​ [ d ​ 𝑾 ℓ , d ​ 𝒃 ℓ ] = ( d ​ 𝑾 ℓ ) ​ 𝒁 + ( d ​ 𝒃 ℓ ) ​ 𝟏 ⊤ , \\frac{\\mathrm{d}f^{\\ell}}{\\mathrm{d}(\\bm{W}^{\\ell},\\bm{b}^{\\ell})}[\\mathrm{d}\\bm{W}^{\\ell},\\mathrm{d}\\bm{b}^{\\", "snippet": "d ​ f ℓ d ​ ( 𝑾 ℓ , 𝒃 ℓ ) ​ [ d ​ 𝑾 ℓ , d ​ 𝒃 ℓ ] = ( d ​ 𝑾 ℓ ) ​ 𝒁 + ( d ​ 𝒃 ℓ ) ​ 𝟏 ⊤ , \\frac{\\mathrm{d}f^{\\ell}}{\\mathrm{d}(\\bm{W}^{\\ell},\\bm{b}^{\\ell})}[\\mathrm{d}\\bm{W}^{\\ell},\\mathrm{d}\\bm{b}^{\\ell}]=(\\mathrm{d}\\bm{W}^{\\ell})\\bm{Z}+(\\mathrm{d}\\bm{b}^{\\ell})\\bm{1}^{\\top}, di"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E86", "title": "T ​ [ 𝑨 , 𝒖 ] = 𝑨 ​ 𝒁 + 𝒖 ​ 𝟏 ⊤ . T[\\bm{A},\\bm{u}]=\\bm{A}\\bm{Z}+\\bm{u}\\bm{1}^{\\top}. italic_T [ bold_italic_A , bold_italic_u ] = bold_italic_A bold_italic_Z + bold_italic_u bold_1 start_POSTSUPERSCRI", "snippet": "T ​ [ 𝑨 , 𝒖 ] = 𝑨 ​ 𝒁 + 𝒖 ​ 𝟏 ⊤ . T[\\bm{A},\\bm{u}]=\\bm{A}\\bm{Z}+\\bm{u}\\bm{1}^{\\top}. italic_T [ bold_italic_A , bold_italic_u ] = bold_italic_A bold_italic_Z + bold_italic_u bold_1 start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . (A.2.86)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E1", "title": "ℒ ​ ( θ , η ) = − u ​ ( θ ) + v ​ ( η ) . \\mathcal{L}(\\theta,\\eta)=-u(\\theta)+v(\\eta). caligraphic_L ( italic_θ , italic_η ) = - italic_u ( italic_θ ) + italic_v ( italic_η ) . (A.3.1)", "snippet": "ℒ ​ ( θ , η ) = − u ​ ( θ ) + v ​ ( η ) . \\mathcal{L}(\\theta,\\eta)=-u(\\theta)+v(\\eta). caligraphic_L ( italic_θ , italic_η ) = - italic_u ( italic_θ ) + italic_v ( italic_η ) . (A.3.1)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E2", "title": "θ ⋆ ∈ arg ​ min θ ∈ Θ ⁡ u ​ ( θ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ v ​ ( η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\math", "snippet": "θ ⋆ ∈ arg ​ min θ ∈ Θ ⁡ u ​ ( θ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ v ​ ( η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}v(\\eta). italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ start_OPER"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E3", "title": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ 𝒮 ​ ( θ ) ⁡ ℒ ​ ( θ , η ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathc", "snippet": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ 𝒮 ​ ( θ ) ⁡ ℒ ​ ( θ , η ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E4", "title": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta", "snippet": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E5", "title": "𝒮 ​ ( θ ) = arg ​ min η ∈ H ⁡ ℒ ​ ( θ , η ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_θ ) = start_OPERATOR roman_arg roman_min ", "snippet": "𝒮 ​ ( θ ) = arg ​ min η ∈ H ⁡ ℒ ​ ( θ , η ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_θ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_η ∈ roman_H end_POSTSUBSCRIPT caligraphi"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E6", "title": "min η ∈ 𝒮 ​ ( θ ) ⁡ ℒ ​ ( θ , η ) = min η ∈ arg ​ min η ′ ∈ H ⁡ ℒ ​ ( θ , η ′ ) ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\op", "snippet": "min η ∈ 𝒮 ​ ( θ ) ⁡ ℒ ​ ( θ , η ) = min η ∈ arg ​ min η ′ ∈ H ⁡ ℒ ​ ( θ , η ′ ) ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\operatorname*{arg\\ min}_{\\eta^{\\prime}\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta^{\\prim"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E7", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\math", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta) roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E8", "title": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\op", "snippet": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta), itali"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.Ex1", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT roman_min start_POSTS", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η ∈ roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_θ , italic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E9", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\ma", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E10", "title": "min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}", "snippet": "min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}). roman_min start_POSTSUBSCRIPT italic_η ∈ roman"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E11", "title": "min η ∈ H ⁡ ℒ ​ ( θ , η ) ≤ ℒ ​ ( θ , η ⋆ ) ≤ ℒ ​ ( θ ⋆ , η ⋆ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta,\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roma", "snippet": "min η ∈ H ⁡ ℒ ​ ( θ , η ) ≤ ℒ ​ ( θ , η ⋆ ) ≤ ℒ ​ ( θ ⋆ , η ⋆ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta,\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_min start_POSTSUBSCRIPT italic_η ∈ roman_H end_POSTSUBSCRIPT caligraphic_L ( i"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E12", "title": "max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) ≤ ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUB", "snippet": "max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) ≤ ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT caligraphic_L ( italic_θ , italic_η ) ≤ caligraphic_L ( italic_θ start_PO"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E13", "title": "ℒ ​ ( θ ⋆ , η ⋆ ) ≤ ℒ ​ ( θ ⋆ , η ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ ⋆ , η ) ⟹ ℒ ​ ( θ ⋆ , η ⋆ ) ≤ min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\", "snippet": "ℒ ​ ( θ ⋆ , η ⋆ ) ≤ ℒ ​ ( θ ⋆ , η ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ ⋆ , η ) ⟹ ℒ ​ ( θ ⋆ , η ⋆ ) ≤ min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta)\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta^{\\star},\\eta)\\implies\\mathcal{L"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E14", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\e", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\et"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E15", "title": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). italic_θ start_POSTSUPERSCRIPT ⋆ end_PO", "snippet": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E16", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRI", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η ∈"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E17", "title": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\e", "snippet": "max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) = ℒ ​ ( θ ⋆ , η ⋆ ) = min η ∈ H ⁡ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\et"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E18", "title": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\op", "snippet": "θ ⋆ ∈ arg ​ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , η ⋆ ∈ arg ​ min η ∈ H ⁡ ℒ ​ ( θ ⋆ , η ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta). itali"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E19", "title": "max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) = ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star})=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_", "snippet": "max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) = ℒ ​ ( θ ⋆ , η ⋆ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star})=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT caligraphic_L ( italic_θ , italic_η start_POSTSUPERSCRIPT ⋆ end_PO"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E20", "title": "ℒ ​ ( θ ⋆ , η ⋆ ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_θ start_POSTSUPERSCRIPT ⋆ e", "snippet": "ℒ ​ ( θ ⋆ , η ⋆ ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , italic_η start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) ≤ ro"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E21", "title": "ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta", "snippet": "ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ min η ∈ H ⁡ ℒ ​ ( θ , η ) ≤ max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}). caligraphic_L ( ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E22", "title": "ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_θ start_POSTSUPERSCRIPT ⋆ end_", "snippet": "ℒ ​ ( θ ⋆ , η ⋆ ) = max θ ∈ Θ ⁡ ℒ ​ ( θ , η ⋆ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , italic_η start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) = roman"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E23", "title": "∇ θ [ min η ∈ H ⁡ ℒ ​ ( θ , η ) ] = ∇ θ ℒ ​ ( θ , stop_grad ​ ( η ⋆ ​ ( θ ) ) ) , \\nabla_{\\theta}\\left[\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\right]=\\nabla_{\\theta}\\mathcal{L}(\\theta,\\texttt", "snippet": "∇ θ [ min η ∈ H ⁡ ℒ ​ ( θ , η ) ] = ∇ θ ℒ ​ ( θ , stop_grad ​ ( η ⋆ ​ ( θ ) ) ) , \\nabla_{\\theta}\\left[\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\right]=\\nabla_{\\theta}\\mathcal{L}(\\theta,\\texttt{stop\\_grad}(\\eta^{\\star}(\\theta))), ∇ start_POSTSUBSCRIPT italic_θ end_POSTSUBS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E28", "title": "θ k + 1 = θ k + h ​ ∇ θ ℒ ​ ( θ k , η k ) , η k + 1 = η k − h ​ ∇ η ℒ ​ ( θ k , η k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta", "snippet": "θ k + 1 = θ k + h ​ ∇ θ ℒ ​ ( θ k , η k ) , η k + 1 = η k − h ​ ∇ η ℒ ​ ( θ k , η k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k}). italic_θ start_POSTSUBSCRIPT italic_k + 1 end"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E29", "title": "∇ 2 ℒ ​ ( θ , η ) = [ ∇ θ 2 ℒ ​ ( θ , η ) d d ​ η ​ ∇ θ 2 ℒ ​ ( θ , η ) d d ​ θ ​ ∇ η ​ θ 2 ℒ ​ ( θ , η ) ∇ η 2 ℒ ​ ( θ ) ] \\nabla^{2}\\mathcal{L}(\\theta,\\eta)=\\begin{bmatrix}\\nabla_{\\theta}^{2}\\mathca", "snippet": "∇ 2 ℒ ​ ( θ , η ) = [ ∇ θ 2 ℒ ​ ( θ , η ) d d ​ η ​ ∇ θ 2 ℒ ​ ( θ , η ) d d ​ θ ​ ∇ η ​ θ 2 ℒ ​ ( θ , η ) ∇ η 2 ℒ ​ ( θ ) ] \\nabla^{2}\\mathcal{L}(\\theta,\\eta)=\\begin{bmatrix}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta,\\eta)&\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\nabla_{\\theta}^{2}\\mathcal{L"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E30", "title": "max ⁡ { ‖ ∇ θ 2 ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 , ‖ ∇ η 2 ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 , ‖ d d ​ η ​ ∇ θ ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 } ≤ β . \\max\\left\\{\\|\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\|_{2},\\le", "snippet": "max ⁡ { ‖ ∇ θ 2 ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 , ‖ ∇ η 2 ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 , ‖ d d ​ η ​ ∇ θ ℒ ​ ( θ ⋆ , η ⋆ ) ‖ 2 } ≤ β . \\max\\left\\{\\|\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\|_{2},\\left\\|\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\right\\|_{2},\\left\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E31", "title": "μ η = λ min ​ ( ∇ η 2 ℒ ​ ( θ ⋆ , η ⋆ ) ) , μ θ = min ⁡ { β , − ( ∇ θ 2 ℒ + [ d d ​ θ ​ ∇ η ℒ ] ​ [ ∇ η 2 ℒ ] − 1 ​ [ d d ​ η ​ ∇ θ ℒ ] ) ​ ( θ ⋆ , η ⋆ ) } . \\mu_{\\eta}=\\lambda_{\\min}(\\nabla_{\\eta}^{2", "snippet": "μ η = λ min ​ ( ∇ η 2 ℒ ​ ( θ ⋆ , η ⋆ ) ) , μ θ = min ⁡ { β , − ( ∇ θ 2 ℒ + [ d d ​ θ ​ ∇ η ℒ ] ​ [ ∇ η 2 ℒ ] − 1 ​ [ d d ​ η ​ ∇ θ ℒ ] ) ​ ( θ ⋆ , η ⋆ ) } . \\mu_{\\eta}=\\lambda_{\\min}(\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})),\\qquad\\mu_{\\theta}=\\min\\left\\{\\beta,-"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E32", "title": "κ η = L / μ η , κ θ = L / μ θ . \\kappa_{\\eta}=L/\\mu_{\\eta},\\qquad\\kappa_{\\theta}=L/\\mu_{\\theta}. italic_κ start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT = italic_L / italic_μ start_POSTSUBSCRIPT itali", "snippet": "κ η = L / μ η , κ θ = L / μ θ . \\kappa_{\\eta}=L/\\mu_{\\eta},\\qquad\\kappa_{\\theta}=L/\\mu_{\\theta}. italic_κ start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT = italic_L / italic_μ start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT , italic_κ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E35", "title": "‖ ( θ k , η k ) − ( θ ⋆ , η ⋆ ) ‖ 2 ≤ c 0 ​ ( 1 − c 1 T ​ κ θ ) k ​ ‖ ( θ 0 , η 0 ) − ( θ ⋆ , η ⋆ ) ‖ 2 , \\|(\\theta_{k},\\eta_{k})-(\\theta^{\\star},\\eta^{\\star})\\|_{2}\\leq c_{0}\\left(1-\\frac{c_{1}}{T\\ka", "snippet": "‖ ( θ k , η k ) − ( θ ⋆ , η ⋆ ) ‖ 2 ≤ c 0 ​ ( 1 − c 1 T ​ κ θ ) k ​ ‖ ( θ 0 , η 0 ) − ( θ ⋆ , η ⋆ ) ‖ 2 , \\|(\\theta_{k},\\eta_{k})-(\\theta^{\\star},\\eta^{\\star})\\|_{2}\\leq c_{0}\\left(1-\\frac{c_{1}}{T\\kappa_{\\theta}}\\right)^{k}\\|(\\theta_{0},\\eta_{0})-(\\theta^{\\star},\\eta^{\\star})\\|_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S4.E1", "title": "softmax ⁡ ( [ x 1 ⋮ x n ] ) = 1 ∑ i = 1 n e x i ​ [ x 1 ⋮ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin", "snippet": "softmax ⁡ ( [ x 1 ⋮ x n ] ) = 1 ∑ i = 1 n e x i ​ [ x 1 ⋮ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}. roman_softmax ( [ start_ARG start_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx100", "title": "‖ θ ⋆ − θ k + 1 ‖ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ∥ italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ∥ start_", "snippet": "‖ θ ⋆ − θ k + 1 ‖ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ∥ italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx101", "title": "‖ θ ⋆ − θ k + 1 ‖ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ∥ italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ∥ start_", "snippet": "‖ θ ⋆ − θ k + 1 ‖ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ∥ italic_θ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx102", "title": "arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ 2 = 1 ⁡ [ ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h 2 ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ ] \\displaystyle\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ ", "snippet": "arg ​ min 𝒗 ∈ ℝ n ‖ 𝒗 ‖ 2 = 1 ⁡ [ ℒ ​ ( θ ) + h ​ ⟨ ∇ ℒ ​ ( θ ) , 𝒗 ⟩ + 1 2 ​ h 2 ​ ⟨ [ ∇ 2 ℒ ​ ( θ ) ] ​ 𝒗 , 𝒗 ⟩ ] \\displaystyle\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}\\left[\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx103", "title": "P k \\displaystyle P_{k} italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = PreconditionerUpdate ​ ( P k − 1 ; θ k , ∇ ℒ ​ ( θ k ) ) \\displaystyle=\\mathrm{PreconditionerUpdate}(P_{k-1};\\theta_{k", "snippet": "P k \\displaystyle P_{k} italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = PreconditionerUpdate ​ ( P k − 1 ; θ k , ∇ ℒ ​ ( θ k ) ) \\displaystyle=\\mathrm{PreconditionerUpdate}(P_{k-1};\\theta_{k},\\nabla\\mathcal{L}(\\theta_{k})) = roman_PreconditionerUpdate ( italic_P start_P"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx104", "title": "𝒈 k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = β ​ 𝒈 k − 1 + ( 1 − β ) ​ ∇ ℒ k ​ ( θ k ) \\displaystyle=\\beta\\bm{g}_{k-1}+(1-\\beta)\\nabla\\mathcal{L}_{k}(\\the", "snippet": "𝒈 k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = β ​ 𝒈 k − 1 + ( 1 − β ) ​ ∇ ℒ k ​ ( θ k ) \\displaystyle=\\beta\\bm{g}_{k-1}+(1-\\beta)\\nabla\\mathcal{L}_{k}(\\theta_{k}) = italic_β bold_italic_g start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx105", "title": "𝒈 k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = β 1 ​ 𝒈 k − 1 + ( 1 − β 1 ) ​ ∇ ℒ k ​ ( θ k ) \\displaystyle=\\beta^{1}\\bm{g}_{k-1}+(1-\\beta^{1})\\nabla\\mathcal", "snippet": "𝒈 k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = β 1 ​ 𝒈 k − 1 + ( 1 − β 1 ) ​ ∇ ℒ k ​ ( θ k ) \\displaystyle=\\beta^{1}\\bm{g}_{k-1}+(1-\\beta^{1})\\nabla\\mathcal{L}_{k}(\\theta_{k}) = italic_β start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx106", "title": "d ​ g \\displaystyle\\mathrm{d}g roman_d italic_g = g ​ ( 𝑾 + d ​ 𝑾 , 𝒃 + d ​ 𝒃 ) − g ​ ( 𝑾 , 𝒃 ) = [ ( 𝑾 + d ​ 𝑾 ) ​ 𝑿 + ( 𝒃 + d ​ 𝒃 ) ​ 𝟏 ⊤ ] − [ 𝑾 ​ 𝑿 + 𝒃 ] \\displaystyle=g(\\bm{W}+\\mathrm{d}\\bm{W},\\b", "snippet": "d ​ g \\displaystyle\\mathrm{d}g roman_d italic_g = g ​ ( 𝑾 + d ​ 𝑾 , 𝒃 + d ​ 𝒃 ) − g ​ ( 𝑾 , 𝒃 ) = [ ( 𝑾 + d ​ 𝑾 ) ​ 𝑿 + ( 𝒃 + d ​ 𝒃 ) ​ 𝟏 ⊤ ] − [ 𝑾 ​ 𝑿 + 𝒃 ] \\displaystyle=g(\\bm{W}+\\mathrm{d}\\bm{W},\\bm{b}+\\mathrm{d}\\bm{b})-g(\\bm{W},\\bm{b})=[(\\bm{W}+\\mathrm{d}\\bm{W})\\bm{X}+(\\bm{b}"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx107", "title": "d ​ p \\displaystyle\\mathrm{d}p roman_d italic_p = p ′ ​ ( a , b ) ​ [ d ​ a , d ​ b ] = p ​ ( a + d ​ a , b + d ​ b ) − p ​ ( a , b ) = ( a + d ​ a ) ​ ( b + d ​ b ) − a ​ b \\displaystyle=p^{\\prime}(a", "snippet": "d ​ p \\displaystyle\\mathrm{d}p roman_d italic_p = p ′ ​ ( a , b ) ​ [ d ​ a , d ​ b ] = p ​ ( a + d ​ a , b + d ​ b ) − p ​ ( a , b ) = ( a + d ​ a ) ​ ( b + d ​ b ) − a ​ b \\displaystyle=p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=p(a+\\mathrm{d}a,b+\\mathrm{d}b)-p(a,b)=(a+\\mathrm{d}"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx108", "title": "f ′ ​ ( x ) ​ [ d ​ x ] \\displaystyle f^{\\prime}(x)[\\mathrm{d}x] italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = p ′ ​ ( v ​ ( x ) ) ​ v ′ ​ ( x ) ​ [ d ​ x ] ", "snippet": "f ′ ​ ( x ) ​ [ d ​ x ] \\displaystyle f^{\\prime}(x)[\\mathrm{d}x] italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = p ′ ​ ( v ​ ( x ) ) ​ v ′ ​ ( x ) ​ [ d ​ x ] = p ′ ​ ( g ​ ( x ) , h ​ ( x ) ) ​ [ g ′ ​ ( x ) ​ [ d ​ x ] , h ′ ​ ( x ) ​ [ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx109", "title": "f ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] \\displaystyle f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}] italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] = ( g ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] ", "snippet": "f ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] \\displaystyle f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}] italic_f start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] = ( g ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] ) ​ h ​ ( 𝑨 ) + g ​ ( 𝑨 ) ​ ( h ′ ​ ( 𝑨 ) ​ [ d ​ 𝑨 ] ) \\displaystyle=(g^{\\prime"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx110", "title": "computing ​ c ′ ​ ( 𝒙 ) ∈ ℝ q × p ​ takes negligible time \\displaystyle\\text{computing}\\ c^{\\prime}(\\bm{x})\\in\\mathbb{R}^{q\\times p}\\ \\text{takes negligible time} computing italic_c start_POSTSUPERSCR", "snippet": "computing ​ c ′ ​ ( 𝒙 ) ∈ ℝ q × p ​ takes negligible time \\displaystyle\\text{computing}\\ c^{\\prime}(\\bm{x})\\in\\mathbb{R}^{q\\times p}\\ \\text{takes negligible time} computing italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx111", "title": "computing ​ a ′ ​ ( b ​ ( c ​ ( 𝒙 ) ) ) ∈ ℝ s × r ​ takes negligible time \\displaystyle\\text{computing}\\ a^{\\prime}(b(c(\\bm{x})))\\in\\mathbb{R}^{s\\times r}\\ \\text{takes negligible time} computing itali", "snippet": "computing ​ a ′ ​ ( b ​ ( c ​ ( 𝒙 ) ) ) ∈ ℝ s × r ​ takes negligible time \\displaystyle\\text{computing}\\ a^{\\prime}(b(c(\\bm{x})))\\in\\mathbb{R}^{s\\times r}\\ \\text{takes negligible time} computing italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( bold_ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx112", "title": "𝒁 θ 1 ​ ( 𝑿 ) ≐ f θ emb ​ ( 𝑿 ) , \\displaystyle\\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X}), bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1", "snippet": "𝒁 θ 1 ​ ( 𝑿 ) ≐ f θ emb ​ ( 𝑿 ) , \\displaystyle\\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X}), bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ e"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx113", "title": "𝒁 1 ≐ f emb ​ ( 𝑿 , θ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ≐ italic_f start_POSTSUPERSCRIPT roma", "snippet": "𝒁 1 ≐ f emb ​ ( 𝑿 , θ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ≐ italic_f start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X , italic_θ start_POSTSUPERSCRIPT roman"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx114", "title": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.45) = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ 𝒚 ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf", "snippet": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.45) = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ 𝒚 ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}\\hat{\\bm{y}} = divide start_ARG roman"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx115", "title": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ h d ​ θ head ⋅ d ​ θ head \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\frac{\\mathrm{d}h}{", "snippet": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ h d ​ θ head ⋅ d ​ θ head \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\frac{\\mathrm{d}h}{\\mathrm{d}\\theta^{\\mathrm{head}}}\\cdot\\mathrm{d}\\theta^{\\mathrm{head}} = divide "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx116", "title": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ ℒ d ​ 𝒁 ℓ + 1 ⋅ d ​ 𝒁 ℓ + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = d", "snippet": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ ℒ d ​ 𝒁 ℓ + 1 ⋅ d ​ 𝒁 ℓ + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = divide start_ARG roman_d caligraphic_L end_ARG start_ARG roman_d bold_italic_Z st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx117", "title": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ ℒ d ​ 𝒁 ℓ + 1 ⋅ d ​ 𝒁 ℓ + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = d", "snippet": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ ℒ d ​ 𝒁 ℓ + 1 ⋅ d ​ 𝒁 ℓ + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = divide start_ARG roman_d caligraphic_L end_ARG start_ARG roman_d bold_italic_Z st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx118", "title": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.68) = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ 𝒚 ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf", "snippet": "d ​ ℒ \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ​ 𝖫 \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.68) = d ​ 𝖫 d ​ 𝒚 ^ ⋅ d ​ 𝒚 ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}\\hat{\\bm{y}} = divide start_ARG roman"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx119", "title": "∇ 𝒁 L + 1 ℒ \\displaystyle\\nabla_{\\bm{Z}^{L+1}}\\mathcal{L} ∇ start_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L = ( d ​ h d ​ 𝒁 L +", "snippet": "∇ 𝒁 L + 1 ℒ \\displaystyle\\nabla_{\\bm{Z}^{L+1}}\\mathcal{L} ∇ start_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L = ( d ​ h d ​ 𝒁 L + 1 ) ∗ ​ ∇ 𝒚 ^ 𝖫 \\displaystyle=\\left(\\frac{\\mathrm{d}h}{\\mathrm{d}\\bm{Z}^{L+1}}\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx120", "title": "d ​ f ℓ \\displaystyle\\mathrm{d}f^{\\ell} roman_d italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = [ ( 𝑾 ℓ + d ​ 𝑾 ℓ ) ​ 𝒁 + ( 𝒃 ℓ + d ​ 𝒃 ℓ ) ​ 𝟏 ⊤ ] − [ 𝑾 ℓ ​ 𝒁 + 𝒃 ℓ ] \\displaystyle=[(\\bm", "snippet": "d ​ f ℓ \\displaystyle\\mathrm{d}f^{\\ell} roman_d italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = [ ( 𝑾 ℓ + d ​ 𝑾 ℓ ) ​ 𝒁 + ( 𝒃 ℓ + d ​ 𝒃 ℓ ) ​ 𝟏 ⊤ ] − [ 𝑾 ℓ ​ 𝒁 + 𝒃 ℓ ] \\displaystyle=[(\\bm{W}^{\\ell}+\\mathrm{d}\\bm{W}^{\\ell})\\bm{Z}+(\\bm{b}^{\\ell}+\\mathrm{d}\\bm{b}^{\\ell}"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx121", "title": "⟨ T ​ [ 𝑨 , 𝒖 ] , 𝑩 ⟩ ℝ m × n \\displaystyle\\langle T[\\bm{A},\\bm{u}],\\bm{B}\\rangle_{\\mathbb{R}^{m\\times n}} ⟨ italic_T [ bold_italic_A , bold_italic_u ] , bold_italic_B ⟩ start_POSTSUBSCRIPT blackboard", "snippet": "⟨ T ​ [ 𝑨 , 𝒖 ] , 𝑩 ⟩ ℝ m × n \\displaystyle\\langle T[\\bm{A},\\bm{u}],\\bm{B}\\rangle_{\\mathbb{R}^{m\\times n}} ⟨ italic_T [ bold_italic_A , bold_italic_u ] , bold_italic_B ⟩ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRI"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx122", "title": "η k + 1 = η k + 1 T + 1 ; η k + 1 t + 1 = η k + 1 t − h ​ ∇ η ℒ ​ ( θ k , η k + 1 t ) , ∀ t ∈ [ T ] ; η k + 1 1 = η k \\displaystyle\\eta_{k+1}=\\eta_{k+1}^{T+1};\\qquad\\eta_{k+1}^{t+1}=\\eta_{k+1}^{t}-h\\n", "snippet": "η k + 1 = η k + 1 T + 1 ; η k + 1 t + 1 = η k + 1 t − h ​ ∇ η ℒ ​ ( θ k , η k + 1 t ) , ∀ t ∈ [ T ] ; η k + 1 1 = η k \\displaystyle\\eta_{k+1}=\\eta_{k+1}^{T+1};\\qquad\\eta_{k+1}^{t+1}=\\eta_{k+1}^{t}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k+1}^{t}),\\quad\\forall t\\in[T];\\qquad\\et"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx123", "title": "θ k + 1 \\displaystyle\\theta_{k+1} italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = θ k + h ​ ∇ θ ℒ ​ ( θ k , η k ) \\displaystyle=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}", "snippet": "θ k + 1 \\displaystyle\\theta_{k+1} italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = θ k + h ​ ∇ θ ℒ ​ ( θ k , η k ) \\displaystyle=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}) = italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ∇ start_P"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx124", "title": "θ k + 1 \\displaystyle\\theta_{k+1} italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = θ k + 1 4 ​ β ​ T ​ ∇ θ ℒ ​ ( θ k , η k ) , \\displaystyle=\\theta_{k}+\\frac{1}{4\\beta T}\\nabla_{\\theta}\\m", "snippet": "θ k + 1 \\displaystyle\\theta_{k+1} italic_θ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = θ k + 1 4 ​ β ​ T ​ ∇ θ ℒ ​ ( θ k , η k ) , \\displaystyle=\\theta_{k}+\\frac{1}{4\\beta T}\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}), = italic_θ start_POSTSUBSCRIPT italic_k end_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx125", "title": "1 2 ​ ‖ ∇ f ​ ( θ ) ‖ 2 2 ≥ μ ​ ( f ​ ( θ ) − f ​ ( θ ⋆ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-f(\\theta^{\\star})\\right), divide start_ARG 1 end_ARG start_ARG ", "snippet": "1 2 ​ ‖ ∇ f ​ ( θ ) ‖ 2 2 ≥ μ ​ ( f ​ ( θ ) − f ​ ( θ ⋆ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-f(\\theta^{\\star})\\right), divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ ∇ italic_f ( italic_θ ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1", "title": "Step-Size Selection", "snippet": "Step-Size Selection The remaining question is what the step size h h italic_h should be? If we choose h h italic_h to be too small, the value of the function may decrease very slowly, as shown by the plot in the middle in Figure A.1 . If h h italic_h is too large, the value might"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx1", "title": "Newton’s Method", "snippet": "Newton’s Method There are some smooth problems and strongly convex problems on which gradient descent nonetheless does quite poorly. For example, let λ ≥ 0 \\lambda\\geq 0 italic_λ ≥ 0 and let ℒ λ : ℝ 2 → ℝ \\mathcal{L}_{\\lambda}\\colon\\mathbb{R}^{2}\\to\\mathbb{R} caligraphic_L start_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx2", "title": "PGD", "snippet": "PGD In practice, we do not have a second-order oracle which allows us to compute ∇ 2 ℒ ​ ( θ ) \\nabla^{2}\\mathcal{L}(\\theta) ∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_θ ) . Instead, we can attempt to learn an approximation to it alongside the parameter "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx1", "title": "Convergence of One-Timescale GDA to Stackelberg Equilibrium", "snippet": "Convergence of One-Timescale GDA to Stackelberg Equilibrium If T = 1 T=1 italic_T = 1 (i.e., named one-timescale because both θ \\theta italic_θ and η \\eta italic_η updates are of the same scale), then the GDA algorithm becomes θ k + 1 = θ k + h ​ ∇ θ ℒ ​ ( θ k , η k ) , η k + 1 ="}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2", "title": "Local Convergence of Two-Timescale GDA to Stackelberg Equilibrium", "snippet": "Local Convergence of Two-Timescale GDA to Stackelberg Equilibrium Strong convexity/concavity is a global property, and none of the games we look into in this book have objectives which are globally strongly concave/strongly convex. In this case, the best we can hope for is local "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#top", "title": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "snippet": ""}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1", "title": "B.1 Differential Entropy of Low-Dimensional Distributions", "snippet": "B.1 Differential Entropy of Low-Dimensional Distributions In this short appendix we discuss the differential entropy of low-dimensional distributions. By definition, the differential entropy of a random variable 𝒙 \\bm{x} bold_italic_x which does not have a density is − ∞ -\\infty "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2", "title": "B.2 Diffusion and Denoising Processes", "snippet": "B.2 Diffusion and Denoising Processes In the main body ( Chapter 3 ), we considered a random variable 𝒙 \\bm{x} bold_italic_x , and a stochastic process defined by ( 3.2.1 ), i.e., 𝒙 t = 𝒙 + t ​ 𝒈 , ∀ t ∈ [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3", "title": "B.3 Lossy Coding and Sphere Packing", "snippet": "B.3 Lossy Coding and Sphere Packing In this section, we prove Theorem 3.6 . Following our conventions throughout this appendix, we write 𝒮 = Supp ⁡ ( 𝒙 ) \\mathcal{S}=\\operatorname{Supp}(\\bm{x}) caligraphic_S = roman_Supp ( bold_italic_x ) for the compact support of the random var"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1", "title": "B.2.1 Diffusion Process Increases Entropy Over Time", "snippet": "B.2.1 Diffusion Process Increases Entropy Over Time In this section appendix we provide a proof of Theorem B.2 . For convenience, we restate it as follows. Theorem B.2 (Diffusion Increases Entropy) . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2", "title": "B.2.2 Denoising Process Reduces Entropy Over Time", "snippet": "B.2.2 Denoising Process Reduces Entropy Over Time Recall that in Section 3.2.1 we start with the random variable 𝒙 T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and iteratively denoise it using iterations of the form 𝒙 ^ s ≐ 𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t = 𝒙 ^ t ] ="}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3", "title": "B.2.3 Technical Lemmas and Intermediate Results", "snippet": "B.2.3 Technical Lemmas and Intermediate Results In this subsection we present technical results which power our main two conceptual theorems. Our presentation will be more or less standard for mathematics; we will start with the higher-level results first, and gradually move back"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1", "title": "B.3.1 Proof of Relationship Between Rate Distortion and Covering", "snippet": "B.3.1 Proof of Relationship Between Rate Distortion and Covering We briefly sketch the proof, then proceed to establishing three fundamental lemmas, then give the proof. The proof will depend on notions introduced in the sketch below. Obtaining an upper bound on the rate distorti"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2", "title": "B.3.2 Proof of Lemma B.6", "snippet": "B.3.2 Proof of Lemma B.6 (Proof of Lemma B.6 ). It suffices to show that any code for 𝒙 \\bm{x} bold_italic_x with expected squared distortion ϵ 2 \\epsilon^{2} italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT produces a code for 𝒙 δ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption1", "title": "Assumption B.1 .", "snippet": "Assumption B.1 . 𝒙 \\bm{x} bold_italic_x is supported on a compact set 𝒮 ⊆ ℝ D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of radius at most R R italic_R , i.e., R ≐ sup 𝝃 ∈ 𝒮 ‖ 𝝃 ‖ 2 R\\doteq\\sup_{\\bm{\\xi}\\in\\m"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem1", "title": "Theorem B.1 .", "snippet": "Theorem B.1 . Let 𝐱 \\bm{x} bold_italic_x be a random variable on ℝ D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . 1. If 𝒙 \\bm{x} bold_italic_x is supported on a compact set 𝒮 ⊆ ℝ D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ⊆ blackboard_R"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption2", "title": "Assumption B.2 .", "snippet": "Assumption B.2 . 𝒙 \\bm{x} bold_italic_x has a twice continuously differentiable density, denoted p p italic_p ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem2", "title": "Theorem B.2 (Diffusion Increases Entropy) .", "snippet": "Theorem B.2 (Diffusion Increases Entropy) . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem3", "title": "Theorem B.3 .", "snippet": "Theorem B.3 . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma1", "title": "Lemma B.1 .", "snippet": "Lemma B.1 . Let 𝐱 \\bm{x} bold_italic_x be any random variable, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma2", "title": "Lemma B.2 .", "snippet": "Lemma B.2 . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma3", "title": "Lemma B.3 (Generalization of [ Gri11 ] , Lemma A.1) .", "snippet": "Lemma B.3 (Generalization of [ Gri11 ] , Lemma A.1) . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma4", "title": "Lemma B.4 (Generalization of [ Gri11 ] Corollary A.2, Part 1) .", "snippet": "Lemma B.4 (Generalization of [ Gri11 ] Corollary A.2, Part 1) . Let f : ℝ D → ℝ D f\\colon\\mathbb{R}^{D}\\to\\mathbb{R}^{D} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT be any differentia"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmcorollary1", "title": "Corollary B.1 (Generalization of [ Gri11 ] Corollary A.2, Part 2) .", "snippet": "Corollary B.1 (Generalization of [ Gri11 ] Corollary A.2, Part 2) . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma5", "title": "Lemma B.5 .", "snippet": "Lemma B.5 . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition1", "title": "Proposition B.1 .", "snippet": "Proposition B.1 . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition2", "title": "Proposition B.2 .", "snippet": "Proposition B.2 . For t > 0 t>0 italic_t > 0 and 𝛏 ∈ ℝ D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_ξ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT it holds ∂ ∂ t ​ φ t ​ ( 𝝃 ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ∂ end_ARG"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition3", "title": "Proposition B.3 ( [ Jon82 ] , Section 11.12) .", "snippet": "Proposition B.3 ( [ Jon82 ] , Section 11.12) . Let f : ( 0 , T ) × ℝ D → ℝ f\\colon(0,T)\\times\\mathbb{R}^{D}\\to\\mathbb{R} italic_f : ( 0 , italic_T ) × blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R be such that: • f f italic_f is a jointly measurab"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition4", "title": "Proposition B.4 ( [ BB11 ] , Proposition 4.20) .", "snippet": "Proposition B.4 ( [ BB11 ] , Proposition 4.20) . Let f f italic_f be k k italic_k -times continuously differentiable with compact support, and let g g italic_g be locally integrable. Then the convolution f ∗ g f*g italic_f ∗ italic_g defined by ( f ∗ g ) ​ ( 𝝃 ) ≐ ∫ ℝ D f ​ ( 𝒖 )"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption3", "title": "Assumption B.3 .", "snippet": "Assumption B.3 . The support 𝒮 ⊂ ℝ D \\mathcal{S}\\subset\\mathbb{R}^{D} caligraphic_S ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of the random variable 𝒙 \\bm{x} bold_italic_x is a finite union of K K italic_K spheres, each with dimension d k d_{k} italic_d st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition1", "title": "Definition B.1 .", "snippet": "Definition B.1 . Let 𝒮 \\mathcal{S} caligraphic_S be a compact set. For any δ > 0 \\delta>0 italic_δ > 0 , define the δ \\delta italic_δ -thickening of 𝒮 \\mathcal{S} caligraphic_S , denoted 𝒮 δ \\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT , by 𝒮 "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition2", "title": "Definition B.2 .", "snippet": "Definition B.2 . Let 𝒙 \\bm{x} bold_italic_x be a random variable such that Supp ⁡ ( 𝒙 ) = 𝒮 \\operatorname{Supp}(\\bm{x})=\\mathcal{S} roman_Supp ( bold_italic_x ) = caligraphic_S is a union of K K italic_K hyperspheres, distributed as in Assumption B.3 . Denote the support of each "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma6", "title": "Lemma B.6 .", "snippet": "Lemma B.6 . Suppose the random variable 𝐱 \\bm{x} bold_italic_x satisfies Assumption B.3 . Then if 0 < δ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_δ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , the thickened random variable 𝐱 δ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E1", "title": "h ​ ( 𝒙 ) ​ “=” ​ lim ε ↘ 0 h ​ ( 𝒙 ε ) . h(\\bm{x})\\ \\text{``=''}\\ \\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon}). italic_h ( bold_italic_x ) “=” roman_lim start_POSTSUBSCRIPT italic_ε ↘ 0 end_PO", "snippet": "h ​ ( 𝒙 ) ​ “=” ​ lim ε ↘ 0 h ​ ( 𝒙 ε ) . h(\\bm{x})\\ \\text{``=''}\\ \\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon}). italic_h ( bold_italic_x ) “=” roman_lim start_POSTSUBSCRIPT italic_ε ↘ 0 end_POSTSUBSCRIPT italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_ε end_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E2", "title": "S ε = ⋃ 𝝃 ∈ 𝒮 B ε ​ ( 𝝃 ) S_{\\varepsilon}=\\bigcup_{\\bm{\\xi}\\in\\mathcal{S}}B_{\\varepsilon}(\\bm{\\xi}) italic_S start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT = ⋃ start_POSTSUBSCRIPT bold_italic_ξ ∈ cali", "snippet": "S ε = ⋃ 𝝃 ∈ 𝒮 B ε ​ ( 𝝃 ) S_{\\varepsilon}=\\bigcup_{\\bm{\\xi}\\in\\mathcal{S}}B_{\\varepsilon}(\\bm{\\xi}) italic_S start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT = ⋃ start_POSTSUBSCRIPT bold_italic_ξ ∈ caligraphic_S end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_ε end_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E3", "title": "p ε ​ ( 𝝃 ) = 𝟏 ​ ( 𝝃 ∈ 𝒮 ε ) ⋅ 1 vol ⁡ ( 𝒮 ε ) . p_{\\varepsilon}(\\bm{\\xi})=\\mathbf{1}(\\bm{\\xi}\\in\\mathcal{S}_{\\varepsilon})\\cdot\\frac{1}{\\operatorname{vol}(\\mathcal{S}_{\\varepsilon})}. italic_p start", "snippet": "p ε ​ ( 𝝃 ) = 𝟏 ​ ( 𝝃 ∈ 𝒮 ε ) ⋅ 1 vol ⁡ ( 𝒮 ε ) . p_{\\varepsilon}(\\bm{\\xi})=\\mathbf{1}(\\bm{\\xi}\\in\\mathcal{S}_{\\varepsilon})\\cdot\\frac{1}{\\operatorname{vol}(\\mathcal{S}_{\\varepsilon})}. italic_p start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( bold_italic_ξ ) = bold_1 ( bold_ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E8", "title": "h ​ ( 𝒙 ) = lim ε ↘ 0 h ​ ( 𝒙 ε ) = lim ε ↘ 0 log ⁡ ( vol ⁡ ( 𝒮 ε ) ) = − ∞ , h(\\bm{x})=\\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon})=\\lim_{\\varepsilon\\searrow 0}\\log(\\operatorname{vol}(\\mathcal", "snippet": "h ​ ( 𝒙 ) = lim ε ↘ 0 h ​ ( 𝒙 ε ) = lim ε ↘ 0 log ⁡ ( vol ⁡ ( 𝒮 ε ) ) = − ∞ , h(\\bm{x})=\\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon})=\\lim_{\\varepsilon\\searrow 0}\\log(\\operatorname{vol}(\\mathcal{S}_{\\varepsilon}))=-\\infty, italic_h ( bold_italic_x ) = roman_lim start_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E9", "title": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒰 ⁡ ( 𝒮 ) ) = log ⁡ vol ⁡ ( 𝒮 ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ≤ italic_h ( caligraphic_U ", "snippet": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒰 ⁡ ( 𝒮 ) ) = log ⁡ vol ⁡ ( 𝒮 ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ≤ italic_h ( caligraphic_U ( caligraphic_S ) ) = roman_log roman_vol ( caligraphic_S ) . (B.1.9)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E10", "title": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒩 ⁡ ( 𝟎 , 𝚺 ) ) = 1 2 ​ log ⁡ ( ( 2 ​ π ​ e ) D ​ det 𝚺 ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{\\Sigma}))=\\frac{1}{2}\\log((2\\pi e)^{D}\\det\\bm{\\Sigma}). italic_h ( b", "snippet": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒩 ⁡ ( 𝟎 , 𝚺 ) ) = 1 2 ​ log ⁡ ( ( 2 ​ π ​ e ) D ​ det 𝚺 ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{\\Sigma}))=\\frac{1}{2}\\log((2\\pi e)^{D}\\det\\bm{\\Sigma}). italic_h ( bold_italic_x ) ≤ italic_h ( caligraphic_N ( bold_0 , bold_Σ ) ) = divide start_A"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E11", "title": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒩 ⁡ ( 𝟎 , a D ​ 𝑰 ) ) = D 2 ​ log ⁡ 2 ​ π ​ e ​ a D . h(\\bm{x})\\leq h\\left(\\operatorname{\\mathcal{N}}\\left(\\bm{0},\\frac{a}{D}\\bm{I}\\right)\\right)=\\frac{D}{2}\\log\\frac{2\\pi ea}{D}. it", "snippet": "h ​ ( 𝒙 ) ≤ h ​ ( 𝒩 ⁡ ( 𝟎 , a D ​ 𝑰 ) ) = D 2 ​ log ⁡ 2 ​ π ​ e ​ a D . h(\\bm{x})\\leq h\\left(\\operatorname{\\mathcal{N}}\\left(\\bm{0},\\frac{a}{D}\\bm{I}\\right)\\right)=\\frac{D}{2}\\log\\frac{2\\pi ea}{D}. italic_h ( bold_italic_x ) ≤ italic_h ( caligraphic_N ( bold_0 , divide start_ARG "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E1", "title": "𝒙 t = 𝒙 + t ​ 𝒈 , ∀ t ∈ [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ∀ italic_t ∈", "snippet": "𝒙 t = 𝒙 + t ​ 𝒈 , ∀ t ∈ [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ∀ italic_t ∈ [ 0 , italic_T ] (B.2.1)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E2", "title": "φ t ​ ( 𝝃 ) ≐ 1 ( 2 ​ π ) D / 2 ​ t D ​ exp ⁡ ( − ‖ 𝝃 ‖ 2 2 2 ​ t 2 ) . \\varphi_{t}(\\bm{\\xi})\\doteq\\frac{1}{(2\\pi)^{D/2}t^{D}}\\exp\\left(-\\frac{\\|\\bm{\\xi}\\|_{2}^{2}}{2t^{2}}\\right). italic_φ start_POST", "snippet": "φ t ​ ( 𝝃 ) ≐ 1 ( 2 ​ π ) D / 2 ​ t D ​ exp ⁡ ( − ‖ 𝝃 ‖ 2 2 2 ​ t 2 ) . \\varphi_{t}(\\bm{\\xi})\\doteq\\frac{1}{(2\\pi)^{D/2}t^{D}}\\exp\\left(-\\frac{\\|\\bm{\\xi}\\|_{2}^{2}}{2t^{2}}\\right). italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) ≐ divide start_ARG 1 end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E3", "title": "p t ​ ( 𝝃 ) = 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] , p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})], italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = blackboard_", "snippet": "p t ​ ( 𝝃 ) = 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] , p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})], italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = blackboard_E [ italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ - bo"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E4", "title": "h ​ ( 𝒙 s ) < h ​ ( 𝒙 t ) , ∀ s , t : 0 ≤ s < t ≤ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < i", "snippet": "h ​ ( 𝒙 s ) < h ​ ( 𝒙 t ) , ∀ s , t : 0 ≤ s < t ≤ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , ∀ ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E5", "title": "Δ ​ f t ​ ( 𝝃 ) = tr ⁡ ( ∇ 2 f t ​ ( 𝝃 ) ) = ∑ i = 1 D ∂ 2 f t ∂ ξ i 2 ​ ( 𝝃 ) . \\Delta f_{t}(\\bm{\\xi})=\\operatorname{tr}(\\nabla^{2}f_{t}(\\bm{\\xi}))=\\sum_{i=1}^{D}\\frac{\\partial^{2}f_{t}}{\\partial\\xi_", "snippet": "Δ ​ f t ​ ( 𝝃 ) = tr ⁡ ( ∇ 2 f t ​ ( 𝝃 ) ) = ∑ i = 1 D ∂ 2 f t ∂ ξ i 2 ​ ( 𝝃 ) . \\Delta f_{t}(\\bm{\\xi})=\\operatorname{tr}(\\nabla^{2}f_{t}(\\bm{\\xi}))=\\sum_{i=1}^{D}\\frac{\\partial^{2}f_{t}}{\\partial\\xi_{i}^{2}}(\\bm{\\xi}). roman_Δ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E6", "title": "∂ p t ∂ t ​ ( 𝝃 ) = t ​ Δ ​ p t ​ ( 𝝃 ) . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=t\\Delta p_{t}(\\bm{\\xi}). divide start_ARG ∂ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_", "snippet": "∂ p t ∂ t ​ ( 𝝃 ) = t ​ Δ ​ p t ​ ( 𝝃 ) . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=t\\Delta p_{t}(\\bm{\\xi}). divide start_ARG ∂ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∂ italic_t end_ARG ( bold_italic_ξ ) = italic_t roman_Δ italic_p start_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E14", "title": "h ​ ( 𝒙 t ) = h ​ ( 𝒙 s ) + ∫ s t d d ​ u ​ h ​ ( 𝒙 u ) ​ d u > h ​ ( 𝒙 s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathrm{d}}{\\mathrm{d}u}h(\\bm{x}_{u})\\mathrm{d}u>h(\\bm{x}_{s}), italic_h ( b", "snippet": "h ​ ( 𝒙 t ) = h ​ ( 𝒙 s ) + ∫ s t d d ​ u ​ h ​ ( 𝒙 u ) ​ d u > h ​ ( 𝒙 s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathrm{d}}{\\mathrm{d}u}h(\\bm{x}_{u})\\mathrm{d}u>h(\\bm{x}_{s}), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_h ( bold_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E15", "title": "𝒙 ^ s ≐ 𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t = 𝒙 ^ t ] = s t ​ 𝒙 ^ t + ( 1 − s t ) ​ 𝒙 ¯ ∗ ​ ( t , 𝒙 ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x", "snippet": "𝒙 ^ s ≐ 𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t = 𝒙 ^ t ] = s t ​ 𝒙 ^ t + ( 1 − s t ) ​ 𝒙 ¯ ∗ ​ ( t , 𝒙 ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x}}_{t}+\\left(1-\\frac{s}{t}\\right)\\bar{\\bm{x}}^{\\ast}(t,\\hat{\\bm{x}}_{t}). over^ "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E16", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 𝒙 t + t 2 ​ ∇ p t ​ ( 𝒙 t ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{x}_{t}+t^{2}\\nabla p_{t}(\\bm{x}_{t}), over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSU", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 𝒙 t + t 2 ​ ∇ p t ​ ( 𝒙 t ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{x}_{t}+t^{2}\\nabla p_{t}(\\bm{x}_{t}), over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E17", "title": "𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t ] = s t ​ 𝒙 t + ( 1 − s t ) ​ ( 𝒙 t + t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) ) = 𝒙 t + ( 1 − s t ) ​ t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) . \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}]=\\fra", "snippet": "𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t ] = s t ​ 𝒙 t + ( 1 − s t ) ​ ( 𝒙 t + t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) ) = 𝒙 t + ( 1 − s t ) ​ t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) . \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}]=\\frac{s}{t}\\bm{x}_{t}+\\left(1-\\frac{s}{t}\\right)\\left(\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E18", "title": "h ( 𝔼 [ 𝒙 s ∣ 𝒙 t ] ) < h ( 𝒙 t ) , ∀ s , t ∈ [ 0 , T ] : 0 < t ≤ R 2 ​ D , 0 ≤ s < t ⋅ min { 1 , R 2 / D − 2 ​ t 2 R 2 / D − t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t", "snippet": "h ( 𝔼 [ 𝒙 s ∣ 𝒙 t ] ) < h ( 𝒙 t ) , ∀ s , t ∈ [ 0 , T ] : 0 < t ≤ R 2 ​ D , 0 ≤ s < t ⋅ min { 1 , R 2 / D − 2 ​ t 2 R 2 / D − t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}),\\qquad\\forall s,t\\in[0,T]\\colon\\quad 0<t\\leq\\frac{R}{\\sqrt{2D}},\\quad 0\\leq s"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E19", "title": "p ¯ ​ ( 𝝃 ) ≐ ( p t ∘ 𝒙 ¯ − 1 ) ​ ( 𝝃 ) det ( 𝒙 ¯ ′ ​ ( 𝒙 ¯ − 1 ​ ( 𝝃 ) ) ) , \\bar{p}(\\bm{\\xi})\\doteq\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi", "snippet": "p ¯ ​ ( 𝝃 ) ≐ ( p t ∘ 𝒙 ¯ − 1 ) ​ ( 𝝃 ) det ( 𝒙 ¯ ′ ​ ( 𝒙 ¯ − 1 ​ ( 𝝃 ) ) ) , \\bar{p}(\\bm{\\xi})\\doteq\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi})))}, over¯ start_ARG italic_p end_ARG ( bold_italic_ξ ) ≐ divide start_ARG ( i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E30", "title": "det ( 𝑴 ) 1 / D = ∏ i = 1 D λ i ​ ( 𝑴 ) 1 / D ≤ ∑ i = 1 D λ i ​ ( 𝑴 ) D = tr ⁡ ( 𝑴 ) D , \\det(\\bm{M})^{1/D}=\\prod_{i=1}^{D}\\lambda_{i}(\\bm{M})^{1/D}\\leq\\frac{\\sum_{i=1}^{D}\\lambda_{i}(\\bm{M})}{D}=\\fra", "snippet": "det ( 𝑴 ) 1 / D = ∏ i = 1 D λ i ​ ( 𝑴 ) 1 / D ≤ ∑ i = 1 D λ i ​ ( 𝑴 ) D = tr ⁡ ( 𝑴 ) D , \\det(\\bm{M})^{1/D}=\\prod_{i=1}^{D}\\lambda_{i}(\\bm{M})^{1/D}\\leq\\frac{\\sum_{i=1}^{D}\\lambda_{i}(\\bm{M})}{D}=\\frac{\\operatorname{tr}(\\bm{M})}{D}, roman_det ( bold_italic_M ) start_POSTSUPERSCRI"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E35", "title": "| Δ log p t ( 𝝃 ) | ≤ max ( D t 2 , | R 2 t 4 − D t 2 | ) = : U t . \\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right", "snippet": "| Δ log p t ( 𝝃 ) | ≤ max ( D t 2 , | R 2 t 4 − D t 2 | ) = : U t . \\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right)=:U_{t}. | roman_Δ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E36", "title": "− ( 1 − s t ) ​ t 2 D ​ U t ≤ ( 1 − s t ) ​ t 2 D ​ Δ ​ log ⁡ p t ​ ( 𝝃 ) ≤ ( 1 − s t ) ​ t 2 D ​ U t . -\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Del", "snippet": "− ( 1 − s t ) ​ t 2 D ​ U t ≤ ( 1 − s t ) ​ t 2 D ​ Δ ​ log ⁡ p t ​ ( 𝝃 ) ≤ ( 1 − s t ) ​ t 2 D ​ U t . -\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}. - divi"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E46", "title": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) − h ​ ( 𝒙 t ) ≤ − M ​ ( s , t , D ) ​ ∫ ℝ D ‖ ∇ p t ​ ( 𝝃 ) ‖ 2 2 p t ​ ( 𝝃 ) ​ d 𝝃 < 0 h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t})\\leq-M(s,t,D)\\int_{\\mathbb{R}^{D}}\\frac{\\|\\nabla ", "snippet": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) − h ​ ( 𝒙 t ) ≤ − M ​ ( s , t , D ) ​ ∫ ℝ D ‖ ∇ p t ​ ( 𝝃 ) ‖ 2 2 p t ​ ( 𝝃 ) ​ d 𝝃 < 0 h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t})\\leq-M(s,t,D)\\int_{\\mathbb{R}^{D}}\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{2}^{2}}{p_{t}(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi}<0 italic_h ( over¯"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E47", "title": "h ​ ( 𝒙 t ) = − ∫ ℝ D p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ​ d 𝝃 . h(\\bm{x}_{t})=-\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x start_POSTSUBSCRIPT it", "snippet": "h ​ ( 𝒙 t ) = − ∫ ℝ D p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ​ d 𝝃 . h(\\bm{x}_{t})=-\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPE"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E48", "title": "g ​ ( 𝝃 ) ≐ − p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ⟹ h ​ ( 𝒙 t ) = ∫ ℝ D g ​ ( 𝝃 ) ​ d 𝝃 . g(\\bm{\\xi})\\doteq-p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\implies h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g(\\bm{\\xi})\\mathr", "snippet": "g ​ ( 𝝃 ) ≐ − p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ⟹ h ​ ( 𝒙 t ) = ∫ ℝ D g ​ ( 𝝃 ) ​ d 𝝃 . g(\\bm{\\xi})\\doteq-p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\implies h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_g ( bold_italic_ξ ) ≐ - italic_p start_POSTSUBSCRIPT italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E49", "title": "g + ​ ( 𝝃 ) ≐ max ⁡ ( g ​ ( 𝝃 ) , 0 ) , g − ​ ( 𝝃 ) ≐ max ⁡ ( − g ​ ( 𝝃 ) , 0 ) ⟹ g = g + − g − and g + , g − ≥ 0 . g_{+}(\\bm{\\xi})\\doteq\\max(g(\\bm{\\xi}),0),\\quad g_{-}(\\bm{\\xi})\\doteq\\max(-g(\\bm{\\xi}", "snippet": "g + ​ ( 𝝃 ) ≐ max ⁡ ( g ​ ( 𝝃 ) , 0 ) , g − ​ ( 𝝃 ) ≐ max ⁡ ( − g ​ ( 𝝃 ) , 0 ) ⟹ g = g + − g − and g + , g − ≥ 0 . g_{+}(\\bm{\\xi})\\doteq\\max(g(\\bm{\\xi}),0),\\quad g_{-}(\\bm{\\xi})\\doteq\\max(-g(\\bm{\\xi}),0)\\quad\\implies\\quad g=g_{+}-g_{-}\\quad\\text{and}\\quad g_{+},g_{-}\\geq 0. ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E50", "title": "h ​ ( 𝒙 t ) = ∫ ℝ D g + ​ ( 𝝃 ) ​ d 𝝃 − ∫ ℝ D g − ​ ( 𝝃 ) ​ d 𝝃 , h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g_{+}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}-\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, italic_h ", "snippet": "h ​ ( 𝒙 t ) = ∫ ℝ D g + ​ ( 𝝃 ) ​ d 𝝃 − ∫ ℝ D g − ​ ( 𝝃 ) ​ d 𝝃 , h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g_{+}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}-\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∫ start_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E51", "title": "g ​ ( 𝝃 ) ≤ 0 ⇔ p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ≥ 0 ⇔ log ⁡ p t ​ ( 𝝃 ) ≥ 0 ⇔ p t ​ ( 𝝃 ) ≥ 1 . g(\\bm{\\xi})\\leq 0\\iff p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\geq 0\\iff\\log p_{t}(\\bm{\\xi})\\geq 0\\iff p_{t}(", "snippet": "g ​ ( 𝝃 ) ≤ 0 ⇔ p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ≥ 0 ⇔ log ⁡ p t ​ ( 𝝃 ) ≥ 0 ⇔ p t ​ ( 𝝃 ) ≥ 1 . g(\\bm{\\xi})\\leq 0\\iff p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\geq 0\\iff\\log p_{t}(\\bm{\\xi})\\geq 0\\iff p_{t}(\\bm{\\xi})\\geq 1. italic_g ( bold_italic_ξ ) ≤ 0 ⇔ italic_p start_POSTSUBSCRIPT i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E52", "title": "g − ​ ( 𝝃 ) = 𝟏 ​ ( p t ​ ( 𝝃 ) ≥ 1 ) ⋅ ( − g ​ ( 𝝃 ) ) = 𝟏 ​ ( p t ​ ( 𝝃 ) ≥ 1 ) ​ p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) . g_{-}(\\bm{\\xi})=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)\\cdot(-g(\\bm{\\xi}))=\\mathbf{1}(p_", "snippet": "g − ​ ( 𝝃 ) = 𝟏 ​ ( p t ​ ( 𝝃 ) ≥ 1 ) ⋅ ( − g ​ ( 𝝃 ) ) = 𝟏 ​ ( p t ​ ( 𝝃 ) ≥ 1 ) ​ p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) . g_{-}(\\bm{\\xi})=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)\\cdot(-g(\\bm{\\xi}))=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi}). italic_g start_POSTSUBS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E53", "title": "max 𝝃 ∈ ℝ D φ t ( 𝝃 − 𝒙 ) = φ t ( 𝟎 ) = 1 ( 2 ​ π ) D / 2 ​ t D = : C t . \\max_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})=\\varphi_{t}(\\bm{0})=\\frac{1}{(2\\pi)^{D/2}t^{D}}=:C_{t}. roman_max", "snippet": "max 𝝃 ∈ ℝ D φ t ( 𝝃 − 𝒙 ) = φ t ( 𝟎 ) = 1 ( 2 ​ π ) D / 2 ​ t D = : C t . \\max_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})=\\varphi_{t}(\\bm{0})=\\frac{1}{(2\\pi)^{D/2}t^{D}}=:C_{t}. roman_max start_POSTSUBSCRIPT bold_italic_ξ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E54", "title": "p t ​ ( 𝝃 ) = 𝔼 ⁡ φ t ​ ( 𝝃 − 𝒙 ) ≤ 𝔼 ⁡ C t = C t . p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\leq\\operatorname{\\mathbb{E}}C_{t}=C_{t}. italic_p start_POSTSUBSCRIPT italic_t", "snippet": "p t ​ ( 𝝃 ) = 𝔼 ⁡ φ t ​ ( 𝝃 − 𝒙 ) ≤ 𝔼 ⁡ C t = C t . p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\leq\\operatorname{\\mathbb{E}}C_{t}=C_{t}. italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = blackboard_E italic_φ start_POSTSUBSCRIPT"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E59", "title": "∫ ℝ D Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ d 𝝃 = − ∫ ℝ D ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ ​ d 𝝃 . \\int_{\\mathbb{R}^{D}}\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]\\mathrm{d}\\bm{\\xi}=", "snippet": "∫ ℝ D Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ d 𝝃 = − ∫ ℝ D ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ ​ d 𝝃 . \\int_{\\mathbb{R}^{D}}\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]\\mathrm{d}\\bm{\\xi}=-\\int_{\\mathbb{R}^{D}}\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\r"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E60", "title": "∫ 𝒦 { ψ ​ ( 𝝃 ) ​ Δ ​ ϕ ​ ( 𝝃 ) + ⟨ ∇ ψ ​ ( 𝝃 ) , ∇ ϕ ​ ( 𝝃 ) ⟩ } ​ d 𝝃 = ∫ ∂ 𝒦 ψ ​ ( 𝝃 ) ​ ⟨ ∇ ϕ ​ ( 𝝃 ) , 𝒏 ​ ( 𝝃 ) ⟩ ​ d σ ​ ( 𝝃 ) \\int_{\\mathcal{K}}\\left\\{\\psi(\\bm{\\xi})\\Delta\\phi(\\bm{\\xi})+\\langl", "snippet": "∫ 𝒦 { ψ ​ ( 𝝃 ) ​ Δ ​ ϕ ​ ( 𝝃 ) + ⟨ ∇ ψ ​ ( 𝝃 ) , ∇ ϕ ​ ( 𝝃 ) ⟩ } ​ d 𝝃 = ∫ ∂ 𝒦 ψ ​ ( 𝝃 ) ​ ⟨ ∇ ϕ ​ ( 𝝃 ) , 𝒏 ​ ( 𝝃 ) ⟩ ​ d σ ​ ( 𝝃 ) \\int_{\\mathcal{K}}\\left\\{\\psi(\\bm{\\xi})\\Delta\\phi(\\bm{\\xi})+\\langle\\nabla\\psi(\\bm{\\xi}),\\nabla\\phi(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi}=\\int"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E73", "title": "− 2 ​ r ​ ‖ 𝒙 ‖ 2 − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ ⟨ 𝝃 , 𝒙 ⟩ − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ r ​ ‖ 𝒙 ‖ 2 − ‖ 𝒙 ‖ 2 2 . -2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2r\\|\\bm{x}\\|_{2", "snippet": "− 2 ​ r ​ ‖ 𝒙 ‖ 2 − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ ⟨ 𝝃 , 𝒙 ⟩ − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ r ​ ‖ 𝒙 ‖ 2 − ‖ 𝒙 ‖ 2 2 . -2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}. - 2 italic_r ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E74", "title": "− 2 ​ R ​ ( r + R ) ≤ 2 ​ ⟨ 𝝃 , 𝒙 ⟩ − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ R ​ r . -2R(r+R)\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2Rr. - 2 italic_R ( italic_r + italic_R ) ≤ 2 ⟨ bold_italic_ξ , bold_it", "snippet": "− 2 ​ R ​ ( r + R ) ≤ 2 ​ ⟨ 𝝃 , 𝒙 ⟩ − ‖ 𝒙 ‖ 2 2 ≤ 2 ​ R ​ r . -2R(r+R)\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2Rr. - 2 italic_R ( italic_r + italic_R ) ≤ 2 ⟨ bold_italic_ξ , bold_italic_x ⟩ - ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E75", "title": "C t ​ e − [ r 2 + 2 ​ R ​ ( r + R ) ] / ( 2 ​ t 2 ) ≤ p t ​ ( 𝝃 ) ≤ C t ​ e [ − r 2 + 2 ​ R ​ r ] / ( 2 ​ t 2 ) . C_{t}e^{-[r^{2}+2R(r+R)]/(2t^{2})}\\leq p_{t}(\\bm{\\xi})\\leq C_{t}e^{[-r^{2}+2Rr]/(2t^{2", "snippet": "C t ​ e − [ r 2 + 2 ​ R ​ ( r + R ) ] / ( 2 ​ t 2 ) ≤ p t ​ ( 𝝃 ) ≤ C t ​ e [ − r 2 + 2 ​ R ​ r ] / ( 2 ​ t 2 ) . C_{t}e^{-[r^{2}+2R(r+R)]/(2t^{2})}\\leq p_{t}(\\bm{\\xi})\\leq C_{t}e^{[-r^{2}+2Rr]/(2t^{2})}. italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_e start_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E84", "title": "− r ​ ( R + r ) t 2 ⋅ C t ​ e [ − r 2 + 2 ​ R ​ r ] / ( 2 ​ t 2 ) ≤ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ≤ − r ​ ( r − R ) t 2 ⋅ C t ​ e − [ r 2 + 2 ​ R ​ ( r + R ) ] / ( 2 ​ t 2 ) . -\\frac{r(R+r)}{t^{2}}\\cdot C_{t}", "snippet": "− r ​ ( R + r ) t 2 ⋅ C t ​ e [ − r 2 + 2 ​ R ​ r ] / ( 2 ​ t 2 ) ≤ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ≤ − r ​ ( r − R ) t 2 ⋅ C t ​ e − [ r 2 + 2 ​ R ​ ( r + R ) ] / ( 2 ​ t 2 ) . -\\frac{r(R+r)}{t^{2}}\\cdot C_{t}e^{[-r^{2}+2Rr]/(2t^{2})}\\leq\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\leq-\\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E85", "title": "[ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ = poly ​ ( r , R , t − 1 , c ) ​ e − Θ r ​ ( r 2 ) [c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle=\\mathrm{poly}(r,R,t^{-1},c)e^", "snippet": "[ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ = poly ​ ( r , R , t − 1 , c ) ​ e − Θ r ​ ( r 2 ) [c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle=\\mathrm{poly}(r,R,t^{-1},c)e^{-\\Theta_{r}(r^{2})} [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E86", "title": "1 r ​ ∫ ∂ B r ​ ( 𝟎 ) [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ​ d 𝝃 = poly ​ ( r , R , t − 1 , c ) ​ e − Θ r ​ ( r 2 ) \\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle", "snippet": "1 r ​ ∫ ∂ B r ​ ( 𝟎 ) [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ​ d 𝝃 = poly ​ ( r , R , t − 1 , c ) ​ e − Θ r ​ ( r 2 ) \\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\mathrm{d}\\bm{\\xi}=\\mathrm{poly}(r,R,t^{-1"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E87", "title": "lim r → ∞ 1 r ​ ∫ ∂ B r ​ ( 𝟎 ) [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ​ d 𝝃 = 0 . \\lim_{r\\to\\infty}\\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\", "snippet": "lim r → ∞ 1 r ​ ∫ ∂ B r ​ ( 𝟎 ) [ c + log ⁡ p t ​ ( 𝝃 ) ] ​ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ​ d 𝝃 = 0 . \\lim_{r\\to\\infty}\\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\mathrm{d}\\bm{\\xi}=0. roman_lim start_POSTSUBSCRIPT italic_r"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E90", "title": "𝒙 ¯ ′ ​ ( 𝝃 ) = 𝑰 + ( 1 − s t ) ​ t 2 ​ ∇ 2 log ⁡ p t ​ ( 𝝃 ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¯ start_ARG bold_italic_x end_", "snippet": "𝒙 ¯ ′ ​ ( 𝝃 ) = 𝑰 + ( 1 − s t ) ​ t 2 ​ ∇ 2 log ⁡ p t ​ ( 𝝃 ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_ξ ) = bold_italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E91", "title": "∇ 2 log ⁡ p t ​ ( 𝝃 ) = p t ​ ( 𝝃 ) ​ ∇ 2 p t ​ ( 𝝃 ) − ( ∇ p t ​ ( 𝝃 ) ) ​ ( ∇ p t ​ ( 𝝃 ) ) ⊤ p t ​ ( 𝝃 ) 2 . \\nabla^{2}\\log p_{t}(\\bm{\\xi})=\\frac{p_{t}(\\bm{\\xi})\\nabla^{2}p_{t}(\\bm{\\xi})-(\\nabla p_", "snippet": "∇ 2 log ⁡ p t ​ ( 𝝃 ) = p t ​ ( 𝝃 ) ​ ∇ 2 p t ​ ( 𝝃 ) − ( ∇ p t ​ ( 𝝃 ) ) ​ ( ∇ p t ​ ( 𝝃 ) ) ⊤ p t ​ ( 𝝃 ) 2 . \\nabla^{2}\\log p_{t}(\\bm{\\xi})=\\frac{p_{t}(\\bm{\\xi})\\nabla^{2}p_{t}(\\bm{\\xi})-(\\nabla p_{t}(\\bm{\\xi}))(\\nabla p_{t}(\\bm{\\xi}))^{\\top}}{p_{t}(\\bm{\\xi})^{2}}. ∇ start_POS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E108", "title": "g ′ ​ ( t ∗ ) ≐ 𝒗 ⊤ ​ [ f ′ ​ ( 𝒙 + t ∗ ​ 𝒗 ) ] ​ 𝒗 > 0 g^{\\prime}(t^{\\ast})\\doteq\\bm{v}^{\\top}\\left[f^{\\prime}(\\bm{x}+t^{\\ast}\\bm{v})\\right]\\bm{v}>0 italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRI", "snippet": "g ′ ​ ( t ∗ ) ≐ 𝒗 ⊤ ​ [ f ′ ​ ( 𝒙 + t ∗ ​ 𝒗 ) ] ​ 𝒗 > 0 g^{\\prime}(t^{\\ast})\\doteq\\bm{v}^{\\top}\\left[f^{\\prime}(\\bm{x}+t^{\\ast}\\bm{v})\\right]\\bm{v}>0 italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_t start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ≐ bold_italic_v star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E109", "title": "sup 𝝃 ∈ ℝ D | Δ ​ log ⁡ p t ​ ( 𝝃 ) | ≤ max ⁡ ( D t 2 , | R t 4 − D t 2 | ) . \\sup_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R}{t^", "snippet": "sup 𝝃 ∈ ℝ D | Δ ​ log ⁡ p t ​ ( 𝝃 ) | ≤ max ⁡ ( D t 2 , | R t 4 − D t 2 | ) . \\sup_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right). roman_sup start_POSTSUBSCRIPT bold_ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E110", "title": "Δ ​ log ⁡ p t ​ ( 𝝃 ) = Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) − ‖ ∇ p t ​ ( 𝝃 ) ‖ 2 2 p t ​ ( 𝝃 ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}-\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{", "snippet": "Δ ​ log ⁡ p t ​ ( 𝝃 ) = Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) − ‖ ∇ p t ​ ( 𝝃 ) ‖ 2 2 p t ​ ( 𝝃 ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}-\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{2}^{2}}{p_{t}(\\bm{\\xi})^{2}}. roman_Δ roman_log italic_p start_POSTSUBSCRIPT ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E113", "title": "q 𝝃 ​ ( 𝒖 ) = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) ∫ ℝ D φ t ​ ( 𝝃 − 𝒗 ) ​ p ​ ( 𝒗 ) ​ d 𝒗 = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) p t ​ ( 𝝃 ) q_{\\bm{\\xi}}(\\bm{u})=\\", "snippet": "q 𝝃 ​ ( 𝒖 ) = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) ∫ ℝ D φ t ​ ( 𝝃 − 𝒗 ) ​ p ​ ( 𝒗 ) ​ d 𝒗 = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] = φ t ​ ( 𝝃 − 𝒖 ) ​ p ​ ( 𝒖 ) p t ​ ( 𝝃 ) q_{\\bm{\\xi}}(\\bm{u})=\\frac{\\varphi_{t}(\\bm{\\xi}-\\bm{u})p(\\bm{u})}{\\int_{\\mathbb{R}^{D}}\\varphi_{t}(\\bm"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E114", "title": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = ∫ ℝ D { ‖ 𝝃 − 𝒖 ‖ 2 2 − D ​ t 2 t 4 } ​ q 𝝃 ​ ( 𝒖 ) ​ d 𝒖 = 1 t 4 ​ 𝔼 ⁡ [ ‖ 𝝃 − 𝒚 𝝃 ‖ 2 2 ] − D t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\int_{\\mathbb{R}^{D}}", "snippet": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = ∫ ℝ D { ‖ 𝝃 − 𝒖 ‖ 2 2 − D ​ t 2 t 4 } ​ q 𝝃 ​ ( 𝒖 ) ​ d 𝒖 = 1 t 4 ​ 𝔼 ⁡ [ ‖ 𝝃 − 𝒚 𝝃 ‖ 2 2 ] − D t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\int_{\\mathbb{R}^{D}}\\left\\{\\frac{\\|\\bm{\\xi}-\\bm{u}\\|_{2}^{2}-Dt^{2}}{t^{4}}\\right\\}q_{\\bm{\\xi}}(\\bm{"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E115", "title": "∇ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = − 𝝃 − 𝔼 ⁡ [ 𝒚 𝝃 ] t 2 . \\frac{\\nabla p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=-\\frac{\\bm{\\xi}-\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]}{t^{2}}. divide start_ARG ∇ italic_p sta", "snippet": "∇ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = − 𝝃 − 𝔼 ⁡ [ 𝒚 𝝃 ] t 2 . \\frac{\\nabla p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=-\\frac{\\bm{\\xi}-\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]}{t^{2}}. divide start_ARG ∇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) end_ARG start_ARG "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E116", "title": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = 𝔼 ⁡ [ ‖ 𝒛 𝝃 ‖ 2 2 ] t 4 − D t 2 , ∇ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = 𝔼 ⁡ [ 𝒛 𝝃 ] t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\", "snippet": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = 𝔼 ⁡ [ ‖ 𝒛 𝝃 ‖ 2 2 ] t 4 − D t 2 , ∇ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) = 𝔼 ⁡ [ 𝒛 𝝃 ] t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\bm{\\xi}}\\|_{2}^{2}]}{t^{4}}-\\frac{D}{t^{2}},\\qquad\\frac{\\nabla p_{t}(\\bm{\\xi})}{"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E121", "title": "tr ⁡ ( Cov ⁡ ( 𝒚 𝝃 ) ) = 𝔼 ⁡ [ ‖ 𝒚 𝝃 ‖ 2 2 ] − ‖ 𝔼 ⁡ [ 𝒚 𝝃 ] ‖ 2 2 ≤ 𝔼 ⁡ [ ‖ 𝒚 𝝃 ‖ 2 2 ] ≤ R 2 . \\operatorname{tr}(\\operatorname{Cov}(\\bm{y}_{\\bm{\\xi}}))=\\operatorname{\\mathbb{E}}[\\|\\bm{y}_{\\bm{\\xi}}\\", "snippet": "tr ⁡ ( Cov ⁡ ( 𝒚 𝝃 ) ) = 𝔼 ⁡ [ ‖ 𝒚 𝝃 ‖ 2 2 ] − ‖ 𝔼 ⁡ [ 𝒚 𝝃 ] ‖ 2 2 ≤ 𝔼 ⁡ [ ‖ 𝒚 𝝃 ‖ 2 2 ] ≤ R 2 . \\operatorname{tr}(\\operatorname{Cov}(\\bm{y}_{\\bm{\\xi}}))=\\operatorname{\\mathbb{E}}[\\|\\bm{y}_{\\bm{\\xi}}\\|_{2}^{2}]-\\|\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]\\|_{2}^{2}\\leq\\operator"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E122", "title": "− D t 2 ≤ Δ ​ log ⁡ p t ​ ( 𝝃 ) ≤ R 2 t 4 − D t 2 , -\\frac{D}{t^{2}}\\leq\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}, - divide start_ARG italic_D end_ARG start_ARG italic_t start_", "snippet": "− D t 2 ≤ Δ ​ log ⁡ p t ​ ( 𝝃 ) ≤ R 2 t 4 − D t 2 , -\\frac{D}{t^{2}}\\leq\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}, - divide start_ARG italic_D end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≤ roman_Δ roman_log italic_p start"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E127", "title": "∂ p t ∂ t ​ ( 𝝃 ) = ∂ ∂ t ​ 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] = 𝔼 ⁡ [ ∂ ∂ t ​ φ t ​ ( 𝝃 − 𝒙 ) ] = ∂ φ t ∂ t ∗ p . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=\\frac{\\partial}{\\partial t}\\operatorname{\\mathbb{E}}", "snippet": "∂ p t ∂ t ​ ( 𝝃 ) = ∂ ∂ t ​ 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] = 𝔼 ⁡ [ ∂ ∂ t ​ φ t ​ ( 𝝃 − 𝒙 ) ] = ∂ φ t ∂ t ∗ p . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=\\frac{\\partial}{\\partial t}\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})]=\\operatorname{\\mathbb{E}}\\left[\\frac{\\partial}{\\p"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E128", "title": "p t = φ t ∗ p ⟹ ∇ p t = ∇ φ t ∗ p ⟹ ∇ 2 p t = ∇ 2 φ t ∗ p ⟹ Δ ​ p t = Δ ​ φ t ∗ p . p_{t}=\\varphi_{t}*p\\implies\\nabla p_{t}=\\nabla\\varphi_{t}*p\\implies\\nabla^{2}p_{t}=\\nabla^{2}\\varphi_{t}*p\\implies\\D", "snippet": "p t = φ t ∗ p ⟹ ∇ p t = ∇ φ t ∗ p ⟹ ∇ 2 p t = ∇ 2 φ t ∗ p ⟹ Δ ​ p t = Δ ​ φ t ∗ p . p_{t}=\\varphi_{t}*p\\implies\\nabla p_{t}=\\nabla\\varphi_{t}*p\\implies\\nabla^{2}p_{t}=\\nabla^{2}\\varphi_{t}*p\\implies\\Delta p_{t}=\\Delta\\varphi_{t}*p. italic_p start_POSTSUBSCRIPT italic_t end_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E133", "title": "∫ t min t max ∫ ℝ D | ∂ f t ∂ t ​ ( 𝝃 ) | ​ d 𝝃 < ∞ . \\int_{t_{\\min}}^{t_{\\max}}\\int_{\\mathbb{R}^{D}}\\left\\lvert\\frac{\\partial f_{t}}{\\partial t}(\\bm{\\xi})\\right\\rvert\\mathrm{d}\\bm{\\xi}<\\infty. ∫ star", "snippet": "∫ t min t max ∫ ℝ D | ∂ f t ∂ t ​ ( 𝝃 ) | ​ d 𝝃 < ∞ . \\int_{t_{\\min}}^{t_{\\max}}\\int_{\\mathbb{R}^{D}}\\left\\lvert\\frac{\\partial f_{t}}{\\partial t}(\\bm{\\xi})\\right\\rvert\\mathrm{d}\\bm{\\xi}<\\infty. ∫ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT end_POS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E134", "title": "d d ​ t ​ ∫ ℝ D f t ​ ( 𝝃 ) ​ d 𝝃 = ∫ ℝ D ∂ ∂ t ​ f t ​ ( 𝝃 ) ​ d 𝝃 , \\frac{\\mathrm{d}}{\\mathrm{d}t}\\int_{\\mathbb{R}^{D}}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}\\frac{\\partial}{\\partial", "snippet": "d d ​ t ​ ∫ ℝ D f t ​ ( 𝝃 ) ​ d 𝝃 = ∫ ℝ D ∂ ∂ t ​ f t ​ ( 𝝃 ) ​ d 𝝃 , \\frac{\\mathrm{d}}{\\mathrm{d}t}\\int_{\\mathbb{R}^{D}}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}\\frac{\\partial}{\\partial t}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, divide start_ARG roman_d end_ARG start_ARG"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E135", "title": "( f ∗ g ) ​ ( 𝝃 ) ≐ ∫ ℝ D f ​ ( 𝒖 ) ​ g ​ ( 𝝃 − 𝒖 ) ​ d 𝒖 (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ∗ italic_g ) ( bold_italic_ξ ) ≐ ∫ start_POST", "snippet": "( f ∗ g ) ​ ( 𝝃 ) ≐ ∫ ℝ D f ​ ( 𝒖 ) ​ g ​ ( 𝝃 − 𝒖 ) ​ d 𝒖 (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ∗ italic_g ) ( bold_italic_ξ ) ≐ ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_PO"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E136", "title": "∇ k ( f ∗ g ) = ( ∇ k f ) ∗ g . \\nabla^{k}(f*g)=(\\nabla^{k}f)*g. ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ∗ italic_g ) = ( ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT i", "snippet": "∇ k ( f ∗ g ) = ( ∇ k f ) ∗ g . \\nabla^{k}(f*g)=(\\nabla^{k}f)*g. ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ∗ italic_g ) = ( ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_f ) ∗ italic_g . (B.2.136)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E137", "title": "∇ k ( f ∗ g ) = f ∗ ( ∇ k g ) . \\nabla^{k}(f*g)=f*(\\nabla^{k}g). ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ∗ italic_g ) = italic_f ∗ ( ∇ start_POSTSUPERSCRIPT italic_k end_POSTSU", "snippet": "∇ k ( f ∗ g ) = f ∗ ( ∇ k g ) . \\nabla^{k}(f*g)=f*(\\nabla^{k}g). ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ∗ italic_g ) = italic_f ∗ ( ∇ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_g ) . (B.2.137)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E1", "title": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log ⁡ vol ⁡ ( B ϵ ) + log ⁡ ( 2 D ​ Γ ​ ( D / 2 ) ​ ( D 2 ​ e ) D / 2 ) , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log\\operatorname{vol}(B_{\\epsilon})+\\log\\left(\\frac{2}", "snippet": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log ⁡ vol ⁡ ( B ϵ ) + log ⁡ ( 2 D ​ Γ ​ ( D / 2 ) ​ ( D 2 ​ e ) D / 2 ) , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log\\operatorname{vol}(B_{\\epsilon})+\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right), caligraphic_R start_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E2", "title": "Γ ​ ( x ) ≤ 2 ​ π ​ x x − 1 / 2 ​ e − x ​ e 1 / ( 12 ​ x ) . \\Gamma(x)\\leq\\sqrt{2\\pi}x^{x-1/2}e^{-x}e^{1/(12x)}. roman_Γ ( italic_x ) ≤ square-root start_ARG 2 italic_π end_ARG italic_x start_POSTSUPE", "snippet": "Γ ​ ( x ) ≤ 2 ​ π ​ x x − 1 / 2 ​ e − x ​ e 1 / ( 12 ​ x ) . \\Gamma(x)\\leq\\sqrt{2\\pi}x^{x-1/2}e^{-x}e^{1/(12x)}. roman_Γ ( italic_x ) ≤ square-root start_ARG 2 italic_π end_ARG italic_x start_POSTSUPERSCRIPT italic_x - 1 / 2 end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E5", "title": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − O ​ ( log ⁡ D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-O(\\log D). caligraphic_R start_POSTSUBSCRIPT i", "snippet": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − O ​ ( log ⁡ D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-O(\\log D). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ italic_h ( bold_italic_x ) - roman"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E6", "title": "ℙ ⁡ [ 𝒙 ∈ A ] = ∫ A 1 vol ⁡ ( 𝒮 ) ​ d 𝒙 . \\operatorname{\\mathbb{P}}[\\bm{x}\\in A]=\\int_{A}\\frac{1}{\\operatorname{vol}(\\mathcal{S})}\\mathrm{d}\\bm{x}. blackboard_P [ bold_italic_x ∈ italic_A ] = ∫ start_", "snippet": "ℙ ⁡ [ 𝒙 ∈ A ] = ∫ A 1 vol ⁡ ( 𝒮 ) ​ d 𝒙 . \\operatorname{\\mathbb{P}}[\\bm{x}\\in A]=\\int_{A}\\frac{1}{\\operatorname{vol}(\\mathcal{S})}\\mathrm{d}\\bm{x}. blackboard_P [ bold_italic_x ∈ italic_A ] = ∫ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG ro"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E7", "title": "h ​ ( 𝒙 ) = log 2 ⁡ vol ⁡ ( 𝒮 ) . h(\\bm{x})=\\log_{2}\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( caligraphic_S ) . (B.3.7", "snippet": "h ​ ( 𝒙 ) = log 2 ⁡ vol ⁡ ( 𝒮 ) . h(\\bm{x})=\\log_{2}\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( caligraphic_S ) . (B.3.7)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E8", "title": "𝒮 δ = { 𝝃 ∈ ℝ D ∣ dist ​ ( 𝝃 , 𝒮 ) ≤ δ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_δ en", "snippet": "𝒮 δ = { 𝝃 ∈ ℝ D ∣ dist ​ ( 𝝃 , 𝒮 ) ≤ δ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = { bold_italic_ξ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E9", "title": "dist ⁡ ( 𝝃 , 𝒮 ) = inf 𝝃 ′ ∈ 𝒮 ‖ 𝝃 − 𝝃 ′ ‖ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}. roman_dist ( bold_italic_ξ", "snippet": "dist ⁡ ( 𝝃 , 𝒮 ) = inf 𝝃 ′ ∈ 𝒮 ‖ 𝝃 − 𝝃 ′ ‖ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}. roman_dist ( bold_italic_ξ , caligraphic_S ) = roman_inf start_POSTSUBSCRIPT bold_italic_ξ start_POSTSUPER"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E10", "title": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT ) ≤ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUB"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E11", "title": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT ) ≤ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUB"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E12", "title": "log 2 ⁡ vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) − log 2 ⁡ vol ⁡ ( B δ + ϵ ) − O ​ ( log ⁡ D ) ≤ R ϵ ​ ( 𝒙 ) . \\log_{2}\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))-\\log_{2}\\operatorname{vol}(B_{\\delta+\\ep", "snippet": "log 2 ⁡ vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) − log 2 ⁡ vol ⁡ ( B δ + ϵ ) − O ​ ( log ⁡ D ) ≤ R ϵ ​ ( 𝒙 ) . \\log_{2}\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))-\\log_{2}\\operatorname{vol}(B_{\\delta+\\epsilon})-O(\\log D)\\leq R_{\\epsilon}(\\bm{x}). roman_log start_POSTSUBSCRIPT 2 end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E13", "title": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) vol ⁡ ( B δ + ϵ ) \\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_itali", "snippet": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) vol ⁡ ( B δ + ϵ ) \\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT ) ) end_ARG start_ARG roman_v"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E14", "title": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) ≥ 𝒩 2 ​ δ ​ ( Supp ⁡ ( 𝒙 ) ) ​ vol ⁡ ( B δ ) . \\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))\\geq\\mathcal{N}_{2\\delta}(\\operatorname{Supp}(\\bm{x}))\\operatorname{vol}", "snippet": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) ≥ 𝒩 2 ​ δ ​ ( Supp ⁡ ( 𝒙 ) ) ​ vol ⁡ ( B δ ) . \\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))\\geq\\mathcal{N}_{2\\delta}(\\operatorname{Supp}(\\bm{x}))\\operatorname{vol}(B_{\\delta}). roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E17", "title": "log 2 ⁡ 𝒩 ϵ ​ ( Supp ⁡ ( 𝒙 ) ) − O ​ ( D ) ≤ R ϵ ​ ( 𝒙 ) , \\log_{2}\\mathcal{N}_{\\epsilon}(\\operatorname{Supp}(\\bm{x}))-O(D)\\leq R_{\\epsilon}(\\bm{x}), roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ", "snippet": "log 2 ⁡ 𝒩 ϵ ​ ( Supp ⁡ ( 𝒙 ) ) − O ​ ( D ) ≤ R ϵ ​ ( 𝒙 ) , \\log_{2}\\mathcal{N}_{\\epsilon}(\\operatorname{Supp}(\\bm{x}))-O(D)\\leq R_{\\epsilon}(\\bm{x}), roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( roman_Supp ( bold"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E23", "title": "inf 𝝃 ′ ∈ 𝒮 k ‖ 𝝃 ∥ − 𝝃 ′ ‖ 2 2 = ‖ 𝝃 ∥ − 𝝃 ∥ ‖ 𝝃 ∥ ‖ 2 ‖ 2 2 . \\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}_{k}}\\,\\left\\|\\bm{\\xi}^{\\|}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}-\\frac{\\bm{\\xi}^{", "snippet": "inf 𝝃 ′ ∈ 𝒮 k ‖ 𝝃 ∥ − 𝝃 ′ ‖ 2 2 = ‖ 𝝃 ∥ − 𝝃 ∥ ‖ 𝝃 ∥ ‖ 2 ‖ 2 2 . \\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}_{k}}\\,\\left\\|\\bm{\\xi}^{\\|}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}-\\frac{\\bm{\\xi}^{\\|}}{\\|\\bm{\\xi}^{\\|}\\|_{2}}\\right\\|_{2}^{2}. roman_inf start_POSTSUBSCRIPT bold_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E24", "title": "π S k ​ ( 𝝃 ) = 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 \\pi_{S_{k}}(\\bm{\\xi})=\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}} italic_π start_POSTSUBSCRIPT italic_S start_POSTSUBS", "snippet": "π S k ​ ( 𝝃 ) = 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 \\pi_{S_{k}}(\\bm{\\xi})=\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}} italic_π start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) = divide st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E29", "title": "‖ 𝝃 k ⟂ ‖ 2 2 + ( ‖ 𝝃 k ∥ ‖ 2 − 1 ) 2 ≤ δ 2 . \\left\\|\\bm{\\xi}^{\\perp}_{k}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{k}\\right\\|_{2}-1\\right)^{2}\\leq\\delta^{2}. ∥ bold_italic_ξ start_POSTSUPERSCRIPT ⟂", "snippet": "‖ 𝝃 k ⟂ ‖ 2 2 + ( ‖ 𝝃 k ∥ ‖ 2 − 1 ) 2 ≤ δ 2 . \\left\\|\\bm{\\xi}^{\\perp}_{k}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{k}\\right\\|_{2}-1\\right)^{2}\\leq\\delta^{2}. ∥ bold_italic_ξ start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E33", "title": "‖ 𝝃 j ⟂ ‖ 2 2 + ( ‖ 𝝃 j ∥ ‖ 2 − 1 ) 2 ≥ ( 1 − δ ) 2 . \\left\\|\\bm{\\xi}^{\\perp}_{j}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{j}\\right\\|_{2}-1\\right)^{2}\\geq(1-\\delta)^{2}. ∥ bold_italic_ξ start_POSTS", "snippet": "‖ 𝝃 j ⟂ ‖ 2 2 + ( ‖ 𝝃 j ∥ ‖ 2 − 1 ) 2 ≥ ( 1 − δ ) 2 . \\left\\|\\bm{\\xi}^{\\perp}_{j}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{j}\\right\\|_{2}-1\\right)^{2}\\geq(1-\\delta)^{2}. ∥ bold_italic_ξ start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E34", "title": "π 𝒮 ​ ( 𝝃 ) = π 𝒮 k ⋆ ​ ( 𝝃 ) , where ​ k ⋆ = arg ​ min k ∈ [ K ] ⁡ dist ⁡ ( 𝝃 , 𝒮 k ) . \\pi_{\\mathcal{S}}(\\bm{\\xi})=\\pi_{\\mathcal{S}_{k_{\\star}}}(\\bm{\\xi}),\\enspace\\text{where}\\enspace k_{\\star}=\\ope", "snippet": "π 𝒮 ​ ( 𝝃 ) = π 𝒮 k ⋆ ​ ( 𝝃 ) , where ​ k ⋆ = arg ​ min k ∈ [ K ] ⁡ dist ⁡ ( 𝝃 , 𝒮 k ) . \\pi_{\\mathcal{S}}(\\bm{\\xi})=\\pi_{\\mathcal{S}_{k_{\\star}}}(\\bm{\\xi}),\\enspace\\text{where}\\enspace k_{\\star}=\\operatorname*{arg\\ min}_{k\\in[K]}\\,\\operatorname{dist}(\\bm{\\xi},\\mathcal{S}_{k}). i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E35", "title": "𝒙 ^ δ = q ​ ( π 𝒮 ​ ( 𝒙 δ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = roman_q ( i", "snippet": "𝒙 ^ δ = q ​ ( π 𝒮 ​ ( 𝒙 δ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = roman_q ( italic_π start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E38", "title": "‖ 𝒙 δ − π 𝒮 ​ ( 𝒙 δ ) ‖ 2 2 ≤ δ 2 , \\left\\|\\bm{x}_{\\delta}-\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})\\right\\|_{2}^{2}\\leq\\delta^{2}, ∥ bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT - italic_π s", "snippet": "‖ 𝒙 δ − π 𝒮 ​ ( 𝒙 δ ) ‖ 2 2 ≤ δ 2 , \\left\\|\\bm{x}_{\\delta}-\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})\\right\\|_{2}^{2}\\leq\\delta^{2}, ∥ bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT - italic_π start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUB"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E39", "title": "𝒮 k , δ = { 𝝃 ∥ + 𝝃 ⟂ ∣ 𝝃 ∥ ∈ Span ( 𝑼 k ) , 𝝃 ⟂ ∈ Span ( 𝑼 k ) ⟂ , ∥ 𝝃 ⟂ ∥ 2 2 + ( ∥ 𝝃 ∥ ∥ 2 − 1 ) 2 ≤ δ } . \\mathcal{S}_{k,\\delta}=\\left\\{\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}\\mid\\bm{\\xi}^{\\|}\\in\\operatorn", "snippet": "𝒮 k , δ = { 𝝃 ∥ + 𝝃 ⟂ ∣ 𝝃 ∥ ∈ Span ( 𝑼 k ) , 𝝃 ⟂ ∈ Span ( 𝑼 k ) ⟂ , ∥ 𝝃 ⟂ ∥ 2 2 + ( ∥ 𝝃 ∥ ∥ 2 − 1 ) 2 ≤ δ } . \\mathcal{S}_{k,\\delta}=\\left\\{\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}\\mid\\bm{\\xi}^{\\|}\\in\\operatorname{Span}(\\bm{U}_{k}),\\bm{\\xi}^{\\perp}\\in\\operatorname{Span}(\\bm{U}_{k})^{\\perp}"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E40", "title": "π 𝒮 k − 1 ( 𝝃 ) = { r 𝝃 + 𝝃 ⟂ ∣ r > 0 , 𝝃 ⟂ ∈ Span ( 𝑼 k ) ⟂ , ∥ 𝝃 ⟂ ∥ 2 2 + ( r − 1 ) 2 ≤ δ } . \\pi_{\\mathcal{S}_{k}}^{-1}(\\bm{\\xi})=\\left\\{r\\bm{\\xi}+\\bm{\\xi}^{\\perp}\\mid r>0,\\bm{\\xi}^{\\perp}\\in\\oper", "snippet": "π 𝒮 k − 1 ( 𝝃 ) = { r 𝝃 + 𝝃 ⟂ ∣ r > 0 , 𝝃 ⟂ ∈ Span ( 𝑼 k ) ⟂ , ∥ 𝝃 ⟂ ∥ 2 2 + ( r − 1 ) 2 ≤ δ } . \\pi_{\\mathcal{S}_{k}}^{-1}(\\bm{\\xi})=\\left\\{r\\bm{\\xi}+\\bm{\\xi}^{\\perp}\\mid r>0,\\bm{\\xi}^{\\perp}\\in\\operatorname{Span}(\\bm{U}_{k})^{\\perp},\\left\\|\\bm{\\xi}^{\\perp}\\right\\|_{2}^{2}+\\left"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E41", "title": "vol ⁡ ( 𝒮 k , δ ) = ∬ Span ( 𝑼 k ) × Span ( 𝑼 k ) ⟂ 𝟏 ‖ 𝝃 ⟂ ‖ 2 2 + ( ‖ 𝝃 ∥ ‖ 2 − 1 ) 2 ≤ δ ​ d 𝝃 ∥ ​ d 𝝃 ⟂ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\iint_{\\operatorname{Span}(\\bm{U}_{k})\\times\\op", "snippet": "vol ⁡ ( 𝒮 k , δ ) = ∬ Span ( 𝑼 k ) × Span ( 𝑼 k ) ⟂ 𝟏 ‖ 𝝃 ⟂ ‖ 2 2 + ( ‖ 𝝃 ∥ ‖ 2 − 1 ) 2 ≤ δ ​ d 𝝃 ∥ ​ d 𝝃 ⟂ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\iint_{\\operatorname{Span}(\\bm{U}_{k})\\times\\operatorname{Span}(\\bm{U}_{k})^{\\perp}}\\mathbf{1}_{\\left\\|\\bm{\\xi}^{\\perp}\\right\\|"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E42", "title": "vol ⁡ ( 𝒮 k , δ ) = ∫ [ 0 , ∞ ) ∫ 𝕊 d k − 1 ∫ Span ( 𝑼 k ) ⟂ r d k − 1 ​ 𝟏 ‖ 𝝃 ⟂ ‖ 2 2 + ( r − 1 ) 2 ≤ δ ​ d r ​ d 𝜽 d k ​ d 𝝃 ⟂ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\int_{[0,\\infty)}\\int_{\\ma", "snippet": "vol ⁡ ( 𝒮 k , δ ) = ∫ [ 0 , ∞ ) ∫ 𝕊 d k − 1 ∫ Span ( 𝑼 k ) ⟂ r d k − 1 ​ 𝟏 ‖ 𝝃 ⟂ ‖ 2 2 + ( r − 1 ) 2 ≤ δ ​ d r ​ d 𝜽 d k ​ d 𝝃 ⟂ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\int_{[0,\\infty)}\\int_{\\mathbb{S}^{d_{k}-1}}\\int_{\\operatorname{Span}(\\bm{U}_{k})^{\\perp}}r^{d_{k}-1}\\math"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E43", "title": "𝔼 ​ [ ‖ π 𝒮 ​ ( 𝒙 δ ) − q ​ ( π 𝒮 ​ ( 𝒙 δ ) ) ‖ 2 2 ] = 𝔼 ​ [ ‖ 𝒙 − q ​ ( 𝒙 ) ‖ 2 2 ] ≤ ϵ 2 , \\mathbb{E}\\left[\\left\\|\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})-\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}))\\r", "snippet": "𝔼 ​ [ ‖ π 𝒮 ​ ( 𝒙 δ ) − q ​ ( π 𝒮 ​ ( 𝒙 δ ) ) ‖ 2 2 ] = 𝔼 ​ [ ‖ 𝒙 − q ​ ( 𝒙 ) ‖ 2 2 ] ≤ ϵ 2 , \\mathbb{E}\\left[\\left\\|\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})-\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}))\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|\\bm{x}-\\mathrm{q}(\\bm{x})\\right\\|_{"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E44", "title": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) , R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}), italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R δ + ϵ ​ ( 𝒙 δ ) ≤ R ϵ ​ ( 𝒙 ) , R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}), italic_R start_POSTSUBSCRIPT italic_δ + italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT ) ≤ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUB"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx126", "title": "h ​ ( 𝒙 ε ) \\displaystyle h(\\bm{x}_{\\varepsilon}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ) = − ∫ ℝ D p ε ​ ( x ) ​ log ⁡ p ε ​ ( x ) ​ d x \\displaystyle=-\\int_{\\mathbb", "snippet": "h ​ ( 𝒙 ε ) \\displaystyle h(\\bm{x}_{\\varepsilon}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ) = − ∫ ℝ D p ε ​ ( x ) ​ log ⁡ p ε ​ ( x ) ​ d x \\displaystyle=-\\int_{\\mathbb{R}^{D}}p_{\\varepsilon}(x)\\log p_{\\varepsilon}(x)\\mathrm{d}x = - ∫ start_POSTSUB"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx127", "title": "d d ​ t ​ h ​ ( 𝒙 t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t", "snippet": "d d ​ t ​ h ​ ( 𝒙 t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = − d d ​ t ​ ∫ ℝ D p t ​ ( 𝝃 ) ​ log ⁡ p t ​ ( 𝝃 ) ​ d 𝝃 \\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx128", "title": "d d ​ t ​ h ​ ( 𝒙 t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t", "snippet": "d d ​ t ​ h ​ ( 𝒙 t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = t ​ ∫ ℝ D ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ ​ d 𝝃 \\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx129", "title": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t})) italic_h ( over¯ start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) = − ∫ 𝒳 ( p t ∘ 𝒙 ¯ ", "snippet": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t})) italic_h ( over¯ start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) = − ∫ 𝒳 ( p t ∘ 𝒙 ¯ − 1 ) ​ ( 𝝃 ) det ( 𝒙 ¯ ′ ​ ( 𝒙 ¯ − 1 ​ ( 𝝃 ) ) ) ​ log ⁡ ( p t ∘ 𝒙 ¯ − 1 ) ​ ( "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx130", "title": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) − h ​ ( 𝒙 t ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t}) italic_h ( over¯ start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCR", "snippet": "h ​ ( 𝒙 ¯ ​ ( 𝒙 t ) ) − h ​ ( 𝒙 t ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t}) italic_h ( over¯ start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) - italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIP"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx131", "title": "∫ ℝ D p t ​ ( 𝝃 ) ​ det ( 𝑰 + ( 1 − s t ) ​ t 2 ​ ∇ 2 log ⁡ p t ​ ( 𝝃 ) ) ​ d ​ 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\det\\left(\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(", "snippet": "∫ ℝ D p t ​ ( 𝝃 ) ​ det ( 𝑰 + ( 1 − s t ) ​ t 2 ​ ∇ 2 log ⁡ p t ​ ( 𝝃 ) ) ​ d ​ 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\det\\left(\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi})\\right)\\mathrm{d}\\bm{\\xi} ∫ start_POSTSUBSCRIPT blackboard_R start_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx132", "title": "( 1 + x ) d \\displaystyle(1+x)^{d} ( 1 + italic_x ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ≤ ( 1 − ( 1 − s t ) ​ t 2 ​ U t D ) D + [ ( 1 + ( 1 − s t ) ​ t 2 ​ U t D ) D − ( 1 − ( 1 − s t )", "snippet": "( 1 + x ) d \\displaystyle(1+x)^{d} ( 1 + italic_x ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ≤ ( 1 − ( 1 − s t ) ​ t 2 ​ U t D ) D + [ ( 1 + ( 1 − s t ) ​ t 2 ​ U t D ) D − ( 1 − ( 1 − s t ) ​ t 2 ​ U t D ) D ] ⏟ M ​ ( s , t , D ) ​ x \\displaystyle\\leq\\left(1-\\frac{\\lef"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx133", "title": "− ( 1 − s t ) ​ t 2 D ​ U t \\displaystyle-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t} - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPE", "snippet": "− ( 1 − s t ) ​ t 2 D ​ U t \\displaystyle-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t} - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG italic_U start_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx134", "title": "∫ ℝ D p t ​ ( 𝝃 ) ​ ( 1 + ( 1 − s t ) ​ t 2 D ​ Δ ​ log ⁡ p t ​ ( 𝝃 ) ) D ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\", "snippet": "∫ ℝ D p t ​ ( 𝝃 ) ​ ( 1 + ( 1 − s t ) ​ t 2 D ​ Δ ​ log ⁡ p t ​ ( 𝝃 ) ) D ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\xi})\\right)^{D}\\mathrm{d}\\bm{\\xi} ∫ start_POSTSUBSCRIPT blackboard_R start_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx135", "title": "∫ ℝ D g − ​ ( 𝝃 ) ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ital", "snippet": "∫ ℝ D g − ​ ( 𝝃 ) ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ( bold_italic_ξ ) roman_d bold_ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx136", "title": "∫ B r ​ ( 𝟎 ) { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 \\displaystyle\\int_{B_{r}(\\bm{0})}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\la", "snippet": "∫ B r ​ ( 𝟎 ) { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 \\displaystyle\\int_{B_{r}(\\bm{0})}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx137", "title": "∫ ℝ D { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\na", "snippet": "∫ ℝ D { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi} "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx138", "title": "p t ​ ( 𝝃 ) \\displaystyle p_{t}(\\bm{\\xi}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\x", "snippet": "p t ​ ( 𝝃 ) \\displaystyle p_{t}(\\bm{\\xi}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})] = blackboard_E [ italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSC"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx139", "title": "⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ \\displaystyle\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle ⟨ ∇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) , bold_italic_ξ ⟩ = ⟨ − 1 t 2 ​ 𝔼 ⁡ [ ( 𝝃", "snippet": "⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ \\displaystyle\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle ⟨ ∇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) , bold_italic_ξ ⟩ = ⟨ − 1 t 2 ​ 𝔼 ⁡ [ ( 𝝃 − 𝒙 ) ​ φ t ​ ( 𝝃 − 𝒙 ) ] , 𝝃 ⟩ \\displaystyle=\\left\\langle-\\frac{1}{t^{2}}\\oper"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx140", "title": "1 t 2 ​ 𝔼 ⁡ [ ( − R ​ r − r 2 ) ​ φ t ​ ( 𝝃 − 𝒙 ) ] ≤ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ≤ 1 t 2 ​ 𝔼 ⁡ [ ( R ​ r − r 2 ) ​ φ t ​ ( 𝝃 − 𝒙 ) ] \\displaystyle\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(-Rr-r^{", "snippet": "1 t 2 ​ 𝔼 ⁡ [ ( − R ​ r − r 2 ) ​ φ t ​ ( 𝝃 − 𝒙 ) ] ≤ ⟨ ∇ p t ​ ( 𝝃 ) , 𝝃 ⟩ ≤ 1 t 2 ​ 𝔼 ⁡ [ ( R ​ r − r 2 ) ​ φ t ​ ( 𝝃 − 𝒙 ) ] \\displaystyle\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(-Rr-r^{2}\\right)\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right]\\leq\\langle\\nabla p_{t}(\\bm{\\xi}),\\b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx141", "title": "∫ ℝ D { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 = 0 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langl", "snippet": "∫ ℝ D { Δ ​ p t ​ ( 𝝃 ) ​ [ c + log ⁡ p t ​ ( 𝝃 ) ] + ⟨ ∇ log ⁡ p t ​ ( 𝝃 ) , ∇ p t ​ ( 𝝃 ) ⟩ } ​ d 𝝃 = 0 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx142", "title": "𝒙 ¯ ′ ​ ( 𝝃 ) \\displaystyle\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi}) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_ξ ) = 𝑰 + ( 1 − s t ) ​ t 2 ​ p t ​ ( 𝝃 ) ​ ∇", "snippet": "𝒙 ¯ ′ ​ ( 𝝃 ) \\displaystyle\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi}) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_ξ ) = 𝑰 + ( 1 − s t ) ​ t 2 ​ p t ​ ( 𝝃 ) ​ ∇ 2 p t ​ ( 𝝃 ) − ( ∇ p t ​ ( 𝝃 ) ) ​ ( ∇ p t ​ ( 𝝃 ) ) ⊤ p t ​ ( 𝝃 ) 2 \\displays"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx143", "title": "𝒗 ⊤ ​ [ 𝒙 ¯ ′ ​ ( 𝝃 ) ] ​ 𝒗 \\displaystyle\\bm{v}^{\\top}[\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})]\\bm{v} bold_italic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT [ over¯ start_ARG bold_italic_x end_ARG start_PO", "snippet": "𝒗 ⊤ ​ [ 𝒙 ¯ ′ ​ ( 𝝃 ) ] ​ 𝒗 \\displaystyle\\bm{v}^{\\top}[\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})]\\bm{v} bold_italic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT [ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( bold_italic_ξ ) ] bold_italic_v (B.2.94) ="}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx144", "title": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) \\displaystyle\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})} divide start_ARG roman_Δ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) end_ARG start", "snippet": "Δ ​ p t ​ ( 𝝃 ) p t ​ ( 𝝃 ) \\displaystyle\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})} divide start_ARG roman_Δ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) e"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx145", "title": "Δ ​ log ⁡ p t ​ ( 𝝃 ) \\displaystyle\\Delta\\log p_{t}(\\bm{\\xi}) roman_Δ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = 𝔼 ⁡ [ ‖ 𝒛 𝝃 ‖ 2 2 ] t 4 − D t 2 − ‖ 𝔼 ⁡ [ 𝒛 ", "snippet": "Δ ​ log ⁡ p t ​ ( 𝝃 ) \\displaystyle\\Delta\\log p_{t}(\\bm{\\xi}) roman_Δ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = 𝔼 ⁡ [ ‖ 𝒛 𝝃 ‖ 2 2 ] t 4 − D t 2 − ‖ 𝔼 ⁡ [ 𝒛 𝝃 ] ‖ 2 2 t 4 \\displaystyle=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\bm{\\xi}}\\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx146", "title": "∂ p t ∂ t ​ ( 𝝃 ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi}) divide start_ARG ∂ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∂ italic_t end_ARG ( bold_italic", "snippet": "∂ p t ∂ t ​ ( 𝝃 ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi}) divide start_ARG ∂ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∂ italic_t end_ARG ( bold_italic_ξ ) = 𝔼 ⁡ [ φ t ​ ( 𝝃 − 𝒙 ) ⋅ ‖ 𝝃 − 𝒙 ‖ 2 2 − D ​ t 2 t 3 ] \\displaystyle=\\oper"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx147", "title": "∂ ∂ t ​ φ t ​ ( 𝝃 ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ∂ end_ARG start_ARG ∂ italic_t end_ARG italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold", "snippet": "∂ ∂ t ​ φ t ​ ( 𝝃 ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ∂ end_ARG start_ARG ∂ italic_t end_ARG italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) = φ t ​ ( 𝝃 ) ⋅ ‖ 𝝃 ‖ 2 2 − D ​ t 2 t 3 \\displaystyle=\\varphi_{t}(\\b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx148", "title": "log ⁡ ( 2 D ​ Γ ​ ( D / 2 ) ​ ( D 2 ​ e ) D / 2 ) \\displaystyle\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right) roman_log ( divide start_ARG 2 end_ARG start_ARG italic_D roman_Γ ", "snippet": "log ⁡ ( 2 D ​ Γ ​ ( D / 2 ) ​ ( D 2 ​ e ) D / 2 ) \\displaystyle\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right) roman_log ( divide start_ARG 2 end_ARG start_ARG italic_D roman_Γ ( italic_D / 2 ) end_ARG ( divide start_ARG italic_D end_ARG start_ARG 2 italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx149", "title": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) vol ⁡ ( B δ + ϵ ) \\displaystyle\\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp", "snippet": "vol ⁡ ( Supp ⁡ ( 𝒙 δ ) ) vol ⁡ ( B δ + ϵ ) \\displaystyle\\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT ) ) end_ARG star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx150", "title": "𝒮 δ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = { 𝝃 ∈ ℝ D ∣ ∃ k ∈ [ K ] : dist ⁡ ( 𝝃 , 𝒮 k ) ≤ δ } \\displaystyle=\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\ex", "snippet": "𝒮 δ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = { 𝝃 ∈ ℝ D ∣ ∃ k ∈ [ K ] : dist ⁡ ( 𝝃 , 𝒮 k ) ≤ δ } \\displaystyle=\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\exists k\\in[K]\\>:\\>\\operatorname{dist}(\\bm{\\xi},\\mathcal{S}_{k})\\leq\\delta\\} = { b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx151", "title": "‖ 𝝃 − 𝝃 ′ ‖ 2 2 = ‖ 𝝃 ∥ + 𝝃 ⟂ − 𝝃 ′ ‖ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2} ∥ bold_italic_ξ - bold_", "snippet": "‖ 𝝃 − 𝝃 ′ ‖ 2 2 = ‖ 𝝃 ∥ + 𝝃 ⟂ − 𝝃 ′ ‖ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2} ∥ bold_italic_ξ - bold_italic_ξ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx152", "title": "𝒮 δ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = ⋃ k ∈ [ K ] { 𝝃 ∈ ℝ D ∣ ‖ 𝝃 − 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 ‖ 2 ≤ δ } . \\displaystyle=\\bigcup_{k\\", "snippet": "𝒮 δ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT = ⋃ k ∈ [ K ] { 𝝃 ∈ ℝ D ∣ ‖ 𝝃 − 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 ‖ 2 ≤ δ } . \\displaystyle=\\bigcup_{k\\in[K]}\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx153", "title": "‖ 𝝃 − 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 ‖ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}}\\right\\|_{2}^{2} ∥ bold_italic_ξ - divide start_AR", "snippet": "‖ 𝝃 − 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 ‖ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}}\\right\\|_{2}^{2} ∥ bold_italic_ξ - divide start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U sta"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx154", "title": "‖ 𝝃 j ∥ ‖ 2 = ‖ 𝑼 j ​ 𝑼 j ⊤ ​ 𝝃 ‖ 2 \\displaystyle\\left\\|\\bm{\\xi}_{j}^{\\|}\\right\\|_{2}=\\left\\|\\bm{U}_{j}\\bm{U}_{j}^{\\top}\\bm{\\xi}\\right\\|_{2} ∥ bold_italic_ξ start_POSTSUBSCRIPT italic_j end_POSTSUBSCR", "snippet": "‖ 𝝃 j ∥ ‖ 2 = ‖ 𝑼 j ​ 𝑼 j ⊤ ​ 𝝃 ‖ 2 \\displaystyle\\left\\|\\bm{\\xi}_{j}^{\\|}\\right\\|_{2}=\\left\\|\\bm{U}_{j}\\bm{U}_{j}^{\\top}\\bm{\\xi}\\right\\|_{2} ∥ bold_italic_ξ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∥ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx155", "title": "𝔼 ​ [ ‖ 𝒙 δ − 𝒙 ^ δ ‖ 2 2 ] \\displaystyle\\mathbb{E}\\left[\\left\\|\\bm{x}_{\\delta}-\\hat{\\bm{x}}_{\\delta}\\right\\|_{2}^{2}\\right] blackboard_E [ ∥ bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRI", "snippet": "𝔼 ​ [ ‖ 𝒙 δ − 𝒙 ^ δ ‖ 2 2 ] \\displaystyle\\mathbb{E}\\left[\\left\\|\\bm{x}_{\\delta}-\\hat{\\bm{x}}_{\\delta}\\right\\|_{2}^{2}\\right] blackboard_E [ ∥ bold_italic_x start_POSTSUBSCRIPT italic_δ end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_δ end_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1", "title": "Finitneness of the Differential Entropy", "snippet": "Finitneness of the Differential Entropy We first show that the entropy exists along the stochastic process and is finite. Lemma B.1 . Let 𝐱 \\bm{x} bold_italic_x be any random variable, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx2", "title": "Integration by Parts in De Brujin Identity", "snippet": "Integration by Parts in De Brujin Identity Finally, we fill in the integration-by-parts argument alluded to in the proofs of Theorems B.2 and B.3 . The argument is conceptually pretty simple but requires some technical estimates to show that the boundary term in the integration-b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3", "title": "Local Invertibility of the Denoiser 𝒙 ¯ \\bar{\\bm{x}} over¯ start_ARG bold_italic_x end_ARG", "snippet": "Local Invertibility of the Denoiser 𝒙 ¯ \\bar{\\bm{x}} over¯ start_ARG bold_italic_x end_ARG Here we provide some results used in the proof of Theorem B.3 which are appropriate generalizations of corresponding results in [ Gri11 ] . Lemma B.3 (Generalization of [ Gri11 ] , Lemma A."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx4", "title": "Controlling the Laplacian Δ ​ log ⁡ p t \\Delta\\log p_{t} roman_Δ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT", "snippet": "Controlling the Laplacian Δ ​ log ⁡ p t \\Delta\\log p_{t} roman_Δ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT Finally, we develop a technical estimate which is required for the proof of Theorem B.3 and actually motivates the assumption for the viable t t ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx5", "title": "Derivative Computations", "snippet": "Derivative Computations Here we calculate some useful derivatives which will be reused throughout the appendix. Proposition B.1 . Let 𝐱 \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bol"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6", "title": "Differentiating Under the Integral Sign", "snippet": "Differentiating Under the Integral Sign In this appendix, we differentiate under the integral sign many times, and it is important to know when we can do this. There are two kinds of differentiating under the integral sign: 1. Differentiating an integral ∫ f t ​ ( 𝝃 ) ​ d 𝝃 \\int "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#top", "title": "Chapter 1 Introduction", "snippet": ""}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1", "title": "1.1 Intelligence, Cybernetics, and Artificial Intelligence", "snippet": "1.1 Intelligence, Cybernetics, and Artificial Intelligence The world in which we are living is neither fully random nor completely unpredictable. 1 1 1 Note there is no need for an intelligent being to learn or memorize anything if the world is fully random. Instead, it follows c"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2", "title": "1.2 What to Learn?", "snippet": "1.2 What to Learn? 1.2.1 Predictability Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3", "title": "1.3 How to Learn?", "snippet": "1.3 How to Learn? 1.3.1 Analytical Approaches Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4", "title": "1.4 A Unifying Approach", "snippet": "1.4 A Unifying Approach So far, we have given a brief account of the main objective and history of machine intelligence and many important ideas and approaches associated with it. In recent years, after the empirical success of deep neural networks, tremendous efforts have been m"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5", "title": "1.5 Bridging Theory and Practice for Machine Intelligence", "snippet": "1.5 Bridging Theory and Practice for Machine Intelligence So far, we have introduced three related frameworks for learning a compact and structured representation 𝒁 \\bm{Z} bold_italic_Z for a given data distribution 𝑿 \\bm{X} bold_italic_X : • The open-ended encoding ( 1.4.10 ); •"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1", "title": "1.2.1 Predictability", "snippet": "1.2.1 Predictability Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at the heart of the th"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2", "title": "1.2.2 Low Dimensionality", "snippet": "1.2.2 Low Dimensionality Learn to Predict. Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , … , S i , … , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSU"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1", "title": "1.3.1 Analytical Approaches", "snippet": "1.3.1 Analytical Approaches Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable or amenable to ef"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2", "title": "1.3.2 Empirical Approaches", "snippet": "1.3.2 Empirical Approaches In practice, for many important real-world data such as images, sounds, and texts, it is difficult to model them with idealistic linear or mixed linear models. For example, there has been a long and rich history in the fields of image processing and com"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1", "title": "1.4.1 Learning Parsimonious Representations", "snippet": "1.4.1 Learning Parsimonious Representations One necessary condition for any learning task to be possible is that the sequences of interest must be computable , at least in the sense of Alan Turing [ Tur36 ] . That is, a sequence can be computed via a program on a typical computer"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS2", "title": "1.4.2 Learning Informative Representations", "snippet": "1.4.2 Learning Informative Representations Note that if the goal was simply to compress the given data just for the sake of compression, then in theory the optimal codes that approach the Kolmogorov complexity would become nearly random or structureless [ Cha66 ] . 50 50 50 Becau"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3", "title": "1.4.3 Learning Consistent Representations", "snippet": "1.4.3 Learning Consistent Representations To summarize our discussions so far, let us denote the data as: 𝑿 = { S 1 , S 2 , … , S i , … , S N } ⊂ ℝ D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POST"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS4", "title": "1.4.4 Learning Self-Consistent Representations", "snippet": "1.4.4 Learning Self-Consistent Representations Note that in the above autoencoding objective, one needs to evaluate how close or consistent the decoded data 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is to the original 𝑿 \\bm{X} bold_italic_X . This often requires some"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1", "title": "Emergence and evolution of intelligence.", "snippet": "Emergence and evolution of intelligence. A necessary condition for the emergence of life on earth about 4 billion years ago is that the earth’s environment is largely predictable. In the environment, life has developed mechanisms that allow it to learn what is predictable about t"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px2", "title": "Evolution of human intelligence.", "snippet": "Evolution of human intelligence. Since the emergence of homo sapiens about 315 thousand years ago, a new and higher form of intelligence emerged which evolves more efficiently and economically. Languages, first spoken 5 5 5 It was believed that Sanskrit was the first spoken langu"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3", "title": "Origin of machine intelligence – Cybernetics.", "snippet": "Origin of machine intelligence – Cybernetics. In 1940s, partly due to the war effort, intelligence in nature had inspired scientists in the 1940s to emulate animal intelligence by machines, which led to the “Cybernetics” movement advocated by Norbert Wiener. Wiener studied zoolog"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px4", "title": "Origin of Artificial Intelligence.", "snippet": "Origin of Artificial Intelligence. From the subtitle of Wiener’s Cybernetics book: “Control and Communication in the Animal and the Machine” , one can tell that the studies in the 1940s mainly aimed to emulate intelligence at the level of animals. As we mentioned before, the rese"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px5", "title": "The renaissance of “Artificial Intelligence” or “Cybernetics”?", "snippet": "The renaissance of “Artificial Intelligence” or “Cybernetics”? As the readers may have known, in the past decade or so, machine intelligence has undergone explosive development, powered mainly by the practice of deep artificial neural networks, triggered by the work of Geoffrey H"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px1", "title": "Scalar Case.", "snippet": "Scalar Case. The simplest predictable discrete sequence is arguably the sequence of natural numbers: S = 1 , 2 , 3 , 4 , 5 , 6 , … , n , n + 1 , … {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , … , italic_n , italic_n + 1 , … (1.2.1) in which the next numb"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px2", "title": "Multi-Variable Case.", "snippet": "Multi-Variable Case. Of course, the value of the next number can also depend on two of its predecessors. For example, the famous Fibonacci sequence is defined to be: S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , … {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px3", "title": "Vector Case.", "snippet": "Vector Case. To simplify the notation, we may define a vector 𝒙 ∈ ℝ d \\bm{x}\\in\\mathbb{R}^{d} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT that collects d d italic_d consecutive values in the sequence 𝒙 n ≐ [ x n + d − 1 , … , x n ] ⊤ , 𝒙 n ∈ ℝ "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px4", "title": "Controlled Prediction.", "snippet": "Controlled Prediction. We may also define a predictable sequence that depends on another predictable sequence as input: 𝒙 n + 1 = f ​ ( 𝒙 n , 𝒖 n ) ∈ ℝ d , n = 1 , 2 , 3 , … , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px5", "title": "Continuous Processes.", "snippet": "Continuous Processes. Predictable sequences have their natural counterparts in the continuous case. We may refer to them as predictable processes. Similar to the sequence of natural numbers, the simplest predictable continuous process is time itself x = t x=t italic_x = italic_t "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px1", "title": "Learn to Predict.", "snippet": "Learn to Predict. Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , … , S i , … , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S st"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2", "title": "Predictability and Low-Dimensionality.", "snippet": "Predictability and Low-Dimensionality. To identify the predictive function f f italic_f , we may notice a common characteristic of segments of any predictable sequence, say given by ( 1.2.21 ). If we take a long segment, say with a length D ≫ d D\\gg d italic_D ≫ italic_d , of the"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3", "title": "Properties of Low-Dimensionality.", "snippet": "Properties of Low-Dimensionality. Of course, temporal correlation in predictable sequences is not the only reason why data are low-dimensional. For example, the space of all images is humongous but most of the space consists of images that resemble structureless random images as "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px1", "title": "Wiener Filter.", "snippet": "Wiener Filter. As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a linear time-invariant (LTI) process: x ​ ["}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px2", "title": "Kalman Filter.", "snippet": "Kalman Filter. The idea of denoising or filtering for a dynamic process was later extended to a linear time-invariant system described by a (finite-dimensional) state-space model by Rudolph Kalman in the 1960s: 𝒛 ​ [ n ] = 𝑨 ​ 𝒛 ​ [ n − 1 ] + 𝑩 ​ 𝒖 ​ [ n ] + ϵ ​ [ n ] . \\bm{z}[n]"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px3", "title": "Identification of Linear Dynamical Systems.", "snippet": "Identification of Linear Dynamical Systems. To derive the Kalman filter, the system parameters ( 𝑨 , 𝑩 , 𝑪 ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) are assumed to be known. If they are not given in advance, it would be a more challenging problem "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1", "title": "Principal Component Analysis.", "snippet": "Principal Component Analysis. From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional linear subspace. Mathematically, we may model such a"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px2", "title": "Independent Component Analysis.", "snippet": "Independent Component Analysis. Independent component analysis (ICA) was originally proposed by [ BJC85 ] as a classic model for learning a good representation . In fact it was originally proposed as a simple mathematical model for our memory. The ICA model takes a deceivingly si"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3", "title": "Sparse Structures and Compressive Sensing.", "snippet": "Sparse Structures and Compressive Sensing. As one may see, if p p italic_p in ( 1.3.13 ) is very small, the probability that any of the components is non-zero is small. In this case, we say 𝒙 \\bm{x} bold_italic_x is sparsely generated and it concentrates on a set of linear subspa"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px4", "title": "Dictionary Learning.", "snippet": "Dictionary Learning. Conceptually, an even harder problem than the sparse coding problem ( 1.3.16 ) is when the observation matrix 𝑨 \\bm{A} bold_italic_A is not even known in advance and we need to learn 𝑨 \\bm{A} bold_italic_A from a set of (possibly noisy) observations, say 𝑿 = "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px1", "title": "Denoising.", "snippet": "Denoising. In the 1950s, statisticians became interested in the problem of denoising data drawn from an arbitrary distribution. Let 𝒙 o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT be a random variable with probability density function p o ​ ( ⋅ ) p_{o}"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px2", "title": "Entropy minimization.", "snippet": "Entropy minimization. In fact, this function has a very intuitive information-theoretic and geometric interpretation. Note that in information theory − log ⁡ p ​ ( 𝒙 ) -\\log p(\\bm{x}) - roman_log italic_p ( bold_italic_x ) generally corresponds to the number of bits needed to enc"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px1", "title": "Artificial neuron.", "snippet": "Artificial neuron. Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals. Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 25 25 25 known as the Linear Thresho"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px2", "title": "Artificial neural network.", "snippet": "Artificial neural network. In the 1950s, Frank Rosenblatt was the first to build a machine with a network of such artificial neurons, shown in Figure 1.15 . The machine is called Mark I Perceptron which consists of an input layer, an output layer, and a single hidden layer consis"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3", "title": "Convolutional neural networks.", "snippet": "Convolutional neural networks. The somewhat disappointing early experimentation with artificial neural networks like Mark I Perceptron in the 50s and 60s suggested that it might not be enough to simply connect neurons in a general fashion as multi-layer perceptrons (MLP). In orde"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px4", "title": "Backpropagation.", "snippet": "Backpropagation. In history, the fate of deep neural networks seems to be tied closely to how they can be trained easily and efficiently. Back propagation (BP) was introduced for this reason. We know that a multiple layer perceptron can be expressed as a composition of a sequence"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px5", "title": "Compressive autoencoding.", "snippet": "Compressive autoencoding. In the late 1980s and 1990s, artificial neural networks were already adopted to learn low-dimensional representations of high-dimensional data such as images. It had been shown that neural networks can be used to learn PCA from the data [ Oja82 , BH89 ] "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px1", "title": "Classification and recognition.", "snippet": "Classification and recognition. As it turns out, the tremendous potential of deep neural networks could only be unleashed once there are enough data and computing power. Fast forward to 2010s, much larger datasets such as ImageNet became available, and GPUs became powerful enough"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px2", "title": "Reinforcement learning.", "snippet": "Reinforcement learning. The early successes of deep networks were mainly for classification tasks in a supervised learning setting, such as speech recognition and image recognition. Deep networks were later adopted by the DeepMind team, led by Demis Hassabis, to learn decision-ma"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px3", "title": "Generation and prediction.", "snippet": "Generation and prediction. One may view early practices of deep networks in the 2010s focused more on extracting relevant information from the data 𝑿 \\bm{X} bold_italic_X and encoding it for certain task-specific representation 𝒁 \\bm{Z} bold_italic_Z (say 𝒁 \\bm{Z} bold_italic_Z r"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4", "title": "Generation via discriminative approaches.", "snippet": "Generation via discriminative approaches. In order for the generated images 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG to be similar to the true natural images 𝑿 \\bm{X} bold_italic_X , we need to be able to evaluate and minimize some distance: min ⁡ d ​ ( 𝑿 , 𝑿 ^ ) . "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px5", "title": "Generation via denoising and diffusion.", "snippet": "Generation via denoising and diffusion. In 2015, shortly after GAN was introduced and became popular, Surya Ganguli and his students realized and suggested that an iterative denoising process modeled by a deep network can be used to learn a general distribution, such as that of n"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1", "title": "Pursuing low-dimensionality via compression.", "snippet": "Pursuing low-dimensionality via compression. From the examples of sequences we gave in Section 1.2.1 , it is clear that some sequences are easy to model and compute and others are more difficult. Obviously, the computational cost of a sequence depends on how complex the predictin"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2", "title": "Computable measure of parsimony.", "snippet": "Computable measure of parsimony. Hence for practical purposes, we need an efficiently computable measure of complexity for sequences that are generated from the same predicting function. 45 45 45 Note that in practice, we typically care about learning the predicting function f f "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.SSS0.Px1", "title": "Bidirectional Autoencoding for Consistency.", "snippet": "Bidirectional Autoencoding for Consistency. In a broader learning context, the main goal of a compressive coding scheme ℰ \\mathcal{E} caligraphic_E is to identify the low-dimensional structures in the data 𝑿 \\bm{X} bold_italic_X so that they can be used to predict things in the o"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.SS0.SSS0.Px1", "title": "Back to Intelligence.", "snippet": "Back to Intelligence. As we have mentioned in the beginning, a common and fundamental task of any intelligent being is to learn predictable information from its sensed data. Now we have understood a little about the computational nature of this task, and one should realize that t"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample1", "title": "Example 1.1 .", "snippet": "Example 1.1 . For example in physics, Newton’s second law of motion describes how to predict the trajectory 𝒙 ​ ( t ) ∈ ℝ 3 \\bm{x}(t)\\in\\mathbb{R}^{3} bold_italic_x ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT of a moving object under a force input 𝑭 ​ "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmremark1", "title": "Remark 1.1 .", "snippet": "Remark 1.1 . The perspective of measuring data complexity with explicit encoding schemes has motivated several learning objectives that were proposed to revise the Kolmogorov complexity for better computability [ WD99 ] , including the minimum message length (MML) proposed later "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample2", "title": "Example 1.2 .", "snippet": "Example 1.2 . For example, image classification is such a case: we assign all images in the same class to a single code and images in different classes to different codes, say “one-hot” vectors: 𝒙 ↦ 𝒛 ∈ { [ 1 , 0 , 0 , … , 0 , 0 ] , [ 0 , 1 , 0 … , 0 , 0 ] , … , [ 0 , 0 , 0 , … ,"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.E1", "title": "phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial int", "snippet": "phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . (1"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E1", "title": "S = 1 , 2 , 3 , 4 , 5 , 6 , … , n , n + 1 , … {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , … , italic_n , italic_n + 1 , … (1.2.1)", "snippet": "S = 1 , 2 , 3 , 4 , 5 , 6 , … , n , n + 1 , … {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , … , italic_n , italic_n + 1 , … (1.2.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E2", "title": "x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 . (1.2.2)", "snippet": "x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 . (1.2.2)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E3", "title": "x n + 1 = f ​ ( x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+1}=f(x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( italic_x start_P", "snippet": "x n + 1 = f ​ ( x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+1}=f(x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E4", "title": "S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , … {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , … (1.2.4)", "snippet": "S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , … {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , … (1.2.4)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E5", "title": "x n + 2 = x n + 1 + x n , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTS", "snippet": "x n + 2 = x n + 1 + x n , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_n "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E6", "title": "x n + 2 = f ​ ( x n + 1 , x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+2}=f(x_{n+1},x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_f ", "snippet": "x n + 2 = f ​ ( x n + 1 , x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+2}=f(x_{n+1},x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT , italic_x start_P"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E7", "title": "x n + d = f ​ ( x n + d − 1 , … , x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + italic_d end_P", "snippet": "x n + d = f ​ ( x n + d − 1 , … , x n ) , x n ∈ ℝ , n = 1 , 2 , 3 , … x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 e"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E8", "title": "𝒙 n ≐ [ x n + d − 1 , … , x n ] ⊤ , 𝒙 n ∈ ℝ d , n = 1 , 2 , 3 , … \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT it", "snippet": "𝒙 n ≐ [ x n + d − 1 , … , x n ] ⊤ , 𝒙 n ∈ ℝ d , n = 1 , 2 , 3 , … \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≐ [ italic_x start_POSTSUBSCRIPT italic_n + italic_d - "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E9", "title": "𝒙 n + 1 = g ​ ( 𝒙 n ) ∈ ℝ d , n = 1 , 2 , 3 , … \\bm{x}_{n+1}=g(\\bm{x}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_g ( bold_it", "snippet": "𝒙 n + 1 = g ​ ( 𝒙 n ) ∈ ℝ d , n = 1 , 2 , 3 , … \\bm{x}_{n+1}=g(\\bm{x}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_g ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ∈ blackboard_R start_POS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E10", "title": "𝒙 n + 1 = f ​ ( 𝒙 n , 𝒖 n ) ∈ ℝ d , n = 1 , 2 , 3 , … , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ", "snippet": "𝒙 n + 1 = f ​ ( 𝒙 n , 𝒖 n ) ∈ ℝ d , n = 1 , 2 , 3 , … , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E11", "title": "𝒙 n + 1 = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝒖 n , 𝑨 ∈ ℝ d × d , 𝑩 ∈ ℝ d × k , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n},\\quad\\bm{A}\\in\\mathbb{R}^{d\\times d},\\bm{B}\\in\\mathbb{R}^{d\\times k}, bold_italic_x start_POSTS", "snippet": "𝒙 n + 1 = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝒖 n , 𝑨 ∈ ℝ d × d , 𝑩 ∈ ℝ d × k , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n},\\quad\\bm{A}\\in\\mathbb{R}^{d\\times d},\\bm{B}\\in\\mathbb{R}^{d\\times k}, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POST"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E12", "title": "𝒖 n = h ​ ( 𝒙 n ) , n = 1 , 2 , 3 , … \\bm{u}_{n}=h(\\bm{x}_{n}),\\quad n=1,2,3,\\ldots bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n", "snippet": "𝒖 n = h ​ ( 𝒙 n ) , n = 1 , 2 , 3 , … \\bm{u}_{n}=h(\\bm{x}_{n}),\\quad n=1,2,3,\\ldots bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_n = 1 , 2 , 3 , … (1.2.12)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E13", "title": "𝒙 n + 1 = f ​ ( 𝒙 n , h ​ ( 𝒙 n ) ) , n = 1 , 2 , 3 , … \\bm{x}_{n+1}=f\\big{(}\\bm{x}_{n},h(\\bm{x}_{n})\\big{)},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = ita", "snippet": "𝒙 n + 1 = f ​ ( 𝒙 n , h ​ ( 𝒙 n ) ) , n = 1 , 2 , 3 , … \\bm{x}_{n+1}=f\\big{(}\\bm{x}_{n},h(\\bm{x}_{n})\\big{)},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_h "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E14", "title": "𝒙 n + 1 = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝒖 n = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝑭 ​ 𝒙 n = ( 𝑨 + 𝑩 ​ 𝑭 ) ​ 𝒙 n , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{F}\\bm{x}_{n}=(\\bm{A}+\\bm{B}\\bm{F})\\bm{x}_{n}, bold", "snippet": "𝒙 n + 1 = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝒖 n = 𝑨 ​ 𝒙 n + 𝑩 ​ 𝑭 ​ 𝒙 n = ( 𝑨 + 𝑩 ​ 𝑭 ) ​ 𝒙 n , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{F}\\bm{x}_{n}=(\\bm{A}+\\bm{B}\\bm{F})\\bm{x}_{n}, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bol"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E15", "title": "𝒙 ˙ ​ ( t ) = f ​ ( 𝒙 ​ ( t ) ) , 𝒙 ∈ ℝ d . \\dot{\\bm{x}}(t)=f(\\bm{x}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d}. over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) ) , bo", "snippet": "𝒙 ˙ ​ ( t ) = f ​ ( 𝒙 ​ ( t ) ) , 𝒙 ∈ ℝ d . \\dot{\\bm{x}}(t)=f(\\bm{x}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d}. over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) ) , bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E16", "title": "𝒙 ˙ ​ ( t ) = f ​ ( 𝒙 ​ ( t ) , 𝒖 ​ ( t ) ) , 𝒙 ∈ ℝ d , 𝒖 ∈ ℝ k , \\dot{\\bm{x}}(t)=f(\\bm{x}(t),\\bm{u}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d},\\bm{u}\\in\\mathbb{R}^{k}, over˙ start_ARG bold_italic_x end_ARG ( i", "snippet": "𝒙 ˙ ​ ( t ) = f ​ ( 𝒙 ​ ( t ) , 𝒖 ​ ( t ) ) , 𝒙 ∈ ℝ d , 𝒖 ∈ ℝ k , \\dot{\\bm{x}}(t)=f(\\bm{x}(t),\\bm{u}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d},\\bm{u}\\in\\mathbb{R}^{k}, over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) , bold_italic_u ( italic_t ) )"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E17", "title": "m ​ 𝒙 ¨ ​ ( t ) = 𝑭 ​ ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17)", "snippet": "m ​ 𝒙 ¨ ​ ( t ) = 𝑭 ​ ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E18", "title": "𝒙 ¨ ​ ( t ) = 𝟎 ⇔ 𝒙 ˙ ​ ( t ) = 𝒗 \\ddot{\\bm{x}}(t)=\\bm{0}\\;\\Leftrightarrow\\;\\dot{\\bm{x}}(t)=\\bm{v} over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_0 ⇔ over˙ start_ARG bold_italic_x end_ARG (", "snippet": "𝒙 ¨ ​ ( t ) = 𝟎 ⇔ 𝒙 ˙ ​ ( t ) = 𝒗 \\ddot{\\bm{x}}(t)=\\bm{0}\\;\\Leftrightarrow\\;\\dot{\\bm{x}}(t)=\\bm{v} over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_0 ⇔ over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_v (1.2.18)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E19", "title": "{ S 1 , S 2 , … , S i , … , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_P", "snippet": "{ S 1 , S 2 , … , S i , … , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT itali"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E20", "title": "S i = [ x j ​ ( i ) , x j ​ ( i ) + 1 , … , x j ​ ( i ) + D − 1 ] ⊤ ∈ ℝ D S_{i}=[x_{j(i)},x_{j(i)+1},\\ldots,x_{j(i)+D-1}]^{\\top}\\in\\mathbb{R}^{D} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIP", "snippet": "S i = [ x j ​ ( i ) , x j ​ ( i ) + 1 , … , x j ​ ( i ) + D − 1 ] ⊤ ∈ ℝ D S_{i}=[x_{j(i)},x_{j(i)+1},\\ldots,x_{j(i)+D-1}]^{\\top}\\in\\mathbb{R}^{D} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) end_POSTSUBSCRIPT , ita"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E21", "title": "x n + d = f ​ ( x n + d − 1 , … , x n ) . x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}). italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + ita", "snippet": "x n + d = f ​ ( x n + d − 1 , … , x n ) . x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}). italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POST"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E22", "title": "𝒙 i = [ x i , x i + 1 , … ​ x i + D − 1 ] ⊤ ∈ ℝ D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start", "snippet": "𝒙 i = [ x i , x i + 1 , … ​ x i + D − 1 ] ⊤ ∈ ℝ D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E23", "title": "x n + 2 = a ⋅ x n + 1 + b ⋅ x n , x_{n+2}=a\\cdot x_{n+1}+b\\cdot x_{n}, italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ⋅ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCR", "snippet": "x n + 2 = a ⋅ x n + 1 + b ⋅ x n , x_{n+2}=a\\cdot x_{n+1}+b\\cdot x_{n}, italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ⋅ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_b ⋅ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , (1.2."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E24", "title": "𝒚 = f ​ ( 𝒙 ) + 𝒏 , \\bm{y}=f(\\bm{x})+\\bm{n}, bold_italic_y = italic_f ( bold_italic_x ) + bold_italic_n , (1.2.24)", "snippet": "𝒚 = f ​ ( 𝒙 ) + 𝒏 , \\bm{y}=f(\\bm{x})+\\bm{n}, bold_italic_y = italic_f ( bold_italic_x ) + bold_italic_n , (1.2.24)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E1", "title": "x ​ [ n ] = h ​ [ n ] ∗ z ​ [ n ] + ϵ ​ [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ∗ italic_z [ italic_n ] + italic_ϵ [ italic_n ] , (1.3.1)", "snippet": "x ​ [ n ] = h ​ [ n ] ∗ z ​ [ n ] + ϵ ​ [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ∗ italic_z [ italic_n ] + italic_ϵ [ italic_n ] , (1.3.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E2", "title": "min h ⁡ 𝔼 ​ [ ϵ ​ [ n ] 2 ] = 𝔼 ​ [ ‖ x ​ [ n ] − h ​ [ n ] ∗ z ​ [ n ] ‖ 2 2 ] . \\min_{h}\\mathbb{E}\\big{[}\\epsilon[n]^{2}\\big{]}=\\mathbb{E}\\big{[}\\|x[n]-h[n]*z[n]\\|_{2}^{2}\\big{]}. roman_min start_PO", "snippet": "min h ⁡ 𝔼 ​ [ ϵ ​ [ n ] 2 ] = 𝔼 ​ [ ‖ x ​ [ n ] − h ​ [ n ] ∗ z ​ [ n ] ‖ 2 2 ] . \\min_{h}\\mathbb{E}\\big{[}\\epsilon[n]^{2}\\big{]}=\\mathbb{E}\\big{[}\\|x[n]-h[n]*z[n]\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT blackboard_E [ italic_ϵ [ italic_n ] star"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E3", "title": "𝒛 ​ [ n ] = 𝑨 ​ 𝒛 ​ [ n − 1 ] + 𝑩 ​ 𝒖 ​ [ n ] + ϵ ​ [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bo", "snippet": "𝒛 ​ [ n ] = 𝑨 ​ 𝒛 ​ [ n − 1 ] + 𝑩 ​ 𝒖 ​ [ n ] + ϵ ​ [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bold_italic_B bold_italic_u [ italic_n ] + bold_italic_ϵ [ italic_n ] . (1.3.3)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E4", "title": "𝒙 ​ [ n ] = 𝑪 ​ 𝒛 ​ [ n ] + 𝒘 ​ [ n ] , \\bm{x}[n]=\\bm{C}\\bm{z}[n]+\\bm{w}[n], bold_italic_x [ italic_n ] = bold_italic_C bold_italic_z [ italic_n ] + bold_italic_w [ italic_n ] , (1.3.4)", "snippet": "𝒙 ​ [ n ] = 𝑪 ​ 𝒛 ​ [ n ] + 𝒘 ​ [ n ] , \\bm{x}[n]=\\bm{C}\\bm{z}[n]+\\bm{w}[n], bold_italic_x [ italic_n ] = bold_italic_C bold_italic_z [ italic_n ] + bold_italic_w [ italic_n ] , (1.3.4)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E5", "title": "min ⁡ 𝔼 ​ [ ‖ 𝒙 ​ [ n ] − 𝑪 ​ 𝒛 ​ [ n ] ‖ 2 2 ] \\min\\mathbb{E}\\big{[}\\|\\bm{x}[n]-\\bm{C}\\bm{z}[n]\\|_{2}^{2}\\big{]} roman_min blackboard_E [ ∥ bold_italic_x [ italic_n ] - bold_italic_C bold_italic_z [ ", "snippet": "min ⁡ 𝔼 ​ [ ‖ 𝒙 ​ [ n ] − 𝑪 ​ 𝒛 ​ [ n ] ‖ 2 2 ] \\min\\mathbb{E}\\big{[}\\|\\bm{x}[n]-\\bm{C}\\bm{z}[n]\\|_{2}^{2}\\big{]} roman_min blackboard_E [ ∥ bold_italic_x [ italic_n ] - bold_italic_C bold_italic_z [ italic_n ] ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E6", "title": "𝒙 = 𝒙 o + ϵ , 𝒙 o ∼ S , \\bm{x}=\\bm{x}_{o}+\\bm{\\epsilon},\\quad\\bm{x}_{o}\\sim S, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_ϵ , bold_italic_x start_POSTSU", "snippet": "𝒙 = 𝒙 o + ϵ , 𝒙 o ∼ S , \\bm{x}=\\bm{x}_{o}+\\bm{\\epsilon},\\quad\\bm{x}_{o}\\sim S, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_ϵ , bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∼ italic_S , (1.3.6)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E7", "title": "𝒙 = 𝒖 1 ​ z 1 + 𝒖 2 ​ z 2 + ⋯ + 𝒖 d ​ z d + ϵ = 𝑼 ​ 𝒛 + ϵ , 𝑼 ∈ ℝ D × d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^", "snippet": "𝒙 = 𝒖 1 ​ z 1 + 𝒖 2 ​ z 2 + ⋯ + 𝒖 d ​ z d + ϵ = 𝑼 ​ 𝒛 + ϵ , 𝑼 ∈ ℝ D × d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIP"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E8", "title": "min 𝑼 ⁡ 𝔼 ​ [ ‖ ϵ ‖ 2 2 ] = 𝔼 ​ [ ‖ 𝒙 − 𝑼 ​ 𝒛 ‖ 2 2 ] . \\min_{\\bm{U}}\\mathbb{E}\\big{[}\\|\\bm{\\epsilon}\\|_{2}^{2}\\big{]}=\\mathbb{E}\\big{[}\\|\\bm{x}-\\bm{U}\\bm{z}\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBS", "snippet": "min 𝑼 ⁡ 𝔼 ​ [ ‖ ϵ ‖ 2 2 ] = 𝔼 ​ [ ‖ 𝒙 − 𝑼 ​ 𝒛 ‖ 2 2 ] . \\min_{\\bm{U}}\\mathbb{E}\\big{[}\\|\\bm{\\epsilon}\\|_{2}^{2}\\big{]}=\\mathbb{E}\\big{[}\\|\\bm{x}-\\bm{U}\\bm{z}\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBSCRIPT bold_italic_U end_POSTSUBSCRIPT blackboard_E [ ∥ bold_italic_ϵ ∥ start_POS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E9", "title": "𝒙 → 𝒙 ^ = 𝑼 ​ 𝑼 ⊤ ​ 𝒙 . \\bm{x}\\rightarrow\\hat{\\bm{x}}=\\bm{U}\\bm{U}^{\\top}\\bm{x}. bold_italic_x → over^ start_ARG bold_italic_x end_ARG = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUP", "snippet": "𝒙 → 𝒙 ^ = 𝑼 ​ 𝑼 ⊤ ​ 𝒙 . \\bm{x}\\rightarrow\\hat{\\bm{x}}=\\bm{U}\\bm{U}^{\\top}\\bm{x}. bold_italic_x → over^ start_ARG bold_italic_x end_ARG = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x . (1.3.9)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E10", "title": "𝒙 → 𝑼 ⊤ 𝒛 → 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT ", "snippet": "𝒙 → 𝑼 ⊤ 𝒛 → 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_OVERACCENT → end_A"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E11", "title": "𝒙 ∼ 𝒩 ​ ( 𝟎 , 𝑼 ​ 𝑼 ⊤ + σ ​ 𝑰 ) , 𝑼 ∈ ℝ D × d , \\bm{x}\\sim\\mathcal{N}(\\bm{0},\\bm{U}\\bm{U}^{\\top}+\\sigma\\bm{I}),\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d}, bold_italic_x ∼ caligraphic_N ( bold_0 , bold_itali", "snippet": "𝒙 ∼ 𝒩 ​ ( 𝟎 , 𝑼 ​ 𝑼 ⊤ + σ ​ 𝑰 ) , 𝑼 ∈ ℝ D × d , \\bm{x}\\sim\\mathcal{N}(\\bm{0},\\bm{U}\\bm{U}^{\\top}+\\sigma\\bm{I}),\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d}, bold_italic_x ∼ caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ bold_it"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E12", "title": "𝒙 = 𝒖 1 ​ z 1 + 𝒖 2 ​ z 2 + ⋯ + 𝒖 d ​ z d + ϵ = 𝑼 ​ 𝒛 + ϵ . \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon}. bold_italic_x = bold_italic_u start_", "snippet": "𝒙 = 𝒖 1 ​ z 1 + 𝒖 2 ​ z 2 + ⋯ + 𝒖 d ​ z d + ϵ = 𝑼 ​ 𝒛 + ϵ . \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon}. bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRI"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E13", "title": "z i = σ i ⋅ w i , σ i ∼ B ​ ( 1 , p ) , z_{i}=\\sigma_{i}\\cdot w_{i},\\quad\\sigma_{i}\\sim B(1,p), italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT italic_i end_POST", "snippet": "z i = σ i ⋅ w i , σ i ∼ B ​ ( 1 , p ) , z_{i}=\\sigma_{i}\\cdot w_{i},\\quad\\sigma_{i}\\sim B(1,p), italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_σ s"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E14", "title": "𝒙 → ℰ 𝒛 → 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT cali", "snippet": "𝒙 → ℰ 𝒛 → 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E15", "title": "𝒵 = { 𝒛 ∈ ℝ n ∣ ‖ 𝒛 ‖ 0 ≤ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\}, caligraphic_Z = { bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∣ ∥ b", "snippet": "𝒵 = { 𝒛 ∈ ℝ n ∣ ‖ 𝒛 ‖ 0 ≤ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\}, caligraphic_Z = { bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∣ ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≤ italic_k } , (1.3.15)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E16", "title": "𝒙 = 𝑨 ​ 𝒛 + ϵ , 𝑨 ∈ ℝ m × n \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ϵ , bold_italic_A ∈ blackboard_R start_POST", "snippet": "𝒙 = 𝑨 ​ 𝒛 + ϵ , 𝑨 ∈ ℝ m × n \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ϵ , bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT (1.3.16)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E17", "title": "min ⁡ ‖ 𝒛 ‖ 1 subject to ‖ 𝒙 − 𝑨 ​ 𝒛 ‖ 2 ≤ ϵ , \\min\\|\\bm{z}\\|_{1}\\quad\\mbox{subject to}\\quad\\|\\bm{x}-\\bm{A}\\bm{z}\\|_{2}\\leq\\epsilon, roman_min ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT", "snippet": "min ⁡ ‖ 𝒛 ‖ 1 subject to ‖ 𝒙 − 𝑨 ​ 𝒛 ‖ 2 ≤ ϵ , \\min\\|\\bm{z}\\|_{1}\\quad\\mbox{subject to}\\quad\\|\\bm{x}-\\bm{A}\\bm{z}\\|_{2}\\leq\\epsilon, roman_min ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT subject to ∥ bold_italic_x - bold_italic_A bold_italic_z ∥ start_POSTSUBSCRIPT "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E18", "title": "𝒙 → ℰ 𝒛 . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z . (1.3.18)", "snippet": "𝒙 → ℰ 𝒛 . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z . (1.3.18)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E19", "title": "intractable ⟹ tractable ⟹ scalable . \\mbox{{intractable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. intractable ⟹ tractable ⟹ scalable . (1.3.19)", "snippet": "intractable ⟹ tractable ⟹ scalable . \\mbox{{intractable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. intractable ⟹ tractable ⟹ scalable . (1.3.19)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E20", "title": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 , 𝑨 ∈ ℝ m × n . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , bold_italic_A ∈ blackboard_R start_POSTSUPE", "snippet": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 , 𝑨 ∈ ℝ m × n . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT . (1.3.20)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E21", "title": "𝑿 → ℰ 𝒁 → 𝑨 𝑿 ​ ? \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\bm{A}\\hskip 5.69054pt}\\bm{X}? bold_italic_X start_ARROW start_OVERACCENT caligraphi", "snippet": "𝑿 → ℰ 𝒁 → 𝑨 𝑿 ​ ? \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\bm{A}\\hskip 5.69054pt}\\bm{X}? bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT bold_i"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E22", "title": "𝒙 = 𝒙 o + σ ​ 𝒈 , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_σ bold_italic_g , (1.3.22)", "snippet": "𝒙 = 𝒙 o + σ ​ 𝒈 , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_σ bold_italic_g , (1.3.22)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E23", "title": "𝒙 ^ o = 𝔼 ​ [ 𝒙 o ∣ 𝒙 ] = 𝒙 + σ 2 ​ ∇ log ⁡ p ​ ( 𝒙 ) . \\hat{\\bm{x}}_{o}=\\mathbb{E}[\\bm{x}_{o}\\mid\\bm{x}]=\\bm{x}+\\sigma^{2}\\nabla\\log p(\\bm{x}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRI", "snippet": "𝒙 ^ o = 𝔼 ​ [ 𝒙 o ∣ 𝒙 ] = 𝒙 + σ 2 ​ ∇ log ⁡ p ​ ( 𝒙 ) . \\hat{\\bm{x}}_{o}=\\mathbb{E}[\\bm{x}_{o}\\mid\\bm{x}]=\\bm{x}+\\sigma^{2}\\nabla\\log p(\\bm{x}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = blackboard_E [ bold_italic_x start_POSTSUBSCRIPT"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E24", "title": "𝒈 ^ = 𝒙 − 𝒙 ^ o σ = − σ ​ ∇ log ⁡ p ​ ( 𝒙 ) , \\hat{\\bm{g}}=\\frac{\\bm{x}-\\hat{\\bm{x}}_{o}}{\\sigma}=-\\sigma\\nabla\\log p(\\bm{x}), over^ start_ARG bold_italic_g end_ARG = divide start_ARG bold_italic_x - ", "snippet": "𝒈 ^ = 𝒙 − 𝒙 ^ o σ = − σ ​ ∇ log ⁡ p ​ ( 𝒙 ) , \\hat{\\bm{g}}=\\frac{\\bm{x}-\\hat{\\bm{x}}_{o}}{\\sigma}=-\\sigma\\nabla\\log p(\\bm{x}), over^ start_ARG bold_italic_g end_ARG = divide start_ARG bold_italic_x - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSC"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E25", "title": "H ​ ( 𝒙 ) = − ∫ p ​ ( 𝒘 ) ​ log ⁡ p ​ ( 𝒘 ) ​ d 𝒘 H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w} italic_H ( bold_italic_x ) = - ∫ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w", "snippet": "H ​ ( 𝒙 ) = − ∫ p ​ ( 𝒘 ) ​ log ⁡ p ​ ( 𝒘 ) ​ d 𝒘 H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w} italic_H ( bold_italic_x ) = - ∫ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w ) roman_d bold_italic_w (1.3.25)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E26", "title": "H ​ ( 𝒙 ) = − ∫ p ​ ( 𝒘 ) ​ log ⁡ p ​ ( 𝒘 ) ​ d 𝒘 → decreasing H ∗ ​ ( 𝒙 ) = − ∫ p ∗ ​ ( 𝒘 ) ​ log ⁡ p ∗ ​ ( 𝒘 ) ​ d 𝒘 . H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w}\\quad\\xrightarrow{\\hskip", "snippet": "H ​ ( 𝒙 ) = − ∫ p ​ ( 𝒘 ) ​ log ⁡ p ​ ( 𝒘 ) ​ d 𝒘 → decreasing H ∗ ​ ( 𝒙 ) = − ∫ p ∗ ​ ( 𝒘 ) ​ log ⁡ p ∗ ​ ( 𝒘 ) ​ d 𝒘 . H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w}\\quad\\xrightarrow{\\hskip 2.84526pt\\mbox{decreasing}\\hskip 2.84526pt}\\quad H^{*}(\\bm{x})=-\\int p^{*}(\\bm{"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E27", "title": "o j = φ ​ ( ∑ i w j ​ i ​ x i ) , o_{j}=\\varphi\\Big{(}\\sum_{i}w_{ji}x_{i}\\Big{)}, italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_φ ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ", "snippet": "o j = φ ​ ( ∑ i w j ​ i ​ x i ) , o_{j}=\\varphi\\Big{(}\\sum_{i}w_{ji}x_{i}\\Big{)}, italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_φ ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT italic_x start_"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E28", "title": "φ ​ ( x ) = max ⁡ { 0 , x } = { x , if ​ x > 0 , 0 , if ​ x ≤ 0 , \\varphi(x)=\\max\\{0,x\\}=\\begin{cases}x,&\\text{if}\\,x>0,\\\\ 0,\\quad&\\text{if}\\,x\\leq 0,\\end{cases} italic_φ ( italic_x ) = roman_max { 0 ", "snippet": "φ ​ ( x ) = max ⁡ { 0 , x } = { x , if ​ x > 0 , 0 , if ​ x ≤ 0 , \\varphi(x)=\\max\\{0,x\\}=\\begin{cases}x,&\\text{if}\\,x>0,\\\\ 0,\\quad&\\text{if}\\,x\\leq 0,\\end{cases} italic_φ ( italic_x ) = roman_max { 0 , italic_x } = { start_ROW start_CELL italic_x , end_CELL start_CELL if italic_x"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E29", "title": "h ​ ( 𝑾 1 , … , 𝑾 L ) = f L ​ ( 𝑾 L ​ f L − 1 ​ ( 𝑾 L − 1 ​ ⋯ ​ f 2 ​ ( 𝑾 2 ​ f 1 ​ ( 𝑾 1 ​ 𝒙 ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}", "snippet": "h ​ ( 𝑾 1 , … , 𝑾 L ) = f L ​ ( 𝑾 L ​ f L − 1 ​ ( 𝑾 L − 1 ​ ⋯ ​ f 2 ​ ( 𝑾 2 ​ f 1 ​ ( 𝑾 1 ​ 𝒙 ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}(\\bm{W}_{1}\\bm{x})))). italic_h ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSU"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E30", "title": "𝑿 → 𝑓 𝒁 → 𝑔 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERAC", "snippet": "𝑿 → 𝑓 𝒁 → 𝑔 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACC"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E31", "title": "min f , g ⁡ ‖ 𝑿 − 𝑿 ^ ‖ 2 2 = ‖ 𝑿 − g ​ ( f ​ ( 𝑿 ) ) ‖ 2 2 , \\min_{f,g}\\big{\\|}\\bm{X}-\\hat{\\bm{X}}\\big{\\|}_{2}^{2}=\\big{\\|}\\bm{X}-g(f(\\bm{X}))\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT italic_f ", "snippet": "min f , g ⁡ ‖ 𝑿 − 𝑿 ^ ‖ 2 2 = ‖ 𝑿 − g ​ ( f ​ ( 𝑿 ) ) ‖ 2 2 , \\min_{f,g}\\big{\\|}\\bm{X}-\\hat{\\bm{X}}\\big{\\|}_{2}^{2}=\\big{\\|}\\bm{X}-g(f(\\bm{X}))\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ∥ bold_italic_X - over^ start_ARG bold_italic_X end"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E32", "title": "𝑿 → 𝑓 𝒁 . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT → end_ARROW bold_italic_Z . (1.3.32)", "snippet": "𝑿 → 𝑓 𝒁 . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT → end_ARROW bold_italic_Z . (1.3.32)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E33", "title": "𝒁 → 𝑔 𝑿 ^ . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG . (1", "snippet": "𝒁 → 𝑔 𝑿 ^ . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG . (1.3.33)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E34", "title": "min ⁡ d ​ ( 𝑿 , 𝑿 ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34)", "snippet": "min ⁡ d ​ ( 𝑿 , 𝑿 ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E35", "title": "𝒁 → 𝑔 𝑿 ^ , 𝑿 → 𝑑 𝟎 , 𝟏 , \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}},\\bm{X}\\xrightarrow{\\hskip 5.69054ptd\\hskip 5.69054pt}\\bm{0},\\bm{1}, bold_italic_Z start_ARROW start_OVERACCE", "snippet": "𝒁 → 𝑔 𝑿 ^ , 𝑿 → 𝑑 𝟎 , 𝟏 , \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}},\\bm{X}\\xrightarrow{\\hskip 5.69054ptd\\hskip 5.69054pt}\\bm{0},\\bm{1}, bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG , b"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E36", "title": "min g ⁡ max d ⁡ ℓ ​ ( 𝑿 , 𝑿 ^ ) , \\min_{g}\\max_{d}\\ell(\\bm{X},\\hat{\\bm{X}}), roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT roman_ℓ (", "snippet": "min g ⁡ max d ⁡ ℓ ​ ( 𝑿 , 𝑿 ^ ) , \\min_{g}\\max_{d}\\ell(\\bm{X},\\hat{\\bm{X}}), roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT roman_ℓ ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) , (1.3.36)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E37", "title": "𝒛 0 ∼ 𝒩 ​ ( 𝟎 , 𝑰 ) → g 0 𝒛 1 → g 1 ⋯ → g L − 1 𝒛 L ∼ p ​ ( 𝒙 ) . \\bm{z}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})\\xrightarrow{\\hskip 5.69054ptg^{0}\\hskip 5.69054pt}\\bm{z}^{1}\\xrightarrow{\\hskip 5.69054ptg^{1", "snippet": "𝒛 0 ∼ 𝒩 ​ ( 𝟎 , 𝑰 ) → g 0 𝒛 1 → g 1 ⋯ → g L − 1 𝒛 L ∼ p ​ ( 𝒙 ) . \\bm{z}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})\\xrightarrow{\\hskip 5.69054ptg^{0}\\hskip 5.69054pt}\\bm{z}^{1}\\xrightarrow{\\hskip 5.69054ptg^{1}\\hskip 5.69054pt}\\cdots\\xrightarrow{\\hskip 5.69054ptg^{L-1}\\hskip 5.69054pt}\\bm"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E1", "title": "computable ⟹ tractable ⟹ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ⟹ tractable ⟹ scalable . (1.4.1)", "snippet": "computable ⟹ tractable ⟹ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ⟹ tractable ⟹ scalable . (1.4.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E2", "title": "K ​ ( S ) = min p : 𝒰 ​ ( p ) = S ⁡ L ​ ( p ) . K(S)=\\min_{p\\,:\\,\\mathcal{U}(p)=S}L(p). italic_K ( italic_S ) = roman_min start_POSTSUBSCRIPT italic_p : caligraphic_U ( italic_p ) = italic_S end_POSTS", "snippet": "K ​ ( S ) = min p : 𝒰 ​ ( p ) = S ⁡ L ​ ( p ) . K(S)=\\min_{p\\,:\\,\\mathcal{U}(p)=S}L(p). italic_K ( italic_S ) = roman_min start_POSTSUBSCRIPT italic_p : caligraphic_U ( italic_p ) = italic_S end_POSTSUBSCRIPT italic_L ( italic_p ) . (1.4.2)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E3", "title": "h ​ ( S ) ≐ − ∫ p ​ ( s ) ​ log ⁡ p ​ ( s ) ​ d s h(S)\\doteq-\\int p(s)\\log p(s)\\mathrm{d}s italic_h ( italic_S ) ≐ - ∫ italic_p ( italic_s ) roman_log italic_p ( italic_s ) roman_d italic_s (1.4.3)", "snippet": "h ​ ( S ) ≐ − ∫ p ​ ( s ) ​ log ⁡ p ​ ( s ) ​ d s h(S)\\doteq-\\int p(s)\\log p(s)\\mathrm{d}s italic_h ( italic_S ) ≐ - ∫ italic_p ( italic_s ) roman_log italic_p ( italic_s ) roman_d italic_s (1.4.3)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E4", "title": "{ S 1 , S 2 , … , S i , … , S N } ⊂ ℝ D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSU", "snippet": "{ S 1 , S 2 , … , S i , … , S N } ⊂ ℝ D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , ital"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E5", "title": "R ​ ( f ∣ ℰ ) = 𝔼 ​ [ L ​ ( ℰ ​ ( S ) ) ] ≈ 1 N ​ ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) . R(f\\mid\\mathcal{E})=\\mathbb{E}[L(\\mathcal{E}(S))]\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})). italic_R ( ita", "snippet": "R ​ ( f ∣ ℰ ) = 𝔼 ​ [ L ​ ( ℰ ​ ( S ) ) ] ≈ 1 N ​ ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) . R(f\\mid\\mathcal{E})=\\mathbb{E}[L(\\mathcal{E}(S))]\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})). italic_R ( italic_f ∣ caligraphic_E ) = blackboard_E [ italic_L ( caligraphic_E ( italic_S ) )"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E6", "title": "R ​ ( f ∣ ℰ 1 ) − R ​ ( f ∣ ℰ 2 ) > 0 , R(f\\mid\\mathcal{E}_{1})-R(f\\mid\\mathcal{E}_{2})>0, italic_R ( italic_f ∣ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ∣ caligra", "snippet": "R ​ ( f ∣ ℰ 1 ) − R ​ ( f ∣ ℰ 2 ) > 0 , R(f\\mid\\mathcal{E}_{1})-R(f\\mid\\mathcal{E}_{2})>0, italic_R ( italic_f ∣ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ∣ caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) > 0 , (1.4.6)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E7", "title": "min ℰ ⁡ R ​ ( f ∣ ℰ ) . \\min_{\\mathcal{E}}R(f\\mid\\mathcal{E}). roman_min start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT italic_R ( italic_f ∣ caligraphic_E ) . (1.4.7)", "snippet": "min ℰ ⁡ R ​ ( f ∣ ℰ ) . \\min_{\\mathcal{E}}R(f\\mid\\mathcal{E}). roman_min start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT italic_R ( italic_f ∣ caligraphic_E ) . (1.4.7)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.Ex1", "title": "1 N ​ ( L ​ ( ℰ ) + ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) ) ≈ 1 N ​ ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) \\frac{1}{N}\\Big{(}L(\\mathcal{E})+\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i}))\\Big{)}\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\", "snippet": "1 N ​ ( L ​ ( ℰ ) + ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) ) ≈ 1 N ​ ∑ i = 1 N L ​ ( ℰ ​ ( S i ) ) \\frac{1}{N}\\Big{(}L(\\mathcal{E})+\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i}))\\Big{)}\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})) divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ( itali"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E8", "title": "K ​ ( S ) < L ​ ( ℰ ​ ( S ) ) . K(S)<L(\\mathcal{E}(S)). italic_K ( italic_S ) < italic_L ( caligraphic_E ( italic_S ) ) . (1.4.8)", "snippet": "K ​ ( S ) < L ​ ( ℰ ​ ( S ) ) . K(S)<L(\\mathcal{E}(S)). italic_K ( italic_S ) < italic_L ( caligraphic_E ( italic_S ) ) . (1.4.8)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E9", "title": "𝑿 = { S 1 , S 2 , … , S i , … , S N } ⊂ ℝ D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start", "snippet": "𝑿 = { S 1 , S 2 , … , S i , … , S N } ⊂ ℝ D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i e"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E10", "title": "𝑿 → ℰ 𝒁 . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z . (1.4.10)", "snippet": "𝑿 → ℰ 𝒁 . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z . (1.4.10)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E11", "title": "min ⁡ ρ ​ ( 𝒁 ) . \\min\\rho(\\bm{Z}). roman_min italic_ρ ( bold_italic_Z ) . (1.4.11)", "snippet": "min ⁡ ρ ​ ( 𝒁 ) . \\min\\rho(\\bm{Z}). roman_min italic_ρ ( bold_italic_Z ) . (1.4.11)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E12", "title": "𝒙 ↦ 𝒛 ∈ { [ 1 , 0 , 0 , … , 0 , 0 ] , [ 0 , 1 , 0 … , 0 , 0 ] , … , [ 0 , 0 , 0 , … , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_i", "snippet": "𝒙 ↦ 𝒛 ∈ { [ 1 , 0 , 0 , … , 0 , 0 ] , [ 0 , 1 , 0 … , 0 , 0 ] , … , [ 0 , 0 , 0 , … , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_italic_x ↦ bold_italic_z ∈ { [ 1 , 0 , 0 , … , 0 , 0 ] , [ 0 , 1 , 0 … , 0 , 0 ] "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E13", "title": "L ​ ( 𝒛 ^ , 𝒛 ) = ∑ k = 1 K − z k ​ log ⁡ z ^ k , L(\\hat{\\bm{z}},\\bm{z})=\\sum_{k=1}^{K}-z_{k}\\log\\hat{z}_{k}, italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) = ∑ start_POSTSUBSCRIPT", "snippet": "L ​ ( 𝒛 ^ , 𝒛 ) = ∑ k = 1 K − z k ​ log ⁡ z ^ k , L(\\hat{\\bm{z}},\\bm{z})=\\sum_{k=1}^{K}-z_{k}\\log\\hat{z}_{k}, italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRI"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E14", "title": "𝑿 → ℰ 𝒁 → 𝒟 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT", "snippet": "𝑿 → ℰ 𝒁 → 𝒟 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERAC"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E15", "title": "min ⁡ d ​ ( 𝑿 , 𝑿 ^ ) + ρ ​ ( 𝒁 ) , \\min\\,d(\\bm{X},\\hat{\\bm{X}})+\\rho(\\bm{Z}), roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_ρ ( bold_italic_Z ) , (1.4.15)", "snippet": "min ⁡ d ​ ( 𝑿 , 𝑿 ^ ) + ρ ​ ( 𝒁 ) , \\min\\,d(\\bm{X},\\hat{\\bm{X}})+\\rho(\\bm{Z}), roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_ρ ( bold_italic_Z ) , (1.4.15)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E16", "title": "𝑿 → ℰ 𝒁 → 𝒟 𝑿 ^ → ℰ 𝒁 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054pt\\mathca", "snippet": "𝑿 → ℰ 𝒁 → 𝒟 𝑿 ^ → ℰ 𝒁 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\hat{\\bm{Z}}, bold_italic_X start_ARROW start_OVERACCENT ca"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E17", "title": "max θ ⁡ min η ⁡ ℓ ​ ( 𝒁 , 𝒁 ^ ) + ρ ​ ( 𝒁 ) , \\max_{\\theta}\\min_{\\eta}\\ell(\\bm{Z},\\hat{\\bm{Z}})+\\rho(\\bm{Z}), roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT ita", "snippet": "max θ ⁡ min η ⁡ ℓ ​ ( 𝒁 , 𝒁 ^ ) + ρ ​ ( 𝒁 ) , \\max_{\\theta}\\min_{\\eta}\\ell(\\bm{Z},\\hat{\\bm{Z}})+\\rho(\\bm{Z}), roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT roman_ℓ ( bold_italic_Z , over^ start_ARG bold_italic_Z "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.E1", "title": "open-ended ⟹ bi-directional ⟹ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, open-ended ⟹ bi-directional ⟹ closed-loop , (1.5.1)", "snippet": "open-ended ⟹ bi-directional ⟹ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, open-ended ⟹ bi-directional ⟹ closed-loop , (1.5.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.E2", "title": "Intelligence ⁡ ( t ) = d d ​ t ​ Information ⁡ ( t ) , Information ⁡ ( t ) = ∫ 0 t Intelligence ⁡ ( s ) ​ d s . \\operatorname{Intelligence}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\operatorname{Information}(", "snippet": "Intelligence ⁡ ( t ) = d d ​ t ​ Information ⁡ ( t ) , Information ⁡ ( t ) = ∫ 0 t Intelligence ⁡ ( s ) ​ d s . \\operatorname{Intelligence}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\operatorname{Information}(t),\\qquad\\operatorname{Information}(t)=\\int_{0}^{t}\\operatorname{Intelligence}(s"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1", "title": "Linear Dynamical Systems", "snippet": "Linear Dynamical Systems Wiener Filter. As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a linear time-invar"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2", "title": "Linear and Mixed Linear Models", "snippet": "Linear and Mixed Linear Models Principal Component Analysis. From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional linear subspace. Math"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3", "title": "General Distributions", "snippet": "General Distributions The distributions of real-world data such as images, videos, and audio are too complex to be modeled by above, somewhat idealistic, linear models or Gaussian processes. We normally do not know a priori they are generated from which family of parametric model"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1", "title": "Classic Artificial Neural Networks", "snippet": "Classic Artificial Neural Networks Artificial neuron. Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals. Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 2"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2", "title": "Modern Deep Neural Networks", "snippet": "Modern Deep Neural Networks For nearly 30 years, from 1980s to 2010s, for the study of machine learning and machine intelligence, neural networks were not considered seriously by the mainstream. Early (deep) neural networks, such as the LeNet, have shown promising performance for"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#top", "title": "Chapter 2 Learning Linear and Independent Structures", "snippet": ""}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1", "title": "2.1 A Low-Dimensional Subspace", "snippet": "2.1 A Low-Dimensional Subspace 2.1.1 Principal Components Analysis (PCA) Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data belong t"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2", "title": "2.2 A Mixture of Complete Low-Dimensional Subspaces", "snippet": "2.2 A Mixture of Complete Low-Dimensional Subspaces As we have seen, low-rank signal models are rich enough to provide a full picture of the interplay between low-dimensionality in data and efficient and scalable computational algorithms for representation and recovery under erro"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3", "title": "2.3 A Mixture of Overcomplete Low-Dimensional Subspaces", "snippet": "2.3 A Mixture of Overcomplete Low-Dimensional Subspaces As we have seen, complete dictionary learning enjoys an elegant computational theory in which we maintain a symmetric autoencoding structure ℰ ​ ( 𝒙 ) = 𝑼 ⊤ ​ 𝒙 \\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x} caligraphic_E ( bold_it"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4", "title": "2.4 Summary and Notes", "snippet": "2.4 Summary and Notes The idealistic models we have presented in this chapter—PCA, ICA, and dictionary learning—were developed over the course of the twentieth century. Many books have been written solely about each method, so we will only attempt here to give a broad overview of"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5", "title": "2.5 Exercises and Extensions", "snippet": "2.5 Exercises and Extensions Exercise 2.1 . Prove that, for any symmetric matrix 𝑨 \\bm{A} bold_italic_A , the solution to the problem max 𝑼 ∈ 𝖮 ​ ( D , d ) ⁡ tr ⁡ ( 𝑼 ⊤ ​ 𝑨 ​ 𝑼 ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\top}\\bm{A}\\bm{U}\\right) roman_max star"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1", "title": "2.1.1 Principal Components Analysis (PCA)", "snippet": "2.1.1 Principal Components Analysis (PCA) Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data belong to a low-dimensional subspace of"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2", "title": "2.1.2 Pursuing Low-rank Structure via Power Iteration", "snippet": "2.1.2 Pursuing Low-rank Structure via Power Iteration There is a computationally efficient way to estimate the top eigenvectors of 𝑿 ​ 𝑿 ⊤ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N or any symmetric positive semide"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS3", "title": "2.1.3 Probabilistic PCA", "snippet": "2.1.3 Probabilistic PCA Notice that the above formulation makes no statistical assumptions on the data-generating process. However, it is common to include statistical elements within a given data model, as it may add further enlightening interpretations about the result of the a"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS4", "title": "2.1.4 Matrix Completion", "snippet": "2.1.4 Matrix Completion In the previous Subsections, we discussed the problem of learning a low-rank geometric or statistical distribution , where the data were sampled from a subspace with additive noise. But this is not the only type of disturbance from a low-dimensional distri"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1", "title": "2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries", "snippet": "2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries Let 𝑼 1 , … , 𝑼 K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , each of size D × d D\\times d italic_D × italic_d , d"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2", "title": "2.2.2 Complete Dictionary Learning", "snippet": "2.2.2 Complete Dictionary Learning In this section, we will derive algorithms for solving the orthogonal dictionary learning problem. To be more precise, we assume that the observed vector 𝒙 ∈ ℝ D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3", "title": "2.2.3 Connection to ICA and Kurtosis", "snippet": "2.2.3 Connection to ICA and Kurtosis With the Bernoulli-Gaussian model, the variables z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are independent and non-Gaussian. Then, there is a clear correspondence between the dictionary learning and the classic independ"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1", "title": "2.3.1 Sparse Coding with an Overcomplete Dictionary", "snippet": "2.3.1 Sparse Coding with an Overcomplete Dictionary In this section, we will consider the data model ( 2.3.1 ), which accommodates sparse linear combinations of many motifs, or atoms . Given data { 𝒙 i } i = 1 N ⊆ ℝ D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2", "title": "2.3.2 Overcomplete Dictionary Learning", "snippet": "2.3.2 Overcomplete Dictionary Learning Recall that we have the data model 𝑿 = 𝑨 ​ 𝒁 + 𝑬 , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14) where 𝒁 \\bm{Z} bold_italic_Z is sparse, and our goal previously was to estimate 𝒁 \\bm{Z} bo"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3", "title": "2.3.3 Learned Deep Sparse Coding", "snippet": "2.3.3 Learned Deep Sparse Coding The main insight from the alternating minimization algorithm for overcomplete dictionary learning in the previous section ( Equations 2.3.18 and 2.3.20 ) is to notice that when we fix 𝐀 \\bm{A} bold_italic_A , the ISTA update for 𝐙 ℓ \\bm{Z}^{\\ell} "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px1", "title": "Problem formulation.", "snippet": "Problem formulation. To write this in mathematical notation, we represent a subspace 𝒮 ⊆ ℝ D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of dimension d d italic_d by an orthonormal matrix 𝑼 ∈ 𝖮 ​ ( D , d ) ⊆ ℝ"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px2", "title": "Subspace encoding-decoding via denoising.", "snippet": "Subspace encoding-decoding via denoising. To simplify this problem, for a fixed 𝑼 ~ \\tilde{\\bm{U}} over~ start_ARG bold_italic_U end_ARG , we have (proof as exercise): min { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px3", "title": "Computing the subspace basis.", "snippet": "Computing the subspace basis. For now, we continue our calculation. Let 𝑿 = [ 𝒙 1 , … , 𝒙 N ] ∈ ℝ D × N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.SSS0.Px1", "title": "Orthogonal dictionary for sparse coding.", "snippet": "Orthogonal dictionary for sparse coding. Now we can formulate the structured representation learning problem for mixtures of low-dimensional subspaces that we will study in this section. We assume that we have samples 𝑿 = [ 𝒙 1 , … ​ 𝒙 N ] \\bm{X}=[\\bm{x}_{1},\\dots\\bm{x}_{N}] bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1", "title": "Dictionary learning via the MSP algorithm.", "snippet": "Dictionary learning via the MSP algorithm. Now suppose that we are given a set of observations: 𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bol"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3.SSS0.Px1", "title": "Incremental ICA: correctness and FastICA algorithm.", "snippet": "Incremental ICA: correctness and FastICA algorithm. The FastICA algorithm, advanced by Hyvärinen and Oja [ HO97 ] , is a fast fixed-point algorithm for solving the k = 1 k=1 italic_k = 1 kurtosis maximization scheme for ICA. The problem at hand is max ‖ 𝒗 ‖ 2 2 = 1 ​ kurt ( 𝑿 ⊤ ​"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px1", "title": "Learned ISTA.", "snippet": "Learned ISTA. The above deep-network interpretation of the alternating minimization is more conceptual than practical, as the process could be rather inefficient and take many layers or iterations to converge. But this is mainly because we try to infer both 𝒁 \\bm{Z} bold_italic_Z"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2", "title": "Sparse Autoencoders.", "snippet": "Sparse Autoencoders. The original motivation for overcomplete dictionary learning was to provide a simple generative model for high-dimensional data. We have seen with LISTA that, in addition, iterative algorithms for learning sparsely-used overcomplete dictionaries provide an in"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px3", "title": "Layerwise learned sparse coding?", "snippet": "Layerwise learned sparse coding? In the supervised setting, LISTA provides a deep neural network analogue of the sparse coding iteration, with layerwise-learned dictionaries, inspired by alternating minimization; even in the unsupervised setting, the same methodology can be appli"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem1", "title": "Theorem 2.1 .", "snippet": "Theorem 2.1 . Suppose that our dataset { 𝐱 i } i = 1 N ⊆ ℝ D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ bl"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark1", "title": "Remark 2.1 .", "snippet": "Remark 2.1 . In some data analysis tasks, the data matrix 𝑿 \\bm{X} bold_italic_X is formatted such that each data point is a row rather than a column as is presented here. In this case the principal components are the top d d italic_d eigenvectors of 𝑿 ⊤ ​ 𝑿 / N \\bm{X}^{\\top}\\bm{"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark2", "title": "Remark 2.2 (Basis Selection via Denoising Eigenvalues) .", "snippet": "Remark 2.2 (Basis Selection via Denoising Eigenvalues) . In many cases, either our data will not truly be distributed according to a subspace-plus-noise model, or we will not know the true underlying dimension d d italic_d of the subspace. In this case, we have to choose d d ital"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark3", "title": "Remark 2.3 (Denoising Samples) .", "snippet": "Remark 2.3 (Denoising Samples) . The expression on the right-hand side of ( 2.1.5 ), that is, min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark4", "title": "Remark 2.4 (Neural Network Interpretation) .", "snippet": "Remark 2.4 (Neural Network Interpretation) . If we do a PCA, we approximately recover the support of the distribution encoded by the parameter 𝑼 ⋆ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT . The learned denoising map then takes the form 𝑼 ⋆ ​ ( 𝑼 ⋆ "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem2", "title": "Theorem 2.2 (Power Iteration) .", "snippet": "Theorem 2.2 (Power Iteration) . Assume that λ 1 > λ i \\lambda_{1}>\\lambda_{i} italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all i > 1 i>1 italic_i > 1 . If we compute the fixed point of ( 2.1.16 ) using the followin"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark5", "title": "Remark 2.5 .", "snippet": "Remark 2.5 . By using the probabilistic viewpoint of PCA, we achieve a clearer and more quantitative understanding of how it relates to denoising. First, consider the denoising problem in ( 2.1.26 ), namely min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operato"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem3", "title": "Theorem 2.3 .", "snippet": "Theorem 2.3 . Suppose that the random variable 𝐱 \\bm{x} bold_italic_x can be written as 𝒙 = 𝑼 ​ 𝒛 + 𝜺 \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε (2.1.34) where 𝐔 ∈ 𝖮 ​ ( D , d ) \\bm{U}\\in\\mathsf{O}(D,d) bold_italic_U ∈ sansser"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark6", "title": "Remark 2.6 (A Mixture of Gaussians v.s. A Superposition of Gaussians) .", "snippet": "Remark 2.6 (A Mixture of Gaussians v.s. A Superposition of Gaussians) . One should be aware that the above model ( 2.2.3 ) is a mixture of Gaussian distributions, not to be confused with a mixture of Gaussian variables by superposition, say 𝒙 = ∑ i = 1 n w i ​ 𝒙 i , 𝒙 i ∼ 𝒩 ​ ( 𝟎"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark7", "title": "Remark 2.7 (The Orthogonal Assumption) .", "snippet": "Remark 2.7 (The Orthogonal Assumption) . At first sight, the assumption that the dictionary 𝑼 \\bm{U} bold_italic_U is orthogonal might seem to be somewhat restrictive. But there is actually no loss of generality. One may consider a complete dictionary to be any square invertible "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark8", "title": "Remark 2.8 .", "snippet": "Remark 2.8 . It is also known that for vectors on a sphere, minimizing the ℓ 1 \\ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm is equivalent to minimizing the ℓ 0 \\ell^{0} roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT norm (for promoting sparsity), arg ​ m"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark9", "title": "Remark 2.9 (Global Optimality of ℓ 4 \\ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT Maximization) .", "snippet": "Remark 2.9 (Global Optimality of ℓ 4 \\ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT Maximization) . The constrained ℓ 4 \\ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization problem is a nonconvex program. In general one should not expect that an"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark10", "title": "Remark 2.10 (Stable Deep Linear Network) .", "snippet": "Remark 2.10 (Stable Deep Linear Network) . The above iterative process of computing the dictionary has a natural incremental “deep learning” interpretation. Let us define δ ​ 𝑨 t + 1 = 𝑨 t + 1 ​ 𝑨 t ⊤ \\delta\\bm{A}_{t+1}=\\bm{A}_{t+1}\\bm{A}_{t}^{\\top} italic_δ bold_italic_A start_P"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexample1", "title": "Example 2.1 .", "snippet": "Example 2.1 . Given sampled images of hand-written digits, Figure 2.6 (a) shows the result of fitting an orthogonal dictionary to the dataset. In contrast, Figure 2.6 (b) shows the result of running an optimization algorithm for learning overcomplete dictionaries (which we will p"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark11", "title": "Remark 2.11 ( ℓ 4 \\ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization versus ℓ 1 \\ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization) .", "snippet": "Remark 2.11 ( ℓ 4 \\ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization versus ℓ 1 \\ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization) . Note that the above problem formulation follows naturally from the LASSO formulation ( 2.3.5 ) for s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise1", "title": "Exercise 2.1 .", "snippet": "Exercise 2.1 . Prove that, for any symmetric matrix 𝑨 \\bm{A} bold_italic_A , the solution to the problem max 𝑼 ∈ 𝖮 ​ ( D , d ) ⁡ tr ⁡ ( 𝑼 ⊤ ​ 𝑨 ​ 𝑼 ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\top}\\bm{A}\\bm{U}\\right) roman_max start_POSTSUBSCRIPT bold_italic_U"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise2", "title": "Exercise 2.2 .", "snippet": "Exercise 2.2 . Let 𝒛 ∼ 𝒩 ​ ( 𝟎 , σ 2 ​ 𝑰 ) \\bm{z}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_z ∼ caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) be a Gaussian random variable with independent components, each with varianc"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise3", "title": "Exercise 2.3 .", "snippet": "Exercise 2.3 . The notion of statistical identifiability discussed above can be related to symmetries of the model class, allowing estimation to be understood in a purely deterministic fashion without any statistical assumptions. Consider the model 𝑿 = 𝑼 ​ 𝒁 \\bm{X}=\\bm{U}\\bm{Z} b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise4", "title": "Exercise 2.4 .", "snippet": "Exercise 2.4 . Consider the model 𝒙 = 𝑼 ​ 𝒛 \\bm{x}=\\bm{U}\\bm{z} bold_italic_x = bold_italic_U bold_italic_z , where 𝑼 ∈ ℝ D × d \\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT with D ≥ d D\\geq d italic_D ≥"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise5", "title": "Exercise 2.5 .", "snippet": "Exercise 2.5 . Let X X italic_X and Y Y italic_Y be zero-mean independent random variables. 1. Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm{kurt}}(Y) roman_kurt ( italic_X + italic_Y ) = roman_kurt ( ital"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise6", "title": "Exercise 2.6 .", "snippet": "Exercise 2.6 . Let f : ℝ d → ℝ f:\\mathbb{R}^{d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R be a given twice-continuously-differentiable objective function. Consider the spherically-constrained optimization problem max ‖ "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise7", "title": "Exercise 2.7 .", "snippet": "Exercise 2.7 . In this exercise, we sketch an argument referred to in the literature as a landscape analysis for the spherically-constrained population kurtosis maximization problem ( 2.2.24 ). We will show that when there is at least one independent component with positive kurto"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise8", "title": "Exercise 2.8 .", "snippet": "Exercise 2.8 . This exercise follows the structure and formalism introduced in Exercise 2.6 , but applies it instead to the orthogonal group 𝖮 ​ ( d ) = { 𝑼 ∈ ℝ d × d ∣ 𝑼 ⊤ ​ 𝑼 = 𝑰 } \\mathsf{O}(d)=\\{\\bm{U}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{U}^{\\top}\\bm{U}=\\bm{I}\\} sansserif_O ( ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E1", "title": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\quad\\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z st", "snippet": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\quad\\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E2", "title": "min 𝑼 ~ , { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 , \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|", "snippet": "min 𝑼 ~ , { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 , \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E5", "title": "min 𝑼 ~ , { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 = min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 . \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\", "snippet": "min 𝑼 ~ , { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 = min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 . \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2}=\\min_{\\tild"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E6", "title": "𝒙 → ℰ = ( 𝑼 ⋆ ) ⊤ 𝒛 → 𝒟 = 𝑼 ⋆ 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=(\\bm{U}^{\\star})^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}^{\\star}\\hskip 5.69054pt", "snippet": "𝒙 → ℰ = ( 𝑼 ⋆ ) ⊤ 𝒛 → 𝒟 = 𝑼 ⋆ 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=(\\bm{U}^{\\star})^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}^{\\star}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = ( bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E9", "title": "max 𝒖 ~ : ‖ 𝒖 ~ ‖ 2 = 1 ⁡ 𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ . \\max_{\\tilde{\\bm{u}}\\colon\\|\\tilde{\\bm{u}}\\|_{2}=1}\\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}. roman_max start_POSTSUBSCRIPT o", "snippet": "max 𝒖 ~ : ‖ 𝒖 ~ ‖ 2 = 1 ⁡ 𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ . \\max_{\\tilde{\\bm{u}}\\colon\\|\\tilde{\\bm{u}}\\|_{2}=1}\\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}. roman_max start_POSTSUBSCRIPT over~ start_ARG bold_italic_u end_ARG : ∥ over~ start_ARG bold_italic_u end_ARG ∥"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E10", "title": "𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ = 𝒖 ~ ⊤ ​ 𝑽 ​ 𝚲 ​ 𝑽 ⊤ ​ 𝒖 = ( 𝑽 ⊤ ​ 𝒖 ~ ) ⊤ ​ 𝚲 ​ ( 𝑽 ⊤ ​ 𝒖 ~ ) . \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{u}}^{\\top}\\bm{V}\\bm{\\Lambda}\\bm{V}", "snippet": "𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ = 𝒖 ~ ⊤ ​ 𝑽 ​ 𝚲 ​ 𝑽 ⊤ ​ 𝒖 = ( 𝑽 ⊤ ​ 𝒖 ~ ) ⊤ ​ 𝚲 ​ ( 𝑽 ⊤ ​ 𝒖 ~ ) . \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{u}}^{\\top}\\bm{V}\\bm{\\Lambda}\\bm{V}^{\\top}\\bm{u}=(\\bm{V}^{\\top}\\tilde{\\bm{u}})^{\\top}\\bm{\\Lambda}(\\bm{V}^{\\top}\\til"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E11", "title": "𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ = 𝒘 ~ ⊤ ​ 𝚲 ​ 𝒘 ~ , \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{w}}^{\\top}\\bm{\\Lambda}\\tilde{\\bm{w}}, over~ start_ARG bold_italic_u end_ARG star", "snippet": "𝒖 ~ ⊤ ​ ( 𝑿 ​ 𝑿 ⊤ / N ) ​ 𝒖 ~ = 𝒘 ~ ⊤ ​ 𝚲 ​ 𝒘 ~ , \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{w}}^{\\top}\\bm{\\Lambda}\\tilde{\\bm{w}}, over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POST"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E12", "title": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z s", "snippet": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E13", "title": "min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBS", "snippet": "min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E14", "title": "denoise ⁡ ( 𝒙 ) = 𝑼 ⋆ ∘ id ∘ ( 𝑼 ⋆ ) ⊤ ​ 𝒙 ⏟ first “layer” ⏟ post-activation of first “layer” ⏟ output of “NN” \\operatorname{denoise}(\\bm{x})=\\underbrace{\\bm{U}^{\\star}\\circ\\underbrace{\\operatorname{i", "snippet": "denoise ⁡ ( 𝒙 ) = 𝑼 ⋆ ∘ id ∘ ( 𝑼 ⋆ ) ⊤ ​ 𝒙 ⏟ first “layer” ⏟ post-activation of first “layer” ⏟ output of “NN” \\operatorname{denoise}(\\bm{x})=\\underbrace{\\bm{U}^{\\star}\\circ\\underbrace{\\operatorname{id}\\circ\\underbrace{(\\bm{U}^{\\star})^{\\top}\\bm{x}}_{\\text{first ``layer''}}}_{\\te"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E15", "title": "NN ⁡ ( 𝒙 ) = 𝑾 ⋆ ∘ ReLU ∘ ( 𝑼 ⋆ ) ⊤ ​ 𝒙 ⏟ first layer ⏟ post-activation of first layer ⏟ output of NN \\operatorname{NN}(\\bm{x})=\\underbrace{\\bm{W}^{\\star}\\circ\\underbrace{\\mathrm{ReLU}\\circ\\underbrace", "snippet": "NN ⁡ ( 𝒙 ) = 𝑾 ⋆ ∘ ReLU ∘ ( 𝑼 ⋆ ) ⊤ ​ 𝒙 ⏟ first layer ⏟ post-activation of first layer ⏟ output of NN \\operatorname{NN}(\\bm{x})=\\underbrace{\\bm{W}^{\\star}\\circ\\underbrace{\\mathrm{ReLU}\\circ\\underbrace{(\\bm{U}^{\\star})^{\\top}\\bm{x}}_{\\text{first layer}}}_{\\text{post-activation of "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E16", "title": "𝒘 = 𝑴 ​ 𝒘 ‖ 𝑴 ​ 𝒘 ‖ 2 . \\bm{w}=\\frac{\\bm{M}\\bm{w}}{\\|\\bm{M}\\bm{w}\\|_{2}}. bold_italic_w = divide start_ARG bold_italic_M bold_italic_w end_ARG start_ARG ∥ bold_italic_M bold_italic_w ∥ start_POSTSUBSC", "snippet": "𝒘 = 𝑴 ​ 𝒘 ‖ 𝑴 ​ 𝒘 ‖ 2 . \\bm{w}=\\frac{\\bm{M}\\bm{w}}{\\|\\bm{M}\\bm{w}\\|_{2}}. bold_italic_w = divide start_ARG bold_italic_M bold_italic_w end_ARG start_ARG ∥ bold_italic_M bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG . (2.1.16)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E17", "title": "𝒗 0 ∼ 𝒩 ⁡ ( 𝟎 , 𝟏 ) , 𝒗 t + 1 ← 𝑴 ​ 𝒗 t ‖ 𝑴 ​ 𝒗 t ‖ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_ital", "snippet": "𝒗 0 ∼ 𝒩 ⁡ ( 𝟎 , 𝟏 ) , 𝒗 t + 1 ← 𝑴 ​ 𝒗 t ‖ 𝑴 ​ 𝒗 t ‖ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_1 )"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E18", "title": "𝒗 t = 𝑴 ​ 𝒗 t − 1 ‖ 𝑴 ​ 𝒗 t − 1 ‖ 2 = 𝑴 2 ​ 𝒗 t − 2 ‖ 𝑴 ​ 𝒗 t − 1 ‖ 2 ​ ‖ 𝑴 ​ 𝒗 t − 2 ‖ 2 = ⋯ = 𝑴 t ​ 𝒗 0 ∏ s = 1 t ‖ 𝑴 ​ 𝒗 s ‖ 2 . \\bm{v}_{t}=\\frac{\\bm{M}\\bm{v}_{t-1}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}}=\\fr", "snippet": "𝒗 t = 𝑴 ​ 𝒗 t − 1 ‖ 𝑴 ​ 𝒗 t − 1 ‖ 2 = 𝑴 2 ​ 𝒗 t − 2 ‖ 𝑴 ​ 𝒗 t − 1 ‖ 2 ​ ‖ 𝑴 ​ 𝒗 t − 2 ‖ 2 = ⋯ = 𝑴 t ​ 𝒗 0 ∏ s = 1 t ‖ 𝑴 ​ 𝒗 s ‖ 2 . \\bm{v}_{t}=\\frac{\\bm{M}\\bm{v}_{t-1}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}}=\\frac{\\bm{M}^{2}\\bm{v}_{t-2}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}\\|\\bm{M}\\bm{v}_{t-2}\\|_{2}}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E19", "title": "𝒗 t = 𝑴 t ​ 𝒗 0 ‖ 𝑴 t ​ 𝒗 0 ‖ 2 . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M st", "snippet": "𝒗 t = 𝑴 t ​ 𝒗 0 ‖ 𝑴 t ​ 𝒗 0 ‖ 2 . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E20", "title": "𝒗 0 = ∑ i = 1 D α i ​ 𝒘 i , \\bm{v}_{0}=\\sum_{i=1}^{D}\\alpha_{i}\\bm{w}_{i}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERS", "snippet": "𝒗 0 = ∑ i = 1 D α i ​ 𝒘 i , \\bm{v}_{0}=\\sum_{i=1}^{D}\\alpha_{i}\\bm{w}_{i}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E21", "title": "𝒗 t = 𝑴 t ​ 𝒗 0 ‖ 𝑴 t ​ 𝒗 0 ‖ 2 = ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ‖ ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ‖ 2 = ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ∑ i = 1 D λ i t ​ | α i | . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M", "snippet": "𝒗 t = 𝑴 t ​ 𝒗 0 ‖ 𝑴 t ​ 𝒗 0 ‖ 2 = ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ‖ ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ‖ 2 = ∑ i = 1 D λ i t ​ α i ​ 𝒘 i ∑ i = 1 D λ i t ​ | α i | . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}=\\frac{\\sum_{i=1}^{D}\\lambda_{i}^{t}\\alpha_{i}\\bm{w}_{i}}{"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E22", "title": "𝒗 t = α 1 ​ 𝒘 1 + ∑ i = 2 D ( λ i / λ 1 ) t ​ α i ​ 𝒘 i | α 1 | + ∑ i = 2 D ( λ i / λ 1 ) t ​ | α i | . \\bm{v}_{t}=\\frac{\\alpha_{1}\\bm{w}_{1}+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\alpha_{i}\\bm{w", "snippet": "𝒗 t = α 1 ​ 𝒘 1 + ∑ i = 2 D ( λ i / λ 1 ) t ​ α i ​ 𝒘 i | α 1 | + ∑ i = 2 D ( λ i / λ 1 ) t ​ | α i | . \\bm{v}_{t}=\\frac{\\alpha_{1}\\bm{w}_{1}+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\alpha_{i}\\bm{w}_{i}}{\\lvert\\alpha_{1}\\rvert+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\lvert\\"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E23", "title": "lim t → ∞ 𝒗 t = α 1 | α 1 | ​ 𝒘 1 = sign ⁡ ( α 1 ) ​ 𝒘 1 , \\lim_{t\\to\\infty}\\bm{v}_{t}=\\frac{\\alpha_{1}}{\\lvert\\alpha_{1}\\rvert}\\bm{w}_{1}=\\operatorname{sign}(\\alpha_{1})\\bm{w}_{1}, roman_lim start_PO", "snippet": "lim t → ∞ 𝒗 t = α 1 | α 1 | ​ 𝒘 1 = sign ⁡ ( α 1 ) ​ 𝒘 1 , \\lim_{t\\to\\infty}\\bm{v}_{t}=\\frac{\\alpha_{1}}{\\lvert\\alpha_{1}\\rvert}\\bm{w}_{1}=\\operatorname{sign}(\\alpha_{1})\\bm{w}_{1}, roman_lim start_POSTSUBSCRIPT italic_t → ∞ end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E24", "title": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 . \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}. bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε . (2.1.24)", "snippet": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 . \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}. bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε . (2.1.24)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E25", "title": "min 𝑼 ~ , 𝒛 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ‖ 2 2 . \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG ", "snippet": "min 𝑼 ~ , 𝒛 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ‖ 2 2 . \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , over~ start_ARG bold_italic_z end_ARG end_POSTSUBSCRIPT "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E26", "title": "min 𝑼 ~ , 𝒛 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ‖ 2 2 = min 𝑼 ~ ⁡ 𝔼 ⁡ min 𝒛 ~ ​ ( 𝒙 ) ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ​ ( 𝒙 ) ‖ 2 2 = min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 , \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatornam", "snippet": "min 𝑼 ~ , 𝒛 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ‖ 2 2 = min 𝑼 ~ ⁡ 𝔼 ⁡ min 𝒛 ~ ​ ( 𝒙 ) ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝒛 ~ ​ ( 𝒙 ) ‖ 2 2 = min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 , \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}=\\min_{\\tilde{\\bm{U}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E29", "title": "𝔼 ⁡ [ 𝒙 ] = 𝑼 ​ 𝔼 ⁡ [ 𝒛 ] + 𝔼 ⁡ [ 𝜺 ] = 𝟎 , \\operatorname{\\mathbb{E}}[\\bm{x}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}]+\\operatorname{\\mathbb{E}}[\\bm{\\varepsilon}]=\\bm{0}, blackboard_E [ bold_italic_x ]", "snippet": "𝔼 ⁡ [ 𝒙 ] = 𝑼 ​ 𝔼 ⁡ [ 𝒛 ] + 𝔼 ⁡ [ 𝜺 ] = 𝟎 , \\operatorname{\\mathbb{E}}[\\bm{x}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}]+\\operatorname{\\mathbb{E}}[\\bm{\\varepsilon}]=\\bm{0}, blackboard_E [ bold_italic_x ] = bold_italic_U blackboard_E [ bold_italic_z ] + blackboard_E [ bold_italic_ε ]"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E30", "title": "Cov ⁡ [ 𝒙 ] = 𝑼 ​ Cov ⁡ [ 𝒛 ] ​ 𝑼 ⊤ + Cov ⁡ [ 𝜺 ] = 𝑼 ​ 𝔼 ⁡ [ 𝒛 ​ 𝒛 ⊤ ] ​ 𝑼 ⊤ + Cov ⁡ [ 𝜺 ] . \\operatorname{Cov}[\\bm{x}]=\\bm{U}\\operatorname{Cov}[\\bm{z}]\\bm{U}^{\\top}+\\operatorname{Cov}[\\bm{\\varepsilo", "snippet": "Cov ⁡ [ 𝒙 ] = 𝑼 ​ Cov ⁡ [ 𝒛 ] ​ 𝑼 ⊤ + Cov ⁡ [ 𝜺 ] = 𝑼 ​ 𝔼 ⁡ [ 𝒛 ​ 𝒛 ⊤ ] ​ 𝑼 ⊤ + Cov ⁡ [ 𝜺 ] . \\operatorname{Cov}[\\bm{x}]=\\bm{U}\\operatorname{Cov}[\\bm{z}]\\bm{U}^{\\top}+\\operatorname{Cov}[\\bm{\\varepsilon}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}\\bm{z}^{\\top}]\\bm{U}^{\\top}+\\operatorn"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E31", "title": "min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bo", "snippet": "min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E32", "title": "𝑼 ⋆ ​ ( 𝑼 ⋆ ) ⊤ = 𝑼 ​ 𝑼 ⊤ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top}=\\bm{U}\\bm{U}^{\\top} bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIP", "snippet": "𝑼 ⋆ ​ ( 𝑼 ⋆ ) ⊤ = 𝑼 ​ 𝑼 ⊤ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top}=\\bm{U}\\bm{U}^{\\top} bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = bold_italic_U bold_italic_U st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E33", "title": "𝒮 = col ( 𝑼 ) = col ( 𝑼 ​ 𝑼 ⊤ ) = col ( 𝑼 ⋆ ​ ( 𝑼 ⋆ ) ⊤ ) = col ( 𝑼 ⋆ ) . \\mathcal{S}=\\mathop{\\mathrm{col}}(\\bm{U})=\\mathop{\\mathrm{col}}(\\bm{U}\\bm{U}^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}(\\bm{", "snippet": "𝒮 = col ( 𝑼 ) = col ( 𝑼 ​ 𝑼 ⊤ ) = col ( 𝑼 ⋆ ​ ( 𝑼 ⋆ ) ⊤ ) = col ( 𝑼 ⋆ ) . \\mathcal{S}=\\mathop{\\mathrm{col}}(\\bm{U})=\\mathop{\\mathrm{col}}(\\bm{U}\\bm{U}^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}). caligraphic_S = roman"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E34", "title": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε (2.1.34)", "snippet": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε (2.1.34)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E35", "title": "𝒀 = 𝑴 ⊙ 𝑿 , \\bm{Y}=\\bm{M}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\", "snippet": "𝒀 = 𝑴 ⊙ 𝑿 , \\bm{Y}=\\bm{M}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scal"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E36", "title": "𝑴 = [ 𝟏 ( D − 1 ) × 1 𝟎 ( D − 1 ) × ( N − 1 ) 1 𝟏 1 × ( N − 1 ) ] . \\bm{M}=\\begin{bmatrix}\\bm{1}_{(D-1)\\times 1}&\\bm{0}_{(D-1)\\times(N-1)}\\\\ 1&\\bm{1}_{1\\times(N-1)}\\end{bmatrix}. bold_italic_M = [ sta", "snippet": "𝑴 = [ 𝟏 ( D − 1 ) × 1 𝟎 ( D − 1 ) × ( N − 1 ) 1 𝟏 1 × ( N − 1 ) ] . \\bm{M}=\\begin{bmatrix}\\bm{1}_{(D-1)\\times 1}&\\bm{0}_{(D-1)\\times(N-1)}\\\\ 1&\\bm{1}_{1\\times(N-1)}\\end{bmatrix}. bold_italic_M = [ start_ARG start_ROW start_CELL bold_1 start_POSTSUBSCRIPT ( italic_D - 1 ) × 1 end_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex1", "title": "𝒛 = ℰ ​ ( 𝒙 ) = 𝑼 ⊤ ​ 𝒙 , 𝒙 ^ = 𝒟 ​ ( 𝒛 ) = 𝑼 ​ 𝒛 , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) =", "snippet": "𝒛 = ℰ ​ ( 𝒙 ) = 𝑼 ⊤ ​ 𝒙 , 𝒙 ^ = 𝒟 ​ ( 𝒛 ) = 𝑼 ​ 𝒛 , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x , over^"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E1", "title": "𝒙 ^ = 𝒟 ​ ( 𝒛 ) = ( ∑ k = 1 K π k ​ ( 𝒛 ) ​ 𝑼 k ) ​ 𝒛 , \\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\left(\\sum_{k=1}^{K}\\pi_{k}(\\bm{z})\\bm{U}_{k}\\right)\\bm{z}, over^ start_ARG bold_italic_x end_ARG = caligraphic", "snippet": "𝒙 ^ = 𝒟 ​ ( 𝒛 ) = ( ∑ k = 1 K π k ​ ( 𝒛 ) ​ 𝑼 k ) ​ 𝒛 , \\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\left(\\sum_{k=1}^{K}\\pi_{k}(\\bm{z})\\bm{U}_{k}\\right)\\bm{z}, over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = ( ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E2", "title": "𝒙 = 𝑼 k ​ 𝒛 for some ​ k ∈ [ K ] , 𝒛 ∈ ℝ d . \\bm{x}=\\bm{U}_{k}\\bm{z}\\quad\\text{for some}\\enspace k\\in[K],\\enspace\\bm{z}\\in\\mathbb{R}^{d}. bold_italic_x = bold_italic_U start_POSTSUBSCRIPT italic_k end", "snippet": "𝒙 = 𝑼 k ​ 𝒛 for some ​ k ∈ [ K ] , 𝒛 ∈ ℝ d . \\bm{x}=\\bm{U}_{k}\\bm{z}\\quad\\text{for some}\\enspace k\\in[K],\\enspace\\bm{z}\\in\\mathbb{R}^{d}. bold_italic_x = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z for some italic_k ∈ [ italic_K ] , bold_italic_z ∈ "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E3", "title": "𝒙 ∼ ∑ k = 1 K π k ​ 𝒩 ​ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) , for some ​ π k ≥ 0 , ∑ k = 1 K π k = 1 . \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}),\\quad\\text{for some}\\enspace\\p", "snippet": "𝒙 ∼ ∑ k = 1 K π k ​ 𝒩 ​ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) , for some ​ π k ≥ 0 , ∑ k = 1 K π k = 1 . \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}),\\quad\\text{for some}\\enspace\\pi_{k}\\geq 0,\\enspace\\sum_{k=1}^{K}\\pi_{k}=1. bold_italic_x ∼ ∑ start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E4", "title": "𝒙 = ∑ i = 1 n w i ​ 𝒙 i , 𝒙 i ∼ 𝒩 ​ ( 𝟎 , 𝑼 i ​ 𝑼 i ⊤ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ∑ start_POSTSUBSC", "snippet": "𝒙 = ∑ i = 1 n w i ​ 𝒙 i , 𝒙 i ∼ 𝒩 ​ ( 𝟎 , 𝑼 i ​ 𝑼 i ⊤ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPER"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E5", "title": "𝒙 = [ | … | 𝑼 1 … 𝑼 K | … | ] ⏟ 𝑼 ​ [ 𝒛 1 ⋮ 𝒛 K ] ⏟ 𝒛 , ‖ [ ‖ 𝒛 1 ‖ 2 ⋮ ‖ 𝒛 K ‖ 2 ] ‖ 0 = 1 . \\bm{x}=\\underbrace{\\begin{bmatrix}|&\\ldots&|\\\\ \\bm{U}_{1}&\\ldots&\\bm{U}_{K}\\\\ |&\\ldots&|\\end{bmatrix}}_{\\b", "snippet": "𝒙 = [ | … | 𝑼 1 … 𝑼 K | … | ] ⏟ 𝑼 ​ [ 𝒛 1 ⋮ 𝒛 K ] ⏟ 𝒛 , ‖ [ ‖ 𝒛 1 ‖ 2 ⋮ ‖ 𝒛 K ‖ 2 ] ‖ 0 = 1 . \\bm{x}=\\underbrace{\\begin{bmatrix}|&\\ldots&|\\\\ \\bm{U}_{1}&\\ldots&\\bm{U}_{K}\\\\ |&\\ldots&|\\end{bmatrix}}_{\\bm{U}}\\underbrace{\\begin{bmatrix}\\bm{z}_{1}\\\\ \\vdots\\\\ \\bm{z}_{K}\\end{bmatrix}}_{"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E6", "title": "‖ 𝒛 ‖ 0 = | { i ∣ z i ≠ 0 } | , \\|\\bm{z}\\|_{0}=\\left\\lvert\\{i\\mid z_{i}\\neq 0\\}\\right\\rvert, ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = | { italic_i ∣ italic_z start_POSTSUBSCRIPT ita", "snippet": "‖ 𝒛 ‖ 0 = | { i ∣ z i ≠ 0 } | , \\|\\bm{z}\\|_{0}=\\left\\lvert\\{i\\mid z_{i}\\neq 0\\}\\right\\rvert, ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = | { italic_i ∣ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≠ 0 } | , (2.2.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E7", "title": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 , ‖ 𝒛 ‖ 0 ≪ d , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}\\ll d, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_P", "snippet": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 , ‖ 𝒛 ‖ 0 ≪ d , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}\\ll d, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≪ italic_d , (2.2.7)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E8", "title": "𝒙 → ℰ = 𝑼 ⊤ 𝒛 → 𝒟 = 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic", "snippet": "𝒙 → ℰ = 𝑼 ⊤ 𝒛 → 𝒟 = 𝑼 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCR"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E9", "title": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε , (2.2.9)", "snippet": "𝒙 = 𝑼 ​ 𝒛 + 𝜺 , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε , (2.2.9)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex2", "title": "z i ∼ Bern ​ ( θ ) ⋅ 𝒩 ​ ( 0 , 1 / θ ) . z_{i}\\sim\\mathrm{Bern}(\\theta)\\cdot\\mathcal{N}(0,1/\\theta). italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ roman_Bern ( italic_θ ) ⋅ caligraphic_N (", "snippet": "z i ∼ Bern ​ ( θ ) ⋅ 𝒩 ​ ( 0 , 1 / θ ) . z_{i}\\sim\\mathrm{Bern}(\\theta)\\cdot\\mathcal{N}(0,1/\\theta). italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ roman_Bern ( italic_θ ) ⋅ caligraphic_N ( 0 , 1 / italic_θ ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E10", "title": "𝑿 ¯ = ( 1 N ​ θ ​ 𝑿 ​ 𝑿 ⊤ ) − 1 2 ​ 𝑿 , \\bar{\\bm{X}}=\\Big{(}\\frac{1}{N\\theta}\\bm{X}\\bm{X}^{\\top}\\Big{)}^{-\\frac{1}{2}}\\bm{X}, over¯ start_ARG bold_italic_X end_ARG = ( divide start_ARG 1 end_ARG start", "snippet": "𝑿 ¯ = ( 1 N ​ θ ​ 𝑿 ​ 𝑿 ⊤ ) − 1 2 ​ 𝑿 , \\bar{\\bm{X}}=\\Big{(}\\frac{1}{N\\theta}\\bm{X}\\bm{X}^{\\top}\\Big{)}^{-\\frac{1}{2}}\\bm{X}, over¯ start_ARG bold_italic_X end_ARG = ( divide start_ARG 1 end_ARG start_ARG italic_N italic_θ end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E11", "title": "𝑿 ¯ = 𝑼 o ​ 𝒁 . \\bar{\\bm{X}}=\\bm{U}_{o}\\bm{Z}. over¯ start_ARG bold_italic_X end_ARG = bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_Z . (2.2.11)", "snippet": "𝑿 ¯ = 𝑼 o ​ 𝒁 . \\bar{\\bm{X}}=\\bm{U}_{o}\\bm{Z}. over¯ start_ARG bold_italic_X end_ARG = bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_Z . (2.2.11)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E12", "title": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start", "snippet": "𝒙 i = 𝑼 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT it"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E13", "title": "𝑨 ​ 𝒙 i = 𝑨 ​ 𝑼 ​ 𝒛 i + 𝑨 ​ 𝜺 i \\bm{A}\\bm{x}_{i}=\\bm{A}\\bm{U}\\bm{z}_{i}+\\bm{A}\\bm{\\varepsilon}_{i} bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_itali", "snippet": "𝑨 ​ 𝒙 i = 𝑨 ​ 𝑼 ​ 𝒛 i + 𝑨 ​ 𝜺 i \\bm{A}\\bm{x}_{i}=\\bm{A}\\bm{U}\\bm{z}_{i}+\\bm{A}\\bm{\\varepsilon}_{i} bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_A"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E14", "title": "arg ​ max 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 4 ⇔ arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 0 . \\operatorname*{arg\\ max}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{4}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\", "snippet": "arg ​ max 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 4 ⇔ arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 0 . \\operatorname*{arg\\ max}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{4}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}. start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E15", "title": "max 𝑨 ~ ∈ 𝖮 ​ ( D ) ⁡ 1 4 ​ ‖ 𝑨 ~ ​ 𝑿 ‖ 4 4 = 1 4 ​ ∑ i = 1 N ‖ 𝑨 ~ ​ 𝒙 i ‖ 4 4 \\max_{\\tilde{\\bm{A}}\\in\\mathsf{O}(D)}\\,\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}=\\frac{1}{4}\\sum_{i=1}^{N}\\", "snippet": "max 𝑨 ~ ∈ 𝖮 ​ ( D ) ⁡ 1 4 ​ ‖ 𝑨 ~ ​ 𝑿 ‖ 4 4 = 1 4 ​ ∑ i = 1 N ‖ 𝑨 ~ ​ 𝒙 i ‖ 4 4 \\max_{\\tilde{\\bm{A}}\\in\\mathsf{O}(D)}\\,\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}=\\frac{1}{4}\\sum_{i=1}^{N}\\left\\|\\tilde{\\bm{A}}\\bm{x}_{i}\\right\\|_{4}^{4} roman_max start_POSTSUBSCRIPT ove"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex3", "title": "arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 1 ⇔ arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\", "snippet": "arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 1 ⇔ arg ​ min 𝒛 ∈ 𝕊 n ⁡ ‖ 𝒛 ‖ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}, start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E16", "title": "min − 1 4 ​ ‖ 𝑨 ~ ​ 𝑿 ‖ 4 4 subject to 𝑨 ~ ⊤ ​ 𝑨 ~ = 𝑰 . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\tilde{\\bm{A}}^{\\top}\\tilde{\\bm{A}}=\\bm{I}. roman_min -", "snippet": "min − 1 4 ​ ‖ 𝑨 ~ ​ 𝑿 ‖ 4 4 subject to 𝑨 ~ ⊤ ​ 𝑨 ~ = 𝑰 . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\tilde{\\bm{A}}^{\\top}\\tilde{\\bm{A}}=\\bm{I}. roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∥ over~ start_ARG bold_italic_A "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E17", "title": "𝑨 ⋆ = 𝒫 O ​ ( D ) ​ [ ( 𝑨 ⋆ ​ 𝑿 ) ⊙ 3 ​ 𝑿 ⊤ ] , \\bm{A}^{\\star}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}^{\\star}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\di", "snippet": "𝑨 ⋆ = 𝒫 O ​ ( D ) ​ [ ( 𝑨 ⋆ ​ 𝑿 ) ⊙ 3 ​ 𝑿 ⊤ ] , \\bm{A}^{\\star}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}^{\\star}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptsty"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E18", "title": "𝑨 t + 1 = 𝒫 O ​ ( D ) ​ [ ( 𝑨 t ​ 𝑿 ) ⊙ 3 ​ 𝑿 ⊤ ] . \\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}_{t}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\disp", "snippet": "𝑨 t + 1 = 𝒫 O ​ ( D ) ​ [ ( 𝑨 t ​ 𝑿 ) ⊙ 3 ​ 𝑿 ⊤ ] . \\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}_{t}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E19", "title": "min − 1 4 ​ ‖ 𝒒 ⊤ ​ 𝑿 ‖ 4 4 subject to 𝒒 ⊤ ​ 𝒒 = 1 . \\min\\,-\\frac{1}{4}\\left\\|\\bm{q}^{\\top}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\bm{q}^{\\top}\\bm{q}=1. roman_min - divide start_ARG 1 end_AR", "snippet": "min − 1 4 ​ ‖ 𝒒 ⊤ ​ 𝑿 ‖ 4 4 subject to 𝒒 ⊤ ​ 𝒒 = 1 . \\min\\,-\\frac{1}{4}\\left\\|\\bm{q}^{\\top}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\bm{q}^{\\top}\\bm{q}=1. roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∥ bold_italic_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex4", "title": "δ ​ 𝑨 t + 1 = 𝒫 O ​ ( D ) ​ [ ( 𝒁 t ) ⊙ 3 ​ 𝒁 t ⊤ ] . \\delta\\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[(\\bm{Z}_{t})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\disp", "snippet": "δ ​ 𝑨 t + 1 = 𝒫 O ​ ( D ) ​ [ ( 𝒁 t ) ⊙ 3 ​ 𝒁 t ⊤ ] . \\delta\\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[(\\bm{Z}_{t})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex5", "title": "𝒁 ⟵ 𝒁 t + 1 = δ ​ 𝑨 t + 1 ​ δ ​ 𝑨 t ​ … ​ δ ​ 𝑨 1 ⏟ forward constructed layers ​ 𝑿 . \\bm{Z}\\;\\longleftarrow\\;\\bm{Z}_{t+1}=\\underbrace{\\delta\\bm{A}_{t+1}\\delta\\bm{A}_{t}\\ldots\\delta\\bm{A}_{1}}_{\\color[", "snippet": "𝒁 ⟵ 𝒁 t + 1 = δ ​ 𝑨 t + 1 ​ δ ​ 𝑨 t ​ … ​ δ ​ 𝑨 1 ⏟ forward constructed layers ​ 𝑿 . \\bm{Z}\\;\\longleftarrow\\;\\bm{Z}_{t+1}=\\underbrace{\\delta\\bm{A}_{t+1}\\delta\\bm{A}_{t}\\ldots\\delta\\bm{A}_{1}}_{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\text{forward construc"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E20", "title": "kurt ( X ) = 𝔼 ⁡ X 4 − 3 ​ ( 𝔼 ⁡ X 2 ) 2 . \\mathop{\\mathrm{kurt}}(X)=\\operatorname{\\mathbb{E}}{X^{4}}-3(\\operatorname{\\mathbb{E}}{X^{2}})^{2}. roman_kurt ( italic_X ) = blackboard_E italic_X start_POS", "snippet": "kurt ( X ) = 𝔼 ⁡ X 4 − 3 ​ ( 𝔼 ⁡ X 2 ) 2 . \\mathop{\\mathrm{kurt}}(X)=\\operatorname{\\mathbb{E}}{X^{4}}-3(\\operatorname{\\mathbb{E}}{X^{2}})^{2}. roman_kurt ( italic_X ) = blackboard_E italic_X start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ( blackboard_E italic_X start_POSTSUPERSC"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E21", "title": "kurt ( 𝒙 ) = 1 N ​ ‖ 𝒙 ‖ 4 4 − 3 N 2 ​ ‖ 𝒙 ‖ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{x})=\\frac{1}{N}\\|\\bm{x}\\|_{4}^{4}-\\frac{3}{N^{2}}\\|\\bm{x}\\|_{2}^{4}. roman_kurt ( bold_italic_x ) = divide start_ARG 1 end", "snippet": "kurt ( 𝒙 ) = 1 N ​ ‖ 𝒙 ‖ 4 4 − 3 N 2 ​ ‖ 𝒙 ‖ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{x})=\\frac{1}{N}\\|\\bm{x}\\|_{4}^{4}-\\frac{3}{N^{2}}\\|\\bm{x}\\|_{2}^{4}. roman_kurt ( bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 4 end_POST"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E22", "title": "max 𝑽 ⊤ ​ 𝑽 = 𝑰 ​ kurt ( 𝑽 ⊤ ​ 𝑿 ) . \\max_{\\bm{V}^{\\top}\\bm{V}=\\bm{I}}\\mathop{\\mathrm{kurt}}(\\bm{V}^{\\top}\\bm{X}). roman_max start_POSTSUBSCRIPT bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRI", "snippet": "max 𝑽 ⊤ ​ 𝑽 = 𝑰 ​ kurt ( 𝑽 ⊤ ​ 𝑿 ) . \\max_{\\bm{V}^{\\top}\\bm{V}=\\bm{I}}\\mathop{\\mathrm{kurt}}(\\bm{V}^{\\top}\\bm{X}). roman_max start_POSTSUBSCRIPT bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_V = bold_italic_I end_POSTSUBSCRIPT roman_kurt ( bold_italic_V st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E23", "title": "max ‖ 𝒗 ‖ 2 2 = 1 ​ kurt ( 𝑿 ⊤ ​ 𝒗 ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_v ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT", "snippet": "max ‖ 𝒗 ‖ 2 2 = 1 ​ kurt ( 𝑿 ⊤ ​ 𝒗 ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_v ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex6", "title": "max ‖ 𝒘 ‖ 2 2 = 1 ⁡ kurt ​ ( 𝒁 ⊤ ​ 𝒘 ) . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\mathrm{kurt}(\\bm{Z}^{\\top}\\bm{w}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_", "snippet": "max ‖ 𝒘 ‖ 2 2 = 1 ⁡ kurt ​ ( 𝒁 ⊤ ​ 𝒘 ) . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\mathrm{kurt}(\\bm{Z}^{\\top}\\bm{w}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_it"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E24", "title": "max ‖ 𝒘 ‖ 2 2 = 1 ​ ∑ i = 1 d kurt ​ ( z i ) ​ w i 4 . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\sum_{i=1}^{d}\\mathrm{kurt}(z_{i})w_{i}^{4}. roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 e", "snippet": "max ‖ 𝒘 ‖ 2 2 = 1 ​ ∑ i = 1 d kurt ​ ( z i ) ​ w i 4 . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\sum_{i=1}^{d}\\mathrm{kurt}(z_{i})w_{i}^{4}. roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E25", "title": "kurt ( 𝑿 ⊤ ​ 𝒖 ) ≈ 1 N ​ ‖ 𝑿 ⊤ ​ 𝒖 ‖ 4 4 − 3 ​ ‖ 𝒖 ‖ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{1}{N}\\|\\bm{X}^{\\top}\\bm{u}\\|_{4}^{4}-3\\|\\bm{u}\\|_{2}^{4}. roman_kurt ( bold_italic_X", "snippet": "kurt ( 𝑿 ⊤ ​ 𝒖 ) ≈ 1 N ​ ‖ 𝑿 ⊤ ​ 𝒖 ‖ 4 4 − 3 ​ ‖ 𝒖 ‖ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{1}{N}\\|\\bm{X}^{\\top}\\bm{u}\\|_{4}^{4}-3\\|\\bm{u}\\|_{2}^{4}. roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) ≈ divide start_ARG "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex7", "title": "∇ 𝒖 ​ kurt ( 𝑿 ⊤ ​ 𝒖 ) ≈ 4 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 12 ​ ‖ 𝒖 ‖ 2 2 ​ 𝒖 . \\nabla_{\\bm{u}}\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{4}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice", "snippet": "∇ 𝒖 ​ kurt ( 𝑿 ⊤ ​ 𝒖 ) ≈ 4 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 12 ​ ‖ 𝒖 ‖ 2 2 ​ 𝒖 . \\nabla_{\\bm{u}}\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{4}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E29", "title": "𝒗 + = 1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 , 𝒖 + = 𝒗 + / ‖ 𝒗 + ‖ 2 . \\begin{split}\\bm{v}^{+}&=\\tfrac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\sca", "snippet": "𝒗 + = 1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 , 𝒖 + = 𝒗 + / ‖ 𝒗 + ‖ 2 . \\begin{split}\\bm{v}^{+}&=\\tfrac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E1", "title": "𝒙 = 𝑨 ​ 𝒛 + 𝜺 , ‖ 𝒛 ‖ 0 = d ≪ m . \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}=d\\ll m. bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0", "snippet": "𝒙 = 𝑨 ​ 𝒛 + 𝜺 , ‖ 𝒛 ‖ 0 = d ≪ m . \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}=d\\ll m. bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_d ≪ italic_m . (2.3.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E2", "title": "𝒙 → ℰ 𝒛 → 𝒟 = 𝑨 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 11.38109pt\\mathcal{E}\\hskip 11.38109pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{A}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW sta", "snippet": "𝒙 → ℰ 𝒛 → 𝒟 = 𝑨 𝒙 ^ . \\bm{x}\\xrightarrow{\\hskip 11.38109pt\\mathcal{E}\\hskip 11.38109pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{A}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z start_ARROW"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E3", "title": "𝒙 i = 𝑨 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N] bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_z star", "snippet": "𝒙 i = 𝑨 ​ 𝒛 i + 𝜺 i , ∀ i ∈ [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N] bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT i"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E4", "title": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E . (2.3.4)", "snippet": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E . (2.3.4)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E5", "title": "min 𝒁 ∈ ℝ d × N ⁡ { ‖ 𝑿 − 𝑨 ​ 𝒁 ‖ F 2 + λ ​ ‖ 𝒁 ‖ 1 } , \\min_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\left\\{\\|\\bm{X}-\\bm{A}\\bm{Z}\\|_{F}^{2}+\\lambda\\|\\bm{Z}\\|_{1}\\right\\}, roman_min start_POSTSUBSCRIPT bold_i", "snippet": "min 𝒁 ∈ ℝ d × N ⁡ { ‖ 𝑿 − 𝑨 ​ 𝒁 ‖ F 2 + λ ​ ‖ 𝒁 ‖ 1 } , \\min_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\left\\{\\|\\bm{X}-\\bm{A}\\bm{Z}\\|_{F}^{2}+\\lambda\\|\\bm{Z}\\|_{1}\\right\\}, roman_min start_POSTSUBSCRIPT bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSC"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E6", "title": "𝒁 t + 1 ← 𝒁 t + η ​ ∇ f ​ ( 𝒁 t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\nabla f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT ital", "snippet": "𝒁 t + 1 ← 𝒁 t + η ​ ∇ f ​ ( 𝒁 t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\nabla f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_η ∇ italic_f ( bold_italic_Z start_POSTSUBSCRIPT"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E7", "title": "𝒁 t + 1 ← 𝒁 t + η ​ ∂ f ​ ( 𝒁 t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\partial f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT it", "snippet": "𝒁 t + 1 ← 𝒁 t + η ​ ∂ f ​ ( 𝒁 t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\partial f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_η ∂ italic_f ( bold_italic_Z start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E12", "title": "𝒁 t + 1 = nonlinearity ​ ( 𝒁 t + linear ⊤ ​ ( linear ​ ( 𝒁 t ) + bias ) ) . \\bm{Z}_{t+1}=\\texttt{nonlinearity}(\\bm{Z}_{t}+\\texttt{linear}^{\\top}(\\texttt{linear}(\\bm{Z}_{t})+\\texttt{bias})). bold_itali", "snippet": "𝒁 t + 1 = nonlinearity ​ ( 𝒁 t + linear ⊤ ​ ( linear ​ ( 𝒁 t ) + bias ) ) . \\bm{Z}_{t+1}=\\texttt{nonlinearity}(\\bm{Z}_{t}+\\texttt{linear}^{\\top}(\\texttt{linear}(\\bm{Z}_{t})+\\texttt{bias})). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = nonlinearity ( bold_ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E13", "title": "𝒁 t + 1 = 𝒁 t + linear 1 ⊤ ( nonlinearity ( linear 2 ( 𝒁 t ) + bias 1 ) + bias 2 , \\bm{Z}_{t+1}=\\bm{Z}_{t}+\\texttt{linear}_{1}^{\\top}(\\texttt{nonlinearity}(\\texttt{linear}_{2}(\\bm{Z}_{t})+\\texttt{bias", "snippet": "𝒁 t + 1 = 𝒁 t + linear 1 ⊤ ( nonlinearity ( linear 2 ( 𝒁 t ) + bias 1 ) + bias 2 , \\bm{Z}_{t+1}=\\bm{Z}_{t}+\\texttt{linear}_{1}^{\\top}(\\texttt{nonlinearity}(\\texttt{linear}_{2}(\\bm{Z}_{t})+\\texttt{bias}_{1})+\\texttt{bias}_{2}, bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E14", "title": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14)", "snippet": "𝑿 = 𝑨 ​ 𝒁 + 𝑬 , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E15", "title": "min 𝑨 ~ , 𝒁 ~ ⁡ { ‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + λ ​ ‖ 𝒁 ~ ‖ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min ", "snippet": "min 𝑨 ~ , 𝒁 ~ ⁡ { ‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + λ ​ ‖ 𝒁 ~ ‖ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG , over~ start_ARG bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E16", "title": "‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + c ​ λ ​ ‖ 𝒁 ~ ‖ 1 . \\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+c\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}. ∥ bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold", "snippet": "‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + c ​ λ ​ ‖ 𝒁 ~ ‖ 1 . \\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+c\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}. ∥ bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E17", "title": "min 𝒁 ~ , 𝑨 ~ : ‖ 𝑨 ~ j ‖ 2 ≤ 1 ⁡ { ‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + λ ​ ‖ 𝒁 ~ ‖ 1 } . \\min_{\\tilde{\\bm{Z}},\\tilde{\\bm{A}}\\,:\\,\\|\\tilde{\\bm{A}}_{j}\\|_{2}\\leq 1}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F", "snippet": "min 𝒁 ~ , 𝑨 ~ : ‖ 𝑨 ~ j ‖ 2 ≤ 1 ⁡ { ‖ 𝑿 − 𝑨 ~ ​ 𝒁 ~ ‖ F 2 + λ ​ ‖ 𝒁 ~ ‖ 1 } . \\min_{\\tilde{\\bm{Z}},\\tilde{\\bm{A}}\\,:\\,\\|\\tilde{\\bm{A}}_{j}\\|_{2}\\leq 1}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min start_POSTSUBSCRIPT over~"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.Ex1", "title": "𝑿 , 𝒁 1 → f ​ ( 𝑨 1 , ⋅ ) 𝒁 2 → f ​ ( 𝑨 2 , ⋅ ) 𝒁 3 → f ​ ( 𝑨 3 , ⋅ ) ⋯ ​ 𝒁 L → f ​ ( 𝑨 L , ⋅ ) 𝒁 L + 1 ≈ 𝒁 . \\bm{X},\\bm{Z}^{1}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{1},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{", "snippet": "𝑿 , 𝒁 1 → f ​ ( 𝑨 1 , ⋅ ) 𝒁 2 → f ​ ( 𝑨 2 , ⋅ ) 𝒁 3 → f ​ ( 𝑨 3 , ⋅ ) ⋯ ​ 𝒁 L → f ​ ( 𝑨 L , ⋅ ) 𝒁 L + 1 ≈ 𝒁 . \\bm{X},\\bm{Z}^{1}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{1},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{Z}^{2}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{2},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{Z"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E23", "title": "min { 𝑨 ℓ } ⁡ ‖ 𝒁 L ​ ( 𝑨 1 , … , 𝑨 L ) − 𝒁 ‖ 2 2 . \\min_{\\{\\bm{A}^{\\ell}\\}}\\big{\\|}\\bm{Z}^{L}(\\bm{A}^{1},\\ldots,\\bm{A}^{L})-\\bm{Z}\\big{\\|}_{2}^{2}. roman_min start_POSTSUBSCRIPT { bold_italic_A start", "snippet": "min { 𝑨 ℓ } ⁡ ‖ 𝒁 L ​ ( 𝑨 1 , … , 𝑨 L ) − 𝒁 ‖ 2 2 . \\min_{\\{\\bm{A}^{\\ell}\\}}\\big{\\|}\\bm{Z}^{L}(\\bm{A}^{1},\\ldots,\\bm{A}^{L})-\\bm{Z}\\big{\\|}_{2}^{2}. roman_min start_POSTSUBSCRIPT { bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } end_POSTSUBSCRIPT ∥ bold_italic_Z"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E24", "title": "min f , g ∥ 𝑯 − g ( f ( 𝑯 ) ) ) ∥ F 2 + λ ∥ f ( 𝑯 ) ∥ 1 , \\min_{f,g}\\|\\bm{H}-g(f(\\bm{H})))\\|_{F}^{2}+\\lambda\\|f(\\bm{H})\\|_{1}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ∥ bol", "snippet": "min f , g ∥ 𝑯 − g ( f ( 𝑯 ) ) ) ∥ F 2 + λ ∥ f ( 𝑯 ) ∥ 1 , \\min_{f,g}\\|\\bm{H}-g(f(\\bm{H})))\\|_{F}^{2}+\\lambda\\|f(\\bm{H})\\|_{1}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ∥ bold_italic_H - italic_g ( italic_f ( bold_italic_H ) ) ) ∥ start_POSTSUBSCRIPT ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E25", "title": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = S η ​ λ ​ ( 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ ( 𝑨 1 ​ 𝒁 1 − 𝑿 ) ) . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=S_{\\eta\\lambda}\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X}", "snippet": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = S η ​ λ ​ ( 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ ( 𝑨 1 ​ 𝒁 1 − 𝑿 ) ) . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=S_{\\eta\\lambda}\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X})\\right). bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ("}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E26", "title": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = max ⁡ { 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ ( 𝑨 1 ​ 𝒁 1 − 𝑿 ) − λ ​ η ​ 𝟏 , 0 } , \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{", "snippet": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = max ⁡ { 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ ( 𝑨 1 ​ 𝒁 1 − 𝑿 ) − λ ​ η ​ 𝟏 , 0 } , \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X})-\\lambda\\eta\\mathbf{1},0\\right\\}, bold_italic_Z start_POSTSUPERSCRIPT 2 end_P"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E27", "title": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = max ⁡ { 2 ​ η ​ ( 𝑨 1 ) ⊤ + ( 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ 𝑨 1 ​ 𝒁 1 − λ ​ η ​ 𝟏 ) , 0 } . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{2\\eta(\\bm{A}^{1})^{\\top}+\\left(\\bm{Z}^{1}-2\\e", "snippet": "𝒁 2 = f ​ ( 𝑨 1 , 𝑿 ) = max ⁡ { 2 ​ η ​ ( 𝑨 1 ) ⊤ + ( 𝒁 1 − 2 ​ η ​ ( 𝑨 1 ) ⊤ ​ 𝑨 1 ​ 𝒁 1 − λ ​ η ​ 𝟏 ) , 0 } . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{2\\eta(\\bm{A}^{1})^{\\top}+\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}\\bm{A}^{1}\\bm{Z}^{1}-\\lambda\\eta\\mathbf{1}\\right),0\\right\\}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.E1", "title": "𝒖 t + 1 = 𝑨 ​ 𝒖 t ‖ 𝑨 ​ 𝒖 t ‖ 2 , \\bm{u}_{t+1}=\\frac{\\bm{A}\\bm{u}_{t}}{\\|\\bm{A}\\bm{u}_{t}\\|_{2}}, bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_A bold", "snippet": "𝒖 t + 1 = 𝑨 ​ 𝒖 t ‖ 𝑨 ​ 𝒖 t ‖ 2 , \\bm{u}_{t+1}=\\frac{\\bm{A}\\bm{u}_{t}}{\\|\\bm{A}\\bm{u}_{t}\\|_{2}}, bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_A bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∥ bol"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E1", "title": "max ‖ 𝒖 ‖ 2 2 = 1 ⁡ f ​ ( 𝒖 ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCR", "snippet": "max ‖ 𝒖 ‖ 2 2 = 1 ⁡ f ​ ( 𝒖 ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT italic_f ( bold_italic_u ) . (2.5.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex1", "title": "U ∩ ℳ = F − 1 ​ ( { 0 } ) U\\cap\\mathcal{M}=F^{-1}(\\{0\\}) italic_U ∩ caligraphic_M = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { 0 } )", "snippet": "U ∩ ℳ = F − 1 ​ ( { 0 } ) U\\cap\\mathcal{M}=F^{-1}(\\{0\\}) italic_U ∩ caligraphic_M = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { 0 } )"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex2", "title": "T 𝒖 ​ ℳ = Ker ⁡ ( D ​ F 𝒖 ) . T_{\\bm{u}}\\mathcal{M}=\\operatorname{Ker}(DF_{\\bm{u}}). italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT caligraphic_M = roman_Ker ( italic_D italic_F start_PO", "snippet": "T 𝒖 ​ ℳ = Ker ⁡ ( D ​ F 𝒖 ) . T_{\\bm{u}}\\mathcal{M}=\\operatorname{Ker}(DF_{\\bm{u}}). italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT caligraphic_M = roman_Ker ( italic_D italic_F start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex3", "title": "T 𝒖 ​ 𝕊 d − 1 = { 𝒗 ∈ ℝ d ∣ ⟨ 𝒗 , 𝒖 ⟩ = 0 } , T_{\\bm{u}}\\mathbb{S}^{d-1}=\\{\\bm{v}\\in\\mathbb{R}^{d}\\mid\\langle\\bm{v},\\bm{u}\\rangle=0\\}, italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT blac", "snippet": "T 𝒖 ​ 𝕊 d − 1 = { 𝒗 ∈ ℝ d ∣ ⟨ 𝒗 , 𝒖 ⟩ = 0 } , T_{\\bm{u}}\\mathbb{S}^{d-1}=\\{\\bm{v}\\in\\mathbb{R}^{d}\\mid\\langle\\bm{v},\\bm{u}\\rangle=0\\}, italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT = { bold_italic_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E2", "title": "grad ​ f ​ ( 𝒖 ) = 𝑷 𝒖 ⟂ ​ ∇ f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_", "snippet": "grad ​ f ​ ( 𝒖 ) = 𝑷 𝒖 ⟂ ​ ∇ f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT ∇ italic_f (2.5.2)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex4", "title": "grad ​ f ​ ( 𝒖 ) = 𝟎 . \\mathrm{grad}\\,f(\\bm{u})=\\mathbf{0}. roman_grad italic_f ( bold_italic_u ) = bold_0 .", "snippet": "grad ​ f ​ ( 𝒖 ) = 𝟎 . \\mathrm{grad}\\,f(\\bm{u})=\\mathbf{0}. roman_grad italic_f ( bold_italic_u ) = bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex5", "title": "proj 𝕊 d − 1 ​ ( 𝒗 ) ≐ min ‖ 𝒖 ‖ 2 2 = 1 ⁡ ‖ 𝒖 − 𝒗 ‖ 2 = 𝒗 ‖ 𝒗 ‖ 2 , \\mathrm{proj}_{\\mathbb{S}^{d-1}}(\\bm{v})\\doteq\\min_{\\|\\bm{u}\\|_{2}^{2}=1}\\,\\|\\bm{u}-\\bm{v}\\|_{2}=\\frac{\\bm{v}}{\\|\\bm{v}\\|_{2}}, rom", "snippet": "proj 𝕊 d − 1 ​ ( 𝒗 ) ≐ min ‖ 𝒖 ‖ 2 2 = 1 ⁡ ‖ 𝒖 − 𝒗 ‖ 2 = 𝒗 ‖ 𝒗 ‖ 2 , \\mathrm{proj}_{\\mathbb{S}^{d-1}}(\\bm{v})\\doteq\\min_{\\|\\bm{u}\\|_{2}^{2}=1}\\,\\|\\bm{u}-\\bm{v}\\|_{2}=\\frac{\\bm{v}}{\\|\\bm{v}\\|_{2}}, roman_proj start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E3", "title": "Hess ​ f ​ ( 𝒖 ) = 𝑷 𝒖 ⟂ ​ ( ∇ 2 f ​ ( 𝒖 ) − ⟨ ∇ f ​ ( 𝒖 ) , 𝒖 ⟩ ​ 𝑰 ) ​ 𝑷 𝒖 ⟂ . \\mathrm{Hess}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\left(\\nabla^{2}f(\\bm{u})-\\langle\\nabla f(\\bm{u}),\\bm{u}\\rangle\\bm{I}\\r", "snippet": "Hess ​ f ​ ( 𝒖 ) = 𝑷 𝒖 ⟂ ​ ( ∇ 2 f ​ ( 𝒖 ) − ⟨ ∇ f ​ ( 𝒖 ) , 𝒖 ⟩ ​ 𝑰 ) ​ 𝑷 𝒖 ⟂ . \\mathrm{Hess}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\left(\\nabla^{2}f(\\bm{u})-\\langle\\nabla f(\\bm{u}),\\bm{u}\\rangle\\bm{I}\\right)\\bm{P}_{\\bm{u}}^{\\perp}. roman_Hess italic_f ( bold_italic_u ) = bold_itali"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E4", "title": "( ∑ i = 1 d kurt ( z i ) ​ w i 4 ) ​ 𝒘 = kurt ( 𝒛 ) ⊙ 𝒘 ⊙ 3 , \\left(\\sum_{i=1}^{d}\\mathop{\\mathrm{kurt}}(z_{i})w_{i}^{4}\\right)\\bm{w}=\\mathop{\\mathrm{kurt}}(\\bm{z})\\mathbin{\\mathchoice{\\raisebox{1.3pt", "snippet": "( ∑ i = 1 d kurt ( z i ) ​ w i 4 ) ​ 𝒘 = kurt ( 𝒛 ) ⊙ 𝒘 ⊙ 3 , \\left(\\sum_{i=1}^{d}\\mathop{\\mathrm{kurt}}(z_{i})w_{i}^{4}\\right)\\bm{w}=\\mathop{\\mathrm{kurt}}(\\bm{z})\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E5", "title": "𝒘 S = ∑ i ∈ S ± 1 kurt ( z i ) ​ ∑ j ∈ S 1 kurt ( z j ) ​ 𝒆 i \\bm{w}_{S}=\\sum_{i\\in S}\\pm\\sqrt{\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{i})\\sum_{j\\in S}\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{j})}}}\\bm{e}_{i} bo", "snippet": "𝒘 S = ∑ i ∈ S ± 1 kurt ( z i ) ​ ∑ j ∈ S 1 kurt ( z j ) ​ 𝒆 i \\bm{w}_{S}=\\sum_{i\\in S}\\pm\\sqrt{\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{i})\\sum_{j\\in S}\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{j})}}}\\bm{e}_{i} bold_italic_w start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E6", "title": "max 𝑸 ⊤ ​ 𝑸 = 𝑰 ⁡ f ​ ( 𝑸 ) . \\max_{\\bm{Q}^{\\top}\\bm{Q}=\\bm{I}}\\,f(\\bm{Q}). roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_PO", "snippet": "max 𝑸 ⊤ ​ 𝑸 = 𝑰 ⁡ f ​ ( 𝑸 ) . \\max_{\\bm{Q}^{\\top}\\bm{Q}=\\bm{I}}\\,f(\\bm{Q}). roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_POSTSUBSCRIPT italic_f ( bold_italic_Q ) . (2.5.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex6", "title": "T 𝑸 ​ 𝖮 ​ ( d ) = { 𝑸 ​ 𝛀 ∈ ℝ d × d ∣ 𝛀 ⊤ = − 𝛀 } , T_{\\bm{Q}}\\mathsf{O}(d)=\\{\\bm{Q}\\bm{\\Omega}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{\\Omega}^{\\top}=-\\bm{\\Omega}\\}, italic_T start_POSTSUBSCRIPT bold_italic_", "snippet": "T 𝑸 ​ 𝖮 ​ ( d ) = { 𝑸 ​ 𝛀 ∈ ℝ d × d ∣ 𝛀 ⊤ = − 𝛀 } , T_{\\bm{Q}}\\mathsf{O}(d)=\\{\\bm{Q}\\bm{\\Omega}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{\\Omega}^{\\top}=-\\bm{\\Omega}\\}, italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) = { bold_italic_Q bold_Ω ∈ blackboa"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex7", "title": "𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( 𝚫 ) = 𝑸 ​ skew ⁡ ( 𝑸 ⊤ ​ 𝚫 ) , \\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}(\\bm{\\Delta})=\\bm{Q}\\operatorname{skew}(\\bm{Q}^{\\top}\\bm{\\Delta}), caligraphic_P start_POSTSUBSCRIPT italic_T ", "snippet": "𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( 𝚫 ) = 𝑸 ​ skew ⁡ ( 𝑸 ⊤ ​ 𝚫 ) , \\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}(\\bm{\\Delta})=\\bm{Q}\\operatorname{skew}(\\bm{Q}^{\\top}\\bm{\\Delta}), caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E7", "title": "grad ​ f ​ ( 𝑸 ) = 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( ∇ f ​ ( 𝑸 ) ) \\mathrm{grad}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla f(\\bm{Q})\\right) roman_grad italic_f ( bold_italic_Q ) = caligraphic_P", "snippet": "grad ​ f ​ ( 𝑸 ) = 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( ∇ f ​ ( 𝑸 ) ) \\mathrm{grad}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla f(\\bm{Q})\\right) roman_grad italic_f ( bold_italic_Q ) = caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex8", "title": "grad ​ f ​ ( 𝑸 ) = 𝟎 . \\mathrm{grad}\\,f(\\bm{Q})=\\mathbf{0}. roman_grad italic_f ( bold_italic_Q ) = bold_0 .", "snippet": "grad ​ f ​ ( 𝑸 ) = 𝟎 . \\mathrm{grad}\\,f(\\bm{Q})=\\mathbf{0}. roman_grad italic_f ( bold_italic_Q ) = bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E8", "title": "Hess ​ f ​ ( 𝑸 ) = 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( ∇ 2 f ​ ( 𝑸 ) − sym ⁡ ( 𝑸 ⊤ ​ ∇ f ​ ( 𝑸 ) ) ⊗ 𝑰 ) ​ 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) , \\mathrm{Hess}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla^{2}f(\\bm{Q})", "snippet": "Hess ​ f ​ ( 𝑸 ) = 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) ​ ( ∇ 2 f ​ ( 𝑸 ) − sym ⁡ ( 𝑸 ⊤ ​ ∇ f ​ ( 𝑸 ) ) ⊗ 𝑰 ) ​ 𝒫 T 𝑸 ​ 𝖮 ​ ( d ) , \\mathrm{Hess}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla^{2}f(\\bm{Q})-\\operatorname{sym}(\\bm{Q}^{\\top}\\nabla f(\\bm{Q}))\\mathbin{\\mathchoice{\\raisebox"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex9", "title": "Hess ​ f ​ ( 𝑸 ) ⪯ 𝟎 . \\mathrm{Hess}\\,f(\\bm{Q})\\preceq\\mathbf{0}. roman_Hess italic_f ( bold_italic_Q ) ⪯ bold_0 .", "snippet": "Hess ​ f ​ ( 𝑸 ) ⪯ 𝟎 . \\mathrm{Hess}\\,f(\\bm{Q})\\preceq\\mathbf{0}. roman_Hess italic_f ( bold_italic_Q ) ⪯ bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex10", "title": "( 𝑩 ⊤ ⊗ 𝑨 ) ​ vec ⁡ ( 𝑿 ) = vec ⁡ ( 𝑨 ​ 𝑿 ​ 𝑩 ) , (\\bm{B}^{\\top}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otime", "snippet": "( 𝑩 ⊤ ⊗ 𝑨 ) ​ vec ⁡ ( 𝑿 ) = vec ⁡ ( 𝑨 ​ 𝑿 ​ 𝑩 ) , (\\bm{B}^{\\top}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\o"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E9", "title": "proj 𝖮 ​ ( d ) ​ ( 𝑿 ) ≐ min 𝑸 ∈ 𝖮 ​ ( d ) ⁡ ‖ 𝑸 − 𝑿 ‖ F 2 . \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})\\doteq\\min_{\\bm{Q}\\in\\mathsf{O}(d)}\\,\\|\\bm{Q}-\\bm{X}\\|_{F}^{2}. roman_proj start_POSTSUBSCRIPT sansser", "snippet": "proj 𝖮 ​ ( d ) ​ ( 𝑿 ) ≐ min 𝑸 ∈ 𝖮 ​ ( d ) ⁡ ‖ 𝑸 − 𝑿 ‖ F 2 . \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})\\doteq\\min_{\\bm{Q}\\in\\mathsf{O}(d)}\\,\\|\\bm{Q}-\\bm{X}\\|_{F}^{2}. roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ≐ roman_min start_POSTSUBS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex11", "title": "proj 𝖮 ​ ( d ) ​ ( 𝑿 ) = 𝑼 ​ 𝑽 ⊤ , \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})=\\bm{U}\\bm{V}^{\\top}, roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) = bold_italic_", "snippet": "proj 𝖮 ​ ( d ) ​ ( 𝑿 ) = 𝑼 ​ 𝑽 ⊤ , \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})=\\bm{U}\\bm{V}^{\\top}, roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) = bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex14", "title": "𝑼 ​ 𝑽 ⊤ = proj 𝖮 ​ ( d ) ​ ( 𝑿 ) . \\bm{U}\\bm{V}^{\\top}=\\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X}). bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT s", "snippet": "𝑼 ​ 𝑽 ⊤ = proj 𝖮 ​ ( d ) ​ ( 𝑿 ) . \\bm{U}\\bm{V}^{\\top}=\\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X}). bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx1", "title": "min { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} r", "snippet": "min { 𝒛 ~ i } i = 1 N ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝒛 ~ i ‖ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} roman_min start_POSTSUBSCRIPT { over~ start_ARG bold_italic_z end_ARG start_POSTS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx2", "title": "arg ​ min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i", "snippet": "arg ​ min 𝑼 ~ ⁡ 1 N ​ ∑ i = 1 N ‖ 𝒙 i − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 i ‖ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2} start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx3", "title": "arg ​ min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2} start_OPERAT", "snippet": "arg ​ min 𝑼 ~ ⁡ 𝔼 ⁡ ‖ 𝒙 − 𝑼 ~ ​ 𝑼 ~ ⊤ ​ 𝒙 ‖ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2} start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx4", "title": "𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 = ⟨ 𝒖 , 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 ⟩ ⏟ λ ​ 𝒖 , \\displaystyle\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot", "snippet": "𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 = ⟨ 𝒖 , 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 ⟩ ⏟ λ ​ 𝒖 , \\displaystyle\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\sca"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx5", "title": "1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 = ( λ N − 3 ) ​ 𝒖 , \\displaystyle\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle", "snippet": "1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 = ( λ N − 3 ) ​ 𝒖 , \\displaystyle\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx6", "title": "1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 ‖ 1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 ‖ 2 = 𝒖 . \\displaystyle\\frac{\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoi", "snippet": "1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 ‖ 1 N ​ 𝑿 ​ ( 𝑿 ⊤ ​ 𝒖 ) ⊙ 3 − 3 ​ 𝒖 ‖ 2 = 𝒖 . \\displaystyle\\frac{\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\sca"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx7", "title": "𝒁 1 \\displaystyle\\bm{Z}_{1} bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∼ \\displaystyle\\sim ∼ 𝒩 ⁡ ( 𝟎 , 𝑰 ) , \\displaystyle\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}), caligraphic_N ( bold_0 ,", "snippet": "𝒁 1 \\displaystyle\\bm{Z}_{1} bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∼ \\displaystyle\\sim ∼ 𝒩 ⁡ ( 𝟎 , 𝑰 ) , \\displaystyle\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}), caligraphic_N ( bold_0 , bold_italic_I ) , (2.3.8) 𝒁 t + 1 \\displaystyle\\bm{Z}_{t+1} bold_italic_Z start"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx8", "title": "S α ​ ( x ) \\displaystyle S_{\\alpha}(x) italic_S start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_x ) ≐ { x − α , x ≥ α , 0 , − α < x < α , x + α , x ≤ − α \\displaystyle\\doteq\\begin{cases}x-\\al", "snippet": "S α ​ ( x ) \\displaystyle S_{\\alpha}(x) italic_S start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_x ) ≐ { x − α , x ≥ α , 0 , − α < x < α , x + α , x ≤ − α \\displaystyle\\doteq\\begin{cases}x-\\alpha,&x\\geq\\alpha,\\\\ 0,&-\\alpha<x<\\alpha,\\\\ x+\\alpha,&x\\leq-\\alpha\\end{cases} ≐ {"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx9", "title": "𝒁 ℓ + 1 = S η ​ λ ​ ( 𝒁 ℓ − 2 ​ η ​ 𝑨 + ⊤ ​ ( 𝑨 + ​ 𝒁 ℓ − 𝑿 ) ) , 𝒁 1 = 𝟎 , ∀ ℓ ∈ [ L ] \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta\\bm{A}_{+}^{\\top}(\\bm{A}_{+}\\bm{Z}^{\\ell}-\\", "snippet": "𝒁 ℓ + 1 = S η ​ λ ​ ( 𝒁 ℓ − 2 ​ η ​ 𝑨 + ⊤ ​ ( 𝑨 + ​ 𝒁 ℓ − 𝑿 ) ) , 𝒁 1 = 𝟎 , ∀ ℓ ∈ [ L ] \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta\\bm{A}_{+}^{\\top}(\\bm{A}_{+}\\bm{Z}^{\\ell}-\\bm{X})\\right),\\quad\\bm{Z}^{1}=\\mathbf{0},\\quad\\forall\\ell\\in[L] bold_italic_Z st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx10", "title": "𝒁 ℓ + 1 = S η ​ λ ​ ( 𝒁 ℓ − 2 ​ η ​ ( 𝑨 ℓ ) ⊤ ​ ( 𝑨 ℓ ​ 𝒁 ℓ − 𝑿 ) ) , ∀ ℓ ∈ [ L ] . \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta(\\bm{A}^{\\ell})^{\\top}(\\bm{A}^{\\ell}\\bm{Z}^{\\el", "snippet": "𝒁 ℓ + 1 = S η ​ λ ​ ( 𝒁 ℓ − 2 ​ η ​ ( 𝑨 ℓ ) ⊤ ​ ( 𝑨 ℓ ​ 𝒁 ℓ − 𝑿 ) ) , ∀ ℓ ∈ [ L ] . \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta(\\bm{A}^{\\ell})^{\\top}(\\bm{A}^{\\ell}\\bm{Z}^{\\ell}-\\bm{X})\\right),\\quad\\forall\\ell\\in[L]. bold_italic_Z start_POSTSUPERSCRIPT ro"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx11", "title": "𝒁 1 = 𝟎 , ( 𝑨 1 ) j ∼ i . i . d . 𝒩 ⁡ ( 𝟎 , 1 D ​ 𝑰 ) , ∀ j ∈ [ m ] , \\displaystyle\\bm{Z}^{1}=\\mathbf{0},\\quad(\\bm{A}_{1})_{j}\\stackrel{{\\scriptstyle\\mathrm{i.i.d.}}}{{\\sim}}\\operatorname{\\mathcal{N}}", "snippet": "𝒁 1 = 𝟎 , ( 𝑨 1 ) j ∼ i . i . d . 𝒩 ⁡ ( 𝟎 , 1 D ​ 𝑰 ) , ∀ j ∈ [ m ] , \\displaystyle\\bm{Z}^{1}=\\mathbf{0},\\quad(\\bm{A}_{1})_{j}\\stackrel{{\\scriptstyle\\mathrm{i.i.d.}}}{{\\sim}}\\operatorname{\\mathcal{N}}(\\bm{0},\\tfrac{1}{D}\\bm{I}),\\enspace\\forall j\\in[m], bold_italic_Z start_POSTSUP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx12", "title": "( 𝑸 ⊤ ​ 𝑿 ) ⊤ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = 𝑸 ⊤ ​ 𝑿 , \\", "snippet": "( 𝑸 ⊤ ​ 𝑿 ) ⊤ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = 𝑸 ⊤ ​ 𝑿 , \\displaystyle=\\bm{Q}^{\\top}\\bm{X}, = bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#top", "title": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "snippet": ""}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1", "title": "3.1 Entropy Minimization and Compression", "snippet": "3.1 Entropy Minimization and Compression 3.1.1 Entropy and Coding Rate In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complex"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2", "title": "3.2 Compression via Denoising", "snippet": "3.2 Compression via Denoising In this section, we will describe a natural and computationally tractable way to learn a distribution p ​ ( 𝒙 ) p(\\bm{x}) italic_p ( bold_italic_x ) by way of learning a parametric encoding of our distribution such that the representation has the min"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3", "title": "3.3 Compression via Lossy Coding", "snippet": "3.3 Compression via Lossy Coding Let us recap what we have covered so far. We have discussed how to fit a denoiser 𝒙 ¯ θ \\bar{\\bm{x}}_{\\theta} over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT using finite samples. We showed that this denoiser e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4", "title": "3.4 Maximizing Information Gain", "snippet": "3.4 Maximizing Information Gain So far in this chapter, we have discussed how to identify a distribution with low-dimensional structures through the principle of compression. As we have seen from the previous two sections, computational compression can be realized through either "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S5", "title": "3.5 Summary and Notes", "snippet": "3.5 Summary and Notes The use of denoising and diffusion for sampling has a rich history. The first work which is clearly about a diffusion model is probably [ SWM+15 ] , but before this there are many works about denoising as a computational and statistical problem. The most rel"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6", "title": "3.6 Exercises and Extensions", "snippet": "3.6 Exercises and Extensions Exercise 3.1 . Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 ). Exercise 3.2 . Consider random vectors 𝒙 ∈ ℝ D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and 𝒚 ∈ ℝ d "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS1", "title": "3.1.1 Entropy and Coding Rate", "snippet": "3.1.1 Entropy and Coding Rate In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complexity but it is not computable and not asso"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS2", "title": "3.1.2 Differential Entropy", "snippet": "3.1.2 Differential Entropy When the random variable 𝒙 ∈ ℝ D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is continuous and has a probability density p p italic_p , one may view that the limit of the above sum ( 3.1.1 ) is"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3", "title": "3.1.3 Minimizing Coding Rate", "snippet": "3.1.3 Minimizing Coding Rate Remember that the learning problem entails the recovery of a (potentially continuous) distribution p ​ ( 𝒙 ) p(\\bm{x}) italic_p ( bold_italic_x ) from a set of samples { 𝒙 1 , … , 𝒙 N } \\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} { bold_italic_x start_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1", "title": "3.2.1 Diffusion and Denoising Processes", "snippet": "3.2.1 Diffusion and Denoising Processes We first want to find a procedure to decrease the entropy of a given very noisy sample into a lower-entropy sample from the data distribution. Here, we describe a potential approach—one of many, but perhaps the most natural way to attack th"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2", "title": "3.2.2 Learning and Sampling a Distribution via Iterative Denoising", "snippet": "3.2.2 Learning and Sampling a Distribution via Iterative Denoising Remember that at the end of Section 3.1.3 , we discussed a pair of desiderata for pursuing a distribution with low-dimensional structure. The first such desideratum is to start with a normal distribution, say with"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1", "title": "3.3.1 Necessity of Lossy Coding", "snippet": "3.3.1 Necessity of Lossy Coding We have previously, multiple times, discussed a difficulty: if we learn the distribution from finite samples in the end, and our function class of denoisers contains enough functions, how do we ensure that we sample from the true distribution (with"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2", "title": "3.3.2 Rate Distortion and Data Geometry", "snippet": "3.3.2 Rate Distortion and Data Geometry Of course, among all encoding schemes that satisfy the above constraint, we would like to choose the one that minimizes the resulting coding rate. For a given random variable 𝒙 \\bm{x} bold_italic_x and a precision ϵ \\epsilon italic_ϵ , this"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3", "title": "3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian", "snippet": "3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian Now suppose we are given a set of data samples in 𝑿 = [ 𝒙 1 , … , 𝒙 N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4", "title": "3.3.4 Clustering a Mixture of Low-Dimensional Gaussians", "snippet": "3.3.4 Clustering a Mixture of Low-Dimensional Gaussians As we have discussed before, the given dataset 𝑿 \\bm{X} bold_italic_X often has low-dimensional intrinsic structures. Hence, encoding it as a general Gaussian would be very redundant. If we can identify those intrinsic struc"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1", "title": "3.4.1 Linear Discriminative Representations", "snippet": "3.4.1 Linear Discriminative Representations Suppose that 𝒙 ∈ ℝ D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is a random vector drawn from a mixture of K K italic_K (component) distributions 𝒟 = { 𝒟 k } k = 1 K \\mathcal{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2", "title": "3.4.2 The Principle of Maximal Coding Rate Reduction", "snippet": "3.4.2 The Principle of Maximal Coding Rate Reduction Although the three properties— between-class discriminative , within-class compressible , and maximally diverse representation —for linear discriminative representations (LDRs) are all highly desired properties of the learned r"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3", "title": "3.4.3 Optimization Properties of Coding Rate Reduction", "snippet": "3.4.3 Optimization Properties of Coding Rate Reduction In this subsection, we study the optimization properties of the MCR 2 function by analyzing its optimal solutions and the structure of its optimization landscape. To get around the technical difficulty introduced by the neura"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px1", "title": "Step 1: different discretizations.", "snippet": "Step 1: different discretizations. The first step we do is motivated by the following point: we do not need to spend so many denoising iterations at large t t italic_t . If we look at Figure 3.5 , we observe that the first 200 200 200 or 300 300 300 iterations, out of the 500 500"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px2", "title": "Step 2: different noise models.", "snippet": "Step 2: different noise models. The second step is to consider slightly different models compared to ( 3.2.1 ). The basic motivation for this is as follows. In practice, the noise distribution 𝒩 ⁡ ( 𝟎 , t L 2 ​ 𝑰 ) \\operatorname{\\mathcal{N}}(\\bm{0},t_{L}^{2}\\bm{I}) caligraphic_N "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px3", "title": "Step 3: optimizing training pipelines.", "snippet": "Step 3: optimizing training pipelines. If we use the procedure dictated by Section 3.2.1 to learn a separate denoiser 𝒙 ¯ ​ ( t , ⋅ ) \\bar{\\bm{x}}(t,\\cdot) over¯ start_ARG bold_italic_x end_ARG ( italic_t , ⋅ ) for each time t t italic_t to be used in the sampling algorithm, we w"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4", "title": "(Optional) Step 4: changing the estimation target.", "snippet": "(Optional) Step 4: changing the estimation target. Note that it is common to instead reorient the whole denoising pipeline around noise predictors , i.e., estimates of 𝔼 ⁡ [ 𝒈 ∣ 𝒙 t ] \\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_g ∣ bold_italic_x sta"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px1", "title": "The clustering problem.", "snippet": "The clustering problem. Now for this specific family of distributions, how can we effectively and efficiently identify those low-dimensional components from a set of samples 𝑿 = [ 𝒙 1 , 𝒙 2 , … , 𝒙 N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X ="}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2", "title": "Clustering via lossy compression.", "snippet": "Clustering via lossy compression. The main difficulty in solving the above clustering problem is that we normally do not know the number of clusters K K italic_K , nor do we know the dimension of each component. There has been a long history for the study of this clustering probl"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3", "title": "Optimization strategies to cluster.", "snippet": "Optimization strategies to cluster. The remaining question is how we optimize the above coding rate objective to find the optimal clusters. There are three natural approaches to this objective: 1. We may start with the whole set 𝑿 \\bm{X} bold_italic_X as a single cluster (i.e. th"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS0.SSS0.Px1", "title": "How to measure the goodness of representations.", "snippet": "How to measure the goodness of representations. One may view a given dataset as samples of a random vector 𝒙 \\bm{x} bold_italic_x with a certain distribution in a high-dimensional space, say ℝ D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . Typi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px1", "title": "Encoding class information via cross entropy.", "snippet": "Encoding class information via cross entropy. Extensive studies have shown that for many practical datasets (e.g., images, audio, and natural languages), the (encoding) mapping from the data 𝒙 \\bm{x} bold_italic_x to its class label 𝒚 \\bm{y} bold_italic_y can be effectively model"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px2", "title": "Minimal discriminative features via information bottleneck.", "snippet": "Minimal discriminative features via information bottleneck. One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features 𝒛 = f ​ ( 𝒙 , θ ) ∈ ℝ d \\bm{z}=f(\\bm{x},\\theta)\\in\\mathbb{R}^{d} b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3", "title": "Linear discriminative representations.", "snippet": "Linear discriminative representations. Whether the given data 𝑿 \\bm{X} bold_italic_X of a mixed distribution 𝒟 \\mathcal{D} caligraphic_D can be effectively classified or clustered depends on how separable (or discriminative) the component distributions 𝒟 k \\mathcal{D}_{k} caligra"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1", "title": "Coding rate of features.", "snippet": "Coding rate of features. Notably, a practical challenge in evaluating the coding rate is that the underlying distribution of the feature representations 𝒁 \\bm{Z} bold_italic_Z is typically unknown. To address this, we may approximate the features 𝒁 = [ 𝒛 1 , … , 𝒛 N ] \\bm{Z}=[\\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.SSS0.Px1", "title": "Regularized MCR 2 .", "snippet": "Regularized MCR 2 . The above theorem characterizes properties of the global optima of the rate reduction objectives. What about other optima, such as local ones? Due to the constraints of the Frobenius norm, it is a difficult task to analyze Problem ( 3.4.15 ) from an optimizati"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample1", "title": "Example 3.1 (Entropy of Gaussian Distributions) .", "snippet": "Example 3.1 (Entropy of Gaussian Distributions) . Through direct calculation, it is possible to show that the entropy of a Gaussian distribution x ∼ 𝒩 ​ ( μ , σ 2 ) x\\sim\\mathcal{N}(\\mu,\\sigma^{2}) italic_x ∼ caligraphic_N ( italic_μ , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem1", "title": "Theorem 3.1 (Information Inequality) .", "snippet": "Theorem 3.1 (Information Inequality) . Let p ​ ( 𝐱 ) , q ​ ( 𝐱 ) p(\\bm{x}),q(\\bm{x}) italic_p ( bold_italic_x ) , italic_q ( bold_italic_x ) be two probability density functions (that have the same support). Then 𝖪𝖫 ⁡ ( p ∥ q ) ≥ 0 \\operatorname{\\mathsf{KL}}(p\\;\\|\\;q)\\geq 0 sanss"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem2", "title": "Theorem 3.2 (Simplified Version of Theorem B.2 ) .", "snippet": "Theorem 3.2 (Simplified Version of Theorem B.2 ) . Suppose that ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT follows the model ( 3.2.1 ). For any t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2", "title": "Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians) .", "snippet": "Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians) . In this example we compute the Bayes optimal denoiser for an incredibly important class of distributions, the Gaussian mixture model. To start, let us fix parameters for the distribution: mixture weights 𝝅 ∈ ℝ K"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem3", "title": "Theorem 3.3 (Tweedie’s Formula) .", "snippet": "Theorem 3.3 (Tweedie’s Formula) . Suppose that ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Let p t p_{t} italic_p start_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample3", "title": "Example 3.3 (Denoising a Two-Point Mixture) .", "snippet": "Example 3.3 (Denoising a Two-Point Mixture) . Let x x italic_x be uniform on the two-point set { − 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } and let ( 𝒙 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem4", "title": "Theorem 3.4 (Simplified Version of Theorem B.3 ) .", "snippet": "Theorem 3.4 (Simplified Version of Theorem B.3 ) . Suppose that ( 𝐱 t ) t ∈ [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Then, under certain t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark1", "title": "Remark 3.1 .", "snippet": "Remark 3.1 . Connections between denoising a distribution and probabilistic PCA. Here, we would like to connect denoising a low-dimensional distribution to probabilistic PCA (see Section 2.1.3 for more details about probabilistic PCA). Suppose that we consider K = 1 K=1 italic_K "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem5", "title": "Theorem 3.5 ( [ LY24 ] Theorem 1, Simplified) .", "snippet": "Theorem 3.5 ( [ LY24 ] Theorem 1, Simplified) . Suppose that 𝔼 ⁡ ‖ 𝐱 ‖ 2 < ∞ \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}<\\infty blackboard_E ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ∞ . If 𝐱 \\bm{x} bold_italic_x is denoised according to the VP process with an expon"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark2", "title": "Remark 3.2 .", "snippet": "Remark 3.2 . What if the data is low-dimensional, say supported on a low-rank subspace of the high dimensional space ℝ D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ? If the data distribution is compactly supported—say if the data is normalized "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark3", "title": "Remark 3.3 .", "snippet": "Remark 3.3 . Various other works define the reverse process as moving backward in the time index t t italic_t using an explicit difference equation, or differential equation in the limit L → ∞ L\\to\\infty italic_L → ∞ , or forward in time using the transformation 𝒚 t = 𝒙 T − t \\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark4", "title": "Remark 3.4 .", "snippet": "Remark 3.4 . The theory presented at the end of the last Section 3.2.1 seems to suggest (loosely speaking) that in practice, using a transformer-like network is a good choice for learning or approximating a denoiser. This is reasonable, but what is the problem with using any old "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample4", "title": "Example 3.4 (Volume, Dimension, and Entropy) .", "snippet": "Example 3.4 (Volume, Dimension, and Entropy) . For the example shown on the top of Figure 3.8 , suppose we have taken some samples from a uniform distribution on a line (say in a 2D plane). The volume of the line or the sample sets is zero. Geometrically, the empirical distributi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample5", "title": "Example 3.5 (Density) .", "snippet": "Example 3.5 (Density) . Consider the two sets of sampled data points shown in Figure 3.8 . Geometrically, they are essentially the same: each set consists of eight points and each point has occurred with equal frequency 1 / 8 1/8 1 / 8 th. The only difference is that for the seco"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample6", "title": "Example 3.6 (Precision) .", "snippet": "Example 3.6 (Precision) . Consider a discrete distribution 𝑿 = [ e , π ] \\bm{X}=[e,\\pi] bold_italic_X = [ italic_e , italic_π ] with equal probability 1 / 2 1/2 1 / 2 taking the values of the Euler number e ≈ 2.71828 e\\approx 2.71828 italic_e ≈ 2.71828 or the number π ≈ 3.14159 \\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark5", "title": "Remark 3.5 .", "snippet": "Remark 3.5 . As it turns out, the rate distortion is an implementable approximation to the entropy of 𝒙 \\bm{x} bold_italic_x in the following sense. Assume that 𝒙 \\bm{x} bold_italic_x and 𝒙 ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG are continuous random vectors. Then t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark6", "title": "Remark 3.6 .", "snippet": "Remark 3.6 . Given a set of data points in 𝑿 = [ 𝒙 1 , … , 𝒙 N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , one can always interpret them as s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample7", "title": "Example 3.7 .", "snippet": "Example 3.7 . Sometimes, one may face an opposite situation when we want to fix the coding rate first and try to find a coding scheme that minimizes the distortion. For example, suppose that we only want to use a fixed number of codes for points sampled from a distribution, and w"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem6", "title": "Theorem 3.6 .", "snippet": "Theorem 3.6 . Suppose that 𝐱 \\bm{x} bold_italic_x is a random variable such that its support K ≐ Supp ⁡ ( 𝐱 ) K\\doteq\\operatorname{Supp}(\\bm{x}) italic_K ≐ roman_Supp ( bold_italic_x ) is a compact set. Define the covering number 𝒩 ϵ ​ ( K ) \\mathcal{N}_{\\epsilon}(K) caligraphic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark7", "title": "Remark 3.7 .", "snippet": "Remark 3.7 . The key ingredient in the proof of the lower bound in Theorem 3.6 is an important result from information theory known as the Shannon lower bound for the rate distortion, named after Claude Shannon, who first derived it in a special case [ Sha59 ] . It asserts the fo"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample8", "title": "Example 3.8 .", "snippet": "Example 3.8 . Figure 3.11 shows an example of a 2D distribution with an ellipsoidal support – approximating the support of a 2D Gaussian distribution. The region is covered by small balls of size ϵ \\epsilon italic_ϵ . All the balls are numbered from 1 1 1 to say n n italic_n . Th"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample9", "title": "Example 3.9 .", "snippet": "Example 3.9 . Figure 3.12 shows an example in which the data 𝑿 \\bm{X} bold_italic_X are distributed around two subspaces (or low-dimensional Gaussians). If they are viewed and coded together as one single Gaussian, the associated discrete (lossy) code book, represented by all the"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample10", "title": "Example 3.10 .", "snippet": "Example 3.10 . To see when the memorization regime is preferred or not, let us consider a number, say N N italic_N , of samples randomly distributed in a unit area on a 2D plane. 19 19 19 Say the points are drawn by a Poisson process with density N N italic_N points per unit area"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample11", "title": "Example 3.11 .", "snippet": "Example 3.11 . Figure 3.14 : Top: 358 noisy samples drawn from two lines and one plane in ℝ 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . Bottom: the effect of varying ϵ \\epsilon italic_ϵ on the clustering result and the coding rate. The red line mar"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample12", "title": "Example 3.12 (Image Segmentation) .", "snippet": "Example 3.12 (Image Segmentation) . The above measure of coding length and the associated clustering algorithm assume the data distribution is a mixture of (low-dimensional) Gaussians. Although this seems somewhat idealistic, the measure and algorithm can already be very useful a"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark8", "title": "Remark 3.8 .", "snippet": "Remark 3.8 . Neural collapse refers to a phenomenon observed in deep neural networks trained for classification, where the learned feature representations and classifier weights exhibit highly symmetric and structured behavior during the terminal phase of training [ PHD20 , ZDZ+2"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark9", "title": "Remark 3.9 .", "snippet": "Remark 3.9 . Linear discriminant analysis (LDA) [ HTF09 ] is a supervised dimensionality reduction technique that aims to find a linear projection of data that maximizes class separability. Specifically, given labeled data, LDA seeks a linear transformation that projects high-dim"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem7", "title": "Theorem 3.7 ( Characterization of Global Optimal Solutions).", "snippet": "Theorem 3.7 ( Characterization of Global Optimal Solutions). Suppose 𝐙 ∗ = [ 𝐙 1 ∗ , … , 𝐙 K ∗ ] \\bm{Z}^{\\ast}=[\\bm{Z}_{1}^{*},\\dots,\\bm{Z}_{K}^{*}] bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample13", "title": "Example 3.13 (Classification of Images on CIFAR-10) .", "snippet": "Example 3.13 (Classification of Images on CIFAR-10) . We here present how the MCR 2 objective helps learn better representations than the cross entropy ( 3.4.2 ) for image classification. Here we adopt the popular neural network architecture, the ResNet-18 [ HZR+16a ] , to model "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem8", "title": "Theorem 3.8 ( Local and Global Optima).", "snippet": "Theorem 3.8 ( Local and Global Optima). Let N k N_{k} italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the number of training samples in the k k italic_k -th class for each k ∈ { 1 , … , K } k\\in\\{1,\\dots,K\\} italic_k ∈ { 1 , … , italic_K } , N max ≐ max ⁡ { N 1 , …"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem9", "title": "Theorem 3.9 ( Benign Global Optimization Landscape).", "snippet": "Theorem 3.9 ( Benign Global Optimization Landscape). Given a coding precision ϵ > 0 \\epsilon>0 italic_ϵ > 0 , if the regularization parameter satisfies ( 3.4.17 ), it holds that any critical point 𝐙 \\bm{Z} bold_italic_Z of the problem ( 3.4.16 ) is either a local maximizer or a s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise1", "title": "Exercise 3.1 .", "snippet": "Exercise 3.1 . Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise2", "title": "Exercise 3.2 .", "snippet": "Exercise 3.2 . Consider random vectors 𝒙 ∈ ℝ D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and 𝒚 ∈ ℝ d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , such that t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise3", "title": "Exercise 3.3 .", "snippet": "Exercise 3.3 . Show the Sherman-Morrison-Woodbury identity, i.e., for matrices 𝑨 \\bm{A} bold_italic_A , 𝑪 \\bm{C} bold_italic_C , 𝑼 \\bm{U} bold_italic_U , 𝑽 \\bm{V} bold_italic_V such that 𝑨 \\bm{A} bold_italic_A , 𝑪 \\bm{C} bold_italic_C , and 𝑨 + 𝑼 ​ 𝑪 ​ 𝑽 \\bm{A}+\\bm{U}\\bm{C}\\bm{V}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise4", "title": "Exercise 3.4 .", "snippet": "Exercise 3.4 . Rederive the following, assuming 𝒙 t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows the generalized noise model ( 3.2.69 ). • Tweedie’s formula: ( 3.2.70 ). • The DDIM iteration: ( 3.2.71 ). • The Bayes optimal denoiser for a Gaussi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise5", "title": "Exercise 3.5 .", "snippet": "Exercise 3.5 . 1. Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures. 2. Reproduce Figure 3.4 and Figure 3.7 . 3. We now introduce a separate process called Flow Matching (FM) , as follows: α t = 1 − t , σ t = t . \\alpha_{t}=1-t,\\qquad\\sigma"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise6", "title": "Exercise 3.6 .", "snippet": "Exercise 3.6 . Please show the following properties of the log ​ det ( ⋅ ) \\log\\det(\\cdot) roman_log roman_det ( ⋅ ) function. 1. Show that f ​ ( 𝑿 ) = log ​ det ( 𝑿 ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E1", "title": "H ​ ( 𝒙 ) ≐ 𝔼 ​ [ log ⁡ 1 / p ​ ( 𝒙 ) ] = − ∑ i = 1 N p ​ ( 𝒙 i ) ​ log ⁡ p ​ ( 𝒙 i ) . H(\\bm{x})\\doteq\\mathbb{E}[\\log 1/p(\\bm{x})]=-\\sum_{i=1}^{N}p(\\bm{x}_{i})\\log p(\\bm{x}_{i}). italic_H ( bold_ital", "snippet": "H ​ ( 𝒙 ) ≐ 𝔼 ​ [ log ⁡ 1 / p ​ ( 𝒙 ) ] = − ∑ i = 1 N p ​ ( 𝒙 i ) ​ log ⁡ p ​ ( 𝒙 i ) . H(\\bm{x})\\doteq\\mathbb{E}[\\log 1/p(\\bm{x})]=-\\sum_{i=1}^{N}p(\\bm{x}_{i})\\log p(\\bm{x}_{i}). italic_H ( bold_italic_x ) ≐ blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ∑ start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E2", "title": "h ​ ( 𝒙 ) ≐ 𝔼 ⁡ [ log ⁡ 1 / p ​ ( 𝒙 ) ] = − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ​ ( 𝝃 ) ​ d 𝝃 . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathr", "snippet": "h ​ ( 𝒙 ) ≐ 𝔼 ⁡ [ log ⁡ 1 / p ​ ( 𝒙 ) ] = − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ​ ( 𝝃 ) ​ d 𝝃 . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x ) ≐ blackboard_E [ roman_log 1 / italic_p"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E3", "title": "h ​ ( x ) = 1 2 ​ log ⁡ ( 2 ​ π ​ σ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_π italic_σ", "snippet": "h ​ ( x ) = 1 2 ​ log ⁡ ( 2 ​ π ​ σ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_π italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG star"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E4", "title": "h ​ ( 𝒙 ) = D 2 ​ ( 1 + log ⁡ ( 2 ​ π ) ) + 1 2 ​ log ​ det ( 𝚺 ) . h(\\bm{x})=\\frac{D}{2}(1+\\log(2\\pi))+\\frac{1}{2}\\log\\det(\\bm{\\Sigma}). italic_h ( bold_italic_x ) = divide start_ARG italic_D end_ARG", "snippet": "h ​ ( 𝒙 ) = D 2 ​ ( 1 + log ⁡ ( 2 ​ π ) ) + 1 2 ​ log ​ det ( 𝚺 ) . h(\\bm{x})=\\frac{D}{2}(1+\\log(2\\pi))+\\frac{1}{2}\\log\\det(\\bm{\\Sigma}). italic_h ( bold_italic_x ) = divide start_ARG italic_D end_ARG start_ARG 2 end_ARG ( 1 + roman_log ( 2 italic_π ) ) + divide start_ARG 1 end_A"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E5", "title": "1 N ​ ∑ i = 1 N − log ⁡ q ​ ( 𝒙 i ) ≈ − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ q ​ ( 𝝃 ) ​ d 𝝃 \\frac{1}{N}\\sum_{i=1}^{N}-\\log q(\\bm{x}_{i})\\quad\\approx\\quad-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\mathrm{d}", "snippet": "1 N ​ ∑ i = 1 N − log ⁡ q ​ ( 𝒙 i ) ≈ − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ q ​ ( 𝝃 ) ​ d 𝝃 \\frac{1}{N}\\sum_{i=1}^{N}-\\log q(\\bm{x}_{i})\\quad\\approx\\quad-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E8", "title": "h ​ ( 𝒙 n ) > h ​ ( 𝒙 e ) > h ​ ( 𝒙 ^ ) . h(\\bm{x}^{n})>h(\\bm{x}^{e})>h(\\hat{\\bm{x}}). italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) > italic_h ( bold_italic_x start_PO", "snippet": "h ​ ( 𝒙 n ) > h ​ ( 𝒙 e ) > h ​ ( 𝒙 ^ ) . h(\\bm{x}^{n})>h(\\bm{x}^{e})>h(\\hat{\\bm{x}}). italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) > italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) > italic_h ( over^ start_ARG bold_i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E9", "title": "p ​ ( 𝒙 n ) → p ​ ( 𝒙 e ) → p ​ ( 𝒙 ^ ) . p(\\bm{x}^{n})\\rightarrow p(\\bm{x}^{e})\\rightarrow p(\\hat{\\bm{x}}). italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) → italic_p ( ", "snippet": "p ​ ( 𝒙 n ) → p ​ ( 𝒙 e ) → p ​ ( 𝒙 ^ ) . p(\\bm{x}^{n})\\rightarrow p(\\bm{x}^{e})\\rightarrow p(\\hat{\\bm{x}}). italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) → italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) → italic_p ( "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E1", "title": "𝒙 t ≐ 𝒙 + t ​ 𝒈 , ∀ t ∈ [ 0 , T ] , \\bm{x}_{t}\\doteq\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T], bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ bold_italic_x + italic_t bold_italic_g , ∀ it", "snippet": "𝒙 t ≐ 𝒙 + t ​ 𝒈 , ∀ t ∈ [ 0 , T ] , \\bm{x}_{t}\\doteq\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T], bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ bold_italic_x + italic_t bold_italic_g , ∀ italic_t ∈ [ 0 , italic_T ] , (3.2.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E2", "title": "d d ​ t ​ h ​ ( 𝒙 t ) > 0 , ∀ t ∈ ( 0 , T ] , \\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t})>0,\\qquad\\forall t\\in(0,T], divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold", "snippet": "d d ​ t ​ h ​ ( 𝒙 t ) > 0 , ∀ t ∈ ( 0 , T ] , \\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t})>0,\\qquad\\forall t\\in(0,T], divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > 0 , ∀ italic_t ∈ ( "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E3", "title": "𝒙 ¯ ∗ ​ ( t , ⋅ ) ∈ arg ​ min 𝒙 ¯ ​ ( t , ⋅ ) ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 𝒙 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 . \\bar{\\bm{x}}^{\\ast}(t,\\cdot)\\in\\operatorname*{arg\\ min}_{\\bar{\\bm{x}}(t,\\cdot)}\\operatorname{\\mathbb{E}}_{\\b", "snippet": "𝒙 ¯ ∗ ​ ( t , ⋅ ) ∈ arg ​ min 𝒙 ¯ ​ ( t , ⋅ ) ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 𝒙 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 . \\bar{\\bm{x}}^{\\ast}(t,\\cdot)\\in\\operatorname*{arg\\ min}_{\\bar{\\bm{x}}(t,\\cdot)}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bm{x}-\\bar{\\bm{x}}(t,\\bm{x}_{t})\\|_{2}^{2}. over¯ start_ARG b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E4", "title": "𝒙 ¯ ∗ ​ ( t , 𝝃 ) ≐ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t = 𝝃 ] . \\bar{\\bm{x}}^{\\ast}(t,\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]. over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ e", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝝃 ) ≐ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t = 𝝃 ] . \\bar{\\bm{x}}^{\\ast}(t,\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]. over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_ξ ) ≐ blackboard_E [ bold_italic_x ∣"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E5", "title": "𝒙 ∼ ∑ k = 1 K π k ​ 𝒩 ⁡ ( 𝝁 k , 𝚺 k ) , \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}), bold_italic_x ∼ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT st", "snippet": "𝒙 ∼ ∑ k = 1 K π k ​ 𝒩 ⁡ ( 𝝁 k , 𝚺 k ) , \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}), bold_italic_x ∼ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E6", "title": "𝒙 t = 𝒙 + t ​ 𝒈 ∼ ∑ k = 1 K π k ​ 𝒩 ⁡ ( 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) . \\bm{x}_{t}=\\bm{x}+t\\bm{g}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). bold_italic_x st", "snippet": "𝒙 t = 𝒙 + t ​ 𝒈 ∼ ∑ k = 1 K π k ​ 𝒩 ⁡ ( 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) . \\bm{x}_{t}=\\bm{x}+t\\bm{g}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E7", "title": "p t ​ ( 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) . p_{t}(\\bm{x}_{t})=\\sum_{k=1}^{K}\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). italic_p start_POSTSUBSCRIPT it", "snippet": "p t ​ ( 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) . p_{t}(\\bm{x}_{t})=\\sum_{k=1}^{K}\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E8", "title": "[ 𝒙 𝒙 t ] = [ 𝝁 y 𝝁 y ] + [ 𝚺 y 1 / 2 𝟎 𝚺 y 1 / 2 t ​ 𝑰 ] ​ [ 𝒖 𝒈 ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}=\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\Sigma}", "snippet": "[ 𝒙 𝒙 t ] = [ 𝝁 y 𝝁 y ] + [ 𝚺 y 1 / 2 𝟎 𝚺 y 1 / 2 t ​ 𝑰 ] ​ [ 𝒖 𝒈 ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}=\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\Sigma}_{y}^{1/2}&\\bm{0}\\\\ \\bm{\\Sigma}_{y}^{1/2}&t\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E9", "title": "[ 𝒙 𝒙 t ] ∼ 𝒩 ⁡ ( [ 𝝁 y 𝝁 y ] , [ 𝚺 y 𝚺 y 𝚺 y 𝚺 y + t 2 ​ 𝑰 ] ) . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}\\sim\\operatorname{\\mathcal{N}}\\left(\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmat", "snippet": "[ 𝒙 𝒙 t ] ∼ 𝒩 ⁡ ( [ 𝝁 y 𝝁 y ] , [ 𝚺 y 𝚺 y 𝚺 y 𝚺 y + t 2 ​ 𝑰 ] ) . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}\\sim\\operatorname{\\mathcal{N}}\\left(\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix},\\begin{bmatrix}\\bm{\\Sigma}_{y}&\\bm{\\Sigma}_{y}\\\\ \\bm{\\Sigma}_{y}&\\bm{\\Sigma"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E10", "title": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t , y ] = 𝝁 y + 𝚺 y ​ ( 𝚺 y + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 y ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y]=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}(\\bm{\\Sigma}_{y}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\b", "snippet": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t , y ] = 𝝁 y + 𝚺 y ​ ( 𝚺 y + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 y ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y]=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}(\\bm{\\Sigma}_{y}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu}_{y}). blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E16", "title": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t , y = k ] = 𝝁 k + 𝚺 k ​ ( 𝚺 k + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 k ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y=k]=\\bm{\\mu}_{k}+\\bm{\\Sigma}_{k}(\\bm{\\Sigma}_{k}+t^{2}\\bm{I})^{-1}(\\bm{x}_", "snippet": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t , y = k ] = 𝝁 k + 𝚺 k ​ ( 𝚺 k + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 k ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y=k]=\\bm{\\mu}_{k}+\\bm{\\Sigma}_{k}(\\bm{\\Sigma}_{k}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu}_{k}). blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E17", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) ∑ i = 1 K π i ​ φ ​ ( 𝒙 t ; 𝝁 i , 𝚺 i + t 2 ​ 𝑰 ) ⋅ ( 𝝁 k + 𝚺 k ​ ( 𝚺 k + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 k ) ) . \\bar{\\bm{x}}^{\\ast}(", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; 𝝁 k , 𝚺 k + t 2 ​ 𝑰 ) ∑ i = 1 K π i ​ φ ​ ( 𝒙 t ; 𝝁 i , 𝚺 i + t 2 ​ 𝑰 ) ⋅ ( 𝝁 k + 𝚺 k ​ ( 𝚺 k + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 k ) ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{K}\\frac{\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Si"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E18", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 𝝁 + 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 ) = 𝝁 + 𝑽 ​ [ λ 1 / ( λ 1 + t 2 ) ⋱ λ D / ( λ D + t 2 ) ] ​ 𝑽 ⊤ ​ ( 𝒙 t − 𝝁 ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{\\mu}+\\bm{\\Sigma}(\\bm", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 𝝁 + 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − 𝝁 ) = 𝝁 + 𝑽 ​ [ λ 1 / ( λ 1 + t 2 ) ⋱ λ D / ( λ D + t 2 ) ] ​ 𝑽 ⊤ ​ ( 𝒙 t − 𝝁 ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{\\mu}+\\bm{\\Sigma}(\\bm{\\Sigma}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu})=\\bm{\\mu}+\\bm{V}\\begin{bmatrix}\\l"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E19", "title": "‖ 𝒙 ¯ ∗ ​ ( t , 𝒙 t ) − 𝝁 ‖ 2 ≤ ‖ 𝒙 t − 𝝁 ‖ 2 . \\|\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})-\\bm{\\mu}\\|_{2}\\leq\\|\\bm{x}_{t}-\\bm{\\mu}\\|_{2}. ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_P", "snippet": "‖ 𝒙 ¯ ∗ ​ ( t , 𝒙 t ) − 𝝁 ‖ 2 ≤ ‖ 𝒙 t − 𝝁 ‖ 2 . \\|\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})-\\bm{\\mu}\\|_{2}\\leq\\|\\bm{x}_{t}-\\bm{\\mu}\\|_{2}. ∥ over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E20", "title": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] = 𝒙 t + t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x ∣ bold", "snippet": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] = 𝒙 t + t 2 ​ ∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = bold_italic_x start"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E33", "title": "x ¯ ∗ ​ ( t , x t ) = φ ​ ( x t ; + 1 , t 2 ) − φ ​ ( x t ; − 1 , t 2 ) φ ​ ( x t ; 1 , t 2 ) + φ ​ ( x t ; − 1 , t 2 ) = tanh ⁡ ( − x t t 2 ) . \\bar{x}^{\\ast}(t,x_{t})=\\frac{\\varphi(x_{t};+1,t^{2})-\\", "snippet": "x ¯ ∗ ​ ( t , x t ) = φ ​ ( x t ; + 1 , t 2 ) − φ ​ ( x t ; − 1 , t 2 ) φ ​ ( x t ; 1 , t 2 ) + φ ​ ( x t ; − 1 , t 2 ) = tanh ⁡ ( − x t t 2 ) . \\bar{x}^{\\ast}(t,x_{t})=\\frac{\\varphi(x_{t};+1,t^{2})-\\varphi(x_{t};-1,t^{2})}{\\varphi(x_{t};1,t^{2})+\\varphi(x_{t};-1,t^{2})}=\\tanh\\le"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E34", "title": "t ℓ = ℓ L ​ T , ℓ ∈ { 0 , 1 , … , L } . t_{\\ell}=\\frac{\\ell}{L}T,\\qquad\\ell\\in\\{0,1,\\dots,L\\}. italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = divide start_ARG roman_ℓ end_ARG start_ARG itali", "snippet": "t ℓ = ℓ L ​ T , ℓ ∈ { 0 , 1 , … , L } . t_{\\ell}=\\frac{\\ell}{L}T,\\qquad\\ell\\in\\{0,1,\\dots,L\\}. italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = divide start_ARG roman_ℓ end_ARG start_ARG italic_L end_ARG italic_T , roman_ℓ ∈ { 0 , 1 , … , italic_L } . (3.2.34)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E40", "title": "h ​ ( 𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t ] ) < h ​ ( 𝒙 t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∣", "snippet": "h ​ ( 𝔼 ⁡ [ 𝒙 s ∣ 𝒙 t ] ) < h ​ ( 𝒙 t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) < italic_h ( b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E41", "title": "min θ ∈ Θ ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta\\in\\Theta}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POS", "snippet": "min θ ∈ Θ ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta\\in\\Theta}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_θ ∈ roman_Θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E42", "title": "𝒙 ∼ 1 K ​ ∑ k = 1 K 𝒩 ⁡ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}) bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG itali", "snippet": "𝒙 ∼ 1 K ​ ∑ k = 1 K 𝒩 ⁡ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}) bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E43", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) ∑ i = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 i ​ 𝑼 i ⊤ + t 2 ​ 𝑰 ) ⋅ ( 𝑼 k ​ 𝑼 k ⊤ ​ ( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 ​ 𝒙 t ) . \\bar{\\bm{x}}^{\\ast}(", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) ∑ i = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 i ​ 𝑼 i ⊤ + t 2 ​ 𝑰 ) ⋅ ( 𝑼 k ​ 𝑼 k ⊤ ​ ( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 ​ 𝒙 t ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{K}\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E44", "title": "( 𝑨 + 𝑼 ​ 𝑪 ​ 𝑽 ) − 1 = 𝑨 − 1 − 𝑨 − 1 ​ 𝑼 ​ ( 𝑪 − 1 + 𝑽 ​ 𝑨 − 1 ​ 𝑼 ) − 1 ​ 𝑽 ​ 𝑨 − 1 . (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\b", "snippet": "( 𝑨 + 𝑼 ​ 𝑪 ​ 𝑽 ) − 1 = 𝑨 − 1 − 𝑨 − 1 ​ 𝑼 ​ ( 𝑪 − 1 + 𝑽 ​ 𝑨 − 1 ​ 𝑼 ) − 1 ​ 𝑽 ​ 𝑨 − 1 . (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1}. ( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E56", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t , \\bar{\\bm{x}}", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E59", "title": "min 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 t ‖ 2 2 = 𝔼 𝒙 , 𝒈 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 , \\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}", "snippet": "min 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 t ‖ 2 2 = 𝔼 𝒙 , 𝒈 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 , \\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}\\right\\|"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E65", "title": "𝒙 T T = 𝒙 + T ​ 𝒈 T = 𝒙 T + 𝒈 → 𝒈 ∼ 𝒩 ⁡ ( 𝟎 , 𝑰 ) . \\frac{\\bm{x}_{T}}{T}=\\frac{\\bm{x}+T\\bm{g}}{T}=\\frac{\\bm{x}}{T}+\\bm{g}\\to\\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}). divide start_ARG bold_i", "snippet": "𝒙 T T = 𝒙 + T ​ 𝒈 T = 𝒙 T + 𝒈 → 𝒈 ∼ 𝒩 ⁡ ( 𝟎 , 𝑰 ) . \\frac{\\bm{x}_{T}}{T}=\\frac{\\bm{x}+T\\bm{g}}{T}=\\frac{\\bm{x}}{T}+\\bm{g}\\to\\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}). divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E66", "title": "𝒙 ^ t ℓ − 1 = ( 1 − 1 ℓ ) ⋅ 𝒙 ^ t ℓ + 1 ℓ ⋅ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}_{t_{\\ell}}+\\frac{1}{\\ell}\\cdot\\bar{\\bm{x}}^{\\ast}(t_{\\e", "snippet": "𝒙 ^ t ℓ − 1 = ( 1 − 1 ℓ ) ⋅ 𝒙 ^ t ℓ + 1 ℓ ⋅ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}_{t_{\\ell}}+\\frac{1}{\\ell}\\cdot\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}). over^ start_ARG bold_italic_x end_ARG start_POSTSU"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E67", "title": "t ℓ = C 1 ​ ( e C 2 ​ ℓ − 1 ) , ∀ ℓ ∈ { 0 , 1 , … , L } t_{\\ell}=C_{1}(e^{C_{2}\\ell}-1),\\qquad\\forall\\ell\\in\\{0,1,\\dots,L\\} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = italic_C start_POST", "snippet": "t ℓ = C 1 ​ ( e C 2 ​ ℓ − 1 ) , ∀ ℓ ∈ { 0 , 1 , … , L } t_{\\ell}=C_{1}(e^{C_{2}\\ell}-1),\\qquad\\forall\\ell\\in\\{0,1,\\dots,L\\} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_e start_POSTSUPERSCRIPT italic_C start_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E68", "title": "𝒙 ^ t ℓ − 1 ≐ t ℓ − 1 t ℓ ​ 𝒙 ^ t ℓ + ( 1 − t ℓ − 1 t ℓ ) ​ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) , \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{t_{\\ell-1}}{t_{\\ell}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(1-\\frac{t_{\\ell-1}}{t_{\\", "snippet": "𝒙 ^ t ℓ − 1 ≐ t ℓ − 1 t ℓ ​ 𝒙 ^ t ℓ + ( 1 − t ℓ − 1 t ℓ ) ​ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) , \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{t_{\\ell-1}}{t_{\\ell}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(1-\\frac{t_{\\ell-1}}{t_{\\ell}}\\right)\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}), over^ start_A"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E69", "title": "𝒙 t ≐ α t ​ 𝒙 + σ t ​ 𝒈 , ∀ t ∈ [ 0 , T ] . \\bm{x}_{t}\\doteq\\alpha_{t}\\bm{x}+\\sigma_{t}\\bm{g},\\qquad\\forall t\\in[0,T]. bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ italic_α start_POS", "snippet": "𝒙 t ≐ α t ​ 𝒙 + σ t ​ 𝒈 , ∀ t ∈ [ 0 , T ] . \\bm{x}_{t}\\doteq\\alpha_{t}\\bm{x}+\\sigma_{t}\\bm{g},\\qquad\\forall t\\in[0,T]. bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≐ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_σ start_POSTSUBSCRI"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E70", "title": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] = 1 α t ​ ( 𝒙 t + σ t 2 ​ ∇ log ⁡ p t ​ ( 𝒙 ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}}\\left(\\bm{x}_{t}+\\sigma_{t}^{2}\\nabla\\log p_{t}(\\bm{x})\\right). bl", "snippet": "𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] = 1 α t ​ ( 𝒙 t + σ t 2 ​ ∇ log ⁡ p t ​ ( 𝒙 ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}}\\left(\\bm{x}_{t}+\\sigma_{t}^{2}\\nabla\\log p_{t}(\\bm{x})\\right). blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E71", "title": "𝒙 ^ t ℓ − 1 = σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{", "snippet": "𝒙 ^ t ℓ − 1 = σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 ^ t ℓ ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\a"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E72", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; α t ​ 𝝁 k , α t 2 ​ 𝚺 k + σ t 2 ​ 𝑰 ) ∑ i = 1 K π i ​ φ ​ ( 𝒙 t ; α t ​ 𝝁 i , α t 2 ​ 𝚺 i + σ t 2 ​ 𝑰 ) ⋅ ( 𝝁 k + α t ​ 𝚺 k ​ ( α t 2 ​ 𝚺 k + σ t 2 ​ ", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = ∑ k = 1 K π k ​ φ ​ ( 𝒙 t ; α t ​ 𝝁 k , α t 2 ​ 𝚺 k + σ t 2 ​ 𝑰 ) ∑ i = 1 K π i ​ φ ​ ( 𝒙 t ; α t ​ 𝝁 i , α t 2 ​ 𝚺 i + σ t 2 ​ 𝑰 ) ⋅ ( 𝝁 k + α t ​ 𝚺 k ​ ( α t 2 ​ 𝚺 k + σ t 2 ​ 𝑰 ) − 1 ​ ( 𝒙 t − α t ​ 𝝁 k ) ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.Ex1", "title": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ 𝒙 ¯ ​ ( t ℓ , 𝒙 ^ t ℓ ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_", "snippet": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ 𝒙 ¯ ​ ( t ℓ , 𝒙 ^ t ℓ ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E73", "title": "min θ ⁡ 𝔼 t , 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t,\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCR", "snippet": "min θ ⁡ 𝔼 t , 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t,\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t , bold_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E74", "title": "min θ ⁡ 𝔼 t ⁡ w t ​ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{", "snippet": "min θ ⁡ 𝔼 t ⁡ w t ​ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 ¯ θ ​ ( t , 𝒙 t ) − 𝒙 ‖ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboar"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E75", "title": "𝒙 t = α t ​ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] + σ t ​ 𝔼 ⁡ [ 𝒈 ∣ 𝒙 t ] ⟹ 𝔼 ⁡ [ 𝒈 ∣ 𝒙 t ] = 1 σ t ​ ( 𝒙 t − α t ​ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] ) , \\bm{x}_{t}=\\alpha_{t}\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]+\\sigma_{t}\\operat", "snippet": "𝒙 t = α t ​ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] + σ t ​ 𝔼 ⁡ [ 𝒈 ∣ 𝒙 t ] ⟹ 𝔼 ⁡ [ 𝒈 ∣ 𝒙 t ] = 1 σ t ​ ( 𝒙 t − α t ​ 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] ) , \\bm{x}_{t}=\\alpha_{t}\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]+\\sigma_{t}\\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}]\\implies\\operatorname{\\mathbb{E}}[\\bm{g}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E76", "title": "𝒈 ¯ ​ ( t , 𝒙 t ) = 1 σ t ​ 𝒙 t − α t σ t ​ 𝒙 ¯ ​ ( t , 𝒙 t ) , \\bar{\\bm{g}}(t,\\bm{x}_{t})=\\frac{1}{\\sigma_{t}}\\bm{x}_{t}-\\frac{\\alpha_{t}}{\\sigma_{t}}\\bar{\\bm{x}}(t,\\bm{x}_{t}), over¯ start_ARG bold_", "snippet": "𝒈 ¯ ​ ( t , 𝒙 t ) = 1 σ t ​ 𝒙 t − α t σ t ​ 𝒙 ¯ ​ ( t , 𝒙 t ) , \\bar{\\bm{g}}(t,\\bm{x}_{t})=\\frac{1}{\\sigma_{t}}\\bm{x}_{t}-\\frac{\\alpha_{t}}{\\sigma_{t}}\\bar{\\bm{x}}(t,\\bm{x}_{t}), over¯ start_ARG bold_italic_g end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E77", "title": "𝔼 t ⁡ w t ​ 𝔼 𝒈 , 𝒙 t ⁡ ‖ 𝒈 − 𝒈 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 = 𝔼 t ⁡ w t ​ α t 2 σ t 2 ​ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 𝒙 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 . \\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{g},\\bm{x}", "snippet": "𝔼 t ⁡ w t ​ 𝔼 𝒈 , 𝒙 t ⁡ ‖ 𝒈 − 𝒈 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 = 𝔼 t ⁡ w t ​ α t 2 σ t 2 ​ 𝔼 𝒙 , 𝒙 t ⁡ ‖ 𝒙 − 𝒙 ¯ ​ ( t , 𝒙 t ) ‖ 2 2 . \\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{g},\\bm{x}_{t}}\\|\\bm{g}-\\bar{\\bm{g}}(t,\\bm{x}_{t})\\|_{2}^{2}=\\operatorname{\\mathbb{E}}_{t}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E78", "title": "𝖳𝖵 ⁡ ( 𝒙 , 𝒚 ) ≐ sup A ⊆ ℝ d | ℙ ⁡ [ 𝒙 ∈ A ] − ℙ ⁡ [ 𝒚 ∈ A ] | . \\operatorname{\\mathsf{TV}}(\\bm{x},\\bm{y})\\doteq\\sup_{A\\subseteq\\mathbb{R}^{d}}\\left\\lvert\\operatorname{\\mathbb{P}}[\\bm{x}\\in A]-\\operat", "snippet": "𝖳𝖵 ⁡ ( 𝒙 , 𝒚 ) ≐ sup A ⊆ ℝ d | ℙ ⁡ [ 𝒙 ∈ A ] − ℙ ⁡ [ 𝒚 ∈ A ] | . \\operatorname{\\mathsf{TV}}(\\bm{x},\\bm{y})\\doteq\\sup_{A\\subseteq\\mathbb{R}^{d}}\\left\\lvert\\operatorname{\\mathbb{P}}[\\bm{x}\\in A]-\\operatorname{\\mathbb{P}}[\\bm{y}\\in A]\\right\\rvert. sansserif_TV ( bold_italic_x , bold"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E79", "title": "𝖳𝖵 ⁡ ( 𝒙 , 𝒙 ^ ) = 𝒪 ~ ​ ( D L ⏟ discretization error + 1 L ​ ∑ ℓ = 1 L α t ℓ σ t ℓ 2 ​ 𝔼 𝒙 , 𝒙 t ℓ ⁡ ‖ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 t ℓ ) − 𝒙 ¯ ​ ( t ℓ , 𝒙 t ℓ ) ‖ 2 2 ⏟ average excess error of the denoiser ) \\", "snippet": "𝖳𝖵 ⁡ ( 𝒙 , 𝒙 ^ ) = 𝒪 ~ ​ ( D L ⏟ discretization error + 1 L ​ ∑ ℓ = 1 L α t ℓ σ t ℓ 2 ​ 𝔼 𝒙 , 𝒙 t ℓ ⁡ ‖ 𝒙 ¯ ∗ ​ ( t ℓ , 𝒙 t ℓ ) − 𝒙 ¯ ​ ( t ℓ , 𝒙 t ℓ ) ‖ 2 2 ⏟ average excess error of the denoiser ) \\operatorname{\\mathsf{TV}}(\\bm{x},\\hat{\\bm{x}})=\\tilde{\\mathcal{O}}\\left(\\underbr"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E80", "title": "𝒪 ~ ​ ( approximate intrinsic dimension L ) \\tilde{\\mathcal{O}}\\left(\\frac{\\text{approximate intrinsic dimension}}{L}\\right) over~ start_ARG caligraphic_O end_ARG ( divide start_ARG approximate intrin", "snippet": "𝒪 ~ ​ ( approximate intrinsic dimension L ) \\tilde{\\mathcal{O}}\\left(\\frac{\\text{approximate intrinsic dimension}}{L}\\right) over~ start_ARG caligraphic_O end_ARG ( divide start_ARG approximate intrinsic dimension end_ARG start_ARG italic_L end_ARG ) (3.2.80)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E81", "title": "𝒙 ¯ ⋆ ​ ( t , 𝒙 t ) = ∑ i = 1 N e − ‖ 𝒙 t − α t ​ 𝒙 i ‖ 2 2 / ( 2 ​ σ t 2 ) ∑ j = 1 N e − ‖ 𝒙 t − α t ​ 𝒙 j ‖ 2 2 / ( 2 ​ σ t 2 ) ​ 𝒙 i . \\bar{\\bm{x}}^{\\star}(t,\\bm{x}_{t})=\\sum_{i=1}^{N}\\frac{e^{-\\|\\", "snippet": "𝒙 ¯ ⋆ ​ ( t , 𝒙 t ) = ∑ i = 1 N e − ‖ 𝒙 t − α t ​ 𝒙 i ‖ 2 2 / ( 2 ​ σ t 2 ) ∑ j = 1 N e − ‖ 𝒙 t − α t ​ 𝒙 j ‖ 2 2 / ( 2 ​ σ t 2 ) ​ 𝒙 i . \\bar{\\bm{x}}^{\\star}(t,\\bm{x}_{t})=\\sum_{i=1}^{N}\\frac{e^{-\\|\\bm{x}_{t}-\\alpha_{t}\\bm{x}_{i}\\|_{2}^{2}/(2\\sigma_{t}^{2})}}{\\sum_{j=1}^{N}e^{-\\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E1", "title": "𝒙 ↦ 𝒙 ^ \\bm{x}\\mapsto\\hat{\\bm{x}} bold_italic_x ↦ over^ start_ARG bold_italic_x end_ARG (3.3.1)", "snippet": "𝒙 ↦ 𝒙 ^ \\bm{x}\\mapsto\\hat{\\bm{x}} bold_italic_x ↦ over^ start_ARG bold_italic_x end_ARG (3.3.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E2", "title": "𝔼 ⁡ [ d ​ ( 𝒙 , 𝒙 ^ ) 2 ] ≤ ϵ 2 . \\operatorname{\\mathbb{E}}[d(\\bm{x},\\hat{\\bm{x}})^{2}]\\leq\\epsilon^{2}. blackboard_E [ italic_d ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) start_POSTSUP", "snippet": "𝔼 ⁡ [ d ​ ( 𝒙 , 𝒙 ^ ) 2 ] ≤ ϵ 2 . \\operatorname{\\mathbb{E}}[d(\\bm{x},\\hat{\\bm{x}})^{2}]\\leq\\epsilon^{2}. blackboard_E [ italic_d ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≤ italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPE"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E3", "title": "ℛ ϵ ​ ( 𝒙 ) = min p ​ ( 𝒙 ^ ∣ 𝒙 ) : 𝔼 ⁡ [ ‖ 𝒙 − 𝒙 ^ ‖ 2 2 ] ≤ ϵ 2 ⁡ I ​ ( 𝒙 ; 𝒙 ^ ) , \\mathcal{R}_{\\epsilon}(\\bm{x})=\\min_{p(\\hat{\\bm{x}}\\mid\\bm{x}):\\operatorname{\\mathbb{E}}[\\|\\bm{x}-\\hat{\\bm{x}}\\|_{", "snippet": "ℛ ϵ ​ ( 𝒙 ) = min p ​ ( 𝒙 ^ ∣ 𝒙 ) : 𝔼 ⁡ [ ‖ 𝒙 − 𝒙 ^ ‖ 2 2 ] ≤ ϵ 2 ⁡ I ​ ( 𝒙 ; 𝒙 ^ ) , \\mathcal{R}_{\\epsilon}(\\bm{x})=\\min_{p(\\hat{\\bm{x}}\\mid\\bm{x}):\\operatorname{\\mathbb{E}}[\\|\\bm{x}-\\hat{\\bm{x}}\\|_{2}^{2}]\\leq\\epsilon^{2}}I(\\bm{x};\\hat{\\bm{x}}), caligraphic_R start_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E4", "title": "I ​ ( 𝒙 ; 𝒙 ^ ) = 𝖪𝖫 ⁡ ( p ​ ( 𝒙 , 𝒙 ^ ) ∥ p ​ ( 𝒙 ) ​ p ​ ( 𝒙 ^ ) ) . I(\\bm{x};\\hat{\\bm{x}})=\\operatorname{\\mathsf{KL}}(p(\\bm{x},\\hat{\\bm{x}})\\;\\|\\;p(\\bm{x})p(\\hat{\\bm{x}})). italic_I ( bold_italic_x", "snippet": "I ​ ( 𝒙 ; 𝒙 ^ ) = 𝖪𝖫 ⁡ ( p ​ ( 𝒙 , 𝒙 ^ ) ∥ p ​ ( 𝒙 ) ​ p ​ ( 𝒙 ^ ) ) . I(\\bm{x};\\hat{\\bm{x}})=\\operatorname{\\mathsf{KL}}(p(\\bm{x},\\hat{\\bm{x}})\\;\\|\\;p(\\bm{x})p(\\hat{\\bm{x}})). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = sansserif_KL ( italic_p ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E5", "title": "I ​ ( 𝒙 ; 𝒙 ^ ) = h ​ ( 𝒙 ) − h ​ ( 𝒙 ∣ 𝒙 ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_", "snippet": "I ​ ( 𝒙 ; 𝒙 ^ ) = h ​ ( 𝒙 ) − h ​ ( 𝒙 ∣ 𝒙 ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_x ) - italic_h ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) , (3.3."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E6", "title": "I ​ ( 𝒙 ; 𝒙 ^ ) = H ​ ( 𝒙 ) − H ​ ( 𝒙 ∣ 𝒙 ^ ) . I(\\bm{x};\\hat{\\bm{x}})=H(\\bm{x})-H(\\bm{x}\\mid\\hat{\\bm{x}}). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_", "snippet": "I ​ ( 𝒙 ; 𝒙 ^ ) = H ​ ( 𝒙 ) − H ​ ( 𝒙 ∣ 𝒙 ^ ) . I(\\bm{x};\\hat{\\bm{x}})=H(\\bm{x})-H(\\bm{x}\\mid\\hat{\\bm{x}}). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ∣ over^ start_ARG bold_italic_x end_ARG ) . (3.3."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E7", "title": "𝒩 ϵ ​ ( K ) ≐ min ⁡ { n ∈ ℕ : ∃ 𝒑 1 , … , 𝒑 n ∈ K ​ s.t. ​ K ⊆ ⋃ i = 1 n B ϵ ​ ( 𝒑 i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\bm{p}_{1},\\dots,\\bm{p}_{n}\\in K\\ \\text{", "snippet": "𝒩 ϵ ​ ( K ) ≐ min ⁡ { n ∈ ℕ : ∃ 𝒑 1 , … , 𝒑 n ∈ K ​ s.t. ​ K ⊆ ⋃ i = 1 n B ϵ ​ ( 𝒑 i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\bm{p}_{1},\\dots,\\bm{p}_{n}\\in K\\ \\text{s.t.}\\ K\\subseteq\\bigcup_{i=1}^{n}B_{\\epsilon}(\\bm{p}_{i})\\right\\}, caligraphic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E8", "title": "ℛ ϵ ​ ( 𝒙 ) ≤ log 2 ⁡ 𝒩 ϵ ​ ( K ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\leq\\log_{2}\\mathcal{N}_{\\epsilon}(K). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≤ roman_log star", "snippet": "ℛ ϵ ​ ( 𝒙 ) ≤ log 2 ⁡ 𝒩 ϵ ​ ( K ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\leq\\log_{2}\\mathcal{N}_{\\epsilon}(K). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≤ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_ϵ e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E9", "title": "ℛ ϵ ​ ( 𝒙 ) ≥ log 2 ⁡ 𝒩 ϵ ​ ( K ) − O ​ ( D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq\\log_{2}\\mathcal{N}_{\\epsilon}(K)-O(D). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x )", "snippet": "ℛ ϵ ​ ( 𝒙 ) ≥ log 2 ⁡ 𝒩 ϵ ​ ( K ) − O ​ ( D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq\\log_{2}\\mathcal{N}_{\\epsilon}(K)-O(D). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUB"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E10", "title": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}, caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POST", "snippet": "ℛ ϵ ​ ( 𝒙 ) ≥ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}, caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ italic_h ( bold_italic_x ) - roman_log start_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E11", "title": "lim ϵ → 0 ℛ ϵ ​ ( 𝒙 ) − [ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − C D ] = 0 . \\lim_{\\epsilon\\to 0}\\mathcal{R}_{\\epsilon}(\\bm{x})-\\left[h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}\\right]=0. ro", "snippet": "lim ϵ → 0 ℛ ϵ ​ ( 𝒙 ) − [ h ​ ( 𝒙 ) − log 2 ⁡ vol ⁡ ( B ϵ ) − C D ] = 0 . \\lim_{\\epsilon\\to 0}\\mathcal{R}_{\\epsilon}(\\bm{x})-\\left[h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}\\right]=0. roman_lim start_POSTSUBSCRIPT italic_ϵ → 0 end_POSTSUBSCRIPT caligraphic_R start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E16", "title": "𝒙 i ↦ 𝒙 ^ i , subject to ‖ 𝒙 i − 𝒙 ^ i ‖ 2 ≤ ϵ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x}}_{i},\\quad\\mbox{subject to}\\quad\\|\\bm{x}_{i}-\\hat{\\bm{x}}_{i}\\|_{2}\\leq\\epsilon. bold_italic_x start_POSTSUBSCRIPT italic_", "snippet": "𝒙 i ↦ 𝒙 ^ i , subject to ‖ 𝒙 i − 𝒙 ^ i ‖ 2 ≤ ϵ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x}}_{i},\\quad\\mbox{subject to}\\quad\\|\\bm{x}_{i}-\\hat{\\bm{x}}_{i}\\|_{2}\\leq\\epsilon. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ↦ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E17", "title": "𝚺 = 1 N ​ 𝑿 ​ 𝑿 ⊤ . {\\bm{\\Sigma}}=\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. bold_Σ = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT", "snippet": "𝚺 = 1 N ​ 𝑿 ​ 𝑿 ⊤ . {\\bm{\\Sigma}}=\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. bold_Σ = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . (3.3.17)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E18", "title": "𝒙 ^ i = 𝒙 i + 𝒘 i , \\hat{\\bm{x}}_{i}=\\bm{x}_{i}+\\bm{w}_{i}, over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSU", "snippet": "𝒙 ^ i = 𝒙 i + 𝒘 i , \\hat{\\bm{x}}_{i}=\\bm{x}_{i}+\\bm{w}_{i}, over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , (3.3.18"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E19", "title": "𝚺 ^ = 𝔼 ​ [ 𝒙 ^ i ​ 𝒙 ^ i ⊤ ] = ϵ 2 D ​ 𝑰 + 1 N ​ 𝑿 ​ 𝑿 ⊤ . \\hat{\\bm{\\Sigma}}=\\mathbb{E}\\left[\\hat{\\bm{x}}_{i}\\hat{\\bm{x}}_{i}^{\\top}\\right]=\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}", "snippet": "𝚺 ^ = 𝔼 ​ [ 𝒙 ^ i ​ 𝒙 ^ i ⊤ ] = ϵ 2 D ​ 𝑰 + 1 N ​ 𝑿 ​ 𝑿 ⊤ . \\hat{\\bm{\\Sigma}}=\\mathbb{E}\\left[\\hat{\\bm{x}}_{i}\\hat{\\bm{x}}_{i}^{\\top}\\right]=\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. over^ start_ARG bold_Σ end_ARG = blackboard_E [ over^ start_ARG bold_italic_x "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E20", "title": "volume ​ ( 𝒙 ^ i ) ∝ det ( 𝚺 ^ ) = det ( ϵ 2 D ​ 𝑰 + 1 N ​ 𝑿 ​ 𝑿 ⊤ ) . \\mbox{volume}(\\hat{\\bm{x}}_{i})\\propto\\sqrt{\\det\\big{(}\\hat{\\bm{\\Sigma}}\\big{)}}=\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}+\\fr", "snippet": "volume ​ ( 𝒙 ^ i ) ∝ det ( 𝚺 ^ ) = det ( ϵ 2 D ​ 𝑰 + 1 N ​ 𝑿 ​ 𝑿 ⊤ ) . \\mbox{volume}(\\hat{\\bm{x}}_{i})\\propto\\sqrt{\\det\\big{(}\\hat{\\bm{\\Sigma}}\\big{)}}=\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}\\right)}. volume ( over^ start_ARG bold_italic_x end_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E21", "title": "volume ​ ( 𝒘 i ) ∝ det ( ϵ 2 D ​ 𝑰 ) . \\mbox{volume}(\\bm{w}_{i})\\propto\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}\\right)}. volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∝ s", "snippet": "volume ​ ( 𝒘 i ) ∝ det ( ϵ 2 D ​ 𝑰 ) . \\mbox{volume}(\\bm{w}_{i})\\propto\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}\\right)}. volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∝ square-root start_ARG roman_det ( divide start_ARG italic_ϵ start_POSTSUPERSCRIPT"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E22", "title": "# ​ ϵ ​ -balls ≈ volume ​ ( 𝒙 ^ i ) volume ​ ( 𝒘 i ) = det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\#\\,\\epsilon\\mbox{-balls}\\approx\\frac{\\mbox{volume}(\\hat{\\bm{x}}_{i})}{\\mbox{volume}(\\bm{w}_{i})}=\\sqrt{\\det\\le", "snippet": "# ​ ϵ ​ -balls ≈ volume ​ ( 𝒙 ^ i ) volume ​ ( 𝒘 i ) = det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\#\\,\\epsilon\\mbox{-balls}\\approx\\frac{\\mbox{volume}(\\hat{\\bm{x}}_{i})}{\\mbox{volume}(\\bm{w}_{i})}=\\sqrt{\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right)}. # italic_ϵ -balls "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E23", "title": "ℛ ϵ ​ ( 𝑿 ) ≈ log 2 ⁡ ( # ​ ϵ ​ -balls ) ≈ R ϵ ​ ( 𝑿 ) ≐ 1 2 ​ log ​ det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx\\log_{2}(\\#\\,\\epsilon\\mbox{-balls})\\approx R_{\\epsilon}(\\bm{", "snippet": "ℛ ϵ ​ ( 𝑿 ) ≈ log 2 ⁡ ( # ​ ϵ ​ -balls ) ≈ R ϵ ​ ( 𝑿 ) ≐ 1 2 ​ log ​ det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx\\log_{2}(\\#\\,\\epsilon\\mbox{-balls})\\approx R_{\\epsilon}(\\bm{X})\\doteq\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E24", "title": "𝒙 ⟶ ball ϵ ⁡ ( 𝒙 ) ⟶ 𝒙 ^ = center of ​ ball ϵ ⁡ ( 𝒙 ) . \\bm{x}\\longrightarrow\\operatorname{ball}_{\\epsilon}(\\bm{x})\\longrightarrow\\hat{\\bm{x}}=\\mbox{center of}\\operatorname{ball}_{\\epsilon}(\\bm{x}). b", "snippet": "𝒙 ⟶ ball ϵ ⁡ ( 𝒙 ) ⟶ 𝒙 ^ = center of ​ ball ϵ ⁡ ( 𝒙 ) . \\bm{x}\\longrightarrow\\operatorname{ball}_{\\epsilon}(\\bm{x})\\longrightarrow\\hat{\\bm{x}}=\\mbox{center of}\\operatorname{ball}_{\\epsilon}(\\bm{x}). bold_italic_x ⟶ roman_ball start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E25", "title": "𝑿 = [ 𝒙 1 , 𝒙 2 , … , 𝒙 N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2", "snippet": "𝑿 = [ 𝒙 1 , 𝒙 2 , … , 𝒙 N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E26", "title": "𝑿 ​ 𝚷 = [ 𝑿 1 , 𝑿 2 , … , 𝑿 K ] , \\bm{X}\\bm{\\Pi}=[\\bm{X}_{1},\\bm{X}_{2},\\dots,\\bm{X}_{K}], bold_italic_X bold_Π = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBS", "snippet": "𝑿 ​ 𝚷 = [ 𝑿 1 , 𝑿 2 , … , 𝑿 K ] , \\bm{X}\\bm{\\Pi}=[\\bm{X}_{1},\\bm{X}_{2},\\dots,\\bm{X}_{K}], bold_italic_X bold_Π = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_X start_POSTSUBSCRIPT italic_K end_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E27", "title": "ℛ ϵ ​ ( 𝑿 ) ≈ R ϵ ​ ( 𝑿 ) = 1 2 ​ log ​ det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}", "snippet": "ℛ ϵ ​ ( 𝑿 ) ≈ R ϵ ​ ( 𝑿 ) = 1 2 ​ log ​ det ( 𝑰 + D N ​ ϵ 2 ​ 𝑿 ​ 𝑿 ⊤ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right). caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E28", "title": "ℛ 0 ​ ( 𝑿 ) = log ⁡ ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) . (3.3.28)", "snippet": "ℛ 0 ​ ( 𝑿 ) = log ⁡ ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) . (3.3.28)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E29", "title": "R ϵ c ​ ( 𝑿 ∣ 𝚷 ) = N 1 N ​ R ϵ ​ ( 𝑿 1 ) + N 2 N ​ R ϵ ​ ( 𝑿 2 ) , R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})=\\frac{N_{1}}{N}R_{\\epsilon}(\\bm{X}_{1})+\\frac{N_{2}}{N}R_{\\epsilon}(\\bm{X}_{2}), italic_R start", "snippet": "R ϵ c ​ ( 𝑿 ∣ 𝚷 ) = N 1 N ​ R ϵ ​ ( 𝑿 1 ) + N 2 N ​ R ϵ ​ ( 𝑿 2 ) , R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})=\\frac{N_{1}}{N}R_{\\epsilon}(\\bm{X}_{1})+\\frac{N_{2}}{N}R_{\\epsilon}(\\bm{X}_{2}), italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E30", "title": "R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≪ R ϵ ​ ( 𝑿 ) , R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≪ R 0 ​ ( 𝑿 ) . R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{\\epsilon}(\\bm{X}),\\quad R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{0}(\\bm{X}). italic_R s", "snippet": "R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≪ R ϵ ​ ( 𝑿 ) , R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≪ R 0 ​ ( 𝑿 ) . R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{\\epsilon}(\\bm{X}),\\quad R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{0}(\\bm{X}). italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E31", "title": "min 𝚷 ⁡ { R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≐ ∑ k = 1 K N k N ​ R ϵ ​ ( 𝑿 k ) } . \\min_{\\bm{\\Pi}}\\left\\{R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{N_{k}}{N}R_{\\epsilon}(\\bm{X}_{k})\\right\\}. roman_m", "snippet": "min 𝚷 ⁡ { R ϵ c ​ ( 𝑿 ∣ 𝚷 ) ≐ ∑ k = 1 K N k N ​ R ϵ ​ ( 𝑿 k ) } . \\min_{\\bm{\\Pi}}\\left\\{R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{N_{k}}{N}R_{\\epsilon}(\\bm{X}_{k})\\right\\}. roman_min start_POSTSUBSCRIPT bold_Π end_POSTSUBSCRIPT { italic_R start_POSTSUBSCRIPT i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E32", "title": "L ​ ( 𝑿 k ) = N k ​ R ϵ ​ ( 𝑿 k ) . L(\\bm{X}_{k})=N_{k}R_{\\epsilon}(\\bm{X}_{k}). italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_P", "snippet": "L ​ ( 𝑿 k ) = N k ​ R ϵ ​ ( 𝑿 k ) . L(\\bm{X}_{k})=N_{k}R_{\\epsilon}(\\bm{X}_{k}). italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.Ex1", "title": "L c ​ ( 𝑿 k , 𝑿 l ) = N k ​ R ϵ ​ ( 𝑿 k ) + N l ​ R ϵ ​ ( 𝑿 l ) − N k ​ log ⁡ N k N k + N l − N l ​ log ⁡ N l N k + N l . L^{c}(\\bm{X}_{k},\\bm{X}_{l})=N_{k}R_{\\epsilon}(\\bm{X}_{k})+N_{l}R_{\\epsilon}(\\", "snippet": "L c ​ ( 𝑿 k , 𝑿 l ) = N k ​ R ϵ ​ ( 𝑿 k ) + N l ​ R ϵ ​ ( 𝑿 l ) − N k ​ log ⁡ N k N k + N l − N l ​ log ⁡ N l N k + N l . L^{c}(\\bm{X}_{k},\\bm{X}_{l})=N_{k}R_{\\epsilon}(\\bm{X}_{k})+N_{l}R_{\\epsilon}(\\bm{X}_{l})-N_{k}\\log\\frac{N_{k}}{N_{k}+N_{l}}-N_{l}\\log\\frac{N_{l}}{N_{k}+N_{l}}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E33", "title": "L ​ ( 𝑿 k ∪ 𝑿 l ) − L c ​ ( 𝑿 k , 𝑿 l ) L(\\bm{X}_{k}\\cup\\bm{X}_{l})-L^{c}(\\bm{X}_{k},\\bm{X}_{l}) italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSC", "snippet": "L ​ ( 𝑿 k ∪ 𝑿 l ) − L c ​ ( 𝑿 k , 𝑿 l ) L(\\bm{X}_{k}\\cup\\bm{X}_{l})-L^{c}(\\bm{X}_{k},\\bm{X}_{l}) italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E1", "title": "𝒙 ∈ ℝ D → f ​ ( 𝒙 ) 𝒛 ∈ ℝ d , \\bm{x}\\in\\mathbb{R}^{D}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}\\in\\mathbb{R}^{d}, bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POS", "snippet": "𝒙 ∈ ℝ D → f ​ ( 𝒙 ) 𝒛 ∈ ℝ d , \\bm{x}\\in\\mathbb{R}^{D}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}\\in\\mathbb{R}^{d}, bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.Ex1", "title": "f ​ ( 𝒙 , θ ) : 𝒙 ↦ 𝒚 f(\\bm{x},\\theta):\\bm{x}\\mapsto\\bm{y} italic_f ( bold_italic_x , italic_θ ) : bold_italic_x ↦ bold_italic_y", "snippet": "f ​ ( 𝒙 , θ ) : 𝒙 ↦ 𝒚 f(\\bm{x},\\theta):\\bm{x}\\mapsto\\bm{y} italic_f ( bold_italic_x , italic_θ ) : bold_italic_x ↦ bold_italic_y"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E2", "title": "min θ ∈ Θ − 𝔼 ​ [ ⟨ 𝒚 , log ⁡ ( f ​ ( 𝒙 , θ ) ) ⟩ ] ≈ − 1 N ​ ∑ i = 1 N ⟨ 𝒚 i , log ⁡ ( f ​ ( 𝒙 i , θ ) ) ⟩ . \\min_{\\theta\\in\\Theta}\\;-\\mathbb{E}[\\langle\\bm{y},\\log(f(\\bm{x},\\theta))\\rangle]\\,\\approx-", "snippet": "min θ ∈ Θ − 𝔼 ​ [ ⟨ 𝒚 , log ⁡ ( f ​ ( 𝒙 , θ ) ) ⟩ ] ≈ − 1 N ​ ∑ i = 1 N ⟨ 𝒚 i , log ⁡ ( f ​ ( 𝒙 i , θ ) ) ⟩ . \\min_{\\theta\\in\\Theta}\\;-\\mathbb{E}[\\langle\\bm{y},\\log(f(\\bm{x},\\theta))\\rangle]\\,\\approx-\\frac{1}{N}\\sum_{i=1}^{N}\\langle\\bm{y}_{i},\\log\\left(f(\\bm{x}_{i},\\theta)\\right)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E3", "title": "𝒙 → f ​ ( 𝒙 , θ ) 𝒛 → g ​ ( 𝒛 ) 𝒚 . \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z})\\hskip 5.69054pt}\\bm{y}. bold_italic_x start_ARROW s", "snippet": "𝒙 → f ​ ( 𝒙 , θ ) 𝒛 → g ​ ( 𝒛 ) 𝒚 . \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z})\\hskip 5.69054pt}\\bm{y}. bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_θ ) end_OVERACCENT → end_ARROW"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E4", "title": "I ​ ( 𝒙 ; 𝒛 ) = H ​ ( 𝒙 ) − H ​ ( 𝒙 ∣ 𝒛 ) , I(\\bm{x};\\bm{z})=H(\\bm{x})-H(\\bm{x}\\mid\\bm{z}), italic_I ( bold_italic_x ; bold_italic_z ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ∣ bold_it", "snippet": "I ​ ( 𝒙 ; 𝒛 ) = H ​ ( 𝒙 ) − H ​ ( 𝒙 ∣ 𝒛 ) , I(\\bm{x};\\bm{z})=H(\\bm{x})-H(\\bm{x}\\mid\\bm{z}), italic_I ( bold_italic_x ; bold_italic_z ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ∣ bold_italic_z ) , (3.4.4)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E5", "title": "max θ ∈ Θ ⁡ IB ​ ( 𝒙 , 𝒚 , 𝒛 ) ≐ I ​ ( 𝒛 ; 𝒚 ) − β ​ I ​ ( 𝒙 ; 𝒛 ) s . t . 𝒛 = f ​ ( 𝒙 , θ ) , \\max_{\\theta\\in\\Theta}\\;\\mbox{IB}(\\bm{x},\\bm{y},\\bm{z})\\doteq I(\\bm{z};\\bm{y})-\\beta I(\\bm{x};\\bm{z})\\qua", "snippet": "max θ ∈ Θ ⁡ IB ​ ( 𝒙 , 𝒚 , 𝒛 ) ≐ I ​ ( 𝒛 ; 𝒚 ) − β ​ I ​ ( 𝒙 ; 𝒛 ) s . t . 𝒛 = f ​ ( 𝒙 , θ ) , \\max_{\\theta\\in\\Theta}\\;\\mbox{IB}(\\bm{x},\\bm{y},\\bm{z})\\doteq I(\\bm{z};\\bm{y})-\\beta I(\\bm{x};\\bm{z})\\quad\\ \\mathrm{s.t.}\\ \\bm{z}=f(\\bm{x},\\theta), roman_max start_POSTSUBSCRIPT italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E6", "title": "𝒙 → f ​ ( 𝒙 ) 𝒛 , \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}, bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT → end_ARROW bold_italic_z , ", "snippet": "𝒙 → f ​ ( 𝒙 ) 𝒛 , \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}, bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT → end_ARROW bold_italic_z , (3.4.6)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E7", "title": "R ϵ ​ ( 𝒁 ) = 1 2 ​ log ​ det ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ​ 𝒁 ⊤ ) . R_{\\epsilon}(\\bm{Z})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_", "snippet": "R ϵ ​ ( 𝒁 ) = 1 2 ​ log ​ det ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ​ 𝒁 ⊤ ) . R_{\\epsilon}(\\bm{Z})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) = divide start_ARG 1 end_ARG start_ARG 2 e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E9", "title": "R ϵ c ​ ( 𝒁 ) ≐ ∑ k = 1 K R ϵ ​ ( 𝒁 k ) , R_{\\epsilon}^{c}(\\bm{Z})\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{Z}_{k}), italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c e", "snippet": "R ϵ c ​ ( 𝒁 ) ≐ ∑ k = 1 K R ϵ ​ ( 𝒁 k ) , R_{\\epsilon}^{c}(\\bm{Z})\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{Z}_{k}), italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E10", "title": "Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) . \\Delta R_{\\epsilon}(\\bm{Z})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}). roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_", "snippet": "Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) . \\Delta R_{\\epsilon}(\\bm{Z})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}). roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E11", "title": "𝑿 → f ​ ( 𝒙 , θ ) 𝒁 → ϵ Δ ​ R ϵ ​ ( 𝒁 ) . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\epsilon\\hskip 5.69054pt}\\Delta R_{\\epsilon}(\\bm{Z}). b", "snippet": "𝑿 → f ​ ( 𝒙 , θ ) 𝒁 → ϵ Δ ​ R ϵ ​ ( 𝒁 ) . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\epsilon\\hskip 5.69054pt}\\Delta R_{\\epsilon}(\\bm{Z}). bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_θ ) "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12X", "title": "max θ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R", "snippet": "max θ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}), roman_Δ italic_R start_POSTSUBSCRI"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12Xa", "title": "s.t. 𝒁 = f ​ ( 𝑿 , θ ) , ‖ 𝒁 k ‖ F 2 = N k , k = 1 , … , K . \\displaystyle\\ \\ \\,\\bm{Z}=f(\\bm{X},\\theta),\\ \\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. bold_italic_Z = italic_f ( bold_italic_X , italic_", "snippet": "s.t. 𝒁 = f ​ ( 𝑿 , θ ) , ‖ 𝒁 k ‖ F 2 = N k , k = 1 , … , K . \\displaystyle\\ \\ \\,\\bm{Z}=f(\\bm{X},\\theta),\\ \\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. bold_italic_Z = italic_f ( bold_italic_X , italic_θ ) , ∥ bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.Ex1", "title": "[ 𝒙 𝒚 ] ∼ 𝒩 ​ ( [ 𝝁 𝒙 𝝁 𝒚 ] , [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\sim\\mathcal{N}\\left(\\begin{bmatrix}\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{\\mu}_{\\bm{y}}\\end{bmatrix},\\begin", "snippet": "[ 𝒙 𝒚 ] ∼ 𝒩 ​ ( [ 𝝁 𝒙 𝝁 𝒚 ] , [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\sim\\mathcal{N}\\left(\\begin{bmatrix}\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{\\mu}_{\\bm{y}}\\end{bmatrix},\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.Ex2", "title": "𝝁 𝒙 = 𝔼 ​ [ 𝒙 ] , 𝝁 𝒚 = 𝔼 ​ [ 𝒚 ] , [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] = 𝔼 ​ [ [ 𝒙 − 𝔼 ​ [ 𝒙 ] 𝒚 − 𝔼 ​ [ 𝒚 ] ] ​ [ 𝒙 − 𝔼 ​ [ 𝒙 ] 𝒚 − 𝔼 ​ [ 𝒚 ] ] ⊤ ] \\bm{\\mu}_{\\bm{x}}=\\mathbb{E}[\\bm{x}],\\quad\\bm{\\mu}_{\\bm{", "snippet": "𝝁 𝒙 = 𝔼 ​ [ 𝒙 ] , 𝝁 𝒚 = 𝔼 ​ [ 𝒚 ] , [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] = 𝔼 ​ [ [ 𝒙 − 𝔼 ​ [ 𝒙 ] 𝒚 − 𝔼 ​ [ 𝒚 ] ] ​ [ 𝒙 − 𝔼 ​ [ 𝒙 ] 𝒚 − 𝔼 ​ [ 𝒚 ] ] ⊤ ] \\bm{\\mu}_{\\bm{x}}=\\mathbb{E}[\\bm{x}],\\quad\\bm{\\mu}_{\\bm{y}}=\\mathbb{E}[\\bm{y}],\\quad\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E1", "title": "p 𝒙 ∣ 𝒚 ∼ 𝒩 ​ ( 𝝁 𝒙 + 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ ( 𝒚 − 𝝁 𝒚 ) , 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ ) . p_{\\bm{x}\\mid\\bm{y}}\\sim\\mathcal{N}\\left(\\bm{\\mu}_{\\bm{x}}+\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}", "snippet": "p 𝒙 ∣ 𝒚 ∼ 𝒩 ​ ( 𝝁 𝒙 + 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ ( 𝒚 − 𝝁 𝒚 ) , 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ ) . p_{\\bm{x}\\mid\\bm{y}}\\sim\\mathcal{N}\\left(\\bm{\\mu}_{\\bm{x}}+\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}(\\bm{y}-\\bm{\\mu}_{\\bm{y}}),\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E2", "title": "[ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] = [ 𝑰 D 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 𝟎 𝑰 d ] ​ [ 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝟎 𝟎 𝚺 𝒚 ] ​ [ 𝑰 D 𝟎 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝑰 d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_", "snippet": "[ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] = [ 𝑰 D 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 𝟎 𝑰 d ] ​ [ 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝟎 𝟎 𝚺 𝒚 ] ​ [ 𝑰 D 𝟎 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝑰 d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmat"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E3", "title": "[ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] − 1 = [ 𝑰 D 𝟎 − 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝑰 d ] ​ [ ( 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ ) − 1 𝟎 𝟎 𝚺 𝒚 − 1 ] ​ [ 𝑰 D − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 𝟎 𝑰 d ] \\begin{bmatrix}\\bm{\\Sigma}_{\\b", "snippet": "[ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] − 1 = [ 𝑰 D 𝟎 − 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ 𝑰 d ] ​ [ ( 𝚺 𝒙 − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 ​ 𝚺 𝒙 ​ 𝒚 ⊤ ) − 1 𝟎 𝟎 𝚺 𝒚 − 1 ] ​ [ 𝑰 D − 𝚺 𝒙 ​ 𝒚 ​ 𝚺 𝒚 − 1 𝟎 𝑰 d ] \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E6", "title": "( 𝑨 + 𝑼 ​ 𝑪 ​ 𝑽 ) − 1 = 𝑨 − 1 − 𝑨 − 1 ​ 𝑼 ​ ( 𝑪 − 1 + 𝑽 ​ 𝑨 − 1 ​ 𝑼 ) − 1 ​ 𝑽 ​ 𝑨 − 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{", "snippet": "( 𝑨 + 𝑼 ​ 𝑪 ​ 𝑽 ) − 1 = 𝑨 − 1 − 𝑨 − 1 ​ 𝑼 ​ ( 𝑪 − 1 + 𝑽 ​ 𝑨 − 1 ​ 𝑼 ) − 1 ​ 𝑽 ​ 𝑨 − 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1} ( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E7", "title": "α t = 1 − t , σ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t .", "snippet": "α t = 1 − t , σ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t . (3.6.7)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx13", "title": "𝖪𝖫 ⁡ ( p ∥ q ) \\displaystyle\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) sansserif_KL ( italic_p ∥ italic_q ) ≐ \\displaystyle\\doteq ≐ − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ q ​ ( 𝝃 ) ​ d 𝝃 − ( − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ", "snippet": "𝖪𝖫 ⁡ ( p ∥ q ) \\displaystyle\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) sansserif_KL ( italic_p ∥ italic_q ) ≐ \\displaystyle\\doteq ≐ − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ q ​ ( 𝝃 ) ​ d 𝝃 − ( − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ​ ( 𝝃 ) ​ d 𝝃 ) \\displaystyle-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\m"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx14", "title": "− 𝖪𝖫 ⁡ ( p ∥ q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ∥ italic_q ) = \\displaystyle= = − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ​ ( 𝝃 ) q ​ ( 𝝃 ) ​ d ​ 𝝃 = ∫ ℝ D p ​ ( 𝝃 ) ​ ", "snippet": "− 𝖪𝖫 ⁡ ( p ∥ q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ∥ italic_q ) = \\displaystyle= = − ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ p ​ ( 𝝃 ) q ​ ( 𝝃 ) ​ d ​ 𝝃 = ∫ ℝ D p ​ ( 𝝃 ) ​ log ⁡ q ​ ( 𝝃 ) p ​ ( 𝝃 ) ​ d ​ 𝝃 \\displaystyle-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx15", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t}) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT ital", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t}) over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = 𝔼 ⁡ [ 𝒙 ∣ 𝒙 t ] \\displaystyle=\\operatorname{\\mathbb{E"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx16", "title": "ℙ ⁡ [ y = k ∣ 𝒙 t ] \\displaystyle\\operatorname{\\mathbb{P}}[y=k\\mid\\bm{x}_{t}] blackboard_P [ italic_y = italic_k ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = p t ∣ y ​ ( 𝒙 t ∣ k ", "snippet": "ℙ ⁡ [ y = k ∣ 𝒙 t ] \\displaystyle\\operatorname{\\mathbb{P}}[y=k\\mid\\bm{x}_{t}] blackboard_P [ italic_y = italic_k ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = p t ∣ y ​ ( 𝒙 t ∣ k ) ​ π k p t ​ ( 𝒙 t ) \\displaystyle=\\frac{p_{t\\mid y}(\\bm{x}_{t}\\mid k)\\pi_{k}}{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx17", "title": "∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) \\displaystyle\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}) ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p", "snippet": "∇ 𝒙 t log ⁡ p t ​ ( 𝒙 t ) \\displaystyle\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}) ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx18", "title": "𝒙 ^ t ℓ − 1 \\displaystyle\\hat{\\bm{x}}_{t_{\\ell-1}} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 𝔼 ⁡ [ 𝒙 t ℓ ", "snippet": "𝒙 ^ t ℓ − 1 \\displaystyle\\hat{\\bm{x}}_{t_{\\ell-1}} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 𝔼 ⁡ [ 𝒙 t ℓ − 1 ∣ 𝒙 t ℓ = 𝒙 ^ t ℓ ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\bm{x}_{t_{\\ell-"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx19", "title": "( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 \\displaystyle(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1} ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_", "snippet": "( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 \\displaystyle(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1} ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_t start_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx20", "title": "φ ​ ( 𝒙 t ; 𝟎 , 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) ∑ i = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 i ​ 𝑼 i ⊤ + t 2 ​ 𝑰 ) \\displaystyle\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\varphi(\\", "snippet": "φ ​ ( 𝒙 t ; 𝟎 , 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) ∑ i = 1 K φ ​ ( 𝒙 t ; 𝟎 , 𝑼 i ​ 𝑼 i ⊤ + t 2 ​ 𝑰 ) \\displaystyle\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}+t^{2}\\bm{I})} divide start_ARG ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx21", "title": "𝑼 k ​ 𝑼 k ⊤ ​ ( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 ​ 𝒙 t \\displaystyle\\bm{U}_{k}\\bm{U}_{k}^{\\top}(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1}\\bm{x}_{t} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSU", "snippet": "𝑼 k ​ 𝑼 k ⊤ ​ ( 𝑼 k ​ 𝑼 k ⊤ + t 2 ​ 𝑰 ) − 1 ​ 𝒙 t \\displaystyle\\bm{U}_{k}\\bm{U}_{k}^{\\top}(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1}\\bm{x}_{t} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx22", "title": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ 𝑼 ​ 𝑼 ⊤ ​ 𝒙 t . \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{U}\\bm{U}^{\\top}\\bm{x}_{t}. over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSC", "snippet": "𝒙 ¯ ∗ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ 𝑼 ​ 𝑼 ⊤ ​ 𝒙 t . \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{U}\\bm{U}^{\\top}\\bm{x}_{t}. over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx23", "title": "𝒙 ¯ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 t , \\displaystyle\\bar{\\bm{x}}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}, over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x", "snippet": "𝒙 ¯ ​ ( t , 𝒙 t ) = 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 t , \\displaystyle\\bar{\\bm{x}}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}, over¯ start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx24", "title": "𝔼 𝒈 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 \\displaystyle\\operatorname{\\mathbb{E}}_{\\bm{g}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2} blackboard_E sta", "snippet": "𝔼 𝒈 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 \\displaystyle\\operatorname{\\mathbb{E}}_{\\bm{g}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2} blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ∥ bold_italic_x - divide start_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx25", "title": "min 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 = 𝔼 𝒙 ⁡ ‖ 𝒙 ‖ 2 2 + ( 1 ( 1 + t 2 ) 2 − 2 1 + t 2 ) ​ 𝔼 𝒙 ⁡ ‖ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 . \\displaystyle\\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatornam", "snippet": "min 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 ⁡ ‖ 𝒙 − 1 1 + t 2 ​ 𝑽 ​ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 = 𝔼 𝒙 ⁡ ‖ 𝒙 ‖ 2 2 + ( 1 ( 1 + t 2 ) 2 − 2 1 + t 2 ) ​ 𝔼 𝒙 ⁡ ‖ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 . \\displaystyle\\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}\\r"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx26", "title": "max 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 ⁡ ‖ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 , \\displaystyle\\max_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{V}^{\\top}\\bm{x}\\|_{2}^{2}, roman_max start_POSTSUBSCRIPT bold_italic", "snippet": "max 𝑽 ∈ 𝖮 ​ ( D , P ) ⁡ 𝔼 𝒙 ⁡ ‖ 𝑽 ⊤ ​ 𝒙 ‖ 2 2 , \\displaystyle\\max_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{V}^{\\top}\\bm{x}\\|_{2}^{2}, roman_max start_POSTSUBSCRIPT bold_italic_V ∈ sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx27", "title": "ℛ ϵ ​ ( 𝒙 ) \\displaystyle\\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ − ∫ K 1 vol ⁡ ( K ) ​ log 2 ⁡ 1 vol ⁡ ( K ) ​ d ​ 𝝃 − log 2 ⁡ v", "snippet": "ℛ ϵ ​ ( 𝒙 ) \\displaystyle\\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_x ) ≥ − ∫ K 1 vol ⁡ ( K ) ​ log 2 ⁡ 1 vol ⁡ ( K ) ​ d ​ 𝝃 − log 2 ⁡ vol ⁡ ( B ϵ ) − C d \\displaystyle\\geq-\\int_{K}\\frac{1}{\\operatorname{vol}(K)}\\log"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx28", "title": "h ​ ( 𝒙 , 𝒈 ) \\displaystyle h(\\bm{x},\\bm{g}) italic_h ( bold_italic_x , bold_italic_g ) = − ∫ p ​ ( 𝝃 ) ​ p ​ ( 𝜸 ) ​ log 2 ⁡ p ​ ( 𝝃 ) ​ p ​ ( 𝜸 ) ​ d 𝝃 ​ d 𝜸 \\displaystyle=-\\int p(\\bm{\\xi})p(\\bm{\\ga", "snippet": "h ​ ( 𝒙 , 𝒈 ) \\displaystyle h(\\bm{x},\\bm{g}) italic_h ( bold_italic_x , bold_italic_g ) = − ∫ p ​ ( 𝝃 ) ​ p ​ ( 𝜸 ) ​ log 2 ⁡ p ​ ( 𝝃 ) ​ p ​ ( 𝜸 ) ​ d 𝝃 ​ d 𝜸 \\displaystyle=-\\int p(\\bm{\\xi})p(\\bm{\\gamma})\\log_{2}p(\\bm{\\xi})p(\\bm{\\gamma})\\mathrm{d}\\bm{\\xi}\\mathrm{d}\\bm{\\gamma} = "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx29", "title": "R ϵ ​ ( 𝒁 k ) = N k 2 ​ N ​ log ​ det ( 𝑰 + d N k ​ ϵ 2 ​ 𝒁 k ​ 𝒁 k ⊤ ) \\displaystyle R_{\\epsilon}(\\bm{Z}_{k})=\\frac{N_{k}}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{N_{k}\\epsilon^{2}}\\bm{Z}_{k}\\bm{Z}_{k}^{\\to", "snippet": "R ϵ ​ ( 𝒁 k ) = N k 2 ​ N ​ log ​ det ( 𝑰 + d N k ​ ϵ 2 ​ 𝒁 k ​ 𝒁 k ⊤ ) \\displaystyle R_{\\epsilon}(\\bm{Z}_{k})=\\frac{N_{k}}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{N_{k}\\epsilon^{2}}\\bm{Z}_{k}\\bm{Z}_{k}^{\\top}\\right) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12", "title": "max θ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R", "snippet": "max θ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}), roman_Δ italic_R start_POSTSUBSCRI"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx30", "title": "R ϵ c ​ ( 𝒁 ∣ 𝚷 ) ≐ ∑ k = 1 K tr ​ ( 𝚷 k ) 2 ​ N ​ log ​ det ( 𝑰 + d tr ​ ( 𝚷 k ) ​ ϵ 2 ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) . \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{\\mathrm{tr}(\\bm{", "snippet": "R ϵ c ​ ( 𝒁 ∣ 𝚷 ) ≐ ∑ k = 1 K tr ​ ( 𝚷 k ) 2 ​ N ​ log ​ det ( 𝑰 + d tr ​ ( 𝚷 k ) ​ ϵ 2 ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) . \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{\\mathrm{tr}(\\bm{\\Pi}_{k})}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsilon^{2"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx31", "title": "max 𝚷 , θ \\displaystyle\\max_{\\bm{\\Pi},\\theta} roman_max start_POSTSUBSCRIPT bold_Π , italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝚷 ) \\displaystyle\\ \\Delta R_{\\epsilon}", "snippet": "max 𝚷 , θ \\displaystyle\\max_{\\bm{\\Pi},\\theta} roman_max start_POSTSUBSCRIPT bold_Π , italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝚷 ) \\displaystyle\\ \\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\mid\\bm{\\Pi})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}\\mi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx32", "title": "max 𝒁 R ϵ ( 𝒁 ) − R ϵ c ( 𝒁 ) s . t . ∥ 𝒁 k ∥ F 2 = N k , k = 1 , … , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},", "snippet": "max 𝒁 R ϵ ( 𝒁 ) − R ϵ c ( 𝒁 ) s . t . ∥ 𝒁 k ∥ F 2 = N k , k = 1 , … , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx33", "title": "max 𝒁 ⁡ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) − λ 2 ​ ‖ 𝒁 ‖ F 2 , \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})-\\frac{\\lambda}{2}\\|\\bm{Z}\\|_{F}^{2}, roman_max start_POSTSUBSCRIPT bol", "snippet": "max 𝒁 ⁡ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ) − λ 2 ​ ‖ 𝒁 ‖ F 2 , \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})-\\frac{\\lambda}{2}\\|\\bm{Z}\\|_{F}^{2}, roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx34", "title": "λ ∈ ( 0 , d ​ ( N / N max − 1 ) N ​ ( N / N max + 1 ) ​ ϵ 2 ] , \\displaystyle\\lambda\\in\\left(0,\\frac{d(\\sqrt{N/N_{\\max}}-1)}{N(\\sqrt{N/N_{\\max}}+1)\\epsilon^{2}}\\right], italic_λ ∈ ( 0 , divide start_A", "snippet": "λ ∈ ( 0 , d ​ ( N / N max − 1 ) N ​ ( N / N max + 1 ) ​ ϵ 2 ] , \\displaystyle\\lambda\\in\\left(0,\\frac{d(\\sqrt{N/N_{\\max}}-1)}{N(\\sqrt{N/N_{\\max}}+1)\\epsilon^{2}}\\right], italic_λ ∈ ( 0 , divide start_ARG italic_d ( square-root start_ARG italic_N / italic_N start_POSTSUBSCRIPT roma"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx35", "title": "𝒁 k ∗ = ( η k + η k 2 − 4 ​ λ 2 ​ N / N k 2 ​ λ ​ α k ) 1 / 2 ​ 𝑼 k ​ 𝑽 k ⊤ , \\displaystyle\\bm{Z}_{k}^{*}=\\left(\\frac{\\eta_{k}+\\sqrt{\\eta_{k}^{2}-4\\lambda^{2}N/N_{k}}}{2\\lambda\\alpha_{k}}\\right)^{1/2}", "snippet": "𝒁 k ∗ = ( η k + η k 2 − 4 ​ λ 2 ​ N / N k 2 ​ λ ​ α k ) 1 / 2 ​ 𝑼 k ​ 𝑽 k ⊤ , \\displaystyle\\bm{Z}_{k}^{*}=\\left(\\frac{\\eta_{k}+\\sqrt{\\eta_{k}^{2}-4\\lambda^{2}N/N_{k}}}{2\\lambda\\alpha_{k}}\\right)^{1/2}\\bm{U}_{k}\\bm{V}_{k}^{\\top}, bold_italic_Z start_POSTSUBSCRIPT italic_k end_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx36", "title": "[ 𝒙 − 𝝁 𝒙 𝒚 − 𝝁 𝒚 ] ⊤ ​ [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] − 1 ​ [ 𝒙 − 𝝁 𝒙 𝒚 − 𝝁 𝒚 ] \\displaystyle\\begin{bmatrix}\\bm{x}-\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{y}-\\bm{\\mu}_{\\bm{y}}\\end{bmatrix}^{\\top}\\begin{bmatrix}\\bm{\\S", "snippet": "[ 𝒙 − 𝝁 𝒙 𝒚 − 𝝁 𝒚 ] ⊤ ​ [ 𝚺 𝒙 𝚺 𝒙 ​ 𝒚 𝚺 𝒙 ​ 𝒚 ⊤ 𝚺 𝒚 ] − 1 ​ [ 𝒙 − 𝝁 𝒙 𝒚 − 𝝁 𝒚 ] \\displaystyle\\begin{bmatrix}\\bm{x}-\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{y}-\\bm{\\mu}_{\\bm{y}}\\end{bmatrix}^{\\top}\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx37", "title": "f ​ ( 𝑿 ) = log ​ det ( 𝑿 ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X )", "snippet": "f ​ ( 𝑿 ) = log ​ det ( 𝑿 ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx38", "title": "log ​ det ( 𝑰 + 𝑿 ⊤ ​ 𝑿 ) = log ​ det ( 𝑰 + 𝑿 ​ 𝑿 ⊤ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_italic_X start_P", "snippet": "log ​ det ( 𝑰 + 𝑿 ⊤ ​ 𝑿 ) = log ​ det ( 𝑰 + 𝑿 ​ 𝑿 ⊤ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) = roman_log roman_det ( bol"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx39", "title": "log ​ det ( 𝑨 ) = ∑ i = 1 n log ⁡ ( λ i ) , \\displaystyle\\log\\det\\left(\\bm{A}\\right)=\\sum_{i=1}^{n}\\log(\\lambda_{i}), roman_log roman_det ( bold_italic_A ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POS", "snippet": "log ​ det ( 𝑨 ) = ∑ i = 1 n log ⁡ ( λ i ) , \\displaystyle\\log\\det\\left(\\bm{A}\\right)=\\sum_{i=1}^{n}\\log(\\lambda_{i}), roman_log roman_det ( bold_italic_A ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log ( italic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#top", "title": "Chapter 4 Deep Representations from Unrolled Optimization", "snippet": ""}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1", "title": "4.1 White-Box Deep Networks via Unrolled Optimization", "snippet": "4.1 White-Box Deep Networks via Unrolled Optimization Now, if we agree that maximizing the rate reduction or information gain leads to the desired representation as discussed in Section 3.4 , the remaining question is how to construct and learn a (nonlinear) mapping from the data"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2", "title": "4.2 White-Box Transformers from Unrolled Optimization", "snippet": "4.2 White-Box Transformers from Unrolled Optimization As we have seen in the previous section, we use the problem of classification to provide a rigorous interpretation for main architectural characteristics of popular deep networks such as the ResNet and the CNN: each layer of s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3", "title": "4.3 Variants of Deep Architectures by Design", "snippet": "4.3 Variants of Deep Architectures by Design So far, we wish that we have provided compelling evidence that the role of (popular) deep networks is to realize certain optimization algorithms for minimizing the coding rate (or maximizing the information gain) of the learned represe"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S4", "title": "4.4 Summary and Notes", "snippet": "4.4 Summary and Notes The materials presented in this chapter are based on a series of recent works on this topic, including [ CYY+22 , WLP+24 , WLY+25 , WDL+25 , YBP+23 ] . These contributions encompass both theoretical advances and practical methodologies for constructing inter"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S5", "title": "4.5 Exercises and Extensions", "snippet": "4.5 Exercises and Extensions Exercise 4.1 . Let 𝒁 = [ 𝒁 1 , … , 𝒁 K ] ∈ ℝ d × m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1", "title": "4.1.1 Deep Networks from Unrolled Gradient Descent", "snippet": "4.1.1 Deep Networks from Unrolled Gradient Descent In the previous chapter, we presented the rate reduction objective ( 3.4.12 ) as a principled objective for learning linear discriminative representations of the data. We have, however, not specified the architecture of the featu"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2", "title": "4.1.2 Convolutional Networks from Invariant Rate Reduction", "snippet": "4.1.2 Convolutional Networks from Invariant Rate Reduction In the previous section, we derived the layer-wise architecture of a deep network, the ReduNet, using unrolled optimization for the rate reduction objective. Specifically, the compression term R ϵ c ​ ( 𝒁 ∣ 𝚷 ) R^{c}_{\\ep"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1", "title": "4.2.1 Unrolled Optimization for Sparse Rate Reduction", "snippet": "4.2.1 Unrolled Optimization for Sparse Rate Reduction We consider a general learning setup associated with real-world signals. Let 𝑿 = [ 𝒙 1 , … , 𝒙 N ] ∈ ℝ D × N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS2", "title": "4.2.2 Overall White-Box Transformer Architecture: CRATE", "snippet": "4.2.2 Overall White-Box Transformer Architecture: CRATE We now design a white-box transformer architecture, named the Coding RATE Transformer ( crate ), by unrolling the above updates. By combining the above two steps ( 4.2.14 ) and ( 4.2.18 ): 1. Local compression of tokens with"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1", "title": "4.3.1 Attention-Only Transformer Architecture", "snippet": "4.3.1 Attention-Only Transformer Architecture In this subsection, we propose a minimalistic transformer architecture consisting of interpretable layers based on the MSSA operator. To derive a fully interpretable transformer architecture with only necessary components, we contend "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2", "title": "4.3.2 Linear-Time Attention: Token Statistics Transformer", "snippet": "4.3.2 Linear-Time Attention: Token Statistics Transformer In this subsection, we propose a new transformer attention operator whose computational complexity scales linearly with the number of tokens based on the coding rate reduction objective. Specifically, we derive a novel var"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1", "title": "Gradient Ascent for Coding Rate Reduction.", "snippet": "Gradient Ascent for Coding Rate Reduction. From the previous chapter, we see that to seek a linear discriminative representation (LDR), mathematically, we are essentially seeking a continuous mapping f ​ ( ⋅ ) : 𝒙 ↦ 𝒛 f(\\cdot):\\bm{x}\\mapsto\\bm{z} italic_f ( ⋅ ) : bold_italic_x ↦ "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2", "title": "Gradient-Guided Feature Map Increment.", "snippet": "Gradient-Guided Feature Map Increment. Notice that in the above, the gradient ascent considers all the features 𝒁 ℓ = [ 𝒛 1 ℓ , … , 𝒛 N ℓ ] \\bm{Z}^{\\ell}=[\\bm{z}^{\\ell}_{1},\\dots,\\bm{z}^{\\ell}_{N}] bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = [ bold_italic_z "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px3", "title": "Deep Network for Optimizing Rate Reduction.", "snippet": "Deep Network for Optimizing Rate Reduction. Notice that the increment is constructed to emulate the gradient ascent for the rate reduction Δ ​ R ϵ \\Delta R_{\\epsilon} roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT . Hence by transforming the features iteratively "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px1", "title": "1D Serial Data and Shift Invariance", "snippet": "1D Serial Data and Shift Invariance To classify one-dimensional data 𝒙 = [ x ​ ( 0 ) , x ​ ( 1 ) , … , x ​ ( D − 1 ) ] ∈ ℝ D \\bm{x}=[x(0),x(1),\\ldots,x(D-1)]\\in\\mathbb{R}^{D} bold_italic_x = [ italic_x ( 0 ) , italic_x ( 1 ) , … , italic_x ( italic_D - 1 ) ] ∈ blackboard_R start_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2", "title": "A Fundamental Trade-off between Invariance and Sparsity.", "snippet": "A Fundamental Trade-off between Invariance and Sparsity. There is one problem though: In general, the set of all circular permutations of a vector 𝒛 \\bm{z} bold_italic_z gives a full-rank matrix. That is, the d d italic_d “augmented” features associated with each sample (hence ea"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px3", "title": "Overall Network Architecture and Comparison.", "snippet": "Overall Network Architecture and Comparison. Following the above derivation, we see that in order to find a linear discriminative representation (LDR) for multiple classes of signals/images that is invariant to translation, sparse coding, a multi-layer architecture with multi-cha"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px1", "title": "Objective for Learning a Structured and Compact Representation.", "snippet": "Objective for Learning a Structured and Compact Representation. Following the framework of rate reduction Section 4.1 , we contend that the goal of representation learning is to find a feature mapping f : 𝑿 ∈ ℝ D × N → 𝒁 ∈ ℝ d × N f\\colon\\bm{X}\\in\\mathbb{R}^{D\\times N}\\to\\bm{Z}\\i"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px2", "title": "Sparse Rate Reduction.", "snippet": "Sparse Rate Reduction. Note that the rate reduction objective ( 4.2.1 ) is invariant to arbitrary joint rotations of the representations and subspaces. In particular, optimizing the rate reduction objective may not naturally lead to axis-aligned (i.e., sparse ) representations. F"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px3", "title": "White-Box Network Architecture via Unrolled Optimization.", "snippet": "White-Box Network Architecture via Unrolled Optimization. Although easy to state, each term in the above objective is computationally challenging to optimize [ WM22 ] . Hence it is natural to adopt an approximation approach that realizes the global transformation f f italic_f to "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px4", "title": "Self-Attention as Gradient Descent on Coding Rate of Token Representations.", "snippet": "Self-Attention as Gradient Descent on Coding Rate of Token Representations. For the first step ( 4.2.7 ), the gradient of the coding rate ∇ 𝒁 R ϵ c \\nabla_{\\bm{Z}}R^{c}_{\\epsilon} ∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_PO"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px5", "title": "MLP as Proximal Gradient Descent for Sparse Coding of Token Representations.", "snippet": "MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. For the second step of alternating minimization, we need to minimize λ ​ ‖ 𝒁 ‖ 1 − R ϵ ​ ( 𝒁 ) \\lambda\\|\\bm{Z}\\|_{1}-R_{\\epsilon}(\\bm{Z}) italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1", "title": "Denoising Operator for Token Representations.", "snippet": "Denoising Operator for Token Representations. Now, we show that the MSSA operator (see ( 4.2.13 )) can incrementally denoise token representations generated from the above model. Spefically, we consider for each ℓ = 1 , … , L \\ell=1,\\dots,L roman_ℓ = 1 , … , italic_L , 𝒁 ( ℓ + 1 "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px2", "title": "Attention-Only Transformer.", "snippet": "Attention-Only Transformer. Now, we formally propose an attention-only transformer architecture. Specifically, by unrolling the iterative optimization steps ( 4.3.2 ) as layers of a deep network, we construct a transformer architecture in Figure 4.17 . Each layer of the proposed "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1", "title": "A New Variational Form for Coding Rates.", "snippet": "A New Variational Form for Coding Rates. To begin, we consider a general form of MCR 2 -like objectives based on concave functions of the spectrum of a matrix. Namely, for a given PSD matrix 𝑴 ∈ 𝖯𝖲𝖣 ​ ( d ) \\bm{M}\\in\\mathsf{PSD}(d) bold_italic_M ∈ sansserif_PSD ( italic_d ) and a"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px2", "title": "Model interpretation.", "snippet": "Model interpretation. Given the proposed attention operator in ( 4.3.11 ), first recall that the rows of 𝚷 \\bm{\\Pi} bold_Π are non-negative and sum to 1 , so our operator takes a weighted average of K K italic_K “attention head”-esque operators and then adds a residual connection"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px3", "title": "Practical Implementation Details.", "snippet": "Practical Implementation Details. Having introduced our proposed attention operator, we now discuss further practical considerations. First, until this point in the presentation, we have avoided discussion of how tokens are “grouped” into various attention heads via the 𝚷 \\bm{\\Pi"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark1", "title": "Remark 4.1 (Interpretation of 𝑬 ℓ \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT and 𝑪 j ℓ \\bm{C}_{j}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT", "snippet": "Remark 4.1 (Interpretation of 𝑬 ℓ \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT and 𝑪 j ℓ \\bm{C}_{j}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT as linear operators) . For an"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample1", "title": "Example 4.1 .", "snippet": "Example 4.1 . To provide some intuition on how ReduNet transforms the features, we provide a simple example with mixed 3D Gaussians and visualize how the features are transformed in Figure 4.5 . Consider a mixture of three Gaussian distributions in ℝ 3 \\mathbb{R}^{3} blackboard_R"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition1", "title": "Proposition 4.1 (Convolution structures of 𝑬 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and 𝑪 k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT s", "snippet": "Proposition 4.1 (Convolution structures of 𝑬 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and 𝑪 k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . The matrix 𝑬 1 = α ​ ( 𝑰 + α ​"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition2", "title": "Proposition 4.2 (Multi-channel convolution structures of 𝑬 ¯ \\bar{\\bm{E}} over¯ start_ARG bold_italic_E end_ARG and 𝑪 ¯ k \\bar{\\bm{C}}_{k} over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT ita", "snippet": "Proposition 4.2 (Multi-channel convolution structures of 𝑬 ¯ \\bar{\\bm{E}} over¯ start_ARG bold_italic_E end_ARG and 𝑪 ¯ k \\bar{\\bm{C}}_{k} over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . The matrix 𝑬 ¯ ≐ α ​ ( 𝑰 + α ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 ¯ ) ​ 𝖼𝗂𝗋𝖼"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark2", "title": "Remark 4.2 (Reducing Computational Complexity in the Frequency Domain) .", "snippet": "Remark 4.2 (Reducing Computational Complexity in the Frequency Domain) . The calculation of 𝑬 ¯ \\bar{\\bm{E}} over¯ start_ARG bold_italic_E end_ARG in ( 4.1.27 ) requires inverting a matrix of size d ​ C × d ​ C dC\\times dC italic_d italic_C × italic_d italic_C , which in general "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample2", "title": "Example 4.2 (Invariant Classification of Digits) .", "snippet": "Example 4.2 (Invariant Classification of Digits) . We next provide an empirical performance of the ReduNet on learning rotation invariant features on the real 10-class MNIST dataset. We impose a polar grid on the image 𝒙 ∈ ℝ H × W \\bm{x}\\in\\mathbb{R}^{H\\times W} bold_italic_x ∈ b"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark3", "title": "Remark 4.3 .", "snippet": "Remark 4.3 . In transformers, each input sample is typically converted into a sequence of tokens . A token is a basic unit of information derived from the raw input: in natural language processing, tokens are typically words or subwords; in computer vision, they correspond to ima"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark4", "title": "Remark 4.4 .", "snippet": "Remark 4.4 . The expression ( 4.2.3 ) for the coding rate can be viewed as a generalization of the coding rate R ϵ c R_{\\epsilon}^{c} italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT used in the original rate reduction obj"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark5", "title": "Remark 4.5 .", "snippet": "Remark 4.5 . In contrast to other unrolled optimization approaches such as the ReduNet (see Section 4.1 ), we explicitly model the distribution of 𝒁 ℓ \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT at each layer, say as a mixture of linear subspaces"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark6", "title": "Remark 4.6 .", "snippet": "Remark 4.6 . The SSA operator in ( 4.2.12 ) resembles the attention operator in a typical transformer [ VSP+17 ] , except that here the linear operators of value, key, and query are all set to be the same as the subspace basis, i.e., 𝑽 k = 𝑲 k = 𝑸 k = 𝑼 k ∗ \\bm{V}_{k}=\\bm{K}_{k}="}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark7", "title": "Remark 4.7 ( The roles of the forward pass and backward propagation ) .", "snippet": "Remark 4.7 ( The roles of the forward pass and backward propagation ) . In contrast to other unrolled optimization approaches such as the ReduNet [ CYY+22 ] , we explicitly model the distribution of each 𝒁 ℓ \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmdefinition1", "title": "Definition 4.1 .", "snippet": "Definition 4.1 . Let C 1 , … , C K C_{1},\\dots,C_{K} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_C start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT be a partition of the index set [ N ] [N] [ italic_N ] and 𝑼 k ∈ 𝒪 d × p k \\bm{U}_{k}\\in\\mathcal{O}^{d\\times p_{k}} "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark8", "title": "Remark 4.8 .", "snippet": "Remark 4.8 . The linear representation hypothesis posits that token representations in LLMs lie in low-dimensional linear subspaces that encode semantic features. Similarly, the superposition hypothesis suggests that these representations can be approximately expressed as a spars"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem1", "title": "Theorem 4.1 .", "snippet": "Theorem 4.1 . Let 𝐙 ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT be defined in Definition 4.1 and φ ​ ( ⋅ ) \\varphi(\\cdot) italic_φ ( ⋅ ) in Eq. ( 4.3.2 ) be φ ​ ( 𝐱 ) = h ​ ( σ ​ ( 𝐱 ) ) \\varphi(\\bm{x})=h\\left(\\sigma(\\bm{x})\\right) italic_φ ( "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark9", "title": "Remark 4.9 .", "snippet": "Remark 4.9 . Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. However, we should point out that in real-world applications, where token representations exhibit more complicated structu"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem2", "title": "Theorem 4.2 .", "snippet": "Theorem 4.2 . Let f : [ 0 , ∞ ) → ℝ f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ∞ ) → blackboard_R be non-decreasing, concave, and obey f ​ ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : 𝖯𝖲𝖣 ​ ( d ) → ℝ F\\colon\\mathsf{PSD}(d)\\to\\mathbb{R} italic_F : sansserif_PSD ( italic_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmcorollary1", "title": "Corollary 4.1 .", "snippet": "Corollary 4.1 . Let f : [ 0 , ∞ ) → ℝ f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ∞ ) → blackboard_R be non-decreasing, concave, and obey f ​ ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : 𝖯𝖲𝖣 ​ ( p ) → ℝ F\\colon\\mathsf{PSD}(p)\\to\\mathbb{R} italic_F : sansserif_PSD ( itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise1", "title": "Exercise 4.1 .", "snippet": "Exercise 4.1 . Let 𝒁 = [ 𝒁 1 , … , 𝒁 K ] ∈ ℝ d × m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise2", "title": "Exercise 4.2 (Neumann series for matrix inverse) .", "snippet": "Exercise 4.2 (Neumann series for matrix inverse) . Let 𝑨 ∈ ℝ n × n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT . If ‖ 𝑨 ‖ < 1 \\|\\bm{A}\\|<1 ∥ bold_italic_A ∥ < 1 , please show ( 𝑰 − 𝑨 ) − 1 = ∑ k = 1 ∞ "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise3", "title": "Exercise 4.3 .", "snippet": "Exercise 4.3 . Please compute the gradients in ( 4.3.9 ) and ( 4.3.10 )."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise4", "title": "Exercise 4.4 .", "snippet": "Exercise 4.4 . Please show Corollary 4.1 when K ​ p ≤ d Kp\\leq d italic_K italic_p ≤ italic_d ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S0.E1", "title": "f ​ ( ⋅ , 𝜽 ) : 𝒙 → f 0 𝒛 0 → ⋯ → 𝒛 ℓ → f ℓ 𝒛 ℓ + 1 → ⋯ → 𝒛 L = 𝒛 . f(\\cdot,\\bm{\\theta})\\colon\\bm{x}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{z}^{\\el", "snippet": "f ​ ( ⋅ , 𝜽 ) : 𝒙 → f 0 𝒛 0 → ⋯ → 𝒛 ℓ → f ℓ 𝒛 ℓ + 1 → ⋯ → 𝒛 L = 𝒛 . f(\\cdot,\\bm{\\theta})\\colon\\bm{x}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{z}^{\\ell+1}\\rightarr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E1", "title": "Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ⏟ R ϵ ​ ( 𝒁 ) − ∑ k = 1 K γ k 2 ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) ⏟ R ϵ c ​ ( 𝒁 ∣ 𝚷 ) , \\begin{split}\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\", "snippet": "Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ⏟ R ϵ ​ ( 𝒁 ) − ∑ k = 1 K γ k 2 ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) ⏟ R ϵ c ​ ( 𝒁 ∣ 𝚷 ) , \\begin{split}\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\underbrace{\\frac{1}{2}\\log\\det\\Big{(}\\bm{I}+{\\alpha}\\bm{Z}\\bm{Z}^{\\to"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E2", "title": "𝒁 ℓ + 1 ∝ 𝒁 ℓ + η ⋅ ∂ Δ ​ R ϵ ∂ 𝒁 ​ ( 𝒁 ℓ ) s.t. 𝒁 ℓ + 1 ⊆ 𝕊 d − 1 , ℓ = 1 , 2 , … , \\bm{Z}^{\\ell+1}\\;\\propto\\;\\bm{Z}^{\\ell}+\\eta\\cdot\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})\\", "snippet": "𝒁 ℓ + 1 ∝ 𝒁 ℓ + η ⋅ ∂ Δ ​ R ϵ ∂ 𝒁 ​ ( 𝒁 ℓ ) s.t. 𝒁 ℓ + 1 ⊆ 𝕊 d − 1 , ℓ = 1 , 2 , … , \\bm{Z}^{\\ell+1}\\;\\propto\\;\\bm{Z}^{\\ell}+\\eta\\cdot\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})\\quad\\mbox{s.t.}\\quad\\bm{Z}^{\\ell+1}\\subseteq\\mathbb{S}^{d-1},\\quad\\ell=1,2,\\ldot"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E3", "title": "1 2 ​ ∂ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ∂ 𝒁 ​ ( 𝒁 ℓ ) = α ​ ( 𝑰 + α ​ 𝒁 ℓ ​ ( 𝒁 ℓ ) ⊤ ) − 1 ⏟ 𝑬 ℓ ∈ ℝ d × d ​ 𝒁 ℓ , \\frac{1}{2}\\frac{\\partial\\log\\det(\\bm{I}\\!+\\!\\alpha\\bm{Z}\\bm{Z}^{\\top})}{\\partial\\bm{Z", "snippet": "1 2 ​ ∂ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ∂ 𝒁 ​ ( 𝒁 ℓ ) = α ​ ( 𝑰 + α ​ 𝒁 ℓ ​ ( 𝒁 ℓ ) ⊤ ) − 1 ⏟ 𝑬 ℓ ∈ ℝ d × d ​ 𝒁 ℓ , \\frac{1}{2}\\frac{\\partial\\log\\det(\\bm{I}\\!+\\!\\alpha\\bm{Z}\\bm{Z}^{\\top})}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\alpha(\\bm{I}\\!+\\!\\alpha\\bm{Z}^{\\ell}(\\bm{Z}^{\\ell"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E4", "title": "1 2 ​ ∂ ( γ k ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) ) ∂ 𝒁 ​ ( 𝒁 ℓ ) = γ k ​ α k ​ ( 𝑰 + α k ​ 𝒁 ℓ ​ 𝚷 k ​ ( 𝒁 ℓ ) ⊤ ) − 1 ⏟ 𝑪 k ℓ ∈ ℝ d × d ​ 𝒁 ℓ ​ 𝚷 k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\lo", "snippet": "1 2 ​ ∂ ( γ k ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 k ​ 𝒁 ⊤ ) ) ∂ 𝒁 ​ ( 𝒁 ℓ ) = γ k ​ α k ​ ( 𝑰 + α k ​ 𝒁 ℓ ​ 𝚷 k ​ ( 𝒁 ℓ ) ⊤ ) − 1 ⏟ 𝑪 k ℓ ∈ ℝ d × d ​ 𝒁 ℓ ​ 𝚷 k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\log\\det(\\bm{I}+\\alpha_{k}\\bm{Z}\\bm{\\Pi}_{k}\\bm{Z}^{\\top})\\right)}{\\partial\\bm{Z}}("}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E5", "title": "∂ Δ ​ R ϵ ∂ 𝒁 ​ ( 𝒁 ℓ ) = 𝑬 ℓ ⏟ Expansion ​ 𝒁 ℓ − ∑ k = 1 K γ k ​ 𝑪 k ℓ ⏟ Compression ​ 𝒁 ℓ ​ 𝚷 k . \\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\bm{E}^{\\ell}}_{\\text{", "snippet": "∂ Δ ​ R ϵ ∂ 𝒁 ​ ( 𝒁 ℓ ) = 𝑬 ℓ ⏟ Expansion ​ 𝒁 ℓ − ∑ k = 1 K γ k ​ 𝑪 k ℓ ⏟ Compression ​ 𝒁 ℓ ​ 𝚷 k . \\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\bm{E}^{\\ell}}_{\\text{Expansion}}\\bm{Z}^{\\ell}\\;-\\;\\sum_{k=1}^{K}\\gamma_{k}\\underbrace{\\bm{C}_{k}^{\\el"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E7", "title": "𝑬 ℓ = α ​ 𝑼 ℓ ​ diag ⁡ ( 1 1 + α ​ λ 1 ℓ , … , 1 1 + α ​ λ d ℓ ) ​ ( 𝑼 ℓ ) ⊤ . \\bm{E}^{\\ell}=\\alpha\\bm{U}^{\\ell}\\,\\operatorname{diag}\\left(\\frac{1}{1+\\alpha\\lambda^{\\ell}_{1}},\\ldots,\\frac{1}{1+\\alpha", "snippet": "𝑬 ℓ = α ​ 𝑼 ℓ ​ diag ⁡ ( 1 1 + α ​ λ 1 ℓ , … , 1 1 + α ​ λ d ℓ ) ​ ( 𝑼 ℓ ) ⊤ . \\bm{E}^{\\ell}=\\alpha\\bm{U}^{\\ell}\\,\\operatorname{diag}\\left(\\frac{1}{1+\\alpha\\lambda^{\\ell}_{1}},\\ldots,\\frac{1}{1+\\alpha\\lambda^{\\ell}_{d}}\\right)\\left(\\bm{U}^{\\ell}\\right)^{\\top}. bold_italic_E start"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E8", "title": "𝒛 ℓ + 1 ∝ 𝒛 ℓ + η ⋅ g ​ ( 𝒛 ℓ , 𝜽 ℓ ) subject to 𝒛 ℓ + 1 ∈ 𝕊 d − 1 \\bm{z}^{\\ell+1}\\;\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\quad\\mbox{subject to}\\quad\\bm{z}^{\\ell+1}\\in\\ma", "snippet": "𝒛 ℓ + 1 ∝ 𝒛 ℓ + η ⋅ g ​ ( 𝒛 ℓ , 𝜽 ℓ ) subject to 𝒛 ℓ + 1 ∈ 𝕊 d − 1 \\bm{z}^{\\ell+1}\\;\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\quad\\mbox{subject to}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1} bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E9", "title": "𝒛 ˙ = g ​ ( 𝒛 , θ ) . \\dot{\\bm{z}}=g(\\bm{z},\\theta). over˙ start_ARG bold_italic_z end_ARG = italic_g ( bold_italic_z , italic_θ ) . (4.1.9)", "snippet": "𝒛 ˙ = g ​ ( 𝒛 , θ ) . \\dot{\\bm{z}}=g(\\bm{z},\\theta). over˙ start_ARG bold_italic_z end_ARG = italic_g ( bold_italic_z , italic_θ ) . (4.1.9)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex2", "title": "𝒁 ˙ = ∂ Δ ​ R ϵ ∂ 𝒁 , \\dot{\\bm{Z}}=\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}, over˙ start_ARG bold_italic_Z end_ARG = divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POST", "snippet": "𝒁 ˙ = ∂ Δ ​ R ϵ ∂ 𝒁 , \\dot{\\bm{Z}}=\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}, over˙ start_ARG bold_italic_Z end_ARG = divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E10", "title": "g ​ ( 𝒛 ℓ , 𝜽 ℓ ) ≐ 𝑬 ℓ ​ 𝒛 ℓ − ∑ k = 1 K γ k ​ π k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ∈ ℝ d , g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell", "snippet": "g ​ ( 𝒛 ℓ , 𝜽 ℓ ) ≐ 𝑬 ℓ ​ 𝒛 ℓ − ∑ k = 1 K γ k ​ π k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ∈ ℝ d , g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm{C}_{k}^{\\ell}\\bm{z}^{\\ell}\\in\\mathbb{R}^{d}, italic_g ( bold_italic_z star"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E11", "title": "𝝅 ^ ​ ( 𝒛 ℓ ) ≐ softmax ⁡ ( − λ ​ [ ‖ 𝑪 1 ℓ ​ 𝒛 ℓ ‖ 2 ⋮ ‖ 𝑪 K ℓ ​ 𝒛 ℓ ‖ 2 ] ) = 1 ∑ k = 1 K exp ⁡ ( − λ ​ ‖ 𝑪 k ℓ ​ 𝒛 ℓ ‖ 2 ) ​ [ exp ⁡ ( − λ ​ ‖ 𝑪 1 ℓ ​ 𝒛 ℓ ‖ 2 ) ⋮ exp ⁡ ( − λ ​ ‖ 𝑪 K ℓ ​ 𝒛 ℓ ‖ 2 ) ", "snippet": "𝝅 ^ ​ ( 𝒛 ℓ ) ≐ softmax ⁡ ( − λ ​ [ ‖ 𝑪 1 ℓ ​ 𝒛 ℓ ‖ 2 ⋮ ‖ 𝑪 K ℓ ​ 𝒛 ℓ ‖ 2 ] ) = 1 ∑ k = 1 K exp ⁡ ( − λ ​ ‖ 𝑪 k ℓ ​ 𝒛 ℓ ‖ 2 ) ​ [ exp ⁡ ( − λ ​ ‖ 𝑪 1 ℓ ​ 𝒛 ℓ ‖ 2 ) ⋮ exp ⁡ ( − λ ​ ‖ 𝑪 K ℓ ​ 𝒛 ℓ ‖ 2 ) ] ∈ [ 0 , 1 ] K . \\widehat{\\bm{\\pi}}(\\bm{z}^{\\ell})\\doteq\\operatorname{\\mathrm{s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13X", "title": "𝒛 ℓ + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ 𝒛 ℓ + η ⋅ 𝑬 ℓ ​ 𝒛 ℓ − η ⋅ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) \\displaystyle\\propto\\;\\bm", "snippet": "𝒛 ℓ + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ 𝒛 ℓ + η ⋅ 𝑬 ℓ ​ 𝒛 ℓ − η ⋅ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) \\displaystyle\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\eta\\cdot\\bm{\\sigma}\\big{(}[\\bm{C"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13Xa", "title": "= 𝒛 ℓ + η ⋅ g ​ ( 𝒛 ℓ , 𝜽 ℓ ) s.t. 𝒛 ℓ + 1 ∈ 𝕊 d − 1 , \\displaystyle=\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\qquad\\mbox{s.t.}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1}, = bold_itali", "snippet": "= 𝒛 ℓ + η ⋅ g ​ ( 𝒛 ℓ , 𝜽 ℓ ) s.t. 𝒛 ℓ + 1 ∈ 𝕊 d − 1 , \\displaystyle=\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\qquad\\mbox{s.t.}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1}, = bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ italic_g ( bo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14X", "title": "f ​ ( 𝒙 , 𝜽 ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_θ ) = f L ∘ f L − 1 ∘ ⋯ ∘ f 1 ∘ f 0 ​ ( 𝒛 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\", "snippet": "f ​ ( 𝒙 , 𝜽 ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_θ ) = f L ∘ f L − 1 ∘ ⋯ ∘ f 1 ∘ f 0 ​ ( 𝒛 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\circ f^{0}(\\bm{z}^{0}), italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14Xa", "title": "f ℓ ​ ( 𝒛 ℓ , 𝜽 ℓ ) ≐ \\displaystyle f^{\\ell}(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POST", "snippet": "f ℓ ​ ( 𝒛 ℓ , 𝜽 ℓ ) ≐ \\displaystyle f^{\\ell}(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14Xb", "title": "g ​ ( 𝒛 ℓ , 𝜽 ℓ ) = \\displaystyle g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;= italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_PO", "snippet": "g ​ ( 𝒛 ℓ , 𝜽 ℓ ) = \\displaystyle g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;= italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) = 𝑬 ℓ ​ 𝒛 ℓ − 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) . \\displ"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E15", "title": "Group Invariance: ​ f ​ ( 𝒙 ∘ 𝔤 , 𝜽 ) ∼ f ​ ( 𝒙 , 𝜽 ) , ∀ 𝔤 ∈ 𝔾 , \\mbox{\\em Group Invariance:}\\;f(\\bm{x}\\circ\\mathfrak{g},\\bm{\\theta})\\sim f(\\bm{x},\\bm{\\theta}),\\ \\forall\\mathfrak{g}\\in\\mathbb{G}, Gro", "snippet": "Group Invariance: ​ f ​ ( 𝒙 ∘ 𝔤 , 𝜽 ) ∼ f ​ ( 𝒙 , 𝜽 ) , ∀ 𝔤 ∈ 𝔾 , \\mbox{\\em Group Invariance:}\\;f(\\bm{x}\\circ\\mathfrak{g},\\bm{\\theta})\\sim f(\\bm{x},\\bm{\\theta}),\\ \\forall\\mathfrak{g}\\in\\mathbb{G}, Group Invariance: italic_f ( bold_italic_x ∘ fraktur_g , bold_italic_θ ) ∼ italic_f"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E16", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒙 ) ≐ [ x ​ ( 0 ) x ​ ( D − 1 ) … x ​ ( 2 ) x ​ ( 1 ) x ​ ( 1 ) x ​ ( 0 ) x ​ ( D − 1 ) ⋯ x ​ ( 2 ) ⋮ x ​ ( 1 ) x ​ ( 0 ) ⋱ ⋮ x ​ ( D − 2 ) ⋮ ⋱ ⋱ x ​ ( D − 1 ) x ​ ( D − 1 ) x ​ ( D − 2 ) … x", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒙 ) ≐ [ x ​ ( 0 ) x ​ ( D − 1 ) … x ​ ( 2 ) x ​ ( 1 ) x ​ ( 1 ) x ​ ( 0 ) x ​ ( D − 1 ) ⋯ x ​ ( 2 ) ⋮ x ​ ( 1 ) x ​ ( 0 ) ⋱ ⋮ x ​ ( D − 2 ) ⋮ ⋱ ⋱ x ​ ( D − 1 ) x ​ ( D − 1 ) x ​ ( D − 2 ) … x ​ ( 1 ) x ​ ( 0 ) ] ∈ ℝ D × D . \\mathsf{circ}(\\bm{x})\\,\\doteq\\,\\left[\\begin{arr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E18", "title": "𝑬 1 = α ​ ( 𝑰 + α ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ⊤ ) − 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRI", "snippet": "𝑬 1 = α ​ ( 𝑰 + α ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ⊤ ) − 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_α ( bold_italic_I + italic_α sansserif_circ ( "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex4", "title": "𝑬 1 ​ 𝒛 = 𝒆 1 ⊛ 𝒛 , \\bm{E}^{1}\\bm{z}=\\bm{e}_{1}\\circledast\\bm{z}, bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_z = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold", "snippet": "𝑬 1 ​ 𝒛 = 𝒆 1 ⊛ 𝒛 , \\bm{E}^{1}\\bm{z}=\\bm{e}_{1}\\circledast\\bm{z}, bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_z = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_z ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex5", "title": "( 𝒆 1 ⊛ 𝒛 ) i ≐ ∑ j = 0 d − 1 e 1 ​ ( j ) ​ x ​ ( i + d − j ​ mod ​ d ) . (\\bm{e}_{1}\\circledast\\bm{z})_{i}\\doteq\\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\\,\\,\\textsf{mod}\\,\\,d). ( bold_italic_e start_POSTSUBSCR", "snippet": "( 𝒆 1 ⊛ 𝒛 ) i ≐ ∑ j = 0 d − 1 e 1 ​ ( j ) ​ x ​ ( i + d − j ​ mod ​ d ) . (\\bm{e}_{1}\\circledast\\bm{z})_{i}\\doteq\\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\\,\\,\\textsf{mod}\\,\\,d). ( bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_z ) start_POSTSUBSCRIPT italic_i end_POSTS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E19", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) + η ⋅ 𝑬 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) − η ⋅ 𝝈 ​ ( [ 𝑪 1 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) , … , 𝑪 K 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) ] ) , \\mathsf{circ}(\\bm{z}^{1})+\\eta\\cdot\\bm{E}^{1}\\mathsf{circ}(\\bm{z}^{1})-\\eta\\cdot\\bm{\\s", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) + η ⋅ 𝑬 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) − η ⋅ 𝝈 ​ ( [ 𝑪 1 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) , … , 𝑪 K 1 ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 ) ] ) , \\mathsf{circ}(\\bm{z}^{1})+\\eta\\cdot\\bm{E}^{1}\\mathsf{circ}(\\bm{z}^{1})-\\eta\\cdot\\bm{\\sigma}\\Big{(}[\\bm{C}_{1}^{1}\\mathsf{circ}(\\bm{z}^{1}),\\ldots,\\bm{C}^{1}_{K}\\maths"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E20", "title": "𝒛 2 ∝ 𝒛 1 + η ⋅ g ​ ( 𝒛 1 , 𝜽 1 ) = 𝒛 1 + η ⋅ 𝒆 1 ⊛ 𝒛 1 − η ⋅ 𝝈 ​ ( [ 𝒄 1 1 ⊛ 𝒛 1 , … , 𝒄 K 1 ⊛ 𝒛 1 ] ) , \\bm{z}^{2}\\propto\\bm{z}^{1}+\\eta\\cdot g(\\bm{z}^{1},\\bm{\\theta}^{1})=\\bm{z}^{1}+\\eta\\cdot\\bm{e}", "snippet": "𝒛 2 ∝ 𝒛 1 + η ⋅ g ​ ( 𝒛 1 , 𝜽 1 ) = 𝒛 1 + η ⋅ 𝒆 1 ⊛ 𝒛 1 − η ⋅ 𝝈 ​ ( [ 𝒄 1 1 ⊛ 𝒛 1 , … , 𝒄 K 1 ⊛ 𝒛 1 ] ) , \\bm{z}^{2}\\propto\\bm{z}^{1}+\\eta\\cdot g(\\bm{z}^{1},\\bm{\\theta}^{1})=\\bm{z}^{1}+\\eta\\cdot\\bm{e}_{1}\\circledast\\bm{z}^{1}-\\eta\\cdot\\bm{\\sigma}\\Big{(}[\\bm{c}_{1}^{1}\\circledast\\"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex6", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒁 2 ) = [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 1 + η ​ g ​ ( 𝒛 1 1 , 𝜽 1 ) ) , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 N 1 + η ​ g ​ ( 𝒛 N 1 , 𝜽 1 ) ) ] . \\mathsf{circ}(\\bm{Z}^{2})=\\big{[}\\mathsf{circ}(\\bm{z}_{1}^{1}+\\eta g(\\bm{z}_{1}^{1},\\", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒁 2 ) = [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 1 1 + η ​ g ​ ( 𝒛 1 1 , 𝜽 1 ) ) , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 N 1 + η ​ g ​ ( 𝒛 N 1 , 𝜽 1 ) ) ] . \\mathsf{circ}(\\bm{Z}^{2})=\\big{[}\\mathsf{circ}(\\bm{z}_{1}^{1}+\\eta g(\\bm{z}_{1}^{1},\\bm{\\theta}^{1})),\\dots,\\mathsf{circ}(\\bm{z}_{N}^{1}+\\eta g(\\bm{z}_{N}^{1},\\bm{\\t"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E21", "title": "𝒛 ​ [ c ] = 𝒌 c ⊛ 𝒙 = 𝖼𝗂𝗋𝖼 ​ ( 𝒌 c ) ​ 𝒙 ∈ ℝ d , c = 1 , … , C . \\bm{z}[c]=\\bm{k}_{c}\\circledast\\bm{x}=\\mathsf{circ}(\\bm{k}_{c})\\bm{x}\\in\\mathbb{R}^{d},\\quad c=1,\\ldots,C. bold_italic_z [ italic_c ] =", "snippet": "𝒛 ​ [ c ] = 𝒌 c ⊛ 𝒙 = 𝖼𝗂𝗋𝖼 ​ ( 𝒌 c ) ​ 𝒙 ∈ ℝ d , c = 1 , … , C . \\bm{z}[c]=\\bm{k}_{c}\\circledast\\bm{x}=\\mathsf{circ}(\\bm{k}_{c})\\bm{x}\\in\\mathbb{R}^{d},\\quad c=1,\\ldots,C. bold_italic_z [ italic_c ] = bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ⊛ bold_italic_x = "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E22X", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ∈ ℝ d ​ C × d , 𝚺 ¯ ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ​ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⊤ , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ⊤", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ∈ ℝ d ​ C × d , 𝚺 ¯ ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ​ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⊤ , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ⊤ ] ∈ ℝ d ​ C × d ​ C , \\displaystyle\\mathsf{circ}(\\bar{\\bm{z}})\\doteq\\left[\\begi"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E23", "title": "𝒙 = 𝒅 k , 1 ⊛ z 1 + … + 𝒅 k , c ⊛ z c = 𝖼𝗂𝗋𝖼 ​ ( 𝑫 k ) ​ 𝒛 , \\bm{x}=\\bm{d}_{k,1}\\circledast z_{1}+\\ldots+\\bm{d}_{k,c}\\circledast z_{c}=\\mathsf{circ}(\\bm{D}_{k})\\bm{z}, bold_italic_x = bold_italic_d st", "snippet": "𝒙 = 𝒅 k , 1 ⊛ z 1 + … + 𝒅 k , c ⊛ z c = 𝖼𝗂𝗋𝖼 ​ ( 𝑫 k ) ​ 𝒛 , \\bm{x}=\\bm{d}_{k,1}\\circledast z_{1}+\\ldots+\\bm{d}_{k,c}\\circledast z_{c}=\\mathsf{circ}(\\bm{D}_{k})\\bm{z}, bold_italic_x = bold_italic_d start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT ⊛ italic_z start_POSTSUBSCRIPT "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E24", "title": "𝒙 = [ 𝖼𝗂𝗋𝖼 ​ ( 𝑫 1 ) , 𝖼𝗂𝗋𝖼 ​ ( 𝑫 2 ) , … , 𝖼𝗂𝗋𝖼 ​ ( 𝑫 K ) ] ​ 𝒛 ¯ , \\bm{x}=\\big{[}\\mathsf{circ}(\\bm{D}_{1}),\\mathsf{circ}(\\bm{D}_{2}),\\ldots,\\mathsf{circ}(\\bm{D}_{K})\\big{]}\\bar{\\bm{z}}, bold_italic_", "snippet": "𝒙 = [ 𝖼𝗂𝗋𝖼 ​ ( 𝑫 1 ) , 𝖼𝗂𝗋𝖼 ​ ( 𝑫 2 ) , … , 𝖼𝗂𝗋𝖼 ​ ( 𝑫 K ) ] ​ 𝒛 ¯ , \\bm{x}=\\big{[}\\mathsf{circ}(\\bm{D}_{1}),\\mathsf{circ}(\\bm{D}_{2}),\\ldots,\\mathsf{circ}(\\bm{D}_{K})\\big{]}\\bar{\\bm{z}}, bold_italic_x = [ sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E25", "title": "[ 𝒌 1 ⊛ 𝒙 , 𝒌 2 ⊛ 𝒙 , … , 𝒌 C ⊛ 𝒙 ] ⊤ = [ 𝖼𝗂𝗋𝖼 ​ ( 𝒌 1 ) ​ 𝒙 , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒌 C ) ​ 𝒙 ] ⊤ ∈ ℝ C × d \\big{[}\\bm{k}_{1}\\circledast\\bm{x},\\bm{k}_{2}\\circledast\\bm{x},\\ldots,\\bm{k}_{C}\\circledast\\bm{x}\\bi", "snippet": "[ 𝒌 1 ⊛ 𝒙 , 𝒌 2 ⊛ 𝒙 , … , 𝒌 C ⊛ 𝒙 ] ⊤ = [ 𝖼𝗂𝗋𝖼 ​ ( 𝒌 1 ) ​ 𝒙 , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒌 C ) ​ 𝒙 ] ⊤ ∈ ℝ C × d \\big{[}\\bm{k}_{1}\\circledast\\bm{x},\\bm{k}_{2}\\circledast\\bm{x},\\ldots,\\bm{k}_{C}\\circledast\\bm{x}\\big{]}^{\\top}=\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E26", "title": "𝒛 ¯ ≐ 𝝉 ​ ( [ 𝖼𝗂𝗋𝖼 ​ ( 𝒌 1 ) ​ 𝒙 , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒌 C ) ​ 𝒙 ] ⊤ ) ∈ ℝ C × d . \\bar{\\bm{z}}\\doteq\\bm{\\tau}\\left(\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{C})\\bm{x}\\big{]}^{\\top}", "snippet": "𝒛 ¯ ≐ 𝝉 ​ ( [ 𝖼𝗂𝗋𝖼 ​ ( 𝒌 1 ) ​ 𝒙 , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒌 C ) ​ 𝒙 ] ⊤ ) ∈ ℝ C × d . \\bar{\\bm{z}}\\doteq\\bm{\\tau}\\left(\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{C})\\bm{x}\\big{]}^{\\top}\\right)\\in\\mathbb{R}^{C\\times d}. over¯ start_ARG bold_italic_z end_ARG ≐ bold_i"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E27", "title": "𝑬 ¯ ≐ α ​ ( 𝑰 + α ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 ¯ ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 ¯ ) ⊤ ) − 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¯ start_ARG bold_", "snippet": "𝑬 ¯ ≐ α ​ ( 𝑰 + α ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 ¯ ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 ¯ ) ⊤ ) − 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¯ start_ARG bold_italic_E end_ARG ≐ italic_α ( bold_italic_I + italic_α sansserif_circ ( over¯ st"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex7", "title": "𝑬 ¯ = [ 𝑬 ¯ 1 , 1 ⋯ 𝑬 ¯ 1 , C ⋮ ⋱ ⋮ 𝑬 ¯ C , 1 ⋯ 𝑬 ¯ C , C ] ∈ ℝ d ​ C × d ​ C , \\bar{\\bm{E}}=\\left[\\begin{matrix}\\bar{\\bm{E}}_{1,1}&\\cdots&\\bar{\\bm{E}}_{1,C}\\\\ \\vdots&\\ddots&\\vdots\\\\ \\bar{\\bm{E}}_{C,1", "snippet": "𝑬 ¯ = [ 𝑬 ¯ 1 , 1 ⋯ 𝑬 ¯ 1 , C ⋮ ⋱ ⋮ 𝑬 ¯ C , 1 ⋯ 𝑬 ¯ C , C ] ∈ ℝ d ​ C × d ​ C , \\bar{\\bm{E}}=\\left[\\begin{matrix}\\bar{\\bm{E}}_{1,1}&\\cdots&\\bar{\\bm{E}}_{1,C}\\\\ \\vdots&\\ddots&\\vdots\\\\ \\bar{\\bm{E}}_{C,1}&\\cdots&\\bar{\\bm{E}}_{C,C}\\\\ \\end{matrix}\\right]\\in\\mathbb{R}^{dC\\times dC}, ov"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex8", "title": "𝑬 ¯ ⋅ vec ​ ( 𝒛 ¯ ) = vec ​ ( 𝒆 ¯ ⊛ 𝒛 ¯ ) . \\bar{\\bm{E}}\\cdot\\textsf{vec}(\\bar{\\bm{z}})=\\textsf{vec}(\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}}). over¯ start_ARG bold_italic_E end_ARG ⋅ vec ( over¯ start_ARG", "snippet": "𝑬 ¯ ⋅ vec ​ ( 𝒛 ¯ ) = vec ​ ( 𝒆 ¯ ⊛ 𝒛 ¯ ) . \\bar{\\bm{E}}\\cdot\\textsf{vec}(\\bar{\\bm{z}})=\\textsf{vec}(\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}}). over¯ start_ARG bold_italic_E end_ARG ⋅ vec ( over¯ start_ARG bold_italic_z end_ARG ) = vec ( over¯ start_ARG bold_italic_e end_ARG ⊛ over¯ s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex9", "title": "( 𝒆 ¯ ⊛ 𝒛 ¯ ) ​ [ c ] ≐ ∑ c ′ = 1 C 𝒆 ¯ ​ [ c , c ′ ] ⊛ 𝒛 ¯ ​ [ c ′ ] , ∀ c = 1 , … , C . (\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}})[c]\\doteq\\sum_{c^{\\prime}=1}^{C}\\bar{\\bm{e}}[c,c^{\\prime}]\\circledast\\bar", "snippet": "( 𝒆 ¯ ⊛ 𝒛 ¯ ) ​ [ c ] ≐ ∑ c ′ = 1 C 𝒆 ¯ ​ [ c , c ′ ] ⊛ 𝒛 ¯ ​ [ c ′ ] , ∀ c = 1 , … , C . (\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}})[c]\\doteq\\sum_{c^{\\prime}=1}^{C}\\bar{\\bm{e}}[c,c^{\\prime}]\\circledast\\bar{\\bm{z}}[c^{\\prime}],\\quad\\forall c=1,\\ldots,C. ( over¯ start_ARG bold_italic_e "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E2", "title": "R ϵ ​ ( 𝒁 ) ≐ 1 2 ​ logdet ​ ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ⊤ ​ 𝒁 ) = 1 2 ​ logdet ​ ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ​ 𝒁 ⊤ ) . R_{\\epsilon}(\\bm{Z})\\doteq\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}^{", "snippet": "R ϵ ​ ( 𝒁 ) ≐ 1 2 ​ logdet ​ ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ⊤ ​ 𝒁 ) = 1 2 ​ logdet ​ ( 𝑰 + d N ​ ϵ 2 ​ 𝒁 ​ 𝒁 ⊤ ) . R_{\\epsilon}(\\bm{Z})\\doteq\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}^{\\top}\\bm{Z}\\right)=\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E3X", "title": "R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_", "snippet": "R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ ∑ k = 1"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E4", "title": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\b", "snippet": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ∈ caligraphic_F end_POSTSUBSCRIPT "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E5X", "title": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 1 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}", "snippet": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 1 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ∈ caligraphic_F end_P"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E6", "title": "f : 𝑿 = 𝒁 0 → f 0 𝒁 1 → ⋯ → 𝒁 ℓ → f ℓ 𝒁 ℓ + 1 → ⋯ → f L − 1 𝒁 L = 𝒁 , f\\colon\\bm{X}=\\bm{Z}^{0}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{1}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xri", "snippet": "f : 𝑿 = 𝒁 0 → f 0 𝒁 1 → ⋯ → 𝒁 ℓ → f ℓ 𝒁 ℓ + 1 → ⋯ → f L − 1 𝒁 L = 𝒁 , f\\colon\\bm{X}=\\bm{Z}^{0}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{1}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{Z}^{\\ell+1}\\rightarrow\\cdo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E8", "title": "𝒁 ℓ + 1 = arg ​ min 𝒁 ⁡ { λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 } . \\bm{Z}^{\\ell+1}=\\operatorname*{arg\\ min}_{{\\bm{Z}}}\\bigg{\\{}\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^", "snippet": "𝒁 ℓ + 1 = arg ​ min 𝒁 ⁡ { λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 } . \\bm{Z}^{\\ell+1}=\\operatorname*{arg\\ min}_{{\\bm{Z}}}\\bigg{\\{}\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}{\\bm{Z}}\\|_{F}^{2}\\bigg{\\}}. bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ +"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E9", "title": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) = p N ​ ϵ 2 ​ ∑ k = 1 K 𝑼 k ​ 𝑼 k ⊤ ​ 𝒁 ​ ( 𝑰 + p N ​ ϵ 2 ​ ( 𝑼 k ⊤ ​ 𝒁 ) ⊤ ​ ( 𝑼 k ⊤ ​ 𝒁 ) ) − 1 . \\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})=\\frac{p}{N\\epsil", "snippet": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) = p N ​ ϵ 2 ​ ∑ k = 1 K 𝑼 k ​ 𝑼 k ⊤ ​ 𝒁 ​ ( 𝑰 + p N ​ ϵ 2 ​ ( 𝑼 k ⊤ ​ 𝒁 ) ⊤ ​ ( 𝑼 k ⊤ ​ 𝒁 ) ) − 1 . \\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})=\\frac{p}{N\\epsilon^{2}}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{Z}\\Big{(}\\bm{I}+\\frac{p}{N\\e"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E14", "title": "𝒁 ℓ + 1 / 2 = ( 1 − κ ​ p N ​ ϵ 2 ) 𝒁 ℓ + κ ​ p N ​ ϵ 2 MSSA ( 𝒁 ℓ | 𝑼 [ K ] ℓ ) . \\bm{Z}^{\\ell+1/2}=\\left(1-\\frac{\\kappa p}{N\\epsilon^{2}}\\right)\\bm{Z}^{\\ell}+\\frac{\\kappa p}{N\\epsilon^{2}}\\mathrm{MS", "snippet": "𝒁 ℓ + 1 / 2 = ( 1 − κ ​ p N ​ ϵ 2 ) 𝒁 ℓ + κ ​ p N ​ ϵ 2 MSSA ( 𝒁 ℓ | 𝑼 [ K ] ℓ ) . \\bm{Z}^{\\ell+1/2}=\\left(1-\\frac{\\kappa p}{N\\epsilon^{2}}\\right)\\bm{Z}^{\\ell}+\\frac{\\kappa p}{N\\epsilon^{2}}\\mathrm{MSSA}\\left(\\bm{Z}^{\\ell}\\ \\middle|\\ \\bm{U}_{[K]}^{\\ell}\\right). bold_italic_Z star"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E15", "title": "R ϵ ​ ( 𝒁 ℓ + 1 / 2 ) ≈ R ϵ ​ ( 𝑫 ℓ ​ 𝒁 ℓ + 1 ) ≈ R ϵ ​ ( 𝒁 ℓ + 1 ) . R_{\\epsilon}(\\bm{Z}^{\\ell+1/2})\\approx R_{\\epsilon}(\\bm{D}^{\\ell}\\bm{Z}^{\\ell+1})\\approx R_{\\epsilon}(\\bm{Z}^{\\ell+1}). italic_R s", "snippet": "R ϵ ​ ( 𝒁 ℓ + 1 / 2 ) ≈ R ϵ ​ ( 𝑫 ℓ ​ 𝒁 ℓ + 1 ) ≈ R ϵ ​ ( 𝒁 ℓ + 1 ) . R_{\\epsilon}(\\bm{Z}^{\\ell+1/2})\\approx R_{\\epsilon}(\\bm{D}^{\\ell}\\bm{Z}^{\\ell+1})\\approx R_{\\epsilon}(\\bm{Z}^{\\ell+1}). italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E16", "title": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ⁡ [ λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}", "snippet": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ⁡ [ λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}\\bm{Z}\\|_{F}^{2}\\right]. bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E17", "title": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ≥ 𝟎 ⁡ [ λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}\\geq\\bm{0}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\e", "snippet": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ≥ 𝟎 ⁡ [ λ ​ ‖ 𝒁 ‖ 1 + 1 2 ​ ‖ 𝒁 ℓ + 1 / 2 − 𝑫 ℓ ​ 𝒁 ‖ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}\\geq\\bm{0}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}\\bm{Z}\\|_{F}^{2}\\right]. bold_italic_Z start_POSTSUPERSCRIP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E20", "title": "𝒁 ℓ + 1 / 2 ≐ 𝒁 ℓ + MSSA ​ ( 𝒁 ℓ ∣ 𝑼 [ K ] ℓ ) , 𝒁 ℓ + 1 ≐ ISTA ​ ( 𝒁 ℓ + 1 / 2 ∣ 𝑫 ℓ ) . \\bm{Z}^{\\ell+1/2}\\doteq\\bm{Z}^{\\ell}+\\texttt{MSSA}(\\bm{Z}^{\\ell}\\mid\\bm{U}_{[K]}^{\\ell}),\\qquad\\bm{Z}^{\\ell+1}", "snippet": "𝒁 ℓ + 1 / 2 ≐ 𝒁 ℓ + MSSA ​ ( 𝒁 ℓ ∣ 𝑼 [ K ] ℓ ) , 𝒁 ℓ + 1 ≐ ISTA ​ ( 𝒁 ℓ + 1 / 2 ∣ 𝑫 ℓ ) . \\bm{Z}^{\\ell+1/2}\\doteq\\bm{Z}^{\\ell}+\\texttt{MSSA}(\\bm{Z}^{\\ell}\\mid\\bm{U}_{[K]}^{\\ell}),\\qquad\\bm{Z}^{\\ell+1}\\doteq\\texttt{ISTA}(\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell}). bold_italic_Z start_POS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E6", "title": "R c , f ​ ( 𝒁 , 𝚷 ) ≐ 1 2 ​ ∑ k = 1 K N k N ​ F ​ ( 1 N k ​ 𝒁 ​ Diag ​ ( 𝝅 k ) ​ 𝒁 ⊤ ) . R_{c,f}(\\bm{Z},\\bm{\\Pi})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}F\\left(\\frac{1}{N_{k}}\\bm{Z}\\mathrm{Diag}", "snippet": "R c , f ​ ( 𝒁 , 𝚷 ) ≐ 1 2 ​ ∑ k = 1 K N k N ​ F ​ ( 1 N k ​ 𝒁 ​ Diag ​ ( 𝝅 k ) ​ 𝒁 ⊤ ) . R_{c,f}(\\bm{Z},\\bm{\\Pi})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}F\\left(\\frac{1}{N_{k}}\\bm{Z}\\mathrm{Diag}(\\bm{\\pi}_{k})\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_c , ital"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E7", "title": "F ​ ( 𝑴 ) ≤ ∑ i = 1 d f ​ ( ( 𝑸 ⊤ ​ 𝑴 ​ 𝑸 ) i ​ i ) . F(\\bm{M})\\leq\\sum_{i=1}^{d}f\\left((\\bm{Q}^{\\top}\\bm{M}\\bm{Q})_{ii}\\right). italic_F ( bold_italic_M ) ≤ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POS", "snippet": "F ​ ( 𝑴 ) ≤ ∑ i = 1 d f ​ ( ( 𝑸 ⊤ ​ 𝑴 ​ 𝑸 ) i ​ i ) . F(\\bm{M})\\leq\\sum_{i=1}^{d}f\\left((\\bm{Q}^{\\top}\\bm{M}\\bm{Q})_{ii}\\right). italic_F ( bold_italic_M ) ≤ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( ( bold_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E8", "title": "R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) ≐ 1 2 ​ ∑ k = 1 K N k N ​ ∑ i = 1 d f ​ ( 1 N k ​ ( 𝑼 k ⊤ ​ 𝒁 ​ Diag ​ ( 𝝅 k ) ​ 𝒁 ⊤ ​ 𝑼 k ) i ​ i ) , R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})\\doteq\\frac{1", "snippet": "R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) ≐ 1 2 ​ ∑ k = 1 K N k N ​ ∑ i = 1 d f ​ ( 1 N k ​ ( 𝑼 k ⊤ ​ 𝒁 ​ Diag ​ ( 𝝅 k ) ​ 𝒁 ⊤ ​ 𝑼 k ) i ​ i ) , R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}\\sum_{i=1}^{d}f\\left(\\frac{1}{N_{k}}(\\bm{U}_{k}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E9", "title": "∇ 𝒁 1 2 ​ ∑ i = 1 d f ​ ( ( 𝒁 ​ Diag ​ ( 𝝅 ) ​ 𝒁 ⊤ ) i ​ i ) = Diag ​ ( ∇ f ​ [ 𝒁 ⊙ 2 ​ 𝝅 ] ) ​ 𝒁 ​ Diag ​ ( 𝝅 ) , \\nabla_{\\bm{Z}}\\ \\frac{1}{2}\\sum_{i=1}^{d}f((\\bm{Z}\\mathrm{Diag}(\\bm{\\pi})\\bm{Z}^{\\to", "snippet": "∇ 𝒁 1 2 ​ ∑ i = 1 d f ​ ( ( 𝒁 ​ Diag ​ ( 𝝅 ) ​ 𝒁 ⊤ ) i ​ i ) = Diag ​ ( ∇ f ​ [ 𝒁 ⊙ 2 ​ 𝝅 ] ) ​ 𝒁 ​ Diag ​ ( 𝝅 ) , \\nabla_{\\bm{Z}}\\ \\frac{1}{2}\\sum_{i=1}^{d}f((\\bm{Z}\\mathrm{Diag}(\\bm{\\pi})\\bm{Z}^{\\top})_{ii})=\\;\\mathrm{Diag}(\\nabla f[\\bm{Z}^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E11", "title": "𝒛 j + = 𝒛 j − τ ​ ∇ 𝒛 j R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) = 𝒛 j − τ N ​ ∑ k = 1 K Π j ​ k ​ 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒛 j \\bm{z}_{j}^{+}=\\bm{z}_{j}-\\tau\\nabla_{\\bm{z}_{j}}R_{c,f}^{\\rm var}", "snippet": "𝒛 j + = 𝒛 j − τ ​ ∇ 𝒛 j R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) = 𝒛 j − τ N ​ ∑ k = 1 K Π j ​ k ​ 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒛 j \\bm{z}_{j}^{+}=\\bm{z}_{j}-\\tau\\nabla_{\\bm{z}_{j}}R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})=\\bm{z}_{j}-\\frac{\\tau}{N}\\sum_{k=1}^{K}\\Pi_{jk"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E12", "title": "𝒛 j + = ∑ k = 1 K Π j ​ k ​ [ 𝒛 j ​ − τ n ​ 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ⏟ action of one attention head ​ 𝒛 j ] . \\bm{z}_{j}^{+}=\\sum_{k=1}^{K}\\Pi_{jk}\\Big{[}\\bm{z}_{j}\\underbrace{-\\frac{\\tau}{", "snippet": "𝒛 j + = ∑ k = 1 K Π j ​ k ​ [ 𝒛 j ​ − τ n ​ 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ⏟ action of one attention head ​ 𝒛 j ] . \\bm{z}_{j}^{+}=\\sum_{k=1}^{K}\\Pi_{jk}\\Big{[}\\bm{z}_{j}\\underbrace{-\\frac{\\tau}{n}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\bm{\\pi}_{k}\\mid\\bm{U}_{k})\\bm{U}_{k}^{\\top}}_{\\text{a"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E13", "title": "R c , f ​ ( 𝒁 , 𝚷 ) ≤ R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) , R_{c,f}(\\bm{Z},\\bm{\\Pi})\\leq R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]}), italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBS", "snippet": "R c , f ​ ( 𝒁 , 𝚷 ) ≤ R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) , R_{c,f}(\\bm{Z},\\bm{\\Pi})\\leq R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]}), italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ) ≤ italic_R start_POSTSUBSCRIPT italic_c , itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E15", "title": "TSSA ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≐ − τ n ​ ∑ k = 1 K 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒁 ​ diag ⁡ ( 𝝅 k ) , \\texttt{TSSA}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq-\\frac{\\tau}{n}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\b", "snippet": "TSSA ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≐ − τ n ​ ∑ k = 1 K 𝑼 k ​ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒁 ​ diag ⁡ ( 𝝅 k ) , \\texttt{TSSA}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq-\\frac{\\tau}{n}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\bm{\\pi}_{k}\\mid\\bm{U}_{k})\\bm{U}_{k}^{\\top}\\bm{Z}\\operatorname{diag}(\\bm{\\pi}_{k}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx40", "title": "α ≐ d N ​ ϵ 2 , α k ≐ d tr ​ ( 𝚷 k ) ​ ϵ 2 , γ k ≐ tr ​ ( 𝚷 k ) N , for ​ k = 1 , … , K . \\displaystyle\\alpha\\doteq\\frac{d}{N\\epsilon^{2}},\\qquad\\alpha_{k}\\doteq\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsi", "snippet": "α ≐ d N ​ ϵ 2 , α k ≐ d tr ​ ( 𝚷 k ) ​ ϵ 2 , γ k ≐ tr ​ ( 𝚷 k ) N , for ​ k = 1 , … , K . \\displaystyle\\alpha\\doteq\\frac{d}{N\\epsilon^{2}},\\qquad\\alpha_{k}\\doteq\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsilon^{2}},\\qquad\\gamma_{k}\\doteq\\frac{\\mathrm{tr}(\\bm{\\Pi}_{k})}{N},\\qquad\\text{f"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx41", "title": "𝑬 ℓ ​ 𝒛 ℓ = α ​ ( 𝒛 ℓ − 𝒁 ℓ ​ 𝒒 ⋆ ℓ ) , where 𝒒 ⋆ ℓ ≐ arg ​ min 𝒒 ℓ ⁡ { α ​ ‖ 𝒛 ℓ − 𝒁 ℓ ​ 𝒒 ℓ ‖ 2 2 + ‖ 𝒒 ℓ ‖ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\bm{z}^{\\ell}=\\alpha(\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{", "snippet": "𝑬 ℓ ​ 𝒛 ℓ = α ​ ( 𝒛 ℓ − 𝒁 ℓ ​ 𝒒 ⋆ ℓ ) , where 𝒒 ⋆ ℓ ≐ arg ​ min 𝒒 ℓ ⁡ { α ​ ‖ 𝒛 ℓ − 𝒁 ℓ ​ 𝒒 ℓ ‖ 2 2 + ‖ 𝒒 ℓ ‖ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\bm{z}^{\\ell}=\\alpha(\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{\\ell}_{\\star}),\\qquad\\mbox{where}\\qquad\\bm{q}^{\\ell}_{\\star}\\doteq\\operatorname*"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx42", "title": "∑ k = 1 K γ k ​ π k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ≈ ∑ k = 1 K γ k ​ π ^ k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ≐ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) , \\displaystyle\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm", "snippet": "∑ k = 1 K γ k ​ π k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ≈ ∑ k = 1 K γ k ​ π ^ k ​ ( 𝒛 ℓ ) ​ 𝑪 k ℓ ​ 𝒛 ℓ ≐ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) , \\displaystyle\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm{C}_{k}^{\\ell}\\bm{z}^{\\ell}\\;\\approx\\;\\sum_{k=1}^{K}\\gamma_{k}\\widehat{\\pi}_{k}("}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13", "title": "𝒛 ℓ + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ 𝒛 ℓ + η ⋅ 𝑬 ℓ ​ 𝒛 ℓ − η ⋅ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) \\displaystyle\\propto\\;\\bm", "snippet": "𝒛 ℓ + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ 𝒛 ℓ + η ⋅ 𝑬 ℓ ​ 𝒛 ℓ − η ⋅ 𝝈 ​ ( [ 𝑪 1 ℓ ​ 𝒛 ℓ , … , 𝑪 K ℓ ​ 𝒛 ℓ ] ) \\displaystyle\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\eta\\cdot\\bm{\\sigma}\\big{(}[\\bm{C"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14", "title": "f ​ ( 𝒙 , 𝜽 ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_θ ) = f L ∘ f L − 1 ∘ ⋯ ∘ f 1 ∘ f 0 ​ ( 𝒛 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\", "snippet": "f ​ ( 𝒙 , 𝜽 ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_θ ) = f L ∘ f L − 1 ∘ ⋯ ∘ f 1 ∘ f 0 ​ ( 𝒛 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\circ f^{0}(\\bm{z}^{0}), italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx43", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ⊤ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( b", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ​ 𝖼𝗂𝗋𝖼 ​ ( 𝒁 1 ) ⊤ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E22", "title": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ∈ ℝ d ​ C × d , 𝚺 ¯ ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ​ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⊤ , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ⊤", "snippet": "𝖼𝗂𝗋𝖼 ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ∈ ℝ d ​ C × d , 𝚺 ¯ ​ ( 𝒛 ¯ ) ≐ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⋮ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ] ​ [ 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ 1 ] ) ⊤ , … , 𝖼𝗂𝗋𝖼 ​ ( 𝒛 ​ [ C ] ) ⊤ ] ∈ ℝ d ​ C × d ​ C , \\displaystyle\\mathsf{circ}(\\bar{\\bm{z}})\\doteq\\left[\\begi"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx44", "title": "max 𝒁 ∈ ℝ d × N ​ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) . \\displaystyle\\mathrm{max}_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\ \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq R_{", "snippet": "max 𝒁 ∈ ℝ d × N ​ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≐ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) . \\displaystyle\\mathrm{max}_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\ \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq R_{\\epsilon}(\\bm{Z})-R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}). roman_max start_POST"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E3", "title": "R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_", "snippet": "R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ ∑ k = 1"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E5", "title": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 1 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}", "snippet": "max f ∈ ℱ ⁡ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 1 ] s.t. ​ 𝒁 = f ​ ( 𝑿 ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ∈ caligraphic_F end_P"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx45", "title": "𝒁 ℓ + 1 / 2 = 𝒁 ℓ − κ ​ ∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ℓ ) . \\displaystyle\\bm{Z}^{\\ell+1/2}=\\bm{Z}^{\\ell}-\\kappa\\nabla_{\\bm{Z}}R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}^{\\ell}). bold_italic_Z start_POSTSUPER", "snippet": "𝒁 ℓ + 1 / 2 = 𝒁 ℓ − κ ​ ∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ℓ ) . \\displaystyle\\bm{Z}^{\\ell+1/2}=\\bm{Z}^{\\ell}-\\kappa\\nabla_{\\bm{Z}}R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}^{\\ell}). bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx46", "title": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) ∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUB", "snippet": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) ∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx47", "title": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≈ p N ​ ϵ 2 ​ 𝒁 − ( p N ​ ϵ 2 ) 2 ​ MSSA ⁡ ( 𝒁 ℓ ∣ 𝑼 [ K ] ℓ ) , \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})\\approx\\frac{p}{N\\epsilon^{2}}\\bm{Z}-\\l", "snippet": "∇ 𝒁 R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) ≈ p N ​ ϵ 2 ​ 𝒁 − ( p N ​ ϵ 2 ) 2 ​ MSSA ⁡ ( 𝒁 ℓ ∣ 𝑼 [ K ] ℓ ) , \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})\\approx\\frac{p}{N\\epsilon^{2}}\\bm{Z}-\\left(\\frac{p}{N\\epsilon^{2}}\\right)^{2}\\operatorname{MSSA}\\left(\\bm{Z}^{\\ell}\\mid"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx48", "title": "SSA ​ ( 𝒁 ∣ 𝑼 k ) ≐ ( 𝑼 k ⊤ ​ 𝒁 ) ​ softmax ​ ( ( 𝑼 k ⊤ ​ 𝒁 ) ⊤ ​ ( 𝑼 k ⊤ ​ 𝒁 ) ) , ∀ k ∈ [ K ] , \\displaystyle\\mathrm{SSA}\\left(\\bm{Z}\\mid\\bm{U}_{k}\\right)\\doteq(\\bm{U}_{k}^{\\top}\\bm{Z})\\mathrm{softm", "snippet": "SSA ​ ( 𝒁 ∣ 𝑼 k ) ≐ ( 𝑼 k ⊤ ​ 𝒁 ) ​ softmax ​ ( ( 𝑼 k ⊤ ​ 𝒁 ) ⊤ ​ ( 𝑼 k ⊤ ​ 𝒁 ) ) , ∀ k ∈ [ K ] , \\displaystyle\\mathrm{SSA}\\left(\\bm{Z}\\mid\\bm{U}_{k}\\right)\\doteq(\\bm{U}_{k}^{\\top}\\bm{Z})\\mathrm{softmax}\\left((\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{U}_{k}^{\\top}\\bm{Z})\\right),\\ \\for"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx49", "title": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ⁡ ‖ 𝒁 ‖ 1 subject to 𝒁 ℓ + 1 / 2 = 𝑫 ℓ ​ 𝒁 . \\displaystyle\\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\|\\bm{Z}\\|_{1}\\quad\\mbox{subject to}\\quad\\bm{Z}^{\\ell+1/2}=\\bm{D}", "snippet": "𝒁 ℓ + 1 ≈ arg ​ min 𝒁 ⁡ ‖ 𝒁 ‖ 1 subject to 𝒁 ℓ + 1 / 2 = 𝑫 ℓ ​ 𝒁 . \\displaystyle\\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\|\\bm{Z}\\|_{1}\\quad\\mbox{subject to}\\quad\\bm{Z}^{\\ell+1/2}=\\bm{D}^{\\ell}\\bm{Z}. bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx50", "title": "𝒁 ℓ + 1 \\displaystyle\\bm{Z}^{\\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = ISTA ​ ( 𝒁 ℓ + 1 / 2 ∣ 𝑫 ℓ ) , \\displaystyle=\\mathrm{ISTA}({\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell", "snippet": "𝒁 ℓ + 1 \\displaystyle\\bm{Z}^{\\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = ISTA ​ ( 𝒁 ℓ + 1 / 2 ∣ 𝑫 ℓ ) , \\displaystyle=\\mathrm{ISTA}({\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell}}), = roman_ISTA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POST"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx51", "title": "𝒛 i = 𝑼 k ​ 𝒂 i ⏟ 𝐬𝐢𝐠𝐧𝐚𝐥 + ∑ j ≠ k K 𝑼 j ​ 𝒆 i , j ⏟ 𝐧𝐨𝐢𝐬𝐞 , ∀ i ∈ C k , \\displaystyle\\bm{z}_{i}=\\underbrace{\\bm{U}_{k}\\bm{a}_{i}}_{\\bf signal}+\\underbrace{\\sum_{j\\neq k}^{K}\\bm{U}_{j}\\bm{e}_{i,j}}_{\\", "snippet": "𝒛 i = 𝑼 k ​ 𝒂 i ⏟ 𝐬𝐢𝐠𝐧𝐚𝐥 + ∑ j ≠ k K 𝑼 j ​ 𝒆 i , j ⏟ 𝐧𝐨𝐢𝐬𝐞 , ∀ i ∈ C k , \\displaystyle\\bm{z}_{i}=\\underbrace{\\bm{U}_{k}\\bm{a}_{i}}_{\\bf signal}+\\underbrace{\\sum_{j\\neq k}^{K}\\bm{U}_{j}\\bm{e}_{i,j}}_{\\bf noise},\\ \\forall i\\in C_{k}, bold_italic_z start_POSTSUBSCRIPT italic_i end_P"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx52", "title": "𝒁 ( ℓ + 1 ) = 𝒁 ( ℓ ) + η ​ ∑ k = 1 K 𝑼 k ​ 𝑼 k T ​ 𝒁 ( ℓ ) ​ φ ​ ( 𝒁 ( ℓ ) T ​ 𝑼 k ​ 𝑼 k T ​ 𝒁 ( ℓ ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z", "snippet": "𝒁 ( ℓ + 1 ) = 𝒁 ( ℓ ) + η ​ ∑ k = 1 K 𝑼 k ​ 𝑼 k T ​ 𝒁 ( ℓ ) ​ φ ​ ( 𝒁 ( ℓ ) T ​ 𝑼 k ​ 𝑼 k T ​ 𝒁 ( ℓ ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\varphi\\left(\\bm{Z}^{(\\ell)^{T}}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx53", "title": "SNR ​ ( 𝒁 k ( ℓ ) ) ≐ ‖ 𝑼 k ​ 𝑼 k T ​ 𝒁 k ( ℓ ) ‖ F ‖ ( 𝑰 − 𝑼 k ​ 𝑼 k T ) ​ 𝒁 k ( ℓ ) ‖ F , ∀ k ∈ [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)})\\doteq\\frac{\\|\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}_{k}", "snippet": "SNR ​ ( 𝒁 k ( ℓ ) ) ≐ ‖ 𝑼 k ​ 𝑼 k T ​ 𝒁 k ( ℓ ) ‖ F ‖ ( 𝑰 − 𝑼 k ​ 𝑼 k T ) ​ 𝒁 k ( ℓ ) ‖ F , ∀ k ∈ [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)})\\doteq\\frac{\\|\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}_{k}^{(\\ell)}\\|_{F}}{\\|(\\bm{I}-\\bm{U}_{k}\\bm{U}_{k}^{T})\\bm{Z}_{k}^{(\\ell)}\\|_{F}},\\"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx54", "title": "[ 𝑼 1 … 𝑼 K ] ∈ 𝒪 d × K ​ p . \\displaystyle\\begin{bmatrix}\\bm{U}_{1}&\\dots&\\bm{U}_{K}\\end{bmatrix}\\in\\mathcal{O}^{d\\times Kp}. [ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_", "snippet": "[ 𝑼 1 … 𝑼 K ] ∈ 𝒪 d × K ​ p . \\displaystyle\\begin{bmatrix}\\bm{U}_{1}&\\dots&\\bm{U}_{K}\\end{bmatrix}\\in\\mathcal{O}^{d\\times Kp}. [ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL bold_italic_U start_POST"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx55", "title": "τ ∈ ( 1 2 , 1 1 + N ​ exp ⁡ ( − 9 ​ p / 32 ) ] . \\displaystyle\\tau\\in\\left(\\frac{1}{2},\\frac{1}{1+N\\exp(-9p/32)}\\right]. italic_τ ∈ ( divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG ", "snippet": "τ ∈ ( 1 2 , 1 1 + N ​ exp ⁡ ( − 9 ​ p / 32 ) ] . \\displaystyle\\tau\\in\\left(\\frac{1}{2},\\frac{1}{1+N\\exp(-9p/32)}\\right]. italic_τ ∈ ( divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG 1 end_ARG start_ARG 1 + italic_N roman_exp ( - 9 italic_p / 32 ) end_ARG ] ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx56", "title": "SNR ​ ( 𝒁 k ( ℓ + 1 ) ) = ( 1 + η ​ τ ) ​ SNR ​ ( 𝒁 k ( ℓ ) ) , ∀ k ∈ [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell+1)})=(1+\\eta\\tau)\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)}),\\ \\forall k\\in[K]. roman_SN", "snippet": "SNR ​ ( 𝒁 k ( ℓ + 1 ) ) = ( 1 + η ​ τ ) ​ SNR ​ ( 𝒁 k ( ℓ ) ) , ∀ k ∈ [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell+1)})=(1+\\eta\\tau)\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)}),\\ \\forall k\\in[K]. roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx57", "title": "∇ 𝒁 R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) = 1 n ​ ∑ k = 1 K 𝑼 k ​ Diag ​ ( ∇ f ​ [ ( 𝑼 k ⊤ ​ 𝒁 ) ⊙ 2 ​ 𝝅 k ⟨ 𝝅 k , 𝟏 ⟩ ] ) ⏟ ≐ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒁 ​ Diag ​ ( 𝝅 k ) . \\displaystyle\\nabla_{\\bm", "snippet": "∇ 𝒁 R c , f var ​ ( 𝒁 , 𝚷 ∣ 𝑼 [ K ] ) = 1 n ​ ∑ k = 1 K 𝑼 k ​ Diag ​ ( ∇ f ​ [ ( 𝑼 k ⊤ ​ 𝒁 ) ⊙ 2 ​ 𝝅 k ⟨ 𝝅 k , 𝟏 ⟩ ] ) ⏟ ≐ 𝑫 ​ ( 𝒁 , 𝝅 k ∣ 𝑼 k ) ​ 𝑼 k ⊤ ​ 𝒁 ​ Diag ​ ( 𝝅 k ) . \\displaystyle\\nabla_{\\bm{Z}}R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})=\\frac{1}{n}\\sum_{k=1}^{K}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx58", "title": "𝚷 = [ 𝝂 ​ ( 𝒛 1 ∣ 𝑼 [ K ] ) ⊤ ⋮ 𝝂 ​ ( 𝒛 n ∣ 𝑼 [ K ] ) ⊤ ] , where 𝝂 ​ ( 𝒛 j ∣ 𝑼 [ K ] ) ≐ softmax ⁡ ( 1 2 ​ η ​ [ ‖ 𝑼 1 ⊤ ​ 𝒛 j ‖ 2 2 ⋮ ‖ 𝑼 K ⊤ ​ 𝒛 j ‖ 2 2 ] ) , ∀ j ∈ [ n ] , \\displaystyle\\bm{\\Pi}=\\b", "snippet": "𝚷 = [ 𝝂 ​ ( 𝒛 1 ∣ 𝑼 [ K ] ) ⊤ ⋮ 𝝂 ​ ( 𝒛 n ∣ 𝑼 [ K ] ) ⊤ ] , where 𝝂 ​ ( 𝒛 j ∣ 𝑼 [ K ] ) ≐ softmax ⁡ ( 1 2 ​ η ​ [ ‖ 𝑼 1 ⊤ ​ 𝒛 j ‖ 2 2 ⋮ ‖ 𝑼 K ⊤ ​ 𝒛 j ‖ 2 2 ] ) , ∀ j ∈ [ n ] , \\displaystyle\\bm{\\Pi}=\\begin{bmatrix}\\bm{\\nu}(\\bm{z}_{1}\\mid\\bm{U}_{[K]})^{\\top}\\\\ \\vdots\\\\ \\bm{\\nu}(\\bm"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx59", "title": "R ​ ( 𝒁 ) = log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 T ) . \\displaystyle R(\\bm{Z})=\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}\\bm{Z}^{T}\\right). italic_R ( bold_italic_Z ) = roman_log roman_det ( bold_italic_I + italic_α bold", "snippet": "R ​ ( 𝒁 ) = log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 T ) . \\displaystyle R(\\bm{Z})=\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}\\bm{Z}^{T}\\right). italic_R ( bold_italic_Z ) = roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx60", "title": "∇ 2 R ​ ( 𝒁 ) ​ [ 𝑫 , 𝑫 ] = α ​ Tr ​ ( 𝑿 − 1 ​ 𝑫 ​ 𝑫 T ) − α 2 2 ​ Tr ​ ( 𝑿 − 1 ​ ( 𝒁 ​ 𝑫 T + 𝑫 ​ 𝒁 T ) ​ 𝑿 − 1 ​ ( 𝒁 ​ 𝑫 T + 𝑫 ​ 𝒁 T ) ) , \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]=\\alpha\\mathr", "snippet": "∇ 2 R ​ ( 𝒁 ) ​ [ 𝑫 , 𝑫 ] = α ​ Tr ​ ( 𝑿 − 1 ​ 𝑫 ​ 𝑫 T ) − α 2 2 ​ Tr ​ ( 𝑿 − 1 ​ ( 𝒁 ​ 𝑫 T + 𝑫 ​ 𝒁 T ) ​ 𝑿 − 1 ​ ( 𝒁 ​ 𝑫 T + 𝑫 ​ 𝒁 T ) ) , \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]=\\alpha\\mathrm{Tr}\\left(\\bm{X}^{-1}\\bm{D}\\bm{D}^{T}\\right)-\\frac{\\alpha^{2}}{2}\\mathrm{Tr}\\le"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx61", "title": "∇ 2 R ​ ( 𝒁 ) ​ [ 𝑫 , 𝑫 ] ≐ ⟨ lim t → 0 ∇ R ​ ( 𝒁 + t ​ 𝑫 ) − ∇ R ​ ( 𝒁 ) t , 𝑫 ⟩ . \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]\\doteq\\left\\langle\\lim_{t\\to 0}\\frac{\\nabla R(\\bm{Z}+t\\bm{D})-\\nabla ", "snippet": "∇ 2 R ​ ( 𝒁 ) ​ [ 𝑫 , 𝑫 ] ≐ ⟨ lim t → 0 ∇ R ​ ( 𝒁 + t ​ 𝑫 ) − ∇ R ​ ( 𝒁 ) t , 𝑫 ⟩ . \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]\\doteq\\left\\langle\\lim_{t\\to 0}\\frac{\\nabla R(\\bm{Z}+t\\bm{D})-\\nabla R(\\bm{Z})}{t},\\bm{D}\\right\\rangle. ∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx62", "title": "R ​ ( 𝒁 ) ≤ ∑ k = 1 K log ​ det ( 𝑰 + α ​ 𝒁 k ​ 𝒁 k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_italic_Z ) ≤ ∑ start_POST", "snippet": "R ​ ( 𝒁 ) ≤ ∑ k = 1 K log ​ det ( 𝑰 + α ​ 𝒁 k ​ 𝒁 k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_italic_Z ) ≤ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POST"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx63", "title": "f ​ ( 𝒁 k ) = 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 k ​ 𝒁 k T ) − m k 2 ​ m ​ log ​ det ( 𝑰 + α k ​ 𝒁 k ​ 𝒁 k T ) − λ 2 ​ ‖ 𝒁 k ‖ F 2 . \\displaystyle f(\\bm{Z}_{k})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{", "snippet": "f ​ ( 𝒁 k ) = 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 k ​ 𝒁 k T ) − m k 2 ​ m ​ log ​ det ( 𝑰 + α k ​ 𝒁 k ​ 𝒁 k T ) − λ 2 ​ ‖ 𝒁 k ‖ F 2 . \\displaystyle f(\\bm{Z}_{k})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right)-\\frac{m_{k}}{2m}\\log\\det\\left(\\bm{I}+\\alpha_{k}\\bm{Z}_{k"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx64", "title": "𝒁 k = 𝑷 k ​ 𝚺 k ​ 𝑸 k T = [ 𝑷 k , 1 𝑷 k , 2 ] ​ [ 𝚺 ~ k 𝟎 𝟎 𝟎 ] ​ [ 𝑸 k , 1 T 𝑸 k , 2 T ] , \\displaystyle\\bm{Z}_{k}=\\bm{P}_{k}\\bm{\\Sigma}_{k}\\bm{Q}_{k}^{T}=\\begin{bmatrix}\\bm{P}_{k,1}&\\bm{P}_{k,2}\\end", "snippet": "𝒁 k = 𝑷 k ​ 𝚺 k ​ 𝑸 k T = [ 𝑷 k , 1 𝑷 k , 2 ] ​ [ 𝚺 ~ k 𝟎 𝟎 𝟎 ] ​ [ 𝑸 k , 1 T 𝑸 k , 2 T ] , \\displaystyle\\bm{Z}_{k}=\\bm{P}_{k}\\bm{\\Sigma}_{k}\\bm{Q}_{k}^{T}=\\begin{bmatrix}\\bm{P}_{k,1}&\\bm{P}_{k,2}\\end{bmatrix}\\begin{bmatrix}\\tilde{\\bm{\\Sigma}}_{k}&\\bm{0}\\\\ \\bm{0}&\\bm{0}\\end{bmatr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx65", "title": "( 𝑰 − 𝑨 ) − 1 = ∑ k = 1 ∞ 𝑨 k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ∑ start_PO", "snippet": "( 𝑰 − 𝑨 ) − 1 = ∑ k = 1 ∞ 𝑨 k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPER"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#top", "title": "Chapter 5 Consistent and Self-Consistent Representations", "snippet": ""}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1", "title": "5.1 Learning Consistent Representations", "snippet": "5.1 Learning Consistent Representations Here we give a formal definition of consistent representations, which are closely related to the concept of autoencoding. Definition 5.1 (Consistent Representations) . Given data 𝑿 \\bm{X} bold_italic_X , an consistent representation is a pa"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2", "title": "5.2 Learning Self-Consistent Representations", "snippet": "5.2 Learning Self-Consistent Representations In earlier chapters, we have studied methods that would allow us to learn a low-dimensional distribution via (lossy) compression. As we have mentioned in Chapter 1 and demonstrated in the previous chapters, the progresses made in machi"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3", "title": "5.3 Continuous Learning Self-Consistent Representations", "snippet": "5.3 Continuous Learning Self-Consistent Representations 5.3.1 Class-wise Incremental Learning As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generati"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4", "title": "5.4 Summary and Notes", "snippet": "5.4 Summary and Notes Historically, autoencoding has been one of the important drivers of research innovation in neural networks for learning, although the most practically impressive demonstrations of deep learning have probably been in other domains (such as discriminative clas"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5", "title": "5.5 Exercises and Extensions", "snippet": "5.5 Exercises and Extensions Exercise 5.1 (Conceptual Understanding of Manifold Flattening) . Consider data lying on a curved manifold ℳ \\mathcal{M} caligraphic_M embedded in ℝ D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1", "title": "5.1.1 Linear Autoencoding via PCA", "snippet": "5.1.1 Linear Autoencoding via PCA According to [ Bal11 ] , the phrase “autoencoder” was first introduced by Hinton and Rumelhart [ RHW86 ] so that a deep representation can be learned via back propagation (BP) in a self-supervision fashion—reconstructing the original data is the "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2", "title": "5.1.2 Nonlinear PCA and Autoencoding", "snippet": "5.1.2 Nonlinear PCA and Autoencoding Of course, one should expect that things will no longer be so simple when we deal with more complex distributions whose underlying low-dimensional structure could be nonlinear. Data on a Nonlinear Submanifold. So, to move beyond the linear str"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS3", "title": "5.1.3 Sparse Autoencoding", "snippet": "5.1.3 Sparse Autoencoding In the above autoencoding schemes, the dimension of the feature space d d italic_d is typically chosen to be much lower than that the original data space D D italic_D so as to explicitly enforce or promote the learned representation to be low-dimensional"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4", "title": "5.1.4 Variational Autoencoding", "snippet": "5.1.4 Variational Autoencoding In the classical conception of autoencoding, following Hinton and Rumelhart [ RHW86 ] , the data distribution plays a very minor role in the formulation, in spite of its centrality to the representation we ultimately learn. Indeed, in the naive fram"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1", "title": "5.2.1 Closed-Loop Transcription via Stackelberg Games", "snippet": "5.2.1 Closed-Loop Transcription via Stackelberg Games How do we try to ensure a learned representation is self-consistent? As usual, let us assume 𝑿 = ∪ k = 1 K 𝑿 k \\bm{X}=\\cup_{k=1}^{K}\\bm{X}_{k} bold_italic_X = ∪ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPE"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2", "title": "5.2.2 A Mixture of Low-Dimensional Gaussians", "snippet": "5.2.2 A Mixture of Low-Dimensional Gaussians In the above, we have argued that it is possible to formulate the problem of learning a data distribution as a closed-loop autoencoding problem. We also saw empirically that such a scheme seems to work. The remaining question is when a"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1", "title": "5.3.1 Class-wise Incremental Learning", "snippet": "5.3.1 Class-wise Incremental Learning As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2", "title": "5.3.2 Sample-wise Continuous Unsupervised Learning", "snippet": "5.3.2 Sample-wise Continuous Unsupervised Learning As we know, the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information, with the CTRL-Binary program: max 𝜽 ⁡ min 𝜼 Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.SSS0.Px1", "title": "Online PCA.", "snippet": "Online PCA. Notice that in the above construction, the linear transform 𝑼 \\bm{U} bold_italic_U used for the encoding and decoding is computed “offline” from all the input data before hand. One question is whether this transform can be learned “online” as the input data come in or"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px1", "title": "Data on a Nonlinear Submanifold.", "snippet": "Data on a Nonlinear Submanifold. So, to move beyond the linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold ℳ \\mathcal{M} caligraphic_M . The intrinsic dimension of the submanifold, say d d italic_d , is typically much lower "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px2", "title": "A Classical Attempt via a Two-Layer Network.", "snippet": "A Classical Attempt via a Two-Layer Network. As we have seen above, in the case of PCA, a one-layer linear neural network is sufficient. That is no longer the case for NLPCA. In 1991, Kramer [ Kra91 ] proposed to solve NLPCA by using a two-layer neural network to represent the en"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px3", "title": "Manifold Flattening via a Deeper Network.", "snippet": "Manifold Flattening via a Deeper Network. Based on the modern practice of deep networks, such classical shallow and wide network architectures are known to be rather difficult to train effectively and efficiently via back propagation (BP), partly due to the diminishing gradient o"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px1", "title": "Measuring distance in the feature space.", "snippet": "Measuring distance in the feature space. However, as we have discussed above, if we do not have the option to compute the distance between 𝑿 \\bm{X} bold_italic_X and 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we are left with the option of comparing their correspond"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2", "title": "Encoder and decoder as a two-player game.", "snippet": "Encoder and decoder as a two-player game. Obviously, to ensure the learned auto-encoding to be self-consistent, the main goal of the decoder g ​ ( ⋅ , 𝜼 ) g(\\cdot,\\bm{\\eta}) italic_g ( ⋅ , bold_italic_η ) is to minimize the distance between 𝒁 \\bm{Z} bold_italic_Z and 𝒁 ^ \\hat{\\bm"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px3", "title": "Visualizing correlation of features 𝒁 \\bm{Z} bold_italic_Z and decoded features 𝒁 ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG .", "snippet": "Visualizing correlation of features 𝒁 \\bm{Z} bold_italic_Z and decoded features 𝒁 ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG . We visualize the cosine similarity between 𝒁 \\bm{Z} bold_italic_Z and 𝒁 ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG learned from the m"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px4", "title": "Visualizing auto-encoding of the data 𝑿 \\bm{X} bold_italic_X and the decoded 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG .", "snippet": "Visualizing auto-encoding of the data 𝑿 \\bm{X} bold_italic_X and the decoded 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG . We compare some representative 𝑿 \\bm{X} bold_italic_X and 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST, CIFAR-10 and ImageNet ("}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px1", "title": "LDR memory sampling and replay.", "snippet": "LDR memory sampling and replay. The simple linear structures of LDR make it uniquely suited for incremental learning: the distribution of features 𝒁 j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of each previously learned class can be explicitly and co"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2", "title": "Incremental learning LDR with an old-memory constraint.", "snippet": "Incremental learning LDR with an old-memory constraint. Notice that, with the learned auto-encoding ( 5.2.9 ), one can replay and use the images, say 𝑿 ^ o ​ l ​ d = g ​ ( 𝒁 o ​ l ​ d , 𝜼 ) \\hat{\\bm{X}}_{old}=g(\\bm{Z}_{old},\\bm{\\eta}) over^ start_ARG bold_italic_X end_ARG start_P"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px3", "title": "Jointly optimal memory via incremental reviewing.", "snippet": "Jointly optimal memory via incremental reviewing. As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for all classes cannot rely on graceful forgetting alone. "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px4", "title": "Experimental verification.", "snippet": "Experimental verification. We show some experimental results on the following datasets: MNIST [ LBB+98a ] and CIFAR-10 [ KNH14 ] . All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFAR-10, the 10 classes are split into 5 tasks with 2 cl"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px5", "title": "Visualizing auto-encoding properties.", "snippet": "Visualizing auto-encoding properties. We begin by qualitatively visualizing some representative images 𝑿 \\bm{X} bold_italic_X and the corresponding replayed 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST and CIFAR-10. The model is learned incrementally with the d"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px6", "title": "Principal subspaces of the learned features.", "snippet": "Principal subspaces of the learned features. Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features 𝒁 j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT for each cl"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px7", "title": "Replay images of samples from principal components.", "snippet": "Replay images of samples from principal components. Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. Figure 5.13 shows the images replayed from sampled features along the "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px8", "title": "Effectiveness of incremental reviewing.", "snippet": "Effectiveness of incremental reviewing. We verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described before. Experiments are conducted on CIFAR-10, with 10 steps. Figure 5.14 left shows replayed images o"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px1", "title": "Sample-wise constraints for unsupervised transcription.", "snippet": "Sample-wise constraints for unsupervised transcription. To improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game ( 5.3.6 ). For simplicity and uniformity"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px2", "title": "Sample-wise self-consistency via closed-loop transcription.", "snippet": "Sample-wise self-consistency via closed-loop transcription. First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote 𝒙 ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG to be close to 𝒙 \\bm{x} bold_italic_x for each "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px3", "title": "Self-supervision via compressing augmented samples.", "snippet": "Self-supervision via compressing augmented samples. Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusion etc) as one “class”—a basi"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px4", "title": "Unsupervised representation learning via closed-loop transcription.", "snippet": "Unsupervised representation learning via closed-loop transcription. So far, we know the CTRL-Binary objective Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) \\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_ital"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5", "title": "Unsupervised CTRL.", "snippet": "Unsupervised CTRL. Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as unsupervised CTRL (u-CTRL): max θ ⁡ min η \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_θ"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px6", "title": "Unsupervised conditional image generation via rate reduction.", "snippet": "Unsupervised conditional image generation via rate reduction. The highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful clusters, and model "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4.SS0.SSS0.Px1", "title": "Shallow vs. deep neural networks, for autoencoding and more.", "snippet": "Shallow vs. deep neural networks, for autoencoding and more. In Section 5.1.2 , we discussed Cybenko’s universal approximation theorem and how it states that in principle, a neural network with a single hidden layer (and suitable elementwise nonlinearities) is sufficient to appro"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition1", "title": "Definition 5.1 (Consistent Representations) .", "snippet": "Definition 5.1 (Consistent Representations) . Given data 𝑿 \\bm{X} bold_italic_X , an consistent representation is a pair of functions ( f : 𝒳 → 𝒵 , g : 𝒵 → 𝒳 ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X → caligraphic_Z , italic_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample1", "title": "Example 5.1 (Normalized Hebbian learning scheme for PCA) .", "snippet": "Example 5.1 (Normalized Hebbian learning scheme for PCA) . Consider a sequence of i.i.d. random vectors 𝒙 1 , … , 𝒙 i , … ∈ ℝ n \\bm{x}_{1},\\ldots,\\bm{x}_{i},\\ldots\\in\\mathbb{R}^{n} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT itali"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition2", "title": "Definition 5.2 (Self-Consistent Representation) .", "snippet": "Definition 5.2 (Self-Consistent Representation) . Given data 𝑿 \\bm{X} bold_italic_X , we call an self-consistent representation to be a pair of functions ( f : 𝒳 → 𝒵 , g : 𝒵 → 𝒳 ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X → cal"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample2", "title": "Example 5.2 .", "snippet": "Example 5.2 . One may wonder why we need the mapping f ​ ( ⋅ , 𝜽 ) f(\\cdot,\\bm{\\theta}) italic_f ( ⋅ , bold_italic_θ ) to function as a discriminator between 𝑿 \\bm{X} bold_italic_X and 𝑿 ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG by maximizing max 𝜽 ⁡ Δ ​ R ϵ ​ ( f ​ ( "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmtheorem1", "title": "Theorem 5.1 ( [ PPC+23 ] , Abridged) .", "snippet": "Theorem 5.1 ( [ PPC+23 ] , Abridged) . Suppose that 𝐗 \\bm{X} bold_italic_X is distributed on a mixture of subspaces. Under certain realistic yet technical conditions, it holds that all sequential equilibria of ( 5.2.22 ) obey: • The 𝒁 k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIP"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise1", "title": "Exercise 5.1 (Conceptual Understanding of Manifold Flattening) .", "snippet": "Exercise 5.1 (Conceptual Understanding of Manifold Flattening) . Consider data lying on a curved manifold ℳ \\mathcal{M} caligraphic_M embedded in ℝ D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface in 3 3 3 -dimensional space)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise2", "title": "Exercise 5.2 (Reproduce Closed-Loop Transcription) .", "snippet": "Exercise 5.2 (Reproduce Closed-Loop Transcription) . Implement a closed-loop transcription pipeline for representation learning on the CIFAR-10 dataset following the methodology in Section 5.2.1 . Reference [ DTL+22 ] for useful hyperparameters and architecture settings. Reproduc"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E1", "title": "f : 𝑿 → f 0 𝒁 0 → ⋯ → 𝒁 ℓ → f ℓ 𝒁 ℓ + 1 → ⋯ → 𝒁 L = 𝒁 . f\\colon\\bm{X}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526pt", "snippet": "f : 𝑿 → f 0 𝒁 0 → ⋯ → 𝒁 ℓ → f ℓ 𝒁 ℓ + 1 → ⋯ → 𝒁 L = 𝒁 . f\\colon\\bm{X}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{Z}^{\\ell+1}\\rightarrow\\cdots\\to\\bm{Z}^{L}=\\bm{Z}. i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E2", "title": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}} bold_italic_X start_ARROW start_O", "snippet": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}} bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z star"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E3", "title": "d ​ ( 𝑿 , 𝑿 ^ ) . d(\\bm{X},\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (5.0.3)", "snippet": "d ​ ( 𝑿 , 𝑿 ^ ) . d(\\bm{X},\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (5.0.3)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E4", "title": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ → ℰ = f 𝒁 ^ ​ ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hski", "snippet": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ → ℰ = f 𝒁 ^ ​ ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\hat{\\bm{Z}}? bold_italic_X start_ARROW"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E1", "title": "𝑿 ^ ≐ g ​ ( 𝒁 ) = g ​ ( f ​ ( 𝑿 ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ≐ italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X ) ) (5.1.1)", "snippet": "𝑿 ^ ≐ g ​ ( 𝒁 ) = g ​ ( f ​ ( 𝑿 ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ≐ italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X ) ) (5.1.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E2", "title": "Δ ​ R ϵ ​ ( 𝒁 ) \\Delta R_{\\epsilon}(\\bm{Z}) roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) (5.1.2)", "snippet": "Δ ​ R ϵ ​ ( 𝒁 ) \\Delta R_{\\epsilon}(\\bm{Z}) roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) (5.1.2)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E3", "title": "d ​ ( 𝑿 , 𝑿 ^ ) = 𝔼 ​ [ ‖ 𝑿 − 𝑿 ^ ‖ 2 2 ] . d(\\bm{X},\\hat{\\bm{X}})=\\mathbb{E}[\\|\\bm{X}-\\hat{\\bm{X}}\\|_{2}^{2}]. italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ∥ bo", "snippet": "d ​ ( 𝑿 , 𝑿 ^ ) = 𝔼 ​ [ ‖ 𝑿 − 𝑿 ^ ‖ 2 2 ] . d(\\bm{X},\\hat{\\bm{X}})=\\mathbb{E}[\\|\\bm{X}-\\hat{\\bm{X}}\\|_{2}^{2}]. italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ∥ bold_italic_X - over^ start_ARG bold_italic_X end_ARG ∥ start_POSTSUBSCRIPT 2 end_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E4", "title": "d ​ ( 𝑿 , 𝑿 ^ ) = 𝒟 K ​ L ​ ( 𝑿 ∥ 𝑿 ^ ) . d(\\bm{X},\\hat{\\bm{X}})=\\mathcal{D}_{KL}(\\bm{X}\\|\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = caligraphic_D start_POSTSU", "snippet": "d ​ ( 𝑿 , 𝑿 ^ ) = 𝒟 K ​ L ​ ( 𝑿 ∥ 𝑿 ^ ) . d(\\bm{X},\\hat{\\bm{X}})=\\mathcal{D}_{KL}(\\bm{X}\\|\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( bold_italic_X ∥ over^ start_ARG bo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E5", "title": "min f , g ⁡ [ − Δ ​ R ϵ ​ ( 𝒁 ) + d ​ ( 𝑿 , 𝑿 ^ ) ] . \\min_{f,g}[-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Δ i", "snippet": "min f , g ⁡ [ − Δ ​ R ϵ ​ ( 𝒁 ) + d ​ ( 𝑿 , 𝑿 ^ ) ] . \\min_{f,g}[-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + itali"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E6", "title": "𝑿 → ℰ = 𝑼 ⊤ 𝒁 → 𝒟 = 𝑼 𝑿 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic", "snippet": "𝑿 → ℰ = 𝑼 ⊤ 𝒁 → 𝒟 = 𝑼 𝑿 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E7", "title": "min 𝑼 ⊤ ​ 𝑼 = 𝑰 ⁡ 𝔼 𝒙 ​ [ ‖ 𝒙 − 𝑼 ​ 𝑼 ⊤ ​ 𝒙 ‖ 2 2 ] \\min_{\\bm{U}^{\\top}\\bm{U}=\\bm{I}}\\,\\mathbb{E}_{\\bm{x}}\\left[\\left\\|\\bm{x}-\\bm{U}\\bm{U}^{\\top}\\bm{x}\\right\\|_{2}^{2}\\right] roman_min start_POSTSUBSC", "snippet": "min 𝑼 ⊤ ​ 𝑼 = 𝑰 ⁡ 𝔼 𝒙 ​ [ ‖ 𝒙 − 𝑼 ​ 𝑼 ⊤ ​ 𝒙 ‖ 2 2 ] \\min_{\\bm{U}^{\\top}\\bm{U}=\\bm{I}}\\,\\mathbb{E}_{\\bm{x}}\\left[\\left\\|\\bm{x}-\\bm{U}\\bm{U}^{\\top}\\bm{x}\\right\\|_{2}^{2}\\right] roman_min start_POSTSUBSCRIPT bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U = b"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E8", "title": "η i = 𝒖 i T ​ 𝒙 i \\eta_{i}=\\bm{u}_{i}^{T}\\bm{x}_{i} italic_η start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_", "snippet": "η i = 𝒖 i T ​ 𝒙 i \\eta_{i}=\\bm{u}_{i}^{T}\\bm{x}_{i} italic_η start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRI"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E9", "title": "𝒖 i + 1 = 𝒖 i + γ ​ η i ​ 𝒙 i ‖ 𝒖 i + γ ​ η i ​ 𝒙 i ‖ 2 \\bm{u}_{i+1}=\\frac{\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}}{\\|\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}\\|_{2}} bold_italic_u start_POSTSUBSCRIPT italic_i ", "snippet": "𝒖 i + 1 = 𝒖 i + γ ​ η i ​ 𝒙 i ‖ 𝒖 i + γ ​ η i ​ 𝒙 i ‖ 2 \\bm{u}_{i+1}=\\frac{\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}}{\\|\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}\\|_{2}} bold_italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_u start_POSTSUBSCRIPT itali"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E10", "title": "𝒛 = 𝑾 2 ​ σ ​ ( 𝑾 1 ​ 𝒙 + 𝒃 ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ ( bold_italic_W start_POSTSUBSCRIPT 1 e", "snippet": "𝒛 = 𝑾 2 ​ σ ​ ( 𝑾 1 ​ 𝒙 + 𝒃 ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_x + bold_italic_b ) , (5.1.10)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E11", "title": "σ ​ ( x ) = 1 1 + e − x . \\sigma(x)=\\frac{1}{1+e^{-x}}. italic_σ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG . (5.1.11", "snippet": "σ ​ ( x ) = 1 1 + e − x . \\sigma(x)=\\frac{1}{1+e^{-x}}. italic_σ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG . (5.1.11)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E12", "title": "min 𝜽 ⁡ 𝔼 ​ [ ‖ 𝒙 − 𝒙 ^ ​ ( 𝜽 ) ‖ 2 2 ] . \\min_{\\bm{\\theta}}\\mathbb{E}[\\|\\bm{x}-\\hat{\\bm{x}}(\\bm{\\theta})\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E [ ∥ bol", "snippet": "min 𝜽 ⁡ 𝔼 ​ [ ‖ 𝒙 − 𝒙 ^ ​ ( 𝜽 ) ‖ 2 2 ] . \\min_{\\bm{\\theta}}\\mathbb{E}[\\|\\bm{x}-\\hat{\\bm{x}}(\\bm{\\theta})\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E [ ∥ bold_italic_x - over^ start_ARG bold_italic_x end_ARG ( bold_italic_θ ) ∥ start_POS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E13", "title": "min f , g ⁡ [ ‖ 𝒁 ‖ 0 − Δ ​ R ϵ ​ ( 𝒁 ) + d ​ ( 𝑿 , 𝑿 ^ ) ] , \\min_{f,g}[\\|\\bm{Z}\\|_{0}-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})], roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTS", "snippet": "min f , g ⁡ [ ‖ 𝒁 ‖ 0 − Δ ​ R ϵ ​ ( 𝒁 ) + d ​ ( 𝑿 , 𝑿 ^ ) ] , \\min_{f,g}[\\|\\bm{Z}\\|_{0}-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})], roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - roman_Δ i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex1", "title": "max 𝜽 ⁡ 𝔼 𝒙 ​ [ log ⁡ p ​ ( 𝒙 ; 𝜽 ) ] . \\max_{\\bm{\\theta}}\\,\\mathbb{E}_{\\bm{x}}[\\log p(\\bm{x};\\,\\bm{\\theta})]. roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBS", "snippet": "max 𝜽 ⁡ 𝔼 𝒙 ​ [ log ⁡ p ​ ( 𝒙 ; 𝜽 ) ] . \\max_{\\bm{\\theta}}\\,\\mathbb{E}_{\\bm{x}}[\\log p(\\bm{x};\\,\\bm{\\theta})]. roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ; bol"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex6", "title": "𝒛 ∣ 𝒙 ∼ 𝒩 ​ ( f 1 ​ ( 𝒙 ) , diag ⁡ ( e f 2 ​ ( 𝒙 ) ) ​ 𝑰 ) . \\bm{z}\\mid\\bm{x}\\sim\\mathcal{N}(f_{1}(\\bm{x}),\\operatorname{diag}(e^{f_{2}(\\bm{x})})\\bm{I}). bold_italic_z ∣ bold_italic_x ∼ caligraphic_N ", "snippet": "𝒛 ∣ 𝒙 ∼ 𝒩 ​ ( f 1 ​ ( 𝒙 ) , diag ⁡ ( e f 2 ​ ( 𝒙 ) ) ​ 𝑰 ) . \\bm{z}\\mid\\bm{x}\\sim\\mathcal{N}(f_{1}(\\bm{x}),\\operatorname{diag}(e^{f_{2}(\\bm{x})})\\bm{I}). bold_italic_z ∣ bold_italic_x ∼ caligraphic_N ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) , roman_dia"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex9", "title": "log ⁡ p ​ ( 𝒙 ; 𝜽 ) ≥ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] . \\log p(\\bm{x};\\,\\bm{\\theta})\\geq\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{", "snippet": "log ⁡ p ​ ( 𝒙 ; 𝜽 ) ≥ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] . \\log p(\\bm{x};\\,\\bm{\\theta})\\geq\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\right]. roman_l"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E14", "title": "max 𝜽 , 𝜼 ⁡ 𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] . \\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\le", "snippet": "max 𝜽 , 𝜼 ⁡ 𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] . \\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\ri"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex16", "title": "𝒛 = d f 1 ​ ( 𝒙 ) + diag ⁡ ( e 1 2 ​ f 2 ​ ( 𝒙 ) ) ​ 𝒈 , 𝒈 ∼ 𝒩 ​ ( 0 , 𝑰 ) , \\bm{z}=_{\\mathrm{d}}f_{1}(\\bm{x})+\\operatorname{diag}(e^{\\tfrac{1}{2}f_{2}(\\bm{x})})\\bm{g},\\quad\\bm{g}\\sim\\mathcal{N}(0,\\bm", "snippet": "𝒛 = d f 1 ​ ( 𝒙 ) + diag ⁡ ( e 1 2 ​ f 2 ​ ( 𝒙 ) ) ​ 𝒈 , 𝒈 ∼ 𝒩 ​ ( 0 , 𝑰 ) , \\bm{z}=_{\\mathrm{d}}f_{1}(\\bm{x})+\\operatorname{diag}(e^{\\tfrac{1}{2}f_{2}(\\bm{x})})\\bm{g},\\quad\\bm{g}\\sim\\mathcal{N}(0,\\bm{I}), bold_italic_z = start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT italic_f sta"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E1", "title": "computable ⟹ tractable ⟹ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ⟹ tractable ⟹ scalable . (5.2.1)", "snippet": "computable ⟹ tractable ⟹ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ⟹ tractable ⟹ scalable . (5.2.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E2", "title": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW star", "snippet": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ . \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z s"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E3", "title": "𝒁 → g ​ ( 𝒛 , η ) 𝑿 ^ , 𝑿 → d ​ ( 𝒙 , θ ) { 𝟎 , 𝟏 } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\eta)\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptd(\\bm{x},\\theta)\\hskip 5.69054", "snippet": "𝒁 → g ​ ( 𝒛 , η ) 𝑿 ^ , 𝑿 → d ​ ( 𝒙 , θ ) { 𝟎 , 𝟏 } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\eta)\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptd(\\bm{x},\\theta)\\hskip 5.69054pt}\\{\\mathbf{0},\\mathbf{1}\\}. bold_italic_Z start_ARROW start_OVERACCENT italic_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E4", "title": "max θ ⁡ min η ⁡ 𝔼 p ​ ( 𝒙 ) ​ [ log ⁡ d ​ ( 𝒙 , θ ) ] + 𝔼 p ​ ( 𝒛 ) ​ [ 1 − log ⁡ d ​ ( g ​ ( 𝒛 , η ) ⏟ 𝒙 ^ ∼ p g , θ ) ] . \\max_{\\theta}\\min_{\\eta}\\mathbb{E}_{p({\\bm{x}})}\\big{[}\\log d(\\bm{x},\\theta)", "snippet": "max θ ⁡ min η ⁡ 𝔼 p ​ ( 𝒙 ) ​ [ log ⁡ d ​ ( 𝒙 , θ ) ] + 𝔼 p ​ ( 𝒛 ) ​ [ 1 − log ⁡ d ​ ( g ​ ( 𝒛 , η ) ⏟ 𝒙 ^ ∼ p g , θ ) ] . \\max_{\\theta}\\min_{\\eta}\\mathbb{E}_{p({\\bm{x}})}\\big{[}\\log d(\\bm{x},\\theta)\\big{]}+\\mathbb{E}_{p({\\bm{z}})}\\big{[}1-\\log d(\\underbrace{g(\\bm{z},\\eta)}_{\\ha"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E5", "title": "𝒟 J ​ S ​ ( p ​ ( 𝒙 ) , p g ​ ( 𝒙 ^ ) ) = 𝒟 K ​ L ​ ( p ∥ ( p + p g ) / 2 ) + 𝒟 K ​ L ​ ( p g ∥ ( p + p g ) / 2 ) . \\mathcal{D}_{JS}(p(\\bm{x}),p_{g}(\\hat{\\bm{x}}))=\\mathcal{D}_{KL}\\big{(}p\\|(p+p_{g})/", "snippet": "𝒟 J ​ S ​ ( p ​ ( 𝒙 ) , p g ​ ( 𝒙 ^ ) ) = 𝒟 K ​ L ​ ( p ∥ ( p + p g ) / 2 ) + 𝒟 K ​ L ​ ( p g ∥ ( p + p g ) / 2 ) . \\mathcal{D}_{JS}(p(\\bm{x}),p_{g}(\\hat{\\bm{x}}))=\\mathcal{D}_{KL}\\big{(}p\\|(p+p_{g})/{2}\\big{)}+\\mathcal{D}_{KL}\\big{(}p_{g}\\|(p+p_{g})/{2}\\big{)}. caligraphic_D sta"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E6", "title": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ → ℰ = f 𝒁 ^ ​ ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hski", "snippet": "𝑿 → ℰ = f 𝒁 → 𝒟 = g 𝑿 ^ → ℰ = f 𝒁 ^ ​ ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\hat{\\bm{Z}}? bold_italic_X start_ARROW"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E7", "title": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACC", "snippet": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW bold_italic_Z , (5.2.7)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E8", "title": "max 𝒁 ⁡ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ⏟ R ϵ ​ ( 𝒁 ) − ∑ k = 1 K γ k 2 ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 j ​ 𝒁 ⊤ ) ⏟ R ϵ c ​ ( 𝒁 ∣ 𝚷 ) , \\begin{split}\\max_{\\bm{Z}}\\;\\Delta R_{\\e", "snippet": "max 𝒁 ⁡ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝚷 ) ≐ 1 2 ​ log ​ det ( 𝑰 + α ​ 𝒁 ​ 𝒁 ⊤ ) ⏟ R ϵ ​ ( 𝒁 ) − ∑ k = 1 K γ k 2 ​ log ​ det ( 𝑰 + α k ​ 𝒁 ​ 𝚷 j ​ 𝒁 ⊤ ) ⏟ R ϵ c ​ ( 𝒁 ∣ 𝚷 ) , \\begin{split}\\max_{\\bm{Z}}\\;\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi})&\\doteq\\underbrace{\\frac{1}{2}\\log\\det\\Big{(}\\bm{I}+{"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E9", "title": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}, ", "snippet": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_itali"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E10", "title": "Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) ≐ R ϵ ​ ( 𝒁 k ∪ 𝒁 ^ k ) − 1 2 ​ ( R ϵ ​ ( 𝒁 k ) + R ϵ ​ ( 𝒁 ^ k ) ) . \\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}\\doteq R_{\\epsilon}\\big{(}\\bm{Z}_{k}\\cup\\hat", "snippet": "Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) ≐ R ϵ ​ ( 𝒁 k ∪ 𝒁 ^ k ) − 1 2 ​ ( R ϵ ​ ( 𝒁 k ) + R ϵ ​ ( 𝒁 ^ k ) ) . \\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}\\doteq R_{\\epsilon}\\big{(}\\bm{Z}_{k}\\cup\\hat{\\bm{Z}}_{k}\\big{)}-\\frac{1}{2}\\big{(}R_{\\epsilon}\\big{(}\\bm{Z}_{k})+R_{\\epsilon"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E11", "title": "d ​ ( 𝒁 , 𝒁 ^ ) ≐ ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) = ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , f ​ ( g ​ ( 𝒁 k , 𝜼 ) , 𝜽 ) ) . d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{", "snippet": "d ​ ( 𝒁 , 𝒁 ^ ) ≐ ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) = ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , f ​ ( g ​ ( 𝒁 k , 𝜼 ) , 𝜽 ) ) . d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},f(g(\\bm{Z}"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E12", "title": "max f ⁡ d ​ ( 𝒁 , 𝒁 ^ ) = max 𝜽 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) = max 𝜽 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( f ​ ( 𝑿 k , 𝜽 ) , f ​ ( 𝑿 ^ k , 𝜽 ) ) . \\max_{f}d(\\bm{Z},\\hat{\\bm{Z}})=\\max_{\\bm{\\theta}}\\sum_{k=1", "snippet": "max f ⁡ d ​ ( 𝒁 , 𝒁 ^ ) = max 𝜽 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k , 𝒁 ^ k ) = max 𝜽 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( f ​ ( 𝑿 k , 𝜽 ) , f ​ ( 𝑿 ^ k , 𝜽 ) ) . \\max_{f}d(\\bm{Z},\\hat{\\bm{Z}})=\\max_{\\bm{\\theta}}\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\max_{\\bm{\\the"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E13", "title": "𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ , 𝑿 → f ​ ( 𝒙 , 𝜽 ) { 𝒁 ^ , 𝒁 } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\h", "snippet": "𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ , 𝑿 → f ​ ( 𝒙 , 𝜽 ) { 𝒁 ^ , 𝒁 } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\{\\hat{\\bm{Z}},\\bm{Z}\\}. bold_italic_Z start_ARROW start_OVERACCE"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E14", "title": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ → f ​ ( 𝒙 , 𝜽 ) 𝒁 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69", "snippet": "𝑿 → f ​ ( 𝒙 , 𝜽 ) 𝒁 → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ → f ​ ( 𝒙 , 𝜽 ) 𝒁 ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.690"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E15", "title": "min g ⁡ d ​ ( 𝒁 , 𝒁 ^ ) ≐ min η ​ ∑ k = 1 K Δ ​ R ​ ( 𝒁 k , 𝒁 ^ k ) = min 𝜼 ​ ∑ k = 1 K Δ ​ R ​ ( 𝒁 k , f ​ ( g ​ ( 𝒁 k , 𝜼 ) , 𝜽 ) ) , \\min_{g}d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\min_{\\eta}\\sum_{k=1}^{K}\\De", "snippet": "min g ⁡ d ​ ( 𝒁 , 𝒁 ^ ) ≐ min η ​ ∑ k = 1 K Δ ​ R ​ ( 𝒁 k , 𝒁 ^ k ) = min 𝜼 ​ ∑ k = 1 K Δ ​ R ​ ( 𝒁 k , f ​ ( g ​ ( 𝒁 k , 𝜼 ) , 𝜽 ) ) , \\min_{g}d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\min_{\\eta}\\sum_{k=1}^{K}\\Delta R\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\De"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E16", "title": "h ​ ( 𝒙 , 𝜽 , 𝜼 ) ≐ f ​ ( g ​ ( f ​ ( 𝒙 , 𝜽 ) , 𝜼 ) , 𝜽 ) : 𝒙 ↦ 𝒛 ^ . h(\\bm{x},\\bm{\\theta},\\bm{\\eta})\\doteq f\\big{(}g\\big{(}f(\\bm{x},\\bm{\\theta}),\\bm{\\eta}\\big{)},\\bm{\\theta}\\big{)}:\\;\\bm{x}\\mapsto\\ha", "snippet": "h ​ ( 𝒙 , 𝜽 , 𝜼 ) ≐ f ​ ( g ​ ( f ​ ( 𝒙 , 𝜽 ) , 𝜼 ) , 𝜽 ) : 𝒙 ↦ 𝒛 ^ . h(\\bm{x},\\bm{\\theta},\\bm{\\eta})\\doteq f\\big{(}g\\big{(}f(\\bm{x},\\bm{\\theta}),\\bm{\\eta}\\big{)},\\bm{\\theta}\\big{)}:\\;\\bm{x}\\mapsto\\hat{\\bm{z}}. italic_h ( bold_italic_x , bold_italic_θ , bold_italic_η ) ≐ italic_f"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E17", "title": "𝒟 ​ ( 𝑿 , 𝑿 ^ ) ≐ max 𝜽 ⁡ min 𝜼 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( f ​ ( 𝑿 k , 𝜽 ) , h ​ ( 𝑿 k , 𝜽 , 𝜼 ) ) . \\mathcal{D}(\\bm{X},\\hat{\\bm{X}})\\doteq\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\Delta R_{\\epsil", "snippet": "𝒟 ​ ( 𝑿 , 𝑿 ^ ) ≐ max 𝜽 ⁡ min 𝜼 ​ ∑ k = 1 K Δ ​ R ϵ ​ ( f ​ ( 𝑿 k , 𝜽 ) , h ​ ( 𝑿 k , 𝜽 , 𝜼 ) ) . \\mathcal{D}(\\bm{X},\\hat{\\bm{X}})\\doteq\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}f(\\bm{X}_{k},\\bm{\\theta}),h(\\bm{X}_{k},\\bm{\\theta},\\bm{\\eta})\\big{)}. "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E21", "title": "max 𝜽 ⁡ min 𝜼 ⁡ 𝒯 𝑿 b ​ ( 𝜽 , 𝜼 ) ≐ Δ ​ R ϵ ​ ( f ​ ( 𝑿 , 𝜽 ) , h ​ ( 𝑿 , 𝜽 , 𝜼 ) ) = Δ ​ R ϵ ​ ( 𝒁 ​ ( 𝜽 ) , 𝒁 ^ ​ ( 𝜽 , 𝜼 ) ) , \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}^{b}_{\\bm{X}}(\\bm{\\theta}", "snippet": "max 𝜽 ⁡ min 𝜼 ⁡ 𝒯 𝑿 b ​ ( 𝜽 , 𝜼 ) ≐ Δ ​ R ϵ ​ ( f ​ ( 𝑿 , 𝜽 ) , h ​ ( 𝑿 , 𝜽 , 𝜼 ) ) = Δ ​ R ϵ ​ ( 𝒁 ​ ( 𝜽 ) , 𝒁 ^ ​ ( 𝜽 , 𝜼 ) ) , \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}^{b}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta})\\doteq\\Delta R_{\\epsilon}\\big{(}f(\\bm{X},\\bm{\\theta}),h(\\bm{X},\\bm{\\t"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E22", "title": "max 𝜽 ⁡ min 𝜼 ⁡ { Δ ​ R ϵ ​ ( 𝒁 ​ ( 𝜽 ) ) + ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k ​ ( 𝜽 ) , 𝒁 ^ k ​ ( 𝜽 , 𝜼 ) ) } \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\left\\{\\Delta R_{\\epsilon}(\\bm{Z}(\\bm{\\theta}))+\\sum_{k=1}^{K}\\D", "snippet": "max 𝜽 ⁡ min 𝜼 ⁡ { Δ ​ R ϵ ​ ( 𝒁 ​ ( 𝜽 ) ) + ∑ k = 1 K Δ ​ R ϵ ​ ( 𝒁 k ​ ( 𝜽 ) , 𝒁 ^ k ​ ( 𝜽 , 𝜼 ) ) } \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\left\\{\\Delta R_{\\epsilon}(\\bm{Z}(\\bm{\\theta}))+\\sum_{k=1}^{K}\\Delta R_{\\epsilon}(\\bm{Z}_{k}(\\bm{\\theta}),\\hat{\\bm{Z}}_{k}(\\bm{\\theta},\\bm{\\eta}"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.E1", "title": "min 𝜼 ⁡ max 𝜽 ⁡ Δ ​ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ^ ) + Δ ​ R ϵ ​ ( 𝒁 n ​ e ​ w , 𝒁 ^ n ​ e ​ w ) . \\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}}\\Delta{R_{\\epsilon}(\\bm{Z})}+\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}+\\Del", "snippet": "min 𝜼 ⁡ max 𝜽 ⁡ Δ ​ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ^ ) + Δ ​ R ϵ ​ ( 𝒁 n ​ e ​ w , 𝒁 ^ n ​ e ​ w ) . \\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}}\\Delta{R_{\\epsilon}(\\bm{Z})}+\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}+\\Delta{R_{\\epsilon}(\\bm{Z}_{new},\\hat{\\bm{Z}}_{new})}. roman_min start_POSTSUBSCRIPT"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.E2", "title": "𝒁 o ​ l ​ d → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ o ​ l ​ d → f ​ ( 𝒙 , 𝜽 ) 𝒁 ^ o ​ l ​ d . \\bm{Z}_{old}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}_{old}\\xrightarrow{\\hskip 5.69054ptf(", "snippet": "𝒁 o ​ l ​ d → g ​ ( 𝒛 , 𝜼 ) 𝑿 ^ o ​ l ​ d → f ​ ( 𝒙 , 𝜽 ) 𝒁 ^ o ​ l ​ d . \\bm{Z}_{old}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}_{old}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\ \\hat{\\bm{Z}}_{old}. bold_italic_Z start_PO"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.Ex1", "title": "Δ ​ R ϵ ​ ( 𝒁 o ​ l ​ d , 𝒁 ^ o ​ l ​ d ) ≐ ∑ j = 1 t Δ ​ R ϵ ​ ( 𝒁 j , o ​ l ​ d , 𝒁 ^ j , o ​ l ​ d ) = 0 . \\Delta R_{\\epsilon}(\\bm{Z}_{old},\\hat{\\bm{Z}}_{old})\\doteq\\sum_{j=1}^{t}\\Delta R_{\\epsilon", "snippet": "Δ ​ R ϵ ​ ( 𝒁 o ​ l ​ d , 𝒁 ^ o ​ l ​ d ) ≐ ∑ j = 1 t Δ ​ R ϵ ​ ( 𝒁 j , o ​ l ​ d , 𝒁 ^ j , o ​ l ​ d ) = 0 . \\Delta R_{\\epsilon}(\\bm{Z}_{old},\\hat{\\bm{Z}}_{old})\\doteq\\sum_{j=1}^{t}\\Delta R_{\\epsilon}(\\bm{Z}_{j,old},\\hat{\\bm{Z}}_{j,old})=0. roman_Δ italic_R start_POSTSUBSCRIPT i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5.E1", "title": "ℳ = { ( 𝒙 , F ​ ( 𝒙 ) ∣ 𝒙 ∈ ℝ D 1 ) } . \\mathcal{M}=\\{(\\bm{x},F(\\bm{x})\\mid\\bm{x}\\in\\mathbb{R}^{D_{1}})\\}. caligraphic_M = { ( bold_italic_x , italic_F ( bold_italic_x ) ∣ bold_italic_x ∈ blackboard_R", "snippet": "ℳ = { ( 𝒙 , F ​ ( 𝒙 ) ∣ 𝒙 ∈ ℝ D 1 ) } . \\mathcal{M}=\\{(\\bm{x},F(\\bm{x})\\mid\\bm{x}\\in\\mathbb{R}^{D_{1}})\\}. caligraphic_M = { ( bold_italic_x , italic_F ( bold_italic_x ) ∣ bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POST"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx66", "title": "p ​ ( 𝒙 ; 𝜽 ) \\displaystyle p(\\bm{x};\\bm{\\theta}) italic_p ( bold_italic_x ; bold_italic_θ ) = ∫ p ​ ( 𝒛 ; 𝜽 ) ​ p ​ ( 𝒙 ∣ 𝒛 ; 𝜽 ) ​ d 𝒛 \\displaystyle=\\int p(\\bm{z};\\,\\bm{\\theta})p(\\bm{x}\\mid\\bm{z};\\,", "snippet": "p ​ ( 𝒙 ; 𝜽 ) \\displaystyle p(\\bm{x};\\bm{\\theta}) italic_p ( bold_italic_x ; bold_italic_θ ) = ∫ p ​ ( 𝒛 ; 𝜽 ) ​ p ​ ( 𝒙 ∣ 𝒛 ; 𝜽 ) ​ d 𝒛 \\displaystyle=\\int p(\\bm{z};\\,\\bm{\\theta})p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\mathrm{d}\\bm{z} = ∫ italic_p ( bold_italic_z ; bold_italic_θ ) ital"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx67", "title": "𝒛 \\displaystyle\\bm{z} bold_italic_z ∼ 𝒩 ​ ( 𝟎 , 𝑰 ) , \\displaystyle\\sim\\mathcal{N}(\\mathbf{0},\\bm{I}), ∼ caligraphic_N ( bold_0 , bold_italic_I ) , 𝒙 ∣ 𝒛 \\displaystyle\\bm{x}\\mid\\bm{z} bold_italic_x ∣ ", "snippet": "𝒛 \\displaystyle\\bm{z} bold_italic_z ∼ 𝒩 ​ ( 𝟎 , 𝑰 ) , \\displaystyle\\sim\\mathcal{N}(\\mathbf{0},\\bm{I}), ∼ caligraphic_N ( bold_0 , bold_italic_I ) , 𝒙 ∣ 𝒛 \\displaystyle\\bm{x}\\mid\\bm{z} bold_italic_x ∣ bold_italic_z ∼ 𝒩 ​ ( g 1 ​ ( 𝒛 ) , diag ⁡ ( e g 2 ​ ( 𝒛 ) ) ​ 𝑰 ) , \\displaysty"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx68", "title": "log ⁡ p ​ ( 𝒙 ; 𝜽 ) \\displaystyle\\log p(\\bm{x};\\,\\bm{\\theta}) roman_log italic_p ( bold_italic_x ; bold_italic_θ ) = log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) p ​ ( 𝒛 ∣ 𝒙 ; 𝜽 ) \\displaystyle=\\log\\frac{p(\\bm{x},\\bm{z};\\", "snippet": "log ⁡ p ​ ( 𝒙 ; 𝜽 ) \\displaystyle\\log p(\\bm{x};\\,\\bm{\\theta}) roman_log italic_p ( bold_italic_x ; bold_italic_θ ) = log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) p ​ ( 𝒛 ∣ 𝒙 ; 𝜽 ) \\displaystyle=\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{p(\\bm{z}\\mid\\bm{x};\\,\\bm{\\theta})} = roman_log divide start_ARG i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx69", "title": "max 𝜽 , 𝜼 ⁡ 𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm", "snippet": "max 𝜽 , 𝜼 ⁡ 𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 , 𝒛 ; 𝜽 ) q ​ ( 𝒛 ∣ 𝒙 ; 𝜼 ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx70", "title": "𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 ∣ 𝒛 ; 𝜽 ) ] \\displaystyle\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\right] ", "snippet": "𝔼 𝒙 ​ 𝔼 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) ​ [ log ⁡ p ​ ( 𝒙 ∣ 𝒛 ; 𝜽 ) ] \\displaystyle\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\right] blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E st"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx71", "title": "max 𝜽 ⁡ min 𝜼 ⁡ 𝒯 𝑿 ​ ( 𝜽 , 𝜼 ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta}) roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start", "snippet": "max 𝜽 ⁡ min 𝜼 ⁡ 𝒯 𝑿 ​ ( 𝜽 , 𝜼 ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta}) roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx72", "title": "min 𝜼 ⁡ max 𝜽 \\displaystyle\\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}} roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ", "snippet": "min 𝜼 ⁡ max 𝜽 \\displaystyle\\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}} roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ^ ) + Δ ​ R ϵ ​ ( 𝒁 n ​ e ​ w , 𝒁 ^ n ​ e ​ w ) \\displaystyle\\"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx73", "title": "max 𝜽 \\displaystyle\\max_{\\bm{\\theta}} roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ^ ) + λ ⋅ Δ ​ R ϵ ​ ( 𝒁 n ​ e ​ w , 𝒁 ^ n ​ e ​ w ) − γ ⋅ Δ ​ R ϵ ​ ", "snippet": "max 𝜽 \\displaystyle\\max_{\\bm{\\theta}} roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT Δ ​ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ^ ) + λ ⋅ Δ ​ R ϵ ​ ( 𝒁 n ​ e ​ w , 𝒁 ^ n ​ e ​ w ) − γ ⋅ Δ ​ R ϵ ​ ( 𝒁 o ​ l ​ d , 𝒁 ^ o ​ l ​ d ) , \\displaystyle\\Delta{R_{\\epsilon}(\\bm{Z})}\\!+\\!"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx74", "title": "max 𝜽 ⁡ min 𝜼 Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min", "snippet": "max 𝜽 ⁡ min 𝜼 Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_Δ italic_R start_POST"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx75", "title": "∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 ^ i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\hat{\\bm{z}}^{i})=0.\\vspace{-2mm} ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ i", "snippet": "∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 ^ i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\hat{\\bm{z}}^{i})=0.\\vspace{-2mm} ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx76", "title": "∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 a i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z}_{a}^{i})=0.\\vspace{-3mm} ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ ita", "snippet": "∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 a i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z}_{a}^{i})=0.\\vspace{-3mm} ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx77", "title": "max θ ⁡ min η \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 ,", "snippet": "max θ ⁡ min η \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) \\displaystyle R_{\\epsilon}(\\bm{Z})+\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx78", "title": "max θ ⁡ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) − λ 1 ​ ∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 a i ) − λ 2 ​ ∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 ^ i ) ; \\displaystyle\\max_{\\theta}\\;R_{\\epsilon}(\\bm{Z})+\\Delta{R_{\\epsilon}(\\", "snippet": "max θ ⁡ R ϵ ​ ( 𝒁 ) + Δ ​ R ϵ ​ ( 𝒁 , 𝒁 ^ ) − λ 1 ​ ∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 a i ) − λ 2 ​ ∑ i ∈ N Δ ​ R ϵ ​ ( 𝒛 i , 𝒛 ^ i ) ; \\displaystyle\\max_{\\theta}\\;R_{\\epsilon}(\\bm{Z})+\\Delta{R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}})-\\lambda_{1}\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx79", "title": "𝚷 ^ = arg ⁡ max ξ ⁡ Δ ​ R ϵ ​ ( 𝒁 | 𝚷 ​ ( ξ ) ) . \\displaystyle\\hat{\\bm{\\Pi}}=\\arg\\max_{\\xi}\\Delta R_{\\epsilon}(\\bm{Z}|\\bm{\\Pi}(\\xi)). over^ start_ARG bold_Π end_ARG = roman_arg roman_max start_POSTSU", "snippet": "𝚷 ^ = arg ⁡ max ξ ⁡ Δ ​ R ϵ ​ ( 𝒁 | 𝚷 ​ ( ξ ) ) . \\displaystyle\\hat{\\bm{\\Pi}}=\\arg\\max_{\\xi}\\Delta R_{\\epsilon}(\\bm{Z}|\\bm{\\Pi}(\\xi)). over^ start_ARG bold_Π end_ARG = roman_arg roman_max start_POSTSUBSCRIPT italic_ξ end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx1", "title": "Probabilistic Perspective on Autoencoding", "snippet": "Probabilistic Perspective on Autoencoding In the manifold model for the data distribution, the key mathematical objects are the support of the data distribution, namely the manifold ℳ \\mathcal{M} caligraphic_M , and the density of the data on the support, say p p italic_p . When "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx2", "title": "VAE Training as Probabilistic Autoencoding", "snippet": "VAE Training as Probabilistic Autoencoding There is a general methodology for maximizing the ELBO objective in Equation 5.1.14 using stochastic gradient descent and various tractable Monte Carlo estimators for the associated gradients. However, the task is simpler under the Gauss"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx3", "title": "Training a VAE", "snippet": "Training a VAE VAEs are typically trained by alternating stochastic gradient ascent on the ELBO objective ( Equation 5.1.15 ), given individual samples 𝒙 \\bm{x} bold_italic_x from the true data distribution and from 𝒛 ∼ q ( ⋅ ∣ 𝒙 ; 𝜼 ) \\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#top", "title": "Chapter 6 Inference with Low-Dimensional Distributions", "snippet": ""}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1", "title": "6.1 Bayesian Inference and Constrained Optimization", "snippet": "6.1 Bayesian Inference and Constrained Optimization Leveraging Low-dimensionality for Stable and Robust Inference. Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data 𝒙 \\bm{x} bold_italic_x and"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2", "title": "6.2 Conditional Inference with a Known Data Distribution", "snippet": "6.2 Conditional Inference with a Known Data Distribution Notice that in the setting we have discussed in previous Chapters, the autoencoding network is trained to reconstruct a set of samples of the random vector 𝒙 \\bm{x} bold_italic_x . This would allow us to regenerate samples "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3", "title": "6.3 Conditional Inference with a Learned Data Representation", "snippet": "6.3 Conditional Inference with a Learned Data Representation In the previous subsection, the reason we can infer 𝒙 \\bm{x} bold_italic_x from the partial observation 𝒚 \\bm{y} bold_italic_y is because (support of) the distribution of 𝑿 \\bm{X} bold_italic_X is known or specified apr"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4", "title": "6.4 Conditional Inference with Paired Data and Measurements", "snippet": "6.4 Conditional Inference with Paired Data and Measurements In many practical applications, we do not know either the distribution of the data 𝒙 \\bm{x} bold_italic_x of interest nor the explicit relationship between the data and certain observed attributes 𝒚 \\bm{y} bold_italic_y "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5", "title": "6.5 Conditional Inference with Measurement Self-Consistency", "snippet": "6.5 Conditional Inference with Measurement Self-Consistency In this last section, we consider the more extreme, but actually ubiquitous, case for distribution learning in which we only have a set of observed samples 𝒀 = { 𝒚 1 , … , 𝒚 N } \\bm{Y}=\\{\\bm{y}_{1},\\ldots,\\bm{y}_{N}\\} bo"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6", "title": "6.6 Summary and Notes", "snippet": "6.6 Summary and Notes Measurement matching without clean samples. In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( 𝒙 , 𝒚 ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7", "title": "6.7 Exercises and Extensions", "snippet": "6.7 Exercises and Extensions Exercise 6.1 (Posterior Variance Correction to DPS) . 1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1", "title": "6.3.1 Image Completion with Masked Auto-Encoding", "snippet": "6.3.1 Image Completion with Masked Auto-Encoding For a general image 𝑿 \\bm{X} bold_italic_X such as the one shown on the left of Figure 6.4 , we can no longer view it as a low-rank matrix. However, humans still demonstrate remarkable ability to complete a scene and recognize fami"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2", "title": "6.3.2 Conditional Sampling with Measurement Matching", "snippet": "6.3.2 Conditional Sampling with Measurement Matching The above (masked) autoencoding problem aims to generate a sample image that is consistent with certain observations or conditions. But let us examine the approach more closely: Given the visual part of an image 𝑿 v = 𝒫 Ω ​ ( 𝑿"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3", "title": "6.3.3 Body Pose Generation Conditioned on Head and Hands", "snippet": "6.3.3 Body Pose Generation Conditioned on Head and Hands The type of conditional estimation or generation problems arise rather naturally in many practical applications. A typical problem of this kind is how to estimate and generate body pose and hand gesture conditioned on a giv"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1", "title": "6.4.1 Class Conditioned Image Generation", "snippet": "6.4.1 Class Conditioned Image Generation While a learned classifier allows us to classify a given image 𝒙 \\bm{x} bold_italic_x to its corresponding class, we often like to generate an image of a given class, by sampling the learned distribution of natural images. To some extent, "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2", "title": "6.4.2 Caption Conditioned Image Generation", "snippet": "6.4.2 Caption Conditioned Image Generation In the previous subsection, we have formulated denoisers for class-conditional denoising with classifier-free guidance, a ubiquitous practical methodology used in the largest-scale diffusion models, and shown how to parameterize them (in"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS1", "title": "6.5.1 Linear Measurement Models", "snippet": "6.5.1 Linear Measurement Models First, for simplicity, let us consider the measurement is a linear function of the data 𝒙 \\bm{x} bold_italic_x of interest: 𝒚 = 𝑨 ​ 𝒙 . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2) Here the matrix 𝑨 ∈ ℝ m × n \\bm{A}\\in"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2", "title": "6.5.2 3D Visual Model from Calibrated Images", "snippet": "6.5.2 3D Visual Model from Calibrated Images In practice, the measurement model is often nonlinear or only partially known. A typical problem of this kind is actually behind how we can learn a working model of the external world from the images perceived, say through our eyes, te"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px1", "title": "Leveraging Low-dimensionality for Stable and Robust Inference.", "snippet": "Leveraging Low-dimensionality for Stable and Robust Inference. Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data 𝒙 \\bm{x} bold_italic_x and its representation 𝒛 \\bm{z} bold_italic_z for vario"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px2", "title": "Statistical interpretation via Bayes’ rule.", "snippet": "Statistical interpretation via Bayes’ rule. Generally speaking, to accomplish such tasks well, we need to get ahold of the conditional distribution p ​ ( 𝒙 ∣ 𝒚 ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ∣ bold_italic_y ) . If we had this, then we would be able to find the max"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px3", "title": "Geometric interpretation as constrained optimization.", "snippet": "Geometric interpretation as constrained optimization. As the support 𝒮 𝒙 \\mathcal{S}_{\\bm{x}} caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT of the distribution of 𝒙 \\bm{x} bold_italic_x is low-dimensional, we may assume that there exists a function F F italic_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px4", "title": "Several representative practical settings for inference.", "snippet": "Several representative practical settings for inference. In practice, however, initial information about the distributions of 𝒙 \\bm{x} bold_italic_x and the relationship between 𝒙 \\bm{x} bold_italic_x and 𝒚 \\bm{y} bold_italic_y can be given in many different ways or forms. In gen"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1", "title": "Low-rank matrix completion.", "snippet": "Low-rank matrix completion. The low-rank matrix completion problem is a classical problem for data completion when its distribution is low-dimensional and known. Consider a random sample of a matrix 𝑿 o = [ 𝒙 1 , … , 𝒙 n ] ∈ ℝ m × n \\bm{X}_{o}=[\\bm{x}_{1},\\ldots,\\bm{x}_{n}]\\in\\ma"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px2", "title": "Further extensions.", "snippet": "Further extensions. It has been shown that images (or more accurately textures) and 3D scenes with low-rank structures can be very effectively completed via solving optimization programs of the above kind, even if there is additional corruption and distortion [ ZLG+10 , LRZ+12 , "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px1", "title": "General linear measurements.", "snippet": "General linear measurements. In fact, we may even consider recovering 𝑿 \\bm{X} bold_italic_X from a more general linear observation model: 𝒀 = 𝑨 ​ 𝑿 0 , 𝑿 t = 𝑿 0 + σ t ​ 𝑮 , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2", "title": "General nonlinear measurements.", "snippet": "General nonlinear measurements. To generalize the above (image) completion problems and make things more rigorous, we may consider that a random vector 𝒙 ∼ p \\bm{x}\\sim p bold_italic_x ∼ italic_p is partially observed through a more general observation function: 𝒚 = h ​ ( 𝒙 ) + 𝒘"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.SSS0.Px1", "title": "Visual World Model from Uncalibrated Image Sequences", "snippet": "Visual World Model from Uncalibrated Image Sequences In the above derivation, we have assumed that the measurement model h ​ ( ⋅ ) h(\\cdot) italic_h ( ⋅ ) is fully known. In the case of stereo vision, this is rather reasonable as the relative pose (and calibration) of the two cam"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px1", "title": "Measurement matching without clean samples.", "snippet": "Measurement matching without clean samples. In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( 𝒙 , 𝒚 ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) —i.e., ground trut"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px2", "title": "Corrections to the Diffusion Posterior Sampling (DPS) approximation.", "snippet": "Corrections to the Diffusion Posterior Sampling (DPS) approximation. In Example 6.2 and in particular in Figure 6.9 , we pointed out a limitation of the DPS approximation Equation 6.3.26 at small levels of measurement noise. This limitation is well-understood, and a principled ap"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px3", "title": "More about measurement matching and diffusion models for inverse problems.", "snippet": "More about measurement matching and diffusion models for inverse problems. Diffusion models have become an extremely popular tool for solving inverse problems arising in scientific applications. Many more methods beyond the simple DPS algorithm we have presented in Algorithm 6.1 "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample1", "title": "Example 6.1 (Image Completion and Text Prediction) .", "snippet": "Example 6.1 (Image Completion and Text Prediction) . The popular natural image completion and natural language prediction are two typical tasks that require us to recover a full data 𝒙 \\bm{x} bold_italic_x from its partial observations 𝒚 \\bm{y} bold_italic_y , with parts of 𝒙 \\bm"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark1", "title": "Remark 6.1 (End-to-End versus Bayesian) .", "snippet": "Remark 6.1 (End-to-End versus Bayesian) . In the modern practices of data-driven machine learning, for certain popular tasks people often directly learn the conditional distribution p ​ ( 𝒙 ∣ 𝒚 ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ∣ bold_italic_y ) or a (probabilistic) "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2", "title": "Example 6.2 .", "snippet": "Example 6.2 . Consider the case where the data distribution is Gaussian with mean 𝝁 ∈ ℝ D \\bm{\\mu}\\in\\mathbb{R}^{D} bold_italic_μ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and covariance 𝚺 ∈ ℝ D × D \\bm{\\Sigma}\\in\\mathbb{R}^{D\\times D} bold_Σ ∈ blackboard_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample3", "title": "Example 6.3 .", "snippet": "Example 6.3 . Let us recall the low-rank mixture of Gaussians data generating process we studied in Example 3.2 (and specifically, the form in Equation 3.2.42 ). Given K ∈ ℕ K\\in\\mathbb{N} italic_K ∈ blackboard_N classes, we assume that 𝒙 ∼ 1 K ​ ∑ k = 1 K 𝒩 ⁡ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ )"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark2", "title": "Remark 6.2 (Parallel Sensing and Distributed Denoising.) .", "snippet": "Remark 6.2 (Parallel Sensing and Distributed Denoising.) . There is something very interesting about the above equation ( 6.5.17 ). It seems to suggest we could try to learn the distribution of 𝒙 \\bm{x} bold_italic_x through a process that coupled with (many of) its (partial) obs"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark3", "title": "Remark 6.3 .", "snippet": "Remark 6.3 . Note that the above goal aligns well with Klein’s Erlangen Program for modern geometry, which is to study invariants of a manifold under a group of transformations. Here, we may view the manifold of interest as the distribution of ego-centric representations of 3D sc"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise1", "title": "Exercise 6.1 (Posterior Variance Correction to DPS) .", "snippet": "Exercise 6.1 (Posterior Variance Correction to DPS) . 1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse at low noise variance issue "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise2", "title": "Exercise 6.2 (Conditional Sampling on MNIST) .", "snippet": "Exercise 6.2 (Conditional Sampling on MNIST) . 1. Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser can be used for unconditional deno"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E1", "title": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.1.1)", "snippet": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.1.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E2", "title": "𝒙 ^ = arg ​ max 𝒙 ⁡ p ​ ( 𝒙 ∣ 𝒚 ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POS", "snippet": "𝒙 ^ = arg ​ max 𝒙 ⁡ p ​ ( 𝒙 ∣ 𝒚 ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_p ( bold_italic_x ∣ bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E3", "title": "𝒙 ^ = 𝔼 ​ [ 𝒙 ∣ 𝒚 ] = ∫ 𝒙 ​ p ​ ( 𝒙 ∣ 𝒚 ) ​ d 𝒙 ; \\hat{\\bm{x}}=\\mathbb{E}[\\bm{x}\\mid\\bm{y}]=\\int\\bm{x}p(\\bm{x}\\mid\\bm{y})\\mathrm{d}\\bm{x}; over^ start_ARG bold_italic_x end_ARG = blackboard_E [ bold_i", "snippet": "𝒙 ^ = 𝔼 ​ [ 𝒙 ∣ 𝒚 ] = ∫ 𝒙 ​ p ​ ( 𝒙 ∣ 𝒚 ) ​ d 𝒙 ; \\hat{\\bm{x}}=\\mathbb{E}[\\bm{x}\\mid\\bm{y}]=\\int\\bm{x}p(\\bm{x}\\mid\\bm{y})\\mathrm{d}\\bm{x}; over^ start_ARG bold_italic_x end_ARG = blackboard_E [ bold_italic_x ∣ bold_italic_y ] = ∫ bold_italic_x italic_p ( bold_italic_x ∣ bold_ital"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E4", "title": "𝒙 ^ ∼ p ​ ( 𝒙 ∣ 𝒚 ) . \\hat{\\bm{x}}\\sim p(\\bm{x}\\mid\\bm{y}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p ( bold_italic_x ∣ bold_italic_y ) . (6.1.4)", "snippet": "𝒙 ^ ∼ p ​ ( 𝒙 ∣ 𝒚 ) . \\hat{\\bm{x}}\\sim p(\\bm{x}\\mid\\bm{y}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p ( bold_italic_x ∣ bold_italic_y ) . (6.1.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E5", "title": "p ​ ( 𝒙 ∣ 𝒚 ) = p ​ ( 𝒚 ∣ 𝒙 ) ​ p ​ ( 𝒙 ) p ​ ( 𝒚 ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ∣ bold_italic_y ) = divide start_ARG italic_p ( bold_", "snippet": "p ​ ( 𝒙 ∣ 𝒚 ) = p ​ ( 𝒚 ∣ 𝒙 ) ​ p ​ ( 𝒙 ) p ​ ( 𝒚 ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ∣ bold_italic_y ) = divide start_ARG italic_p ( bold_italic_y ∣ bold_italic_x ) italic_p ( bold_italic_x ) end_ARG start_ARG italic_p"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E6", "title": "𝒙 ^ = arg ​ max 𝒙 ⁡ [ log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + log ⁡ p ​ ( 𝒙 ) ] , \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}[\\log p(\\bm{y}\\mid\\bm{x})+\\log p(\\bm{x})], over^ start_ARG bold_italic_x end_ARG = start_O", "snippet": "𝒙 ^ = arg ​ max 𝒙 ⁡ [ log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + log ⁡ p ​ ( 𝒙 ) ] , \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}[\\log p(\\bm{y}\\mid\\bm{x})+\\log p(\\bm{x})], over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E7", "title": "𝒙 k + 1 = 𝒙 k + α ⋅ ( ∇ 𝒙 log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + ∇ 𝒙 log ⁡ p ​ ( 𝒙 ) ) . \\bm{x}_{k+1}=\\bm{x}_{k}+\\alpha\\cdot\\big{(}\\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})\\big{)}. bold_ita", "snippet": "𝒙 k + 1 = 𝒙 k + α ⋅ ( ∇ 𝒙 log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + ∇ 𝒙 log ⁡ p ​ ( 𝒙 ) ) . \\bm{x}_{k+1}=\\bm{x}_{k}+\\alpha\\cdot\\big{(}\\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})\\big{)}. bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E8", "title": "F ​ ( 𝒙 ) = 𝟎 ⇔ 𝒙 ∈ 𝒮 𝒙 F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ⇔ bold_italic_x ∈ caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBS", "snippet": "F ​ ( 𝒙 ) = 𝟎 ⇔ 𝒙 ∈ 𝒮 𝒙 F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ⇔ bold_italic_x ∈ caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT (6.1.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E9", "title": "F ​ ( 𝒙 ) = min 𝒙 p ∈ 𝒮 𝒙 ⁡ ‖ 𝒙 − 𝒙 p ‖ 2 . F(\\bm{x})=\\min_{\\bm{x}_{p}\\in\\mathcal{S}_{\\bm{x}}}\\|\\bm{x}-\\bm{x}_{p}\\|_{2}. italic_F ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT bold_italic_x start_", "snippet": "F ​ ( 𝒙 ) = min 𝒙 p ∈ 𝒮 𝒙 ⁡ ‖ 𝒙 − 𝒙 p ‖ 2 . F(\\bm{x})=\\min_{\\bm{x}_{p}\\in\\mathcal{S}_{\\bm{x}}}\\|\\bm{x}-\\bm{x}_{p}\\|_{2}. italic_F ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_S start_POSTSUBSCRIPT bol"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E10", "title": "max 𝒙 − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 s.t. F ​ ( 𝒙 ) = 𝟎 . \\max_{\\bm{x}}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\quad\\mbox{s.t.}\\quad F(\\bm{x})=\\bm{0}. roman_max start_POSTSUBSCRIPT bold_italic_x end_PO", "snippet": "max 𝒙 − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 s.t. F ​ ( 𝒙 ) = 𝟎 . \\max_{\\bm{x}}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\quad\\mbox{s.t.}\\quad F(\\bm{x})=\\bm{0}. roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E11", "title": "max 𝒙 ⁡ [ − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 + 𝝀 ⊤ ​ F ​ ( 𝒙 ) − μ 2 ​ ‖ F ​ ( 𝒙 ) ‖ 2 2 ] \\max_{\\bm{x}}\\left[-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\bm{\\lambda}^{\\top}F(\\bm{x})-\\frac{\\mu}{2}\\|F(\\bm{x})\\", "snippet": "max 𝒙 ⁡ [ − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 + 𝝀 ⊤ ​ F ​ ( 𝒙 ) − μ 2 ​ ‖ F ​ ( 𝒙 ) ‖ 2 2 ] \\max_{\\bm{x}}\\left[-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\bm{\\lambda}^{\\top}F(\\bm{x})-\\frac{\\mu}{2}\\|F(\\bm{x})\\|_{2}^{2}\\right] roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ["}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E12", "title": "max 𝒙 ⁡ [ log ⁡ exp ⁡ ( − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 ) + log ⁡ exp ⁡ ( − μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝝀 / μ ‖ 2 2 ) ] , \\max_{\\bm{x}}\\left[\\log\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)}+\\log\\e", "snippet": "max 𝒙 ⁡ [ log ⁡ exp ⁡ ( − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 ) + log ⁡ exp ⁡ ( − μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝝀 / μ ‖ 2 2 ) ] , \\max_{\\bm{x}}\\left[\\log\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)}+\\log\\exp\\Big{(}-\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}\\Big{)}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E13", "title": "p ​ ( 𝒚 ∣ 𝒙 ) ∝ exp ⁡ ( − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 ) , p ​ ( 𝒙 ) ∝ exp ⁡ ( − μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝒄 ‖ 2 2 ) . p(\\bm{y}\\mid\\bm{x})\\propto\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)},\\qu", "snippet": "p ​ ( 𝒚 ∣ 𝒙 ) ∝ exp ⁡ ( − 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 ) , p ​ ( 𝒙 ) ∝ exp ⁡ ( − μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝒄 ‖ 2 2 ) . p(\\bm{y}\\mid\\bm{x})\\propto\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)},\\quad p(\\bm{x})\\propto\\exp\\Big{(}-\\frac{\\mu}{2}\\|F(\\bm{x})-\\bm{c}\\|_{2}^{2}\\Big{)}."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E14", "title": "∇ 𝒙 log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + ∇ 𝒙 log ⁡ p ​ ( 𝒙 ) = ∂ h ∂ 𝒙 ​ ( 𝒙 ) ​ ( 𝒚 − h ​ ( 𝒙 ) ) + μ ​ ∂ F ∂ 𝒙 ​ ( 𝒙 ) ​ ( 𝒄 − F ​ ( 𝒙 ) ) , \\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})=\\f", "snippet": "∇ 𝒙 log ⁡ p ​ ( 𝒚 ∣ 𝒙 ) + ∇ 𝒙 log ⁡ p ​ ( 𝒙 ) = ∂ h ∂ 𝒙 ​ ( 𝒙 ) ​ ( 𝒚 − h ​ ( 𝒙 ) ) + μ ​ ∂ F ∂ 𝒙 ​ ( 𝒙 ) ​ ( 𝒄 − F ​ ( 𝒙 ) ) , \\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})=\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x})\\big{(}\\bm{y}-h(\\bm{x})\\big{)}+\\mu\\frac{\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E15", "title": "min 𝒙 ⁡ 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 + μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝝀 / μ ‖ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}, roman_min ", "snippet": "min 𝒙 ⁡ 1 2 ​ ‖ h ​ ( 𝒙 ) − 𝒚 ‖ 2 2 + μ 2 ​ ‖ F ​ ( 𝒙 ) − 𝝀 / μ ‖ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT divide start_ARG 1 end_ARG s"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E1", "title": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.2.1)", "snippet": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.2.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E2", "title": "f : 𝒫 Ω ​ ( 𝒙 ) ↦ 𝒙 ^ , f:\\mathcal{P}_{\\Omega}(\\bm{x})\\mapsto\\hat{\\bm{x}}, italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_x ) ↦ over^ start_ARG bold_italic_x end_", "snippet": "f : 𝒫 Ω ​ ( 𝒙 ) ↦ 𝒙 ^ , f:\\mathcal{P}_{\\Omega}(\\bm{x})\\mapsto\\hat{\\bm{x}}, italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_x ) ↦ over^ start_ARG bold_italic_x end_ARG , (6.2.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E3", "title": "rank ​ ( 𝑿 o ) = r < min ⁡ { m , n } . \\mbox{rank}(\\bm{X}_{o})=r<\\min\\{m,n\\}. rank ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = italic_r < roman_min { italic_m , italic_n } . (6.", "snippet": "rank ​ ( 𝑿 o ) = r < min ⁡ { m , n } . \\mbox{rank}(\\bm{X}_{o})=r<\\min\\{m,n\\}. rank ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = italic_r < roman_min { italic_m , italic_n } . (6.2.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E4", "title": "𝒀 = 𝒫 Ω ​ ( 𝑿 o ) . \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}_{o}). bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ", "snippet": "𝒀 = 𝒫 Ω ​ ( 𝑿 o ) . \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}_{o}). bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) . (6.2.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E5", "title": "min 𝑿 ⁡ rank ​ ( 𝑿 ) subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) . \\min_{\\bm{X}}\\mbox{rank}(\\bm{X})\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min start_POSTSUBSCRIPT bold_italic_X end_POSTSU", "snippet": "min 𝑿 ⁡ rank ​ ( 𝑿 ) subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) . \\min_{\\bm{X}}\\mbox{rank}(\\bm{X})\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT rank ( bold_italic_X ) subject to bold_italic_Y = caligraphic_P start_PO"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E6", "title": "min ⁡ R ϵ ​ ( 𝑿 ) = 1 2 ​ log ​ det ( 𝑰 + α ​ 𝑿 ​ 𝑿 ⊤ ) subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) . \\min R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{X}\\bm{X}^{\\top}\\right)\\quad\\mbox{subject to}\\qu", "snippet": "min ⁡ R ϵ ​ ( 𝑿 ) = 1 2 ​ log ​ det ( 𝑰 + α ​ 𝑿 ​ 𝑿 ⊤ ) subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) . \\min R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{X}\\bm{X}^{\\top}\\right)\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min italic_R start_POSTSUBSCRIPT it"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E7", "title": "min ⁡ ‖ 𝑿 ‖ ∗ subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) , \\min\\|\\bm{X}\\|_{*}\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}), roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT subject", "snippet": "min ⁡ ‖ 𝑿 ‖ ∗ subject to 𝒀 = 𝒫 Ω ​ ( 𝑿 ) , \\min\\|\\bm{X}\\|_{*}\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}), roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E8", "title": "min ⁡ ‖ 𝑿 ‖ ∗ + λ ​ ‖ 𝒀 − 𝒫 Ω ​ ( 𝑿 ) ‖ F 2 , \\min\\|\\bm{X}\\|_{*}+\\lambda\\|\\bm{Y}-\\mathcal{P}_{\\Omega}(\\bm{X})\\|_{F}^{2}, roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT + italic_λ ", "snippet": "min ⁡ ‖ 𝑿 ‖ ∗ + λ ​ ‖ 𝒀 − 𝒫 Ω ​ ( 𝑿 ) ‖ F 2 , \\min\\|\\bm{X}\\|_{*}+\\lambda\\|\\bm{Y}-\\mathcal{P}_{\\Omega}(\\bm{X})\\|_{F}^{2}, roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT + italic_λ ∥ bold_italic_Y - caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E9", "title": "𝒀 ∘ τ = 𝑿 o + 𝑬 , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ∘ italic_τ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9)", "snippet": "𝒀 ∘ τ = 𝑿 o + 𝑬 , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ∘ italic_τ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E1", "title": "min f , g ⁡ L MAE ​ ( f , g ) ≐ 𝔼 ​ [ ‖ ( g ∘ f ) ​ ( 𝒫 Ω ​ ( 𝑿 ) ) − 𝑿 ‖ 2 2 ] . \\min_{f,g}L_{\\mathrm{MAE}}(f,g)\\doteq\\mathbb{E}\\big{[}\\|(g\\circ f)(\\mathcal{P}_{\\Omega}(\\bm{X}))-\\bm{X}\\|_{2}^{2}]. ro", "snippet": "min f , g ⁡ L MAE ​ ( f , g ) ≐ 𝔼 ​ [ ‖ ( g ∘ f ) ​ ( 𝒫 Ω ​ ( 𝑿 ) ) − 𝑿 ‖ 2 2 ] . \\min_{f,g}L_{\\mathrm{MAE}}(f,g)\\doteq\\mathbb{E}\\big{[}\\|(g\\circ f)(\\mathcal{P}_{\\Omega}(\\bm{X}))-\\bm{X}\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT italic_L start"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E2", "title": "f : 𝑿 ↦ 𝒁 f:\\bm{X}\\mapsto\\bm{Z} italic_f : bold_italic_X ↦ bold_italic_Z (6.3.2)", "snippet": "f : 𝑿 ↦ 𝒁 f:\\bm{X}\\mapsto\\bm{Z} italic_f : bold_italic_X ↦ bold_italic_Z (6.3.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E3", "title": "𝔼 𝒁 = f ​ ( 𝑿 ) ​ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] = 𝔼 𝒁 = f ​ ( 𝑿 ) ​ [ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] , \\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[\\Delta R_{\\epsilon}(\\bm{Z}\\mid", "snippet": "𝔼 𝒁 = f ​ ( 𝑿 ) ​ [ Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] = 𝔼 𝒁 = f ​ ( 𝑿 ) ​ [ R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) − λ ​ ‖ 𝒁 ‖ 0 ] , \\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]=\\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[R_{\\epsilon}("}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex1", "title": "p 𝑿 m ∣ 𝑿 v ​ ( 𝚵 m ∣ 𝚵 v ) p_{\\bm{X}_{m}\\mid\\bm{X}_{v}}(\\bm{\\Xi}_{m}\\mid\\bm{\\Xi}_{v}) italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_italic_X start_P", "snippet": "p 𝑿 m ∣ 𝑿 v ​ ( 𝚵 m ∣ 𝚵 v ) p_{\\bm{X}_{m}\\mid\\bm{X}_{v}}(\\bm{\\Xi}_{m}\\mid\\bm{\\Xi}_{v}) italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_Ξ start_POSTSUB"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E4", "title": "arg ​ min h = g ∘ f ⁡ L MAE ​ ( h ) = 𝚵 v ↦ 𝚵 v + 𝔼 ​ [ 𝑿 m ∣ 𝑿 v = 𝚵 v ] . \\operatorname*{arg\\ min}_{h=g\\circ f}\\,L_{\\mathrm{MAE}}(h)=\\bm{\\Xi}_{v}\\mapsto\\bm{\\Xi}_{v}+\\mathbb{E}[\\bm{X}_{m}\\mid\\bm{X}_{", "snippet": "arg ​ min h = g ∘ f ⁡ L MAE ​ ( h ) = 𝚵 v ↦ 𝚵 v + 𝔼 ​ [ 𝑿 m ∣ 𝑿 v = 𝚵 v ] . \\operatorname*{arg\\ min}_{h=g\\circ f}\\,L_{\\mathrm{MAE}}(h)=\\bm{\\Xi}_{v}\\mapsto\\bm{\\Xi}_{v}+\\mathbb{E}[\\bm{X}_{m}\\mid\\bm{X}_{v}=\\bm{\\Xi}_{v}]. start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E5", "title": "𝒀 = 𝑨 ​ 𝑿 0 , 𝑿 t = 𝑿 0 + σ t ​ 𝑮 , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_ital", "snippet": "𝒀 = 𝑨 ​ 𝑿 0 , 𝑿 t = 𝑿 0 + σ t ​ 𝑮 , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_X start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E6", "title": "𝑿 ^ ∗ = arg ​ min 𝑿 ^ ⁡ 𝔼 ​ [ ‖ 𝑨 ​ ( 𝑿 ^ ​ ( 𝑨 ​ 𝑿 t , 𝑨 ) − 𝑿 0 ) ‖ 2 ] \\hat{\\bm{X}}_{*}=\\operatorname*{arg\\ min}_{\\hat{\\bm{X}}}\\mathbb{E}[\\|\\bm{A}(\\hat{\\bm{X}}(\\bm{A}\\bm{X}_{t},\\bm{A})-\\bm{X}_{0})\\", "snippet": "𝑿 ^ ∗ = arg ​ min 𝑿 ^ ⁡ 𝔼 ​ [ ‖ 𝑨 ​ ( 𝑿 ^ ​ ( 𝑨 ​ 𝑿 t , 𝑨 ) − 𝑿 0 ) ‖ 2 ] \\hat{\\bm{X}}_{*}=\\operatorname*{arg\\ min}_{\\hat{\\bm{X}}}\\mathbb{E}[\\|\\bm{A}(\\hat{\\bm{X}}(\\bm{A}\\bm{X}_{t},\\bm{A})-\\bm{X}_{0})\\|^{2}] over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ∗ end_POSTSUBSC"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E7", "title": "𝑨 ​ 𝑿 ^ ∗ ​ ( 𝑨 ​ ( 𝑿 t ) , 𝑨 ) = 𝑨 ​ 𝔼 ​ [ 𝑿 0 ∣ 𝑨 ​ 𝑿 t , 𝑨 ] . \\bm{A}\\hat{\\bm{X}}_{*}(\\bm{A}(\\bm{X}_{t}),\\bm{A})=\\bm{A}\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_A over^ start_A", "snippet": "𝑨 ​ 𝑿 ^ ∗ ​ ( 𝑨 ​ ( 𝑿 t ) , 𝑨 ) = 𝑨 ​ 𝔼 ​ [ 𝑿 0 ∣ 𝑨 ​ 𝑿 t , 𝑨 ] . \\bm{A}\\hat{\\bm{X}}_{*}(\\bm{A}(\\bm{X}_{t}),\\bm{A})=\\bm{A}\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_A over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ( bold_italic_A"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E8", "title": "𝑿 t − s = γ t ​ 𝑿 t + ( 1 − γ t ) ​ 𝔼 ​ [ 𝑿 0 ∣ 𝑨 ​ 𝑿 t , 𝑨 ] . \\bm{X}_{t-s}=\\gamma_{t}\\bm{X}_{t}+(1-\\gamma_{t})\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_X start_POSTSUBSCRIPT ita", "snippet": "𝑿 t − s = γ t ​ 𝑿 t + ( 1 − γ t ) ​ 𝔼 ​ [ 𝑿 0 ∣ 𝑨 ​ 𝑿 t , 𝑨 ] . \\bm{X}_{t-s}=\\gamma_{t}\\bm{X}_{t}+(1-\\gamma_{t})\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_X start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = italic_γ start_POSTSUBSCRIPT italic_t end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E9", "title": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9)", "snippet": "𝒚 = h ​ ( 𝒙 ) + 𝒘 , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E10", "title": "𝒙 ^ ∼ p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝒚 = 𝝂 ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{y}=\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y en", "snippet": "𝒙 ^ ∼ p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝒚 = 𝝂 ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{y}=\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_y = bold_italic_ν ) . (6.3.10)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E11", "title": "p 𝒙 t c ∣ 𝒚 ( ⋅ ∣ 𝝂 ) = ∫ p 𝒙 t c ∣ 𝒙 c ( ⋅ ∣ 𝝃 ) ⏟ = 𝒩 ​ ( 𝝃 , t 2 ​ 𝑰 ) ⋅ p 𝒙 c ∣ 𝒚 ⏟ = p 𝒙 ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) ​ d 𝝃 = ∫ p 𝒙 t ∣ 𝒙 , 𝒚 ( ⋅ ∣ 𝝃 , 𝝂 ) ⋅ p 𝒙 ∣ 𝒚 ( 𝝃 ∣ 𝝂 ) d 𝝃 = ∫ p 𝒙 t , 𝒙 ∣ 𝒚 ​ ( ⋅ , 𝝃 ", "snippet": "p 𝒙 t c ∣ 𝒚 ( ⋅ ∣ 𝝂 ) = ∫ p 𝒙 t c ∣ 𝒙 c ( ⋅ ∣ 𝝃 ) ⏟ = 𝒩 ​ ( 𝝃 , t 2 ​ 𝑰 ) ⋅ p 𝒙 c ∣ 𝒚 ⏟ = p 𝒙 ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) ​ d 𝝃 = ∫ p 𝒙 t ∣ 𝒙 , 𝒚 ( ⋅ ∣ 𝝃 , 𝝂 ) ⋅ p 𝒙 ∣ 𝒚 ( 𝝃 ∣ 𝝂 ) d 𝝃 = ∫ p 𝒙 t , 𝒙 ∣ 𝒚 ​ ( ⋅ , 𝝃 ∣ 𝝂 ) ​ d 𝝃 = p 𝒙 t ∣ 𝒚 ( ⋅ ∣ 𝝂 ) . \\begin{split}p_{\\bm{x}^{\\mathrm{c}}_{t}\\mid\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E12", "title": "p 𝒙 t ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) = p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) ​ p 𝒙 t ​ ( 𝝃 ) p 𝒚 ​ ( 𝝂 ) . p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\frac{p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})p_{\\bm{x}_{t}}(\\bm{\\xi})}", "snippet": "p 𝒙 t ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) = p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) ​ p 𝒙 t ​ ( 𝝃 ) p 𝒚 ​ ( 𝝂 ) . p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\frac{p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})p_{\\bm{x}_{t}}(\\bm{\\xi})}{p_{\\bm{y}}(\\bm{\\nu})}. italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUB"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E13", "title": "∇ 𝝃 log ⁡ p 𝒙 t ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) = ∇ 𝝃 log ⁡ p t ​ ( 𝝃 ) ⏟ score matching + ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) ⏟ measurement matching , \\nabla_{\\bm{\\xi}}\\log p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})", "snippet": "∇ 𝝃 log ⁡ p 𝒙 t ∣ 𝒚 ​ ( 𝝃 ∣ 𝝂 ) = ∇ 𝝃 log ⁡ p t ​ ( 𝝃 ) ⏟ score matching + ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) ⏟ measurement matching , \\nabla_{\\bm{\\xi}}\\log p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\underbrace{\\nabla_{\\bm{\\xi}}\\log p_{t}(\\bm{\\xi})}_{\\text{score matching}}+\\und"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex4", "title": "[ 𝒙 𝒚 ] = d [ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ​ [ 𝒈 𝒘 ] + [ 𝝁 𝑨 ​ 𝝁 ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}", "snippet": "[ 𝒙 𝒚 ] = d [ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ​ [ 𝒈 𝒘 ] + [ 𝝁 𝑨 ​ 𝝁 ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{g}\\\\ \\bm{w}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\mu}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex5", "title": "[ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ​ [ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ⊤ = [ 𝚺 𝚺 ​ 𝑨 ⊤ 𝑨 ​ 𝚺 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end", "snippet": "[ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ​ [ 𝚺 1 / 2 𝟎 𝑨 ​ 𝚺 1 / 2 σ ​ 𝑰 ] ⊤ = [ 𝚺 𝚺 ​ 𝑨 ⊤ 𝑨 ​ 𝚺 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E15", "title": "p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝝂 ) = 𝒩 ( 𝝁 + 𝚺 ​ 𝑨 ⊤ ​ ( 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ) − 1 ​ ( 𝝂 − 𝑨 ​ 𝝁 ) ⏟ 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) , 𝚺 − 𝚺 ​ 𝑨 ⊤ ​ ( 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ) − 1 ​ 𝑨 ​ 𝚺 ⏟ 𝚺 𝒙 ∣ 𝒚 ) . p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,", "snippet": "p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝝂 ) = 𝒩 ( 𝝁 + 𝚺 ​ 𝑨 ⊤ ​ ( 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ) − 1 ​ ( 𝝂 − 𝑨 ​ 𝝁 ) ⏟ 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) , 𝚺 − 𝚺 ​ 𝑨 ⊤ ​ ( 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ) − 1 ​ 𝑨 ​ 𝚺 ⏟ 𝚺 𝒙 ∣ 𝒚 ) . p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu})=\\mathcal{N}\\left(\\underbrace{\\bm{\\mu}+\\bm{\\Sigma}\\bm{A}^{\\top}\\lef"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E16", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] = 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) + 𝚺 𝒙 ∣ 𝒚 ​ ( 𝚺 𝒙 ∣ 𝒚 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]=\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\b", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] = 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) + 𝚺 𝒙 ∣ 𝒚 ​ ( 𝚺 𝒙 ∣ 𝒚 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 𝒙 ∣ 𝒚 ​ ( 𝝂 ) ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]=\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\bm{\\nu})+\\bm{\\Sigma}_{\\bm{x}\\mid\\bm{y}}\\left(\\bm{\\Sigma}_{\\bm{x}\\mid\\bm{y}}+t^{2}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E17", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] = 𝝁 + 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 ) , \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\bm{\\mu}+\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}(\\bm{\\xi}-\\bm{\\mu}), blackboard_E", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] = 𝝁 + 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 ) , \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\bm{\\mu}+\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}(\\bm{\\xi}-\\bm{\\mu}), blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E18", "title": "[ 𝒙 𝒙 t 𝒚 ] = d [ 𝚺 1 / 2 𝟎 𝟎 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ​ [ 𝒈 𝒈 ′ 𝒘 ] + [ 𝝁 𝝁 𝑨 ​ 𝝁 ] , \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathb", "snippet": "[ 𝒙 𝒙 t 𝒚 ] = d [ 𝚺 1 / 2 𝟎 𝟎 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ​ [ 𝒈 𝒈 ′ 𝒘 ] + [ 𝝁 𝝁 𝑨 ​ 𝝁 ] , \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\mathbf{0}\\\\ \\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex6", "title": "[ 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ​ [ 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ⊤ = [ 𝚺 + t 2 ​ 𝑰 𝚺 ​ 𝑨 ⊤ 𝑨 ​ 𝚺 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}", "snippet": "[ 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ​ [ 𝚺 1 / 2 t ​ 𝑰 𝟎 𝑨 ​ 𝚺 1 / 2 𝟎 σ ​ 𝑰 ] ⊤ = [ 𝚺 + t 2 ​ 𝑰 𝚺 ​ 𝑨 ⊤ 𝑨 ​ 𝚺 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E19", "title": "p 𝒚 ∣ 𝒙 t ( ⋅ ∣ 𝝃 ) = 𝒩 ( 𝑨 ​ 𝝁 + 𝑨 ​ 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 ) ⏟ 𝝁 𝒚 ∣ 𝒙 t ​ ( 𝝃 ) , 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 − 𝑨 ​ 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 ​ 𝑨 ⊤ ⏟ 𝚺 𝒚 ∣ 𝒙 t ) . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\", "snippet": "p 𝒚 ∣ 𝒙 t ( ⋅ ∣ 𝝃 ) = 𝒩 ( 𝑨 ​ 𝝁 + 𝑨 ​ 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ ( 𝝃 − 𝝁 ) ⏟ 𝝁 𝒚 ∣ 𝒙 t ​ ( 𝝃 ) , 𝑨 ​ 𝚺 ​ 𝑨 ⊤ + σ 2 ​ 𝑰 − 𝑨 ​ 𝚺 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 ​ 𝑨 ⊤ ⏟ 𝚺 𝒚 ∣ 𝒙 t ) . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\,\\cdot\\,\\mid\\bm{\\xi})=\\mathcal{N}\\left(\\underbrace{\\bm{A}\\bm{\\mu}+\\bm{A}\\bm{\\Sig"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E21", "title": "𝚺 𝒚 ∣ 𝒙 t = σ 2 ​ 𝑰 + 𝑨 ​ 𝚺 1 / 2 ​ ( 𝑰 − 𝚺 1 / 2 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 1 / 2 ) ​ 𝚺 ​ 𝑨 ⊤ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}=\\sigma^{2}\\bm{I}+\\bm{A}\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2", "snippet": "𝚺 𝒚 ∣ 𝒙 t = σ 2 ​ 𝑰 + 𝑨 ​ 𝚺 1 / 2 ​ ( 𝑰 − 𝚺 1 / 2 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 1 / 2 ) ​ 𝚺 ​ 𝑨 ⊤ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}=\\sigma^{2}\\bm{I}+\\bm{A}\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Sigma}\\bm"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E24", "title": "λ i ​ t 2 λ i + t 2 ≈ 0 . \\frac{\\lambda_{i}t^{2}}{\\lambda_{i}+t^{2}}\\approx 0. divide start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSC", "snippet": "λ i ​ t 2 λ i + t 2 ≈ 0 . \\frac{\\lambda_{i}t^{2}}{\\lambda_{i}+t^{2}}\\approx 0. divide start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT +"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E25", "title": "𝚺 𝒚 ∣ 𝒙 t ≈ σ 2 ​ 𝑰 . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}\\approx\\sigma^{2}\\bm{I}. bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRI", "snippet": "𝚺 𝒚 ∣ 𝒙 t ≈ σ 2 ​ 𝑰 . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}\\approx\\sigma^{2}\\bm{I}. bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≈ italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I . (6.3.2"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E26", "title": "∇ 𝝃 log p 𝒚 ∣ 𝒙 t ( 𝝂 ∣ 𝝃 ) ≈ ∇ 𝝃 log p 𝒚 ∣ 𝒙 ( 𝝂 ∣ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] ) . \\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})\\approx\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}", "snippet": "∇ 𝝃 log p 𝒚 ∣ 𝒙 t ( 𝝂 ∣ 𝝃 ) ≈ ∇ 𝝃 log p 𝒚 ∣ 𝒙 ( 𝝂 ∣ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] ) . \\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})\\approx\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]). ∇ start_POSTSUBSCRIPT bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E27", "title": "p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) = ∫ p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝝃 ′ ) ​ p 𝒙 ∣ 𝒙 t ​ ( 𝝃 ′ ∣ 𝝃 ) ​ d 𝝃 ′ . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})=\\int p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi}^{\\prime})p_{\\bm{x}\\mid", "snippet": "p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) = ∫ p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝝃 ′ ) ​ p 𝒙 ∣ 𝒙 t ​ ( 𝝃 ′ ∣ 𝝃 ) ​ d 𝝃 ′ . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})=\\int p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi}^{\\prime})p_{\\bm{x}\\mid\\bm{x}_{t}}(\\bm{\\xi}^{\\prime}\\mid\\bm{\\xi})\\mathrm{d}\\bm{\\xi}^{\\prime}. italic_p "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E28", "title": "𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] ≈ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] + t 2 ∇ 𝝃 log p 𝒚 ∣ 𝒙 ( 𝝂 ∣ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]\\approx\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}", "snippet": "𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] ≈ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] + t 2 ∇ 𝝃 log p 𝒚 ∣ 𝒙 ( 𝝂 ∣ 𝔼 [ 𝒙 ∣ 𝒙 t = 𝝃 ] ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]\\approx\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\mathbb{E}[\\bm{x}\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E29", "title": "𝒙 ¯ θ ​ ( t , 𝝃 , 𝝂 ) = 𝒙 ¯ θ ​ ( t , 𝝃 ) + t 2 ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝒙 ¯ θ ​ ( t , 𝝃 ) ) . \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi},\\bm{\\nu})=\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})+t^{2}\\nabla_{\\bm{\\xi}}\\lo", "snippet": "𝒙 ¯ θ ​ ( t , 𝝃 , 𝝂 ) = 𝒙 ¯ θ ​ ( t , 𝝃 ) + t 2 ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝒙 ¯ θ ​ ( t , 𝝃 ) ) . \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi},\\bm{\\nu})=\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})). over¯ sta"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E30", "title": "p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝝃 ) ∝ exp ⁡ ( − 1 2 ​ σ 2 ​ ‖ h ​ ( 𝝃 ) − 𝝂 ‖ 2 2 ) . p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi})\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\|h(\\bm{\\xi})-\\bm{\\nu}\\right\\|_{2}^{2}\\right", "snippet": "p 𝒚 ∣ 𝒙 ​ ( 𝝂 ∣ 𝝃 ) ∝ exp ⁡ ( − 1 2 ​ σ 2 ​ ‖ h ​ ( 𝝃 ) − 𝝂 ‖ 2 2 ) . p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi})\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\|h(\\bm{\\xi})-\\bm{\\nu}\\right\\|_{2}^{2}\\right). italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex9", "title": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ α t ℓ ) ( 𝒙 ¯ θ ( t ℓ , 𝒙 ^ t ℓ ) − σ t ℓ 2 2 ​ α t ℓ ​ σ 2 ∇ 𝝃 [ ∥ h ( 𝒙 ¯ θ ( t ℓ , 𝝃 ) ) − 𝝂 ∥ 2 2 ] | 𝝃 = 𝒙 ^ t ℓ . ) \\hat{\\bm", "snippet": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ α t ℓ ) ( 𝒙 ¯ θ ( t ℓ , 𝒙 ^ t ℓ ) − σ t ℓ 2 2 ​ α t ℓ ​ σ 2 ∇ 𝝃 [ ∥ h ( 𝒙 ¯ θ ( t ℓ , 𝝃 ) ) − 𝝂 ∥ 2 2 ] | 𝝃 = 𝒙 ^ t ℓ . ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E1", "title": "h : 𝒙 ↦ 𝒚 . h:\\bm{x}\\mapsto\\bm{y}. italic_h : bold_italic_x ↦ bold_italic_y . (6.4.1)", "snippet": "h : 𝒙 ↦ 𝒚 . h:\\bm{x}\\mapsto\\bm{y}. italic_h : bold_italic_x ↦ bold_italic_y . (6.4.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E2", "title": "f : 𝒙 ↦ 𝒛 . f:\\bm{x}\\mapsto\\bm{z}. italic_f : bold_italic_x ↦ bold_italic_z . (6.4.2)", "snippet": "f : 𝒙 ↦ 𝒛 . f:\\bm{x}\\mapsto\\bm{z}. italic_f : bold_italic_x ↦ bold_italic_z . (6.4.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E3", "title": "f : 𝒙 ↦ ( 𝒛 , 𝒚 ) f:\\bm{x}\\mapsto(\\bm{z},\\bm{y}) italic_f : bold_italic_x ↦ ( bold_italic_z , bold_italic_y ) (6.4.3)", "snippet": "f : 𝒙 ↦ ( 𝒛 , 𝒚 ) f:\\bm{x}\\mapsto(\\bm{z},\\bm{y}) italic_f : bold_italic_x ↦ ( bold_italic_z , bold_italic_y ) (6.4.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E4", "title": "f : ( 𝒙 , 𝒘 ) ↦ ( 𝒛 , 𝒚 ) f:(\\bm{x},\\bm{w})\\mapsto(\\bm{z},\\bm{y}) italic_f : ( bold_italic_x , bold_italic_w ) ↦ ( bold_italic_z , bold_italic_y ) (6.4.4)", "snippet": "f : ( 𝒙 , 𝒘 ) ↦ ( 𝒛 , 𝒚 ) f:(\\bm{x},\\bm{w})\\mapsto(\\bm{z},\\bm{y}) italic_f : ( bold_italic_x , bold_italic_w ) ↦ ( bold_italic_z , bold_italic_y ) (6.4.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E5", "title": "𝒙 ^ ∼ p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝝂 ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSC", "snippet": "𝒙 ^ ∼ p 𝒙 ∣ 𝒚 ( ⋅ ∣ 𝝂 ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν ) . (6.4.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E6", "title": "( 𝒙 , y ) ∼ p 𝒙 , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_italic_x , italic_y ) ∼ italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT . (6.4.6)", "snippet": "( 𝒙 , y ) ∼ p 𝒙 , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_italic_x , italic_y ) ∼ italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT . (6.4.6)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E7", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , y = ν ] = 𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] + σ t 2 α t ​ ∇ 𝝃 log ⁡ p y ∣ 𝒙 t ​ ( ν ∣ 𝝃 ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\frac{\\sigma", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , y = ν ] = 𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] + σ t 2 α t ​ ∇ 𝝃 log ⁡ p y ∣ 𝒙 t ​ ( ν ∣ 𝝃 ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{y\\mid\\bm{x}_{t}}(\\nu\\mid\\bm{\\xi})."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E8", "title": "f θ c : ( t , 𝒙 t ) ↦ softmax ⁡ ( 𝑾 head ​ 𝒛 ​ ( t , 𝒙 t ) ) . f_{\\theta_{\\mathrm{c}}}:(t,\\bm{x}_{t})\\mapsto\\operatorname{\\mathrm{softmax}}(\\bm{W}_{\\mathrm{head}}\\bm{z}(t,\\bm{x}_{t})). italic_f start_", "snippet": "f θ c : ( t , 𝒙 t ) ↦ softmax ⁡ ( 𝑾 head ​ 𝒛 ​ ( t , 𝒙 t ) ) . f_{\\theta_{\\mathrm{c}}}:(t,\\bm{x}_{t})\\mapsto\\operatorname{\\mathrm{softmax}}(\\bm{W}_{\\mathrm{head}}\\bm{z}(t,\\bm{x}_{t})). italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUB"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E9", "title": "𝒙 ¯ θ naive ​ ( t , 𝒙 t , y ) = 𝒙 ¯ θ d ​ ( t , 𝒙 t ) + σ t 2 α t ​ ∇ 𝒙 t ⟨ log ⁡ f θ c ​ ( t , 𝒙 t ) , 𝒆 y ⟩ \\bar{\\bm{x}}_{\\theta}^{\\mathrm{naive}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(", "snippet": "𝒙 ¯ θ naive ​ ( t , 𝒙 t , y ) = 𝒙 ¯ θ d ​ ( t , 𝒙 t ) + σ t 2 α t ​ ∇ 𝒙 t ⟨ log ⁡ f θ c ​ ( t , 𝒙 t ) , 𝒆 y ⟩ \\bar{\\bm{x}}_{\\theta}^{\\mathrm{naive}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,\\bm{x}_{t})+\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{x}_{t}}\\left\\langle\\l"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E10", "title": "𝒙 ¯ θ CG ​ ( t , 𝒙 t , y ) = 𝒙 ¯ θ d ​ ( t , 𝒙 t ) + γ ​ σ t 2 α t ​ ∇ 𝒙 t ⟨ log ⁡ f θ c ​ ( t , 𝒙 t ) , 𝒆 y ⟩ \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,", "snippet": "𝒙 ¯ θ CG ​ ( t , 𝒙 t , y ) = 𝒙 ¯ θ d ​ ( t , 𝒙 t ) + γ ​ σ t 2 α t ​ ∇ 𝒙 t ⟨ log ⁡ f θ c ​ ( t , 𝒙 t ) , 𝒆 y ⟩ \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,\\bm{x}_{t})+\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{x}_{t}}\\left\\lang"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E12", "title": "𝒙 ¯ θ CG , ideal ​ ( t , 𝝃 , ν ) = 𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] + γ ​ σ t 2 α t ​ ∇ 𝝃 log ⁡ p y ∣ 𝒙 t ​ ( ν ∣ 𝝃 ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu)=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\", "snippet": "𝒙 ¯ θ CG , ideal ​ ( t , 𝝃 , ν ) = 𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] + γ ​ σ t 2 α t ​ ∇ 𝝃 log ⁡ p y ∣ 𝒙 t ​ ( ν ∣ 𝝃 ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu)=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{y\\mid\\b"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E13", "title": "log ⁡ p y ∣ 𝒙 t = log ⁡ p 𝒙 t ∣ y + log ⁡ p y − log ⁡ p 𝒙 t , \\log p_{y\\mid\\bm{x}_{t}}=\\log p_{\\bm{x}_{t}\\mid y}+\\log p_{y}-\\log p_{\\bm{x}_{t}}, roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_", "snippet": "log ⁡ p y ∣ 𝒙 t = log ⁡ p 𝒙 t ∣ y + log ⁡ p y − log ⁡ p 𝒙 t , \\log p_{y\\mid\\bm{x}_{t}}=\\log p_{\\bm{x}_{t}\\mid y}+\\log p_{y}-\\log p_{\\bm{x}_{t}}, roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roma"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E15", "title": "𝒙 ¯ θ CFG ​ ( t , 𝒙 t , y ) = ( 1 − γ ) ​ 𝒙 ¯ θ ​ ( t , 𝒙 t , ∅ ) + γ ​ 𝒙 ¯ θ ​ ( t , 𝒙 t , y ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CFG}}(t,\\bm{x}_{t},y)=(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},\\var", "snippet": "𝒙 ¯ θ CFG ​ ( t , 𝒙 t , y ) = ( 1 − γ ) ​ 𝒙 ¯ θ ​ ( t , 𝒙 t , ∅ ) + γ ​ 𝒙 ¯ θ ​ ( t , 𝒙 t , y ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CFG}}(t,\\bm{x}_{t},y)=(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},\\varnothing)+\\gamma\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},y). over¯ start_ARG bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E16", "title": "y + = { ∅ with probability ​ p uncond ; y else . y^{+}=\\begin{cases}\\varnothing&\\text{with probability }p_{\\mathrm{uncond}};\\\\ y&\\text{else}.\\end{cases} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERS", "snippet": "y + = { ∅ with probability ​ p uncond ; y else . y^{+}=\\begin{cases}\\varnothing&\\text{with probability }p_{\\mathrm{uncond}};\\\\ y&\\text{else}.\\end{cases} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { start_ROW start_CELL ∅ end_CELL start_CELL with probability italic_p s"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.Ex3", "title": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ ( ( 1 − γ ) ​ 𝒙 ¯ θ ​ ( t ℓ , 𝒙 ^ t ℓ , ∅ ) + γ ​ 𝒙 ¯ θ ​ ( t ℓ , 𝒙 ^ t ℓ , ν ) ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\fr", "snippet": "𝒙 ^ t ℓ − 1 ≐ σ t ℓ − 1 σ t ℓ ​ 𝒙 ^ t ℓ + ( α t ℓ − 1 − σ t ℓ − 1 σ t ℓ ​ α t ℓ ) ​ ( ( 1 − γ ) ​ 𝒙 ¯ θ ​ ( t ℓ , 𝒙 ^ t ℓ , ∅ ) + γ ​ 𝒙 ¯ θ ​ ( t ℓ , 𝒙 ^ t ℓ , ν ) ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E17", "title": "𝒙 ∼ 1 K ​ ∑ k = 1 K 𝒩 ⁡ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG it", "snippet": "𝒙 ∼ 1 K ​ ∑ k = 1 K 𝒩 ⁡ ( 𝟎 , 𝑼 k ​ 𝑼 k ⊤ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E18", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , y = ν ] = 1 1 + t 2 ​ 𝑼 ν ​ 𝑼 ν ⊤ ​ 𝝃 \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\frac{1}{1+t^{2}}\\bm{U}_{\\nu}\\bm{U}_{\\nu}^{\\top}\\bm{\\xi} blackboard_E [ bold_italic_x ∣ bold_i", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , y = ν ] = 1 1 + t 2 ​ 𝑼 ν ​ 𝑼 ν ⊤ ​ 𝝃 \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\frac{1}{1+t^{2}}\\bm{U}_{\\nu}\\bm{U}_{\\nu}^{\\top}\\bm{\\xi} blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , italic_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E19", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] = 1 1 + t 2 ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝝃 ‖ 2 2 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 . \\mathbb{E}[\\bm{x}\\", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 ] = 1 1 + t 2 ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝝃 ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝝃 ‖ 2 2 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝝃 . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\frac{1}{1+t^{2}}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E20", "title": "𝒙 ¯ CFG , ideal ​ ( t , 𝒙 t , y ) = 1 1 + t 2 ​ ( ( 1 − γ ) ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ​ 𝑼 k ", "snippet": "𝒙 ¯ CFG , ideal ​ ( t , 𝒙 t , y ) = 1 1 + t 2 ​ ( ( 1 − γ ) ​ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ​ 𝑼 k ​ 𝑼 k ⊤ + γ ​ 𝑼 y ​ 𝑼 y ⊤ ) ​ 𝒙 t . \\bar{\\bm{x}}^{\\mathrm{CFG,\\,ideal}}(t,\\bm{x}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E21", "title": "𝒙 ¯ CFG , ideal ​ ( t , 𝒙 t , y ) = 1 1 + t 2 ( [ γ + ( 1 − γ ) exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ] 𝑼 y 𝑼 y ⊤ + ", "snippet": "𝒙 ¯ CFG , ideal ​ ( t , 𝒙 t , y ) = 1 1 + t 2 ( [ γ + ( 1 − γ ) exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ] 𝑼 y 𝑼 y ⊤ + ( 1 − γ ) ∑ k ≠ y exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E22", "title": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) = 1 , \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U", "snippet": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 k ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) = 1 , \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{k}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E23", "title": "γ + ( 1 − γ ) ​ exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ≈ 1 , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2}", "snippet": "γ + ( 1 − γ ) ​ exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ≈ 1 , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E24", "title": "γ + ( 1 − γ ) ​ exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ≈ γ , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2}", "snippet": "γ + ( 1 − γ ) ​ exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 i ⊤ ​ 𝒙 t ‖ 2 2 ) ≈ γ , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E25", "title": "exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ′ ⊤ ​ 𝒙 t ‖ 2 2 ) = exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ( ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 − ‖ 𝑼 y ′ ⊤ ​ 𝒙 t ‖ 2 2 ) ) ", "snippet": "exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 ) exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ‖ 𝑼 y ′ ⊤ ​ 𝒙 t ‖ 2 2 ) = exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ( ‖ 𝑼 y ⊤ ​ 𝒙 t ‖ 2 2 − ‖ 𝑼 y ′ ⊤ ​ 𝒙 t ‖ 2 2 ) ) , \\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E26", "title": "𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 k = 𝒗 k , 𝑼 k ′ ​ 𝑼 k ′ ⊤ ​ 𝒗 k = 𝟎 , k ′ ≠ k . \\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{k}=\\bm{v}_{k},\\quad\\bm{U}_{k^{\\prime}}\\bm{U}_{k^{\\prime}}^{\\top}\\bm{v}_{k}=\\mathbf{0},\\enspace k^{\\pr", "snippet": "𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 k = 𝒗 k , 𝑼 k ′ ​ 𝑼 k ′ ⊤ ​ 𝒗 k = 𝟎 , k ′ ≠ k . \\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{k}=\\bm{v}_{k},\\quad\\bm{U}_{k^{\\prime}}\\bm{U}_{k^{\\prime}}^{\\top}\\bm{v}_{k}=\\mathbf{0},\\enspace k^{\\prime}\\neq k. bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_it"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E27", "title": "( 𝒙 t , 𝒗 ) ↦ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t . (\\bm{x}_{t},\\bm{v})\\maps", "snippet": "( 𝒙 t , 𝒗 ) ↦ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t . (\\bm{x}_{t},\\bm{v})\\mapsto\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E29", "title": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 y ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 y ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t ≈ 𝑼 y ​ 𝑼 y ⊤ ​ 𝒙 t . \\sum_{k=1}^{K}", "snippet": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 y ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 y ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t ≈ 𝑼 y ​ 𝑼 y ⊤ ​ 𝒙 t . \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E30", "title": "( 𝒙 t , 𝒗 ) ↦ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t (\\bm{x}_{t},\\bm{v})\\mapsto", "snippet": "( 𝒙 t , 𝒗 ) ↦ ∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t (\\bm{x}_{t},\\bm{v})\\mapsto\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E31", "title": "MHCA ​ ( 𝒛 t , 𝒀 + ) = 𝑼 out ​ [ SA ⁡ ( [ 𝑼 qry 1 ] ⊤ ​ ψ ​ ( 𝒛 t ) , [ 𝑼 key 1 ] ⊤ ​ τ ​ ( 𝒀 + ) , [ 𝑼 val 1 ] ⊤ ​ τ ​ ( 𝒀 + ) ) ⋮ SA ⁡ ( [ 𝑼 qry K ] ⊤ ​ ψ ​ ( 𝒛 t ) , [ 𝑼 key K ] ⊤ ​ τ ​ ( 𝒀 + ) , [", "snippet": "MHCA ​ ( 𝒛 t , 𝒀 + ) = 𝑼 out ​ [ SA ⁡ ( [ 𝑼 qry 1 ] ⊤ ​ ψ ​ ( 𝒛 t ) , [ 𝑼 key 1 ] ⊤ ​ τ ​ ( 𝒀 + ) , [ 𝑼 val 1 ] ⊤ ​ τ ​ ( 𝒀 + ) ) ⋮ SA ⁡ ( [ 𝑼 qry K ] ⊤ ​ ψ ​ ( 𝒛 t ) , [ 𝑼 key K ] ⊤ ​ τ ​ ( 𝒀 + ) , [ 𝑼 val K ] ⊤ ​ τ ​ ( 𝒀 + ) ) ] , \\mathrm{MHCA}(\\bm{z}_{t},\\bm{Y}^{+})=\\bm{U}_{\\m"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E1", "title": "F ​ ( 𝒙 ) = 𝟎 , G ​ ( 𝒚 ) = 𝟎 . F(\\bm{x})=\\bm{0},\\quad G(\\bm{y})=\\bm{0}. italic_F ( bold_italic_x ) = bold_0 , italic_G ( bold_italic_y ) = bold_0 . (6.5.1)", "snippet": "F ​ ( 𝒙 ) = 𝟎 , G ​ ( 𝒚 ) = 𝟎 . F(\\bm{x})=\\bm{0},\\quad G(\\bm{y})=\\bm{0}. italic_F ( bold_italic_x ) = bold_0 , italic_G ( bold_italic_y ) = bold_0 . (6.5.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E2", "title": "𝒚 = 𝑨 ​ 𝒙 . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2)", "snippet": "𝒚 = 𝑨 ​ 𝒙 . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E3", "title": "𝒚 t = 𝒚 0 + t ​ 𝒈 , 𝒚 0 = 𝑨 ​ ( 𝒙 0 ) , \\bm{y}_{t}=\\bm{y}_{0}+t\\bm{g},\\quad\\bm{y}_{0}=\\bm{A}(\\bm{x}_{0}), bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRI", "snippet": "𝒚 t = 𝒚 0 + t ​ 𝒈 , 𝒚 0 = 𝑨 ​ ( 𝒙 0 ) , \\bm{y}_{t}=\\bm{y}_{0}+t\\bm{g},\\quad\\bm{y}_{0}=\\bm{A}(\\bm{x}_{0}), bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t bold_italic_g , bold_italic_y start_POSTSUBSCR"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E4", "title": "𝒚 t = 𝑨 ​ 𝒙 t . \\bm{y}_{t}=\\bm{A}\\bm{x}_{t}. bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (6.5.4)", "snippet": "𝒚 t = 𝑨 ​ 𝒙 t . \\bm{y}_{t}=\\bm{A}\\bm{x}_{t}. bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (6.5.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E5", "title": "𝒚 t − s ≈ 𝒚 t + s ​ t ​ ∇ log ⁡ p t ​ ( 𝒚 t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nabla\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_y st", "snippet": "𝒚 t − s ≈ 𝒚 t + s ​ t ​ ∇ log ⁡ p t ​ ( 𝒚 t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nabla\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ∇ roman_log ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E6", "title": "𝑨 ​ 𝒙 t − s ≈ 𝑨 ​ 𝒙 t + s ​ t ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) , \\bm{A}\\bm{x}_{t-s}\\approx\\bm{A}\\bm{x}_{t}+st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}), bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t - i", "snippet": "𝑨 ​ 𝒙 t − s ≈ 𝑨 ​ 𝒙 t + s ​ t ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) , \\bm{A}\\bm{x}_{t-s}\\approx\\bm{A}\\bm{x}_{t}+st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}), bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_A bold_italic_x start_POSTSUBSCRIPT ital"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E7", "title": "𝑨 ​ ( 𝒙 t − s − 𝒙 t ) ≈ s ​ t ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) . \\bm{A}(\\bm{x}_{t-s}-\\bm{x}_{t})\\approx st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t - it", "snippet": "𝑨 ​ ( 𝒙 t − s − 𝒙 t ) ≈ s ​ t ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) . \\bm{A}(\\bm{x}_{t-s}-\\bm{x}_{t})\\approx st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E8", "title": "𝒙 t − s ≈ 𝒙 t + s ​ t ​ 𝑨 † ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) . \\bm{x}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}^{\\dagger}\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_", "snippet": "𝒙 t − s ≈ 𝒙 t + s ​ t ​ 𝑨 † ​ ∇ log ⁡ p t ​ ( 𝑨 ​ 𝒙 t ) . \\bm{x}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}^{\\dagger}\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E9", "title": "𝒚 i = h ​ ( 𝒙 , θ i ) + 𝒘 i , \\bm{y}^{i}=h(\\bm{x},\\theta^{i})+\\bm{w}^{i}, bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT i", "snippet": "𝒚 i = h ​ ( 𝒙 , θ i ) + 𝒘 i , \\bm{y}^{i}=h(\\bm{x},\\theta^{i})+\\bm{w}^{i}, bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E10", "title": "𝒚 0 = h ​ ( 𝒙 , θ 0 ) + 𝒘 0 , 𝒚 1 = h ​ ( 𝒙 , θ 1 ) + 𝒘 1 , \\bm{y}^{0}=h(\\bm{x},\\theta^{0})+\\bm{w}^{0},\\quad\\bm{y}^{1}=h(\\bm{x},\\theta^{1})+\\bm{w}^{1}, bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSU", "snippet": "𝒚 0 = h ​ ( 𝒙 , θ 0 ) + 𝒘 0 , 𝒚 1 = h ​ ( 𝒙 , θ 1 ) + 𝒘 1 , \\bm{y}^{0}=h(\\bm{x},\\theta^{0})+\\bm{w}^{0},\\quad\\bm{y}^{1}=h(\\bm{x},\\theta^{1})+\\bm{w}^{1}, bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT 0 end_POST"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E11", "title": "𝔼 ​ [ 𝒚 ∣ 𝒚 t = 𝝂 ] = 𝝂 + t 2 ​ ∇ 𝝂 log ⁡ p t ​ ( 𝝂 ) . \\mathbb{E}[\\bm{y}\\mid\\bm{y}_{t}=\\bm{\\nu}]=\\bm{\\nu}+t^{2}\\nabla_{\\bm{\\nu}}\\log p_{t}(\\bm{\\nu}). blackboard_E [ bold_italic_y ∣ bold_italic_y star", "snippet": "𝔼 ​ [ 𝒚 ∣ 𝒚 t = 𝝂 ] = 𝝂 + t 2 ​ ∇ 𝝂 log ⁡ p t ​ ( 𝝂 ) . \\mathbb{E}[\\bm{y}\\mid\\bm{y}_{t}=\\bm{\\nu}]=\\bm{\\nu}+t^{2}\\nabla_{\\bm{\\nu}}\\log p_{t}(\\bm{\\nu}). blackboard_E [ bold_italic_y ∣ bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ν ] = bold_italic_ν + i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E12", "title": "𝒚 t − s = 𝒚 t + s ​ t ​ ∇ 𝒚 log ⁡ p t ​ ( 𝒚 t ) . \\bm{y}_{t-s}=\\bm{y}_{t}+st\\nabla_{\\bm{y}}\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic", "snippet": "𝒚 t − s = 𝒚 t + s ​ t ​ ∇ 𝒚 log ⁡ p t ​ ( 𝒚 t ) . \\bm{y}_{t-s}=\\bm{y}_{t}+st\\nabla_{\\bm{y}}\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ∇ start_PO"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E13", "title": "𝒚 = h ​ ( 𝒙 ) . \\bm{y}=h(\\bm{x}). bold_italic_y = italic_h ( bold_italic_x ) . (6.5.13)", "snippet": "𝒚 = h ​ ( 𝒙 ) . \\bm{y}=h(\\bm{x}). bold_italic_y = italic_h ( bold_italic_x ) . (6.5.13)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E14", "title": "h ​ ( 𝒙 t − s ) ≈ h ​ ( 𝒙 t ) + s ​ t ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "h ​ ( 𝒙 t − s ) ≈ h ​ ( 𝒙 t ) + s ​ t ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ≈ italic_h ( bold_italic_x start_POSTSUB"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E15", "title": "h ​ ( 𝒙 t − s ) ≈ h ​ ( 𝒙 t ) + ∂ h ∂ 𝒙 ​ ( 𝒙 t ) ⋅ 𝒗 ​ s ≐ h ​ ( 𝒙 t ) + 𝑨 ​ ( 𝒙 t ) ​ 𝒗 ​ s . h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x}_{t})\\cdot\\bm{v}s\\doteq h(\\", "snippet": "h ​ ( 𝒙 t − s ) ≈ h ​ ( 𝒙 t ) + ∂ h ∂ 𝒙 ​ ( 𝒙 t ) ⋅ 𝒗 ​ s ≐ h ​ ( 𝒙 t ) + 𝑨 ​ ( 𝒙 t ) ​ 𝒗 ​ s . h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x}_{t})\\cdot\\bm{v}s\\doteq h(\\bm{x}_{t})+\\bm{A}(\\bm{x}_{t})\\bm{v}s. italic_h ( bold_italic_x start_POSTSUBSCRI"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E16", "title": "𝑨 ​ ( 𝒙 t ) ​ 𝒗 = t ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) . \\bm{A}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT", "snippet": "𝑨 ​ ( 𝒙 t ) ​ 𝒗 = t ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) . \\bm{A}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v = italic_t ∇ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIP"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E17", "title": "𝒙 ^ t − s ≈ 𝒙 t + s ​ t ​ 𝑨 ​ ( 𝒙 t ) † ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) . \\hat{\\bm{x}}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}(\\bm{x}_{t})^{\\dagger}\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). over^ start_ARG ", "snippet": "𝒙 ^ t − s ≈ 𝒙 t + s ​ t ​ 𝑨 ​ ( 𝒙 t ) † ​ ∇ 𝒚 log ⁡ p t ​ ( h ​ ( 𝒙 t ) ) . \\hat{\\bm{x}}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}(\\bm{x}_{t})^{\\dagger}\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E18", "title": "𝒚 i = h i ​ ( 𝒙 ) + 𝒘 i , i = 1 , … , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i e", "snippet": "𝒚 i = h i ​ ( 𝒙 ) + 𝒘 i , i = 1 , … , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E19", "title": "𝑨 i ​ ( 𝒙 t ) ​ 𝒗 = t ​ ∇ 𝒚 i log ⁡ p t ​ ( h i ​ ( 𝒙 t ) ) , \\bm{A}^{i}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t})), bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPER", "snippet": "𝑨 i ​ ( 𝒙 t ) ​ 𝒗 = t ​ ∇ 𝒚 i log ⁡ p t ​ ( h i ​ ( 𝒙 t ) ) , \\bm{A}^{i}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t})), bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E20", "title": "𝒗 i = t ​ 𝑨 i ​ ( 𝒙 t ) † ​ [ ∇ 𝒚 i log ⁡ p t ​ ( h i ​ ( 𝒙 t ) ) ] , i = 1 , … , K , \\bm{v}^{i}=t\\bm{A}^{i}(\\bm{x}_{t})^{\\dagger}\\big{[}\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t}))\\big{]},\\quad i", "snippet": "𝒗 i = t ​ 𝑨 i ​ ( 𝒙 t ) † ​ [ ∇ 𝒚 i log ⁡ p t ​ ( h i ​ ( 𝒙 t ) ) ] , i = 1 , … , K , \\bm{v}^{i}=t\\bm{A}^{i}(\\bm{x}_{t})^{\\dagger}\\big{[}\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t}))\\big{]},\\quad i=1,\\ldots,K, bold_italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E21", "title": "𝒚 k = h ​ ( 𝒙 k , θ k ) , k = 1 , … , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POST", "snippet": "𝒚 k = h ​ ( 𝒙 k , θ k ) , k = 1 , … , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT italic"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E22", "title": "𝒀 = H ​ ( 𝒙 , Θ ) . \\bm{Y}=H(\\bm{x},\\Theta). bold_italic_Y = italic_H ( bold_italic_x , roman_Θ ) . (6.5.22)", "snippet": "𝒀 = H ​ ( 𝒙 , Θ ) . \\bm{Y}=H(\\bm{x},\\Theta). bold_italic_Y = italic_H ( bold_italic_x , roman_Θ ) . (6.5.22)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.E1", "title": "𝔼 𝒈 ​ [ ‖ 𝒙 − f ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 ] = 𝔼 𝒈 ​ [ ‖ 𝒙 + t ​ 𝒈 − f ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 + 2 ​ t 2 ​ ∇ ⋅ f ​ ( 𝒙 + t ​ 𝒈 ) ] − t 2 ​ D , \\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}-f(\\bm{x}+t\\bm{g})\\right\\|", "snippet": "𝔼 𝒈 ​ [ ‖ 𝒙 − f ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 ] = 𝔼 𝒈 ​ [ ‖ 𝒙 + t ​ 𝒈 − f ​ ( 𝒙 + t ​ 𝒈 ) ‖ 2 2 + 2 ​ t 2 ​ ∇ ⋅ f ​ ( 𝒙 + t ​ 𝒈 ) ] − t 2 ​ D , \\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}-f(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2}\\right]=\\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}+t\\bm{g}-f(\\bm{x}+t\\bm{g})"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.Ex1", "title": "∇ ⋅ f = ∑ i = 1 D ∂ i f i . \\nabla\\cdot f=\\sum_{i=1}^{D}\\partial_{i}f_{i}. ∇ ⋅ italic_f = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ∂ star", "snippet": "∇ ⋅ f = ∑ i = 1 D ∂ i f i . \\nabla\\cdot f=\\sum_{i=1}^{D}\\partial_{i}f_{i}. ∇ ⋅ italic_f = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ∂ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.E2", "title": "𝔼 𝒙 t ​ [ ‖ 𝒙 t − f ​ ( 𝒙 t ) ‖ 2 2 + 2 ​ t 2 ​ ∇ ⋅ f ​ ( 𝒙 t ) ] = 𝔼 𝒙 t ​ [ ‖ 𝒙 t − f ​ ( 𝒙 t ) ‖ 2 2 ] − 2 ​ t 2 ​ ∫ ⟨ ∇ p 𝒙 t ​ ( 𝝃 ) , f ​ ( 𝝃 ) ⟩ ​ d 𝝃 . \\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{", "snippet": "𝔼 𝒙 t ​ [ ‖ 𝒙 t − f ​ ( 𝒙 t ) ‖ 2 2 + 2 ​ t 2 ​ ∇ ⋅ f ​ ( 𝒙 t ) ] = 𝔼 𝒙 t ​ [ ‖ 𝒙 t − f ​ ( 𝒙 t ) ‖ 2 2 ] − 2 ​ t 2 ​ ∫ ⟨ ∇ p 𝒙 t ​ ( 𝝃 ) , f ​ ( 𝝃 ) ⟩ ​ d 𝝃 . \\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{x}_{t}-f(\\bm{x}_{t})\\right\\|_{2}^{2}+2t^{2}\\nabla\\cdot f(\\bm{x}_{t})\\right]=\\mat"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx80", "title": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] \\displaystyle\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}] blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_it", "snippet": "𝔼 ​ [ 𝒙 ∣ 𝒙 t = 𝝃 , 𝒚 = 𝝂 ] \\displaystyle\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}] blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , bold_italic_y = bold_italic_ν ] = 𝝃 + t 2 ​ ∇ 𝝃 log ⁡ p t ​ ( 𝝃 ) + t 2"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx81", "title": "t 2 ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) \\displaystyle t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT", "snippet": "t 2 ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) \\displaystyle t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx82", "title": "𝚺 1 / 2 ​ ( 𝑰 − 𝚺 1 / 2 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 1 / 2 ) ​ 𝚺 1 / 2 \\displaystyle\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Si", "snippet": "𝚺 1 / 2 ​ ( 𝑰 − 𝚺 1 / 2 ​ ( 𝚺 + t 2 ​ 𝑰 ) − 1 ​ 𝚺 1 / 2 ) ​ 𝚺 1 / 2 \\displaystyle\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Sigma}^{1/2} bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx83", "title": "γ ​ σ t 2 α t ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) \\displaystyle\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_γ divide start_ARG itali", "snippet": "γ ​ σ t 2 α t ​ ∇ 𝝃 log ⁡ p 𝒚 ∣ 𝒙 t ​ ( 𝝂 ∣ 𝝃 ) \\displaystyle\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_γ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx84", "title": "𝒙 ¯ θ CG , ideal ​ ( t , 𝝃 , ν ) \\displaystyle\\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu) over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POS", "snippet": "𝒙 ¯ θ CG , ideal ​ ( t , 𝝃 , ν ) \\displaystyle\\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu) over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx85", "title": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 y ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 y ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t = exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ ", "snippet": "∑ k = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒗 y ) ∑ i = 1 K exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝑼 i ​ 𝑼 i ⊤ ​ 𝒗 y ) ​ 𝑼 k ​ 𝑼 k ⊤ ​ 𝒙 t = exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝒗 y ) exp ⁡ ( 1 2 ​ t 2 ​ ( 1 + t 2 ) ​ 𝒙 t ⊤ ​ 𝒗 y ) + K − 1 ​ 𝑼 y ​ 𝑼 "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#top", "title": "Chapter 7 Learning Representations for Real-World Data", "snippet": ""}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1", "title": "7.1 Technical Setup and Outline of the Chapter", "snippet": "7.1 Technical Setup and Outline of the Chapter In previous chapters, we alluded to different setups in which we used representation-learning techniques to process real data at scale. In this chapter, we will describe such setups in great detail. The objective of this section is t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2", "title": "7.2 Simplified Contrastive Learning", "snippet": "7.2 Simplified Contrastive Learning Learning high-quality and faithful representations of data is a fundamental problem in deep learning, known as self-supervised learning . There have been many approaches proposed for this task, many of which do not evidently use the techniques "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3", "title": "7.3 Image Classification", "snippet": "7.3 Image Classification In the previous section, we simplified an overly complex learning objective using our intuition of representation learning through the lens of compression. However, many of the most popular learning procedures are incredibly simple. In these cases, it is "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4", "title": "7.4 Causal Language Modeling", "snippet": "7.4 Causal Language Modeling We now study causal language modeling , a method for training large language models (LLMs). This is the same setup used to train, among many others, GPT-2 and many more language models. 7.4.1 Data The data we will use to investigate the performance of"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5", "title": "7.5 Scaling White-Box Transformers", "snippet": "7.5 Scaling White-Box Transformers In this section, we will discuss three ways in which various parts of CRATE-type models can be scaled up or made more efficient while still remaining white-box. These developments mix both conceptual and empirical insights, and can be viewed as "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6", "title": "7.6 Masked Autoencoding for Imagery Data", "snippet": "7.6 Masked Autoencoding for Imagery Data The second application we discuss is nonlinear image completion , also known as masked autoencoding (MAE), which is a direct generalization of the low-rank matrix completion problem discussed in Chapter 2 . Masked autoencoding, since its i"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S7", "title": "7.7 Summary and Notes", "snippet": "7.7 Summary and Notes All work in this chapter is downstream of the Transformer architecture, which was introduced by [ VSP+17 ] . The Transformer architecture is formally described in Section 7.2 . A main empirical innovation in recent years, spurred by the prevalence and perfor"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8", "title": "7.8 Exercises and Extensions", "snippet": "7.8 Exercises and Extensions Exercise 7.1 . Read the DINO paper [ CTM+21 ] . Exercise 7.2 . DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS1", "title": "7.2.1 Data", "snippet": "7.2.1 Data The data that we will use to explore and simplify the DINO methodology is all 2-dimensional image data. For training , we will use the ImageNet-1K and ImageNet-21K datasets. Each sample in the dataset is an RGB image, of varying resolution, and a label indicating the o"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2", "title": "7.2.2 Task and Objective Function", "snippet": "7.2.2 Task and Objective Function Our task is to learn a good representation of the data. Contrastive learning, by and large, does this by defining what properties of the input image we wish the features to reflect, construct images which share these properties but vary others, a"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3", "title": "7.2.3 Architecture: Vision Transformer", "snippet": "7.2.3 Architecture: Vision Transformer For the architecture, we use a standard vision transformer. Here is how such an architecture works, formally, in the context of image data. Recall from Section 7.1 that there are four components to an encoder architecture, namely an embeddin"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4", "title": "7.2.4 Optimization Strategy", "snippet": "7.2.4 Optimization Strategy Figure 7.8 : The DINO pipeline. Student features and teacher features are computed for each input. The objective attempts to align the student features with the teacher features by projecting both sets of features into a high-dimensional probability si"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5", "title": "7.2.5 Evaluation Methodology", "snippet": "7.2.5 Evaluation Methodology There are several ways to evaluate a trained transformer model. We highlight two in this section. Let us define the center crop view v cc : ℐ → ℐ v_{\\mathrm{cc}}\\colon\\mathcal{I}\\to\\mathcal{I} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT : "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6", "title": "7.2.6 Experimental Setup and Results", "snippet": "7.2.6 Experimental Setup and Results Since SimDINO is directly built upon DINO, we compare the optimal settings for DINO as given by their original paper [ CTM+21 ] with the same settings applied to SimDINO for fair comparison. Objective function. We use 10 10 10 local views (i.e"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS1", "title": "7.3.1 Task and Objective", "snippet": "7.3.1 Task and Objective Unlike before, our task is not just to learn a good representation of the data, but also simultaneously build a classifier. Formally, we have labeled data pairs ( 𝑿 , 𝒚 ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , where 𝒚 ∈ { 0 , 1 } N cls \\bm{y}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS2", "title": "7.3.2 The CRATE Architecture", "snippet": "7.3.2 The CRATE Architecture The architecture that we use is the CRATE architecture, described in some level of detail in Chapter 4 . The overall setup is similar to that of the regular transformer in Section 7.2.3 , with a few changes. While the embedding step is the same as bot"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS3", "title": "7.3.3 Optimization", "snippet": "7.3.3 Optimization We train our classifier using a simple end-to-end stochastic optimization procedure, where we sub-sample data and views, compute the average loss and its gradient over these samples, and use an optimization algorithm to change the parameters. At each timestep k"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS4", "title": "7.3.4 Evaluation Methodology", "snippet": "7.3.4 Evaluation Methodology We use the same evaluation procedure as Section 7.2.5 . To summarize, for all evaluations (as well as training) we use a center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT which reshapes the input image and t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5", "title": "7.3.5 Experimental Setup and Results", "snippet": "7.3.5 Experimental Setup and Results Since CRATE is directly based on the transformer, we compare the optimal settings for ViT as given by [ DBK+21 , TCD+20 ] with the same settings applied to CRATE for fair comparison. Model architecture. The center crop resizes the whole image "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS1", "title": "7.4.1 Data", "snippet": "7.4.1 Data The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) [ GC19 ] , an open-source reproduction of the unreleased WebText dataset used by OpenAI to train GPT2. Each sample in OWT is a web document, typically sourced from"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2", "title": "7.4.2 Task and Objective", "snippet": "7.4.2 Task and Objective For causal language modeling pre-training, the idea is that we want to train the model to output human-like text . The most popular way to do this by far is to use a two-stage training process: 5 5 5 Modern language model training has several additional t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3", "title": "7.4.3 Architecture: Causal CRATE", "snippet": "7.4.3 Architecture: Causal CRATE For the architecture, we use a standard GPT-2 style transformer, substituting CRATE layers in for the transformer layers. 9 9 9 In direct contravention of the conventions in this book and those of many other communities, the NLP community calls su"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS4", "title": "7.4.4 Optimization Strategy", "snippet": "7.4.4 Optimization Strategy We train our language model using end-to-end stochastic optimization. One remaining issue is that, in practice, different documents in a batch have different lengths (in terms of the number of tokens required for each sequence), but as of the time of w"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS5", "title": "7.4.5 Evaluation Methodology", "snippet": "7.4.5 Evaluation Methodology There are several ways to evaluate a trained transformer language model. • On a holdout dataset of arbitrary text, we can evaluate ℒ CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT on it; lower losses are b"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6", "title": "7.4.6 Experimental Setup and Results", "snippet": "7.4.6 Experimental Setup and Results Since our causal CRATE architecture is directly built upon GPT-2, we compare the optimal settings for GPT-2 as given by the NanoGPT repository [ Kar22 ] with the same settings applied to CRATE for fair comparison. Model architecture. We use th"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS1", "title": "7.5.1 Increasing Network Width: CRATE- α \\alpha italic_α", "snippet": "7.5.1 Increasing Network Width: CRATE- α \\alpha italic_α Figure 7.14 : One layer of the CRATE- α \\alpha italic_α backbone. The difference from CRATE is that the ISTA θ ℓ \\operatorname{ISTA}_{\\theta}^{\\ell} roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERS"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS2", "title": "7.5.2 Linear Time Complexity Transformers", "snippet": "7.5.2 Linear Time Complexity Transformers In practice, deep learning models suffer bottlenecks to space and time complexity, representing problem sizes which they cannot scale beyond given fixed resources. One such bottleneck, particularly meaningful when dealing with data where "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS3", "title": "7.5.3 Attention-Only Transformers", "snippet": "7.5.3 Attention-Only Transformers Another bottleneck to remove from deep learning models, specifically transformer-like architectures, is the memory bottleneck which comes from massive matrix multiplications in MLPs, where the internal dimension is far greater than the feature di"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS1", "title": "7.6.1 Task and Objective", "snippet": "7.6.1 Task and Objective As the name suggests, masked autoencoding involves a view v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT which, given an input, performs a random resized crop (cf Section 7.2.2 ) to turn the input image into a square image of size ( C ,"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2", "title": "7.6.2 Architecture", "snippet": "7.6.2 Architecture Figure 7.19 : One layer of the encoder and decoder in a CRATE autoencoder backbone. The encoder and decoder layers both feed their inputs through multi-head subspace self-attention and a dictionary learning or dictionary encoding step. Note that the encoder and"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS3", "title": "7.6.3 Optimization", "snippet": "7.6.3 Optimization As in Section 7.3.3 , we use a simple optimization setup: we sample images and masks, compute the loss on those samples and the gradients of this loss, and update the parameters using a generic optimization algorithm and the aforementioned gradients. For each t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS4", "title": "7.6.4 Evaluation", "snippet": "7.6.4 Evaluation This is the first autoencoder network we discuss in this chapter. We use the same center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT as in Sections 7.2.5 and 7.3.4 , resizing the final image to a square with side length "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5", "title": "7.6.5 Experiments", "snippet": "7.6.5 Experiments Since CRATE-MAE is directly based on the ViT-MAE, we compare the optimal settings for ViT-MAE as given by [ HCX+22 ] with the same settings applied to CRATE-MAE for fair comparison. Model architecture. During training, the masked crop v m v_{m} italic_v start_PO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px1", "title": "Embedding.", "snippet": "Embedding. Given imagery data 𝑿 ∈ ℐ \\bm{X}\\in\\mathcal{I} bold_italic_X ∈ caligraphic_I , we embed it as a sequence of tokens in ℝ d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT using the map f θ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSU"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px2", "title": "Backbone.", "snippet": "Backbone. Given a sequence of embeddings 𝒁 θ 1 ​ ( 𝑿 ) ≐ f θ emb ​ ( 𝑿 ) ∈ ( ℝ d ) ∗ \\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px3", "title": "Feature extractor.", "snippet": "Feature extractor. We use a post-processing step f θ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the class token feature , which (recall) is the feature meant to contain a"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px4", "title": "Task-specific (“DINO”) head.", "snippet": "Task-specific (“DINO”) head. For DINO, we use the task-specific DINO head h 𝑾 , 𝝁 h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT . For SimDINO, we use no task-specific head at all , as previously described."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1", "title": "Optimizing DINO.", "snippet": "Optimizing DINO. We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses two sets of weights for the same architecture : student weights θ s \\theta_{\\mathrm{s}} italic_θ start_POSTSUBSCRIPT roman_s end_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px2", "title": "Optimizing SimDINO.", "snippet": "Optimizing SimDINO. The simplified DINO population-level objective is very similar in spirit but much simpler in execution, i.e., ℒ SimDINO − st ( θ s , θ t ) ≐ 𝔼 [ d ℓ 2 ( 𝒛 θ t ( 𝑿 g ) , 𝒛 θ s ( 𝑿 c ) ) ] − γ 2 log det ( 𝑰 + d ε 2 Cov ( 𝒛 θ s ( 𝑿 g ) ) ) ) . \\mathcal{L}_{\\mathr"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px1", "title": "Linear probing.", "snippet": "Linear probing. The first, and most architecture-agnostic, way to evaluate an encoder model 𝑿 ↦ 𝒛 θ ​ ( 𝑿 ) \\bm{X}\\mapsto\\bm{z}_{\\theta}(\\bm{X}) bold_italic_X ↦ bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) is to employ linear probing . Linear pro"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px2", "title": "k k italic_k -nearest neighbors.", "snippet": "k k italic_k -nearest neighbors. We can also evaluate the performance of the features on classification tasks without needing to explicitly train a classifier by using the k k italic_k -nearest neighbor algorithm to get an average predicted label. Namely, given a dataset { 𝒛 b } "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px3", "title": "Fidelity of the attention maps.", "snippet": "Fidelity of the attention maps. Another way to check the performance of the representations, for a transformer-based encoder, is to examine the fidelity of the attention maps 𝑨 L , k ∈ ℝ n × n \\bm{A}^{L,k}\\in\\mathbb{R}^{n\\times n} bold_italic_A start_POSTSUPERSCRIPT italic_L , it"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px4", "title": "Object detection and segmentation.", "snippet": "Object detection and segmentation. We can evaluate how the representations capture the fine-grained (i.e., smaller or more detailed) properties of the input by using them for semantic segmentation . Roughly, this means that we use the features to construct bounding boxes for all "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px1", "title": "Objective function.", "snippet": "Objective function. We use 10 10 10 local views (i.e., M loc = 10 M_{\\mathrm{loc}}=10 italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10 ) of resolution 96 × 96 96\\times 96 96 × 96 (i.e., S loc = 96 S_{\\mathrm{loc}}=96 italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px2", "title": "Model architecture.", "snippet": "Model architecture. For all inputs we set the patch size to be 16 × 16 16\\times 16 16 × 16 (namely, P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16 ). We use the small, base, and "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px3", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, both our DINO reproduction and SimDINO use the ImageNet-1K dataset across all methods. We use AdamW [ LH17 ] as the optimizer, which is a very standard choice. We follow the following hyperparameter recommendations: • The batch size is"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px4", "title": "Evaluation results.", "snippet": "Evaluation results. In terms of downsream classification performance, we obtain the performance in Table 7.1 . We observe that the performance of SimDINO is much higher than in DINO under fair comparison. Also, it is much more stable: the prescribed settings of DINO cannot train "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. The center crop resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a center crop of size 224 × 224 224\\times 224 224 × 2"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, we use the ImageNet-1K dataset. We use the LION optimizer [ CLH+24 ] to pre-train both our ViT replication as well as CRATE. We set the base learning rate as 2.4 × 10 − 4 2.4\\times 10^{-4} 2.4 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.3 demonstrates that CRATE models achieve parity or improvement compared to the popular Vision Transformer (ViT) architecture at similar parameter counts, at least in terms of the linear separability of their features w.r.t. different classes. In terms "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px1", "title": "Embedding.", "snippet": "Embedding. We first embed the token sequence 𝑿 ∈ [ V ] N \\bm{X}\\in[V]^{N} bold_italic_X ∈ [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT to Euclidean space. This is often done by associating each index in [ V ] [V] [ italic_V ] with a vector in ℝ d \\mathbb{R}^{d}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px2", "title": "Backbone.", "snippet": "Backbone. We process the embeddings using a CRATE-like backbone which uses causal masking. To motivate causal masking, consider the causal language modeling loss ℒ CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT defined in ( 7.4.1 ). T"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px3", "title": "Feature extractor.", "snippet": "Feature extractor. We use a post-processing step f θ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the feature vector of the last known token so as to predict the next token"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px4", "title": "Task-specific head.", "snippet": "Task-specific head. For our classification head h θ h_{\\theta} italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , the GPT-2 architecture uses a simple linear layer and a softmax to get the desired probability vectors: h θ ​ ( 𝒛 ) ≐ softmax ⁡ ( 𝑾 out ​ 𝒛 + 𝒃 out ) , h_{\\the"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. We use the GPT-2 tokenizer, which has vocabulary size V = 50257 V=50257 italic_V = 50257 , including a special token for <|pad|> . 13 13 13 The <|bos|> token is not included in this setup, although it is very common in modern language models. The context lengt"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For training causal CRATE, we follow the implementations in the NanoGPT repository [ Kar22 ] . Specifically, we use a batch size of 384 and train for 600,000 steps with the Adam optimizer [ KB14 ] . For the Adam optimizer, we use ( β 1 , β 2 ) = ( 0.9 ,"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.5 demonstrates that CRATE models achieve reasonable performance on the causal language modeling loss across a variety of datasets compared to GPT-2 models with similar parameter counts and similar architectures. Table 7.5: Zero-shot cross-entropy loss "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px1", "title": "The encoder.", "snippet": "The encoder. The encoder is the same as the CRATE encoder in Section 7.3.2 , with the caveat that there is no feature extractor f θ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT . However,"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px2", "title": "The decoder backbone.", "snippet": "The decoder backbone. The decoder backbone is the CRATE decoder described in Chapter 5 . For completeness’ sake, we describe it now. Given a feature sequence 𝒁 θ ​ ( 𝑿 ) ≐ f θ ​ ( 𝑿 ) ∈ ( ℝ d ) ∗ \\bm{Z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px3", "title": "The un-embedding module.", "snippet": "The un-embedding module. To transform 𝒁 ~ θ , η ​ ( 𝑿 ) \\tilde{\\bm{Z}}_{\\theta,\\eta}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X ) back into an estimate for 𝑿 \\bm{X} bold_italic_X , we need to undo the ef"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. During training, the masked crop v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSU"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, we use the ImageNet-1K dataset. We use the AdamW optimizer to pre-train both our ViT-MAE replication as well as CRATE-MAE. We set the base learning rate as 3 × 10 − 5 3\\times 10^{-5} 3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.12 demonstrates that CRATE-MAE models achieve, roughly speaking, parity compared to the popular ViT-MAE architecture at similar parameter counts, and also that the feature learning performance (as measured by performance on downstream classification ta"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise1", "title": "Exercise 7.1 .", "snippet": "Exercise 7.1 . Read the DINO paper [ CTM+21 ] ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise2", "title": "Exercise 7.2 .", "snippet": "Exercise 7.2 . DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of images with similar local information are similar. Formulate an "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise3", "title": "Exercise 7.3 .", "snippet": "Exercise 7.3 . This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations. (a) Propose an alternative to the term involving R ε R_{\\varepsilon} italic_R start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT in ( 7.2.34 ) fo"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise4", "title": "Exercise 7.4 .", "snippet": "Exercise 7.4 . Prove that ( 7.2.37 ) and ( 7.2.38 ) are convex optimization problems."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise5", "title": "Exercise 7.5 .", "snippet": "Exercise 7.5 . (a) Implement the CRATE and CRATE- α \\alpha italic_α models. (b) Compare their performance and efficiency on the CIFAR-10 dataset. (c) Compare their interpretability in two ways: • The sparsity ‖ 𝒁 ‖ 0 \\|\\bm{Z}\\|_{0} ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E1", "title": "d CE ​ ( 𝒑 , 𝒒 ) ≐ CE ⁡ ( 𝒑 , 𝒒 ) , ∀ 𝒑 , 𝒒 ∈ Δ m d_{\\operatorname{CE}}(\\bm{p},\\bm{q})\\doteq\\operatorname{CE}(\\bm{p},\\bm{q}),\\quad\\forall\\bm{p},\\bm{q}\\in\\Delta_{m} italic_d start_POSTSUBSCRIPT roman_C", "snippet": "d CE ​ ( 𝒑 , 𝒒 ) ≐ CE ⁡ ( 𝒑 , 𝒒 ) , ∀ 𝒑 , 𝒒 ∈ Δ m d_{\\operatorname{CE}}(\\bm{p},\\bm{q})\\doteq\\operatorname{CE}(\\bm{p},\\bm{q}),\\quad\\forall\\bm{p},\\bm{q}\\in\\Delta_{m} italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p , bold_italic_q ) ≐ roman_CE ( bold_italic_p"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E2", "title": "CE ⁡ ( 𝒑 , 𝒒 ) ≐ − ∑ i = 1 m p i ​ log ⁡ q i , ∀ 𝒑 = ( p 1 , … , p m ) , 𝒒 = ( q 1 , … , q m ) ∈ Δ m . \\operatorname{CE}(\\bm{p},\\bm{q})\\doteq-\\sum_{i=1}^{m}p_{i}\\log q_{i},\\quad\\forall\\bm{p}=(p_{1},\\d", "snippet": "CE ⁡ ( 𝒑 , 𝒒 ) ≐ − ∑ i = 1 m p i ​ log ⁡ q i , ∀ 𝒑 = ( p 1 , … , p m ) , 𝒒 = ( q 1 , … , q m ) ∈ Δ m . \\operatorname{CE}(\\bm{p},\\bm{q})\\doteq-\\sum_{i=1}^{m}p_{i}\\log q_{i},\\quad\\forall\\bm{p}=(p_{1},\\dots,p_{m}),\\bm{q}=(q_{1},\\dots,q_{m})\\in\\Delta_{m}. roman_CE ( bold_italic_p , b"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E4", "title": "𝖪𝖫 ⁡ ( 𝒑 ∥ 𝒒 ) ≐ ∑ i = 1 m p i ​ log ⁡ ( p i / q i ) , \\operatorname{\\mathsf{KL}}(\\bm{p}\\;\\|\\;\\bm{q})\\doteq\\sum_{i=1}^{m}p_{i}\\log(p_{i}/q_{i}), sansserif_KL ( bold_italic_p ∥ bold_italic_q ) ≐ ∑ star", "snippet": "𝖪𝖫 ⁡ ( 𝒑 ∥ 𝒒 ) ≐ ∑ i = 1 m p i ​ log ⁡ ( p i / q i ) , \\operatorname{\\mathsf{KL}}(\\bm{p}\\;\\|\\;\\bm{q})\\doteq\\sum_{i=1}^{m}p_{i}\\log(p_{i}/q_{i}), sansserif_KL ( bold_italic_p ∥ bold_italic_q ) ≐ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m en"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E5", "title": "h 𝑾 , 𝝁 ​ ( 𝒛 ) ≐ softmax ⁡ ( [ 𝑾 ​ 𝒛 − 𝝁 ] / τ ) , ∀ 𝒛 ∈ ℝ d , h_{\\bm{W},\\bm{\\mu}}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}([\\bm{W}\\bm{z}-\\bm{\\mu}]/\\tau),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d}, ita", "snippet": "h 𝑾 , 𝝁 ​ ( 𝒛 ) ≐ softmax ⁡ ( [ 𝑾 ​ 𝒛 − 𝝁 ] / τ ) , ∀ 𝒛 ∈ ℝ d , h_{\\bm{W},\\bm{\\mu}}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}([\\bm{W}\\bm{z}-\\bm{\\mu}]/\\tau),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d}, italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E6", "title": "softmax ⁡ ( [ x 1 ⋮ x s ] ) ≐ 1 ∑ i = 1 s e x i ​ [ e x 1 ⋮ e x s ] \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{s}\\end{bmatrix}\\right)\\doteq\\frac{1}{\\sum_{i=1}^{s}e^{x_{i}}", "snippet": "softmax ⁡ ( [ x 1 ⋮ x s ] ) ≐ 1 ∑ i = 1 s e x i ​ [ e x 1 ⋮ e x s ] \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{s}\\end{bmatrix}\\right)\\doteq\\frac{1}{\\sum_{i=1}^{s}e^{x_{i}}}\\begin{bmatrix}e^{x_{1}}\\\\ \\vdots\\\\ e^{x_{s}}\\end{bmatrix} roman_softmax ( [ st"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E7", "title": "min θ , 𝑾 , 𝝁 ⁡ ℒ DINO ​ ( θ , 𝑾 , 𝝁 ) where ℒ DINO ​ ( θ , 𝑾 , 𝝁 ) ≐ 𝔼 ⁡ [ d CE ​ ( 𝒑 θ , 𝑾 , 𝝁 ​ ( 𝑿 g ) , 𝒑 θ , 𝑾 ​ ( 𝑿 c ) ) ] , \\min_{\\theta,\\bm{W},\\bm{\\mu}}\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm", "snippet": "min θ , 𝑾 , 𝝁 ⁡ ℒ DINO ​ ( θ , 𝑾 , 𝝁 ) where ℒ DINO ​ ( θ , 𝑾 , 𝝁 ) ≐ 𝔼 ⁡ [ d CE ​ ( 𝒑 θ , 𝑾 , 𝝁 ​ ( 𝑿 g ) , 𝒑 θ , 𝑾 ​ ( 𝑿 c ) ) ] , \\min_{\\theta,\\bm{W},\\bm{\\mu}}\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm{W},\\bm{\\mu})\\qquad\\text{where}\\qquad\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm{W},\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E8", "title": "d ℓ 2 ​ ( 𝒙 , 𝒚 ) ≐ 1 2 ​ ‖ 𝒙 − 𝒚 ‖ 2 2 , ∀ 𝒙 , 𝒚 ∈ ℝ d . d_{\\ell^{2}}(\\bm{x},\\bm{y})\\doteq\\frac{1}{2}\\|\\bm{x}-\\bm{y}\\|_{2}^{2},\\qquad\\forall\\bm{x},\\bm{y}\\in\\mathbb{R}^{d}. italic_d start_POSTSUBSCRIP", "snippet": "d ℓ 2 ​ ( 𝒙 , 𝒚 ) ≐ 1 2 ​ ‖ 𝒙 − 𝒚 ‖ 2 2 , ∀ 𝒙 , 𝒚 ∈ ℝ d . d_{\\ell^{2}}(\\bm{x},\\bm{y})\\doteq\\frac{1}{2}\\|\\bm{x}-\\bm{y}\\|_{2}^{2},\\qquad\\forall\\bm{x},\\bm{y}\\in\\mathbb{R}^{d}. italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_i"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E9", "title": "ℒ SimDINO ​ ( θ ) ≐ 𝔼 ⁡ [ d ℓ 2 ​ ( 𝒛 θ ​ ( 𝑿 g ) , 𝒛 θ ​ ( 𝑿 c ) ) ] − γ 2 ​ log ​ det ( 𝑰 + d ε 2 ​ Cov ⁡ ( 𝒛 θ ​ ( 𝑿 g ) ) ) , \\mathcal{L}_{\\mathrm{SimDINO}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[", "snippet": "ℒ SimDINO ​ ( θ ) ≐ 𝔼 ⁡ [ d ℓ 2 ​ ( 𝒛 θ ​ ( 𝑿 g ) , 𝒛 θ ​ ( 𝑿 c ) ) ] − γ 2 ​ log ​ det ( 𝑰 + d ε 2 ​ Cov ⁡ ( 𝒛 θ ​ ( 𝑿 g ) ) ) , \\mathcal{L}_{\\mathrm{SimDINO}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[d_{\\ell^{2}}(\\bm{z}_{\\theta}(\\bm{X}_{g}),\\bm{z}_{\\theta}(\\bm{X}_{c}))]-\\frac{\\ga"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E10", "title": "𝑿 patch ↦ [ 𝒛 cls 1 , 𝑾 emb ​ 𝑿 ] + 𝑬 pos . \\bm{X}^{\\mathrm{patch}}\\mapsto[\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}\\bm{X}]+\\bm{E}^{\\mathrm{pos}}. bold_italic_X start_POSTSUPERSCRIPT roman_patch", "snippet": "𝑿 patch ↦ [ 𝒛 cls 1 , 𝑾 emb ​ 𝑿 ] + 𝑬 pos . \\bm{X}^{\\mathrm{patch}}\\mapsto[\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}\\bm{X}]+\\bm{E}^{\\mathrm{pos}}. bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ↦ [ bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E11", "title": "f θ emb ​ ( 𝑿 ) ≐ [ 𝒛 cls 1 , 𝑾 emb ​ f patch ​ ( 𝑿 ) + 𝑬 pos ] . f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}", "snippet": "f θ emb ​ ( 𝑿 ) ≐ [ 𝒛 cls 1 , 𝑾 emb ​ f patch ​ ( 𝑿 ) + 𝑬 pos ] . f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{\\mathrm{pos}}\\end{bmatrix}. italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBS"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E12", "title": "f θ bb = f θ L ∘ ⋯ ∘ f θ 1 . f_{\\theta}^{\\mathrm{bb}}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}. italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPE", "snippet": "f θ bb = f θ L ∘ ⋯ ∘ f θ 1 . f_{\\theta}^{\\mathrm{bb}}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}. italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPE"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E19", "title": "𝑨 θ k , ℓ ​ ( 𝒁 ) ≐ 𝑨 ​ ( [ 𝑼 qry k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 key k , ℓ ] ⊤ ​ 𝒁 ) , SA θ k , ℓ ⁡ ( 𝒁 ) ≐ SA ⁡ ( [ 𝑼 qry k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 key k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 val k , ℓ ] ⊤ ​ 𝒁 ) \\bm{A}_{\\theta}^{k,\\ell}", "snippet": "𝑨 θ k , ℓ ​ ( 𝒁 ) ≐ 𝑨 ​ ( [ 𝑼 qry k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 key k , ℓ ] ⊤ ​ 𝒁 ) , SA θ k , ℓ ⁡ ( 𝒁 ) ≐ SA ⁡ ( [ 𝑼 qry k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 key k , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 val k , ℓ ] ⊤ ​ 𝒁 ) \\bm{A}_{\\theta}^{k,\\ell}(\\bm{Z})\\doteq\\bm{A}([\\bm{U}_{\\mathrm{qry}}^{k,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mat"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E20", "title": "MLP θ ℓ ⁡ ( 𝒁 ) ≐ 𝑾 down ℓ ​ ReLU ⁡ ( 𝑾 up ℓ ​ 𝒁 + 𝒃 up ℓ ​ 𝟏 n ⊤ ) + 𝒃 down ℓ ​ 𝟏 n ⊤ \\operatorname{MLP}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{W}_{\\mathrm{down}}^{\\ell}\\operatorname{ReLU}(\\bm{W}_{\\mathrm{", "snippet": "MLP θ ℓ ⁡ ( 𝒁 ) ≐ 𝑾 down ℓ ​ ReLU ⁡ ( 𝑾 up ℓ ​ 𝒁 + 𝒃 up ℓ ​ 𝟏 n ⊤ ) + 𝒃 down ℓ ​ 𝟏 n ⊤ \\operatorname{MLP}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{W}_{\\mathrm{down}}^{\\ell}\\operatorname{ReLU}(\\bm{W}_{\\mathrm{up}}^{\\ell}\\bm{Z}+\\bm{b}_{\\mathrm{up}}^{\\ell}\\bm{1}_{n}^{\\top})+\\bm{b}_{\\mathrm{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E21", "title": "LN θ i , ℓ ⁡ ( 𝒁 ) = LN θ i , ℓ ⁡ ( [ 𝒛 1 , … , 𝒛 n ] ) = [ LN θ i , ℓ ⁡ ( 𝒛 1 ) , … , LN θ i , ℓ ⁡ ( 𝒛 n ) ] \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{Z})=\\operatorname{LN}_{\\theta}^{i,\\ell}(\\begin{bma", "snippet": "LN θ i , ℓ ⁡ ( 𝒁 ) = LN θ i , ℓ ⁡ ( [ 𝒛 1 , … , 𝒛 n ] ) = [ LN θ i , ℓ ⁡ ( 𝒛 1 ) , … , LN θ i , ℓ ⁡ ( 𝒛 n ) ] \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{Z})=\\operatorname{LN}_{\\theta}^{i,\\ell}(\\begin{bmatrix}\\bm{z}_{1},\\dots,\\bm{z}_{n}\\end{bmatrix})=\\begin{bmatrix}\\operatorname{LN}_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E22", "title": "LN θ i , ℓ ⁡ ( 𝒛 ) = 𝒛 − mean ⁡ ( 𝒛 ) ​ 𝟏 d ‖ 𝒛 − mean ⁡ ( 𝒛 ) ​ 𝟏 d ‖ 2 ⊙ 𝜶 i , ℓ + 𝜷 i , ℓ where mean ⁡ ( 𝒛 ) = 1 d ​ 𝟏 d ⊤ ​ 𝒛 \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z})=\\frac{\\bm{z}-\\operatorname", "snippet": "LN θ i , ℓ ⁡ ( 𝒛 ) = 𝒛 − mean ⁡ ( 𝒛 ) ​ 𝟏 d ‖ 𝒛 − mean ⁡ ( 𝒛 ) ​ 𝟏 d ‖ 2 ⊙ 𝜶 i , ℓ + 𝜷 i , ℓ where mean ⁡ ( 𝒛 ) = 1 d ​ 𝟏 d ⊤ ​ 𝒛 \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z})=\\frac{\\bm{z}-\\operatorname{mean}(\\bm{z})\\bm{1}_{d}}{\\|\\bm{z}-\\operatorname{mean}(\\bm{z})\\bm{1}_{d}\\|_{2}}\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E23", "title": "𝒛 θ ​ ( 𝑿 ) ≐ f θ ext ​ ( 𝒁 θ ​ ( 𝑿 ) ) = f θ ext ​ ( [ 𝒛 θ 1 ​ ( 𝑿 ) , … , 𝒛 θ n ​ ( 𝑿 ) ] ) ≐ MLP θ ext ⁡ ( 𝒛 θ 1 ​ ( 𝑿 ) ) ‖ MLP θ ext ⁡ ( 𝒛 θ 1 ​ ( 𝑿 ) ) ‖ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\th", "snippet": "𝒛 θ ​ ( 𝑿 ) ≐ f θ ext ​ ( 𝒁 θ ​ ( 𝑿 ) ) = f θ ext ​ ( [ 𝒛 θ 1 ​ ( 𝑿 ) , … , 𝒛 θ n ​ ( 𝑿 ) ] ) ≐ MLP θ ext ⁡ ( 𝒛 θ 1 ​ ( 𝑿 ) ) ‖ MLP θ ext ⁡ ( 𝒛 θ 1 ​ ( 𝑿 ) ) ‖ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}))=f_{\\theta}^{\\mathrm{ext}}([\\bm{z}_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E24", "title": "ℒ DINO − st ​ ( θ s , θ t , 𝑾 s , 𝑾 t , 𝝁 ) ≐ 𝔼 ⁡ [ d CE ​ ( 𝒑 θ t , 𝑾 t , 𝝁 ​ ( 𝑿 g ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 c ) ) ] . \\mathcal{L}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mat", "snippet": "ℒ DINO − st ​ ( θ s , θ t , 𝑾 s , 𝑾 t , 𝝁 ) ≐ 𝔼 ⁡ [ d CE ​ ( 𝒑 θ t , 𝑾 t , 𝝁 ​ ( 𝑿 g ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 c ) ) ] . \\mathcal{L}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{s}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu})\\doteq\\operatorname{\\ma"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E25", "title": "𝒛 θ s ​ ( 𝑿 b , ℓ ( k ) , i ) ≐ ( f θ s ext ∘ f θ s ) ​ ( 𝑿 b , ℓ ( k ) , i ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 b , ℓ ( k ) , i ) ≐ h 𝑾 s , 𝟎 m ​ ( 𝒛 θ s ​ ( 𝑿 b , ℓ ( k ) , i ​ ( θ ) ) ) \\bm{z}_{\\theta_{\\mathrm{s}", "snippet": "𝒛 θ s ​ ( 𝑿 b , ℓ ( k ) , i ) ≐ ( f θ s ext ∘ f θ s ) ​ ( 𝑿 b , ℓ ( k ) , i ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 b , ℓ ( k ) , i ) ≐ h 𝑾 s , 𝟎 m ​ ( 𝒛 θ s ​ ( 𝑿 b , ℓ ( k ) , i ​ ( θ ) ) ) \\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\circ f_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E33", "title": "ℒ SimDINO − st ( θ s , θ t ) ≐ 𝔼 [ d ℓ 2 ( 𝒛 θ t ( 𝑿 g ) , 𝒛 θ s ( 𝑿 c ) ) ] − γ 2 log det ( 𝑰 + d ε 2 Cov ( 𝒛 θ s ( 𝑿 g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s", "snippet": "ℒ SimDINO − st ( θ s , θ t ) ≐ 𝔼 [ d ℓ 2 ( 𝒛 θ t ( 𝑿 g ) , 𝒛 θ s ( 𝑿 c ) ) ] − γ 2 log det ( 𝑰 + d ε 2 Cov ( 𝒛 θ s ( 𝑿 g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}})\\doteq\\operatorname{\\mathbb{E}}\\left[d_{\\ell^{2}}(\\bm{z}_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E37", "title": "min 𝑾 ∈ ℝ N cls × d ⁡ 𝔼 ⁡ [ CE ⁡ ( 𝒚 , 𝑾 ​ 𝒛 θ ​ ( 𝑿 cc ) ) ] . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{\\m", "snippet": "min 𝑾 ∈ ℝ N cls × d ⁡ 𝔼 ⁡ [ CE ⁡ ( 𝒚 , 𝑾 ​ 𝒛 θ ​ ( 𝑿 cc ) ) ] . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{\\mathrm{cc}}))]. roman_min start_POSTSUBSCRIPT bold_italic_W ∈ blackboard_R start_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E38", "title": "min 𝑾 ∈ ℝ N cls × d ⁡ 1 B ​ ∑ b = 1 B CE ⁡ ( 𝒚 b , 𝑾 ​ 𝒛 θ ​ ( 𝑿 b , cc ) ) . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b},\\bm{W}\\bm{z}_{\\", "snippet": "min 𝑾 ∈ ℝ N cls × d ⁡ 1 B ​ ∑ b = 1 B CE ⁡ ( 𝒚 b , 𝑾 ​ 𝒛 θ ​ ( 𝑿 b , cc ) ) . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}})). roman_min start_POSTSUBSCRIPT bold_italic_W ∈ b"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E39", "title": "𝒚 ^ θ ​ ( 𝑿 ∣ { ( 𝑿 b , 𝒚 b ) } b = 1 B ) = 𝟏 ​ ( i ⋆ ) where i ⋆ ≐ arg ​ max i ∈ [ Q ] ​ ∑ b = 1 B 𝒚 b ​ 𝟏 ​ [ 𝒛 θ ​ ( 𝑿 cc , b ) ∈ NN k ⁡ ( 𝒛 θ ​ ( 𝑿 cc ) ) ] . \\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\b", "snippet": "𝒚 ^ θ ​ ( 𝑿 ∣ { ( 𝑿 b , 𝒚 b ) } b = 1 B ) = 𝟏 ​ ( i ⋆ ) where i ⋆ ≐ arg ​ max i ∈ [ Q ] ​ ∑ b = 1 B 𝒚 b ​ 𝟏 ​ [ 𝒛 θ ​ ( 𝑿 cc , b ) ∈ NN k ⁡ ( 𝒛 θ ​ ( 𝑿 cc ) ) ] . \\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{1}(i^{\\star})\\quad\\text{where}\\quad i^{\\st"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E40", "title": "𝔼 𝑿 , 𝒚 ⁡ [ 𝟏 ​ ( 𝒚 ^ θ ​ ( 𝑿 ∣ { ( 𝑿 b , 𝒚 b ) } b = 1 B ) = 𝒚 ) ] \\operatorname{\\mathbb{E}}_{\\bm{X},\\bm{y}}[\\mathbf{1}(\\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{y})]", "snippet": "𝔼 𝑿 , 𝒚 ⁡ [ 𝟏 ​ ( 𝒚 ^ θ ​ ( 𝑿 ∣ { ( 𝑿 b , 𝒚 b ) } b = 1 B ) = 𝒚 ) ] \\operatorname{\\mathbb{E}}_{\\bm{X},\\bm{y}}[\\mathbf{1}(\\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{y})] blackboard_E start_POSTSUBSCRIPT bold_italic_X , bold_italic_y end_POSTSUBSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E1", "title": "h θ ​ ( 𝒛 ) ≐ softmax ⁡ ( 𝑾 head ​ 𝒛 + 𝒃 head ) , ∀ 𝒛 ∈ ℝ d h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{head}}\\bm{z}+\\bm{b}^{\\mathrm{head}}),\\qquad\\forall\\bm{z}\\in\\mathbb{R", "snippet": "h θ ​ ( 𝒛 ) ≐ softmax ⁡ ( 𝑾 head ​ 𝒛 + 𝒃 head ) , ∀ 𝒛 ∈ ℝ d h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{head}}\\bm{z}+\\bm{b}^{\\mathrm{head}}),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d} italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E2", "title": "min θ ⁡ { ℒ CE ​ ( θ ) ≐ 𝔼 ⁡ [ CE ⁡ ( 𝒚 , 𝒑 θ ​ ( 𝑿 cc ) ) ] } . \\min_{\\theta}\\left\\{\\mathcal{L}_{\\operatorname{CE}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{p}_{\\theta}(\\bm", "snippet": "min θ ⁡ { ℒ CE ​ ( θ ) ≐ 𝔼 ⁡ [ CE ⁡ ( 𝒚 , 𝒑 θ ​ ( 𝑿 cc ) ) ] } . \\min_{\\theta}\\left\\{\\mathcal{L}_{\\operatorname{CE}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{p}_{\\theta}(\\bm{X}_{\\mathrm{cc}}))]\\right\\}. roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E5", "title": "MSSA θ ℓ ⁡ ( 𝒁 ) ≐ 𝑼 out ℓ ​ [ SA ⁡ ( [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ⋮ SA ⁡ ( [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ] + 𝒃 out ℓ ​ 𝟏 n ⊤ \\operatorname", "snippet": "MSSA θ ℓ ⁡ ( 𝒁 ) ≐ 𝑼 out ℓ ​ [ SA ⁡ ( [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ⋮ SA ⁡ ( [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ] + 𝒃 out ℓ ​ 𝟏 n ⊤ \\operatorname{MSSA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{U}_{\\mathrm{out}}^{\\ell}\\begin{bmatrix}\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E6", "title": "ISTA θ ℓ ⁡ ( 𝒁 ) ≐ ReLU ⁡ ( 𝒁 − β ​ ( 𝑫 ℓ ) ⊤ ​ ( 𝑫 ℓ ​ 𝒁 − 𝒁 ) + β ​ λ ​ 𝟏 d ​ 𝟏 n ⊤ ) , \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}", "snippet": "ISTA θ ℓ ⁡ ( 𝒁 ) ≐ ReLU ⁡ ( 𝒁 − β ​ ( 𝑫 ℓ ) ⊤ ​ ( 𝑫 ℓ ​ 𝒁 − 𝒁 ) + β ​ λ ​ 𝟏 d ​ 𝟏 n ⊤ ) , \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}^{\\ell}\\bm{Z}-\\bm{Z})+\\beta\\lambda\\bm{1}_{d}\\bm{1}_{n}^{\\top}), roman_ISTA start"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E7", "title": "ℒ ^ CE ( k ) ​ ( θ ) ≐ 1 B ​ ∑ b = 1 B CE ⁡ ( 𝒚 b ( k ) , 𝒑 θ ​ ( 𝑿 b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b}^{(", "snippet": "ℒ ^ CE ( k ) ​ ( θ ) ≐ 1 B ​ ∑ b = 1 B CE ⁡ ( 𝒚 b ( k ) , 𝒑 θ ​ ( 𝑿 b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b}^{(k)},\\bm{p}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}}^{(k)})). over^ start_ARG caligraphic_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E8", "title": "θ ( k + 1 ) ≐ OptUpdate ( k ) ​ ( θ ( k ) ; ∇ θ ℒ ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}). italic_θ start_P", "snippet": "θ ( k + 1 ) ≐ OptUpdate ( k ) ​ ( θ ( k ) ; ∇ θ ℒ ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}). italic_θ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ≐ OptUpdate start_POSTSUPERS"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E1", "title": "min θ ⁡ { ℒ CLM ​ ( θ ) ≐ 𝔼 𝑿 ⁡ [ 1 N − 1 ​ ∑ n = 1 N − 1 CE ⁡ ( 𝟏 ​ ( 𝒙 n + 1 ) , 𝒑 θ ​ ( 𝑿 : n ) ) ] } \\min_{\\theta}\\left\\{\\mathcal{L}_{\\mathrm{CLM}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}_{\\bm{X}}\\", "snippet": "min θ ⁡ { ℒ CLM ​ ( θ ) ≐ 𝔼 𝑿 ⁡ [ 1 N − 1 ​ ∑ n = 1 N − 1 CE ⁡ ( 𝟏 ​ ( 𝒙 n + 1 ) , 𝒑 θ ​ ( 𝑿 : n ) ) ] } \\min_{\\theta}\\left\\{\\mathcal{L}_{\\mathrm{CLM}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}_{\\bm{X}}\\left[\\frac{1}{N-1}\\sum_{n=1}^{N-1}\\operatorname{CE}(\\bm{1}(\\bm{x}_{n+1}),\\bm{p}_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E2", "title": "f θ emb ​ ( 𝑿 ) ≐ [ 𝑬 𝒙 1 , … , 𝑬 𝒙 N ] + 𝑬 : N pos f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq[\\bm{E}_{\\bm{x}_{1}},\\dots,\\bm{E}_{\\bm{x}_{N}}]+\\bm{E}_{:N}^{\\mathrm{pos}} italic_f start_POSTSUBSCRIPT itali", "snippet": "f θ emb ​ ( 𝑿 ) ≐ [ 𝑬 𝒙 1 , … , 𝑬 𝒙 N ] + 𝑬 : N pos f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq[\\bm{E}_{\\bm{x}_{1}},\\dots,\\bm{E}_{\\bm{x}_{N}}]+\\bm{E}_{:N}^{\\mathrm{pos}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E3", "title": "𝒁 θ ​ ( 𝑿 : n ) = 𝒁 θ ​ ( 𝑿 ) : n \\bm{Z}_{\\theta}(\\bm{X}_{:n})=\\bm{Z}_{\\theta}(\\bm{X})_{:n} bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n ", "snippet": "𝒁 θ ​ ( 𝑿 : n ) = 𝒁 θ ​ ( 𝑿 ) : n \\bm{Z}_{\\theta}(\\bm{X}_{:n})=\\bm{Z}_{\\theta}(\\bm{X})_{:n} bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) = bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E7", "title": "CausalSA ( 𝑸 , 𝑲 , 𝑽 ) t = ∑ i = 1 t 𝑽 i softmax ( [ 𝑲 : t ] ⊤ 𝑸 t ) i \\operatorname{CausalSA}(\\bm{Q},\\bm{K},\\bm{V})_{t}=\\sum_{i=1}^{t}\\bm{V}_{i}\\operatorname{\\mathrm{softmax}}\\left([\\bm{K}_{:t}]^{\\to", "snippet": "CausalSA ( 𝑸 , 𝑲 , 𝑽 ) t = ∑ i = 1 t 𝑽 i softmax ( [ 𝑲 : t ] ⊤ 𝑸 t ) i \\operatorname{CausalSA}(\\bm{Q},\\bm{K},\\bm{V})_{t}=\\sum_{i=1}^{t}\\bm{V}_{i}\\operatorname{\\mathrm{softmax}}\\left([\\bm{K}_{:t}]^{\\top}\\bm{Q}_{t}\\right)_{i} roman_CausalSA ( bold_italic_Q , bold_italic_K , bold_it"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E8", "title": "f θ ext ​ ( 𝒁 θ ​ ( 𝑿 : n ) ) ≐ ( 𝒁 θ ​ ( 𝑿 ) ) n f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}_{:n}))\\doteq(\\bm{Z}_{\\theta}(\\bm{X}))_{n} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT sta", "snippet": "f θ ext ​ ( 𝒁 θ ​ ( 𝑿 : n ) ) ≐ ( 𝒁 θ ​ ( 𝑿 ) ) n f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}_{:n}))\\doteq(\\bm{Z}_{\\theta}(\\bm{X}))_{n} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSC"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E9", "title": "h θ ​ ( 𝒛 ) ≐ softmax ⁡ ( 𝑾 out ​ 𝒛 + 𝒃 out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_θ end_PO", "snippet": "h θ ​ ( 𝒛 ) ≐ softmax ⁡ ( 𝑾 out ​ 𝒛 + 𝒃 out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) ≐ roman_softmax ( bold_italic_W start_POSTSUPERSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E10", "title": "ℒ ^ CLM ( k ) ( θ ) ≐ 1 B ​ ( N max ( k ) − 1 ) ∑ b = 1 B ∑ n = 1 N max ( k ) − 1 CE ( 𝟏 ( 𝒙 b , n + 1 ( k ) ) , 𝒑 θ ( 𝑿 b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}(\\theta)\\doteq\\frac", "snippet": "ℒ ^ CLM ( k ) ( θ ) ≐ 1 B ​ ( N max ( k ) − 1 ) ∑ b = 1 B ∑ n = 1 N max ( k ) − 1 CE ( 𝟏 ( 𝒙 b , n + 1 ( k ) ) , 𝒑 θ ( 𝑿 b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}(\\theta)\\doteq\\frac{1}{B(N_{\\max}^{(k)}-1)}\\sum_{b=1}^{B}\\sum_{n=1}^{N_{\\max}^{(k)}-1}\\operatorname"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E11", "title": "θ ( k + 1 ) ≐ OptUpdate ( k ) ​ ( θ ( k ) ; ∇ θ ℒ ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}). italic_θ start_POSTS", "snippet": "θ ( k + 1 ) ≐ OptUpdate ( k ) ​ ( θ ( k ) ; ∇ θ ℒ ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}). italic_θ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ≐ OptUpdate start_POSTSUPERSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E1", "title": "𝒁 θ ℓ + 1 = ISTA θ ℓ ⁡ ( 𝒁 θ ℓ + 1 / 2 ∣ 𝒁 θ ℓ + 1 / 2 ) \\bm{Z}_{\\theta}^{\\ell+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2}) bold_italic_Z start_POST", "snippet": "𝒁 θ ℓ + 1 = ISTA θ ℓ ⁡ ( 𝒁 θ ℓ + 1 / 2 ∣ 𝒁 θ ℓ + 1 / 2 ) \\bm{Z}_{\\theta}^{\\ell+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTS"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E2", "title": "ISTA θ ℓ ⁡ ( 𝒁 ∣ 𝒀 ) ≐ ReLU ⁡ ( 𝒁 − β ​ ( 𝑫 ℓ ) ⊤ ​ ( 𝑫 ℓ ​ 𝒁 − 𝒀 ) + β ​ λ ​ 𝟏 s ​ 𝟏 n ⊤ ) \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}\\mid\\bm{Y})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{", "snippet": "ISTA θ ℓ ⁡ ( 𝒁 ∣ 𝒀 ) ≐ ReLU ⁡ ( 𝒁 − β ​ ( 𝑫 ℓ ) ⊤ ​ ( 𝑫 ℓ ​ 𝒁 − 𝒀 ) + β ​ λ ​ 𝟏 s ​ 𝟏 n ⊤ ) \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}\\mid\\bm{Y})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}^{\\ell}\\bm{Z}-\\bm{Y})+\\beta\\lambda\\bm{1}_{s}\\bm{1}_{n}^{\\top}) roman"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E3", "title": "𝒁 θ ℓ + 1 = 𝑨 θ ℓ , T ; 𝑨 θ ℓ , t + 1 = ISTA θ ℓ ⁡ ( 𝑨 θ ℓ , t ∣ 𝒁 θ ℓ + 1 / 2 ) ∀ 0 ≤ t < T ; 𝑨 θ ℓ , 0 = 𝟎 s × n , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=", "snippet": "𝒁 θ ℓ + 1 = 𝑨 θ ℓ , T ; 𝑨 θ ℓ , t + 1 = ISTA θ ℓ ⁡ ( 𝑨 θ ℓ , t ∣ 𝒁 θ ℓ + 1 / 2 ) ∀ 0 ≤ t < T ; 𝑨 θ ℓ , 0 = 𝟎 s × n , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{A}_{\\theta}^{\\ell,t}\\mid\\bm{Z}_{\\theta}^"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E4", "title": "𝒁 θ ℓ + 1 = 𝑫 ℓ ​ 𝑨 θ ℓ , T ; 𝑨 θ ℓ , t + 1 = ISTA θ ℓ ⁡ ( 𝑨 θ ℓ , t ∣ 𝒁 θ ℓ + 1 / 2 ) ; 𝑨 θ ℓ , 0 = 𝟎 , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{D}^{\\ell}\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}", "snippet": "𝒁 θ ℓ + 1 = 𝑫 ℓ ​ 𝑨 θ ℓ , T ; 𝑨 θ ℓ , t + 1 = ISTA θ ℓ ⁡ ( 𝑨 θ ℓ , t ∣ 𝒁 θ ℓ + 1 / 2 ) ; 𝑨 θ ℓ , 0 = 𝟎 , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{D}^{\\ell}\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{A}_{\\theta}^{\\ell,t}\\mid\\bm{Z}_{\\theta}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E5", "title": "𝒁 θ ℓ + 1 ​ ( 𝑿 ) ≐ ODL θ ℓ ⁡ ( 𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})\\doteq\\operatorname{ODL}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X})). bold_italic_Z start_POSTSUBSCRIP", "snippet": "𝒁 θ ℓ + 1 ​ ( 𝑿 ) ≐ ODL θ ℓ ⁡ ( 𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})\\doteq\\operatorname{ODL}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X})). bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E8", "title": "𝒁 θ ℓ + 1 ​ ( 𝑿 ) = 𝒁 θ ℓ ​ ( 𝑿 ) + MSSA θ ℓ ⁡ ( LN θ ℓ ⁡ ( 𝒁 θ ℓ ​ ( 𝑿 ) ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\", "snippet": "𝒁 θ ℓ + 1 ​ ( 𝑿 ) = 𝒁 θ ℓ ​ ( 𝑿 ) + MSSA θ ℓ ⁡ ( LN θ ℓ ⁡ ( 𝒁 θ ℓ ​ ( 𝑿 ) ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X}))). bold_italic_Z start_POSTSUBSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E1", "title": "min θ , η ⁡ { ℒ MAE ​ ( θ , η ) ≐ 𝔼 ⁡ ‖ 𝑿 ^ θ , η ​ ( 𝑿 m ) − 𝑿 ‖ F 2 } \\min_{\\theta,\\eta}\\left\\{\\mathcal{L}_{\\mathrm{MAE}}(\\theta,\\eta)\\doteq\\operatorname{\\mathbb{E}}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{", "snippet": "min θ , η ⁡ { ℒ MAE ​ ( θ , η ) ≐ 𝔼 ⁡ ‖ 𝑿 ^ θ , η ​ ( 𝑿 m ) − 𝑿 ‖ F 2 } \\min_{\\theta,\\eta}\\left\\{\\mathcal{L}_{\\mathrm{MAE}}(\\theta,\\eta)\\doteq\\operatorname{\\mathbb{E}}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{m})-\\bm{X}\\|_{F}^{2}\\right\\} roman_min start_POSTSUBSCRIPT italic_θ , italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E2", "title": "g η bb = g η L ∘ ⋯ ∘ g η 1 . g_{\\eta}^{\\mathrm{bb}}=g_{\\eta}^{L}\\circ\\cdots\\circ g_{\\eta}^{1}. italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIP", "snippet": "g η bb = g η L ∘ ⋯ ∘ g η 1 . g_{\\eta}^{\\mathrm{bb}}=g_{\\eta}^{L}\\circ\\cdots\\circ g_{\\eta}^{1}. italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E5", "title": "f θ emb ​ ( 𝑿 ) ≐ [ 𝒛 cls 1 , 𝑾 emb ​ f patch ​ ( 𝑿 ) + 𝑬 pos ] f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{", "snippet": "f θ emb ​ ( 𝑿 ) ≐ [ 𝒛 cls 1 , 𝑾 emb ​ f patch ​ ( 𝑿 ) + 𝑬 pos ] f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{\\mathrm{pos}}\\end{bmatrix} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E6", "title": "g η unemb ​ ( 𝒁 ~ ) ≐ g η unemb ​ ( [ 𝒛 ~ 1 , … , 𝒛 ~ n ] ) = g unpatch ​ ( 𝑾 unemb ​ ( [ 𝒛 ~ 2 , … , 𝒛 ~ n ] − 𝑬 ~ pos ) ) , g_{\\eta}^{\\mathrm{unemb}}(\\tilde{\\bm{Z}})\\doteq g_{\\eta}^{\\mathrm{unemb}}(", "snippet": "g η unemb ​ ( 𝒁 ~ ) ≐ g η unemb ​ ( [ 𝒛 ~ 1 , … , 𝒛 ~ n ] ) = g unpatch ​ ( 𝑾 unemb ​ ( [ 𝒛 ~ 2 , … , 𝒛 ~ n ] − 𝑬 ~ pos ) ) , g_{\\eta}^{\\mathrm{unemb}}(\\tilde{\\bm{Z}})\\doteq g_{\\eta}^{\\mathrm{unemb}}(\\begin{bmatrix}\\tilde{\\bm{z}}^{1},\\dots,\\tilde{\\bm{z}}^{n}\\end{bmatrix})=g^{\\mat"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E7", "title": "ℒ ^ MAE ( k ) ​ ( θ , η ) ≐ 1 B ​ ∑ b = 1 B ‖ 𝑿 ^ θ , η ​ ( 𝑿 b , r ( k ) ) − 𝑿 b ( k ) ‖ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\|\\hat{\\bm{X}}_{\\theta", "snippet": "ℒ ^ MAE ( k ) ​ ( θ , η ) ≐ 1 B ​ ∑ b = 1 B ‖ 𝑿 ^ θ , η ​ ( 𝑿 b , r ( k ) ) − 𝑿 b ( k ) ‖ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{b,r}^{(k)})-\\bm{X}_{b}^{(k)}\\|_{F}^{2}. over^ start_ARG caligraph"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E8", "title": "( θ ( k + 1 ) , η ( k + 1 ) ) ≐ OptUpdate ( k ) ​ ( θ ( k ) , η ( k ) ; ∇ ( θ , η ) ℒ ^ MAE ( k ) ) . (\\theta^{(k+1)},\\eta^{(k+1)})\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)},\\eta^{(k)};\\nabla_{(\\thet", "snippet": "( θ ( k + 1 ) , η ( k + 1 ) ) ≐ OptUpdate ( k ) ​ ( θ ( k ) , η ( k ) ; ∇ ( θ , η ) ℒ ^ MAE ( k ) ) . (\\theta^{(k+1)},\\eta^{(k+1)})\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)},\\eta^{(k)};\\nabla_{(\\theta,\\eta)}\\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}). ( italic_θ start_POSTSUPERSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E9", "title": "f θ ext ​ ( 𝒁 ) ≐ f θ ext ​ ( [ 𝒛 1 , … , 𝒛 n ] ) = 𝒛 1 , f_{\\theta}^{\\mathrm{ext}}(\\bm{Z})\\doteq f_{\\theta}^{\\mathrm{ext}}([\\bm{z}^{1},\\dots,\\bm{z}^{n}])=\\bm{z}^{1}, italic_f start_POSTSUBSCRIPT ital", "snippet": "f θ ext ​ ( 𝒁 ) ≐ f θ ext ​ ( [ 𝒛 1 , … , 𝒛 n ] ) = 𝒛 1 , f_{\\theta}^{\\mathrm{ext}}(\\bm{Z})\\doteq f_{\\theta}^{\\mathrm{ext}}([\\bm{z}^{1},\\dots,\\bm{z}^{n}])=\\bm{z}^{1}, italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bol"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx86", "title": "CE ⁡ ( 𝒑 , 𝒒 ) \\displaystyle\\operatorname{CE}(\\bm{p},\\bm{q}) roman_CE ( bold_italic_p , bold_italic_q ) = − ∑ i = 1 m p i ​ log ⁡ q i = ∑ i = 1 m p i ​ log ⁡ ( p i / q i ) − ∑ i = 1 m p i ​ log ⁡ p i ", "snippet": "CE ⁡ ( 𝒑 , 𝒒 ) \\displaystyle\\operatorname{CE}(\\bm{p},\\bm{q}) roman_CE ( bold_italic_p , bold_italic_q ) = − ∑ i = 1 m p i ​ log ⁡ q i = ∑ i = 1 m p i ​ log ⁡ ( p i / q i ) − ∑ i = 1 m p i ​ log ⁡ p i = 𝖪𝖫 ⁡ ( 𝒑 ∥ 𝒒 ) + H ​ ( 𝒑 ) \\displaystyle=-\\sum_{i=1}^{m}p_{i}\\log q_{i}=\\sum_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx87", "title": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = 𝒁 θ ℓ ​ ( 𝑿 ) + MHSA θ ℓ ⁡ ( LN θ 1 , ℓ ⁡ ( 𝒁 θ ℓ ​ ( 𝑿 ) ) ) \\displays"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx88", "title": "MHSA θ ℓ ⁡ ( 𝒁 ) \\displaystyle\\operatorname{MHSA}_{\\theta}^{\\ell}(\\bm{Z}) roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ", "snippet": "MHSA θ ℓ ⁡ ( 𝒁 ) \\displaystyle\\operatorname{MHSA}_{\\theta}^{\\ell}(\\bm{Z}) roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ 𝑼 out ℓ ​ [ SA ⁡ ( [ 𝑼 qry 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 key 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 val 1 ,"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx89", "title": "softmax ⁡ ( 𝑴 ) \\displaystyle\\operatorname{\\mathrm{softmax}}(\\bm{M}) roman_softmax ( bold_italic_M ) ≐ [ softmax ⁡ ( 𝒎 1 ) ⋯ softmax ⁡ ( 𝒎 p ) ] , \\displaystyle\\doteq\\begin{bmatrix}\\operatorname{\\math", "snippet": "softmax ⁡ ( 𝑴 ) \\displaystyle\\operatorname{\\mathrm{softmax}}(\\bm{M}) roman_softmax ( bold_italic_M ) ≐ [ softmax ⁡ ( 𝒎 1 ) ⋯ softmax ⁡ ( 𝒎 p ) ] , \\displaystyle\\doteq\\begin{bmatrix}\\operatorname{\\mathrm{softmax}}(\\bm{m}_{1})&\\cdots&\\operatorname{\\mathrm{softmax}}(\\bm{m}_{p})\\end{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx90", "title": "𝒛 θ s ​ ( 𝑿 b , g ( k ) , i ) ≐ ( f θ s ext ∘ f θ s ) ​ ( 𝑿 b , g ( k ) , i ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 b , g ( k ) , i ) ≐ h 𝑾 s , 𝟎 m ​ ( 𝒛 θ s ​ ( 𝑿 b , g ( k ) , i ) ) , \\displaystyle\\bm{z}_{\\theta_{\\ma", "snippet": "𝒛 θ s ​ ( 𝑿 b , g ( k ) , i ) ≐ ( f θ s ext ∘ f θ s ) ​ ( 𝑿 b , g ( k ) , i ) , 𝒑 θ s , 𝑾 s ​ ( 𝑿 b , g ( k ) , i ) ≐ h 𝑾 s , 𝟎 m ​ ( 𝒛 θ s ​ ( 𝑿 b , g ( k ) , i ) ) , \\displaystyle\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,g}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\cir"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx91", "title": "ℒ ^ DINO − st ( k ) ​ ( θ s , θ t , 𝑾 s , 𝑾 t , 𝝁 ) ≐ 1 B ​ M glo ​ ( M glo + M loc − 1 ) ​ ∑ b = 1 B ∑ i = 1 M glo \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{", "snippet": "ℒ ^ DINO − st ( k ) ​ ( θ s , θ t , 𝑾 s , 𝑾 t , 𝝁 ) ≐ 1 B ​ M glo ​ ( M glo + M loc − 1 ) ​ ∑ b = 1 B ∑ i = 1 M glo \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{s}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx92", "title": "( θ s ( k + 1 ) , 𝑾 s ( k + 1 ) ) \\displaystyle(\\theta_{\\mathrm{s}}^{(k+1)},\\bm{W}_{\\mathrm{s}}^{(k+1)}) ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 )", "snippet": "( θ s ( k + 1 ) , 𝑾 s ( k + 1 ) ) \\displaystyle(\\theta_{\\mathrm{s}}^{(k+1)},\\bm{W}_{\\mathrm{s}}^{(k+1)}) ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx93", "title": "ℒ ^ SimDINO − st ( k ) ( θ s , θ t ) ≐ 1 B ​ M glo ​ ( M glo + M loc − 1 ) ∑ b = 1 B ∑ i = 1 M glo [ ∑ j = 1 M loc d ℓ 2 ( 𝒛 θ t ( 𝑿 b , g ( k ) , i ) , 𝒛 θ s ( 𝑿 b , ℓ ( k ) , j ) ) \\displaystyle\\hat", "snippet": "ℒ ^ SimDINO − st ( k ) ( θ s , θ t ) ≐ 1 B ​ M glo ​ ( M glo + M loc − 1 ) ∑ b = 1 B ∑ i = 1 M glo [ ∑ j = 1 M loc d ℓ 2 ( 𝒛 θ t ( 𝑿 b , g ( k ) , i ) , 𝒛 θ s ( 𝑿 b , ℓ ( k ) , j ) ) \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{SimDINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx94", "title": "θ s ( k + 1 ) \\displaystyle\\theta_{\\mathrm{s}}^{(k+1)} italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT = OptUpdate ( k ) ​ ( θ s ( k )", "snippet": "θ s ( k + 1 ) \\displaystyle\\theta_{\\mathrm{s}}^{(k+1)} italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT = OptUpdate ( k ) ​ ( θ s ( k ) ; ∇ θ s ℒ ^ SimDINO − st ( k ) ) \\displaystyle=\\textsc{OptUpdate}^{(k)}(\\theta_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx95", "title": "𝑿 ↦ ⋯ ↦ 𝒁 L − 1 = [ 𝒛 1 L − 1 ⏟ class token , 𝒛 2 L − 1 ​ … , 𝒛 n L − 1 ⏟ patch tokens ] ↦ 𝑨 k , L = [ 𝑨 1 , 1 k , L 𝑨 1 , 2 : k , L 𝑨 2 ⁣ : , 1 k , L 𝑨 2 ⁣ : , 2 : k , L ] . \\displaystyle\\bm{X}\\mapst", "snippet": "𝑿 ↦ ⋯ ↦ 𝒁 L − 1 = [ 𝒛 1 L − 1 ⏟ class token , 𝒛 2 L − 1 ​ … , 𝒛 n L − 1 ⏟ patch tokens ] ↦ 𝑨 k , L = [ 𝑨 1 , 1 k , L 𝑨 1 , 2 : k , L 𝑨 2 ⁣ : , 1 k , L 𝑨 2 ⁣ : , 2 : k , L ] . \\displaystyle\\bm{X}\\mapsto\\cdots\\mapsto\\bm{Z}^{L-1}=[\\underbrace{\\bm{z}_{1}^{L-1}}_{\\text{class token}},\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx96", "title": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = 𝒁 θ ℓ ​ ( 𝑿 ) + MSSA θ ℓ ⁡ ( LN θ 1 , ℓ ⁡ ( 𝒁 θ ℓ ​ ( 𝑿 ) ) ) , \\displa"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx97", "title": "CausalMSSA θ ℓ ⁡ ( 𝒁 ) ≐ 𝑼 out ℓ ​ [ CausalSA ⁡ ( [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ⋮ CausalSA ⁡ ( [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ] + 𝒃 out ℓ ​ 𝟏", "snippet": "CausalMSSA θ ℓ ⁡ ( 𝒁 ) ≐ 𝑼 out ℓ ​ [ CausalSA ⁡ ( [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ⋮ CausalSA ⁡ ( [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 K , ℓ ] ⊤ ​ 𝒁 , [ 𝑼 1 , ℓ ] ⊤ ​ 𝒁 ) ] + 𝒃 out ℓ ​ 𝟏 N ⊤ \\displaystyle\\operatorname{CausalMSSA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{U}_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx98", "title": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "𝒁 θ ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = 𝒁 θ ℓ ​ ( 𝑿 ) + TSSA θ ℓ ⁡ ( LN θ 1 , ℓ ⁡ ( 𝒁 θ ℓ ​ ( 𝑿 ) ) ) \\displays"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx99", "title": "𝒁 ~ θ , η ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\tilde{\\bm{Z}}_{\\theta,\\eta}^{\\ell+1/2}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPER", "snippet": "𝒁 ~ θ , η ℓ + 1 / 2 ​ ( 𝑿 ) \\displaystyle\\tilde{\\bm{Z}}_{\\theta,\\eta}^{\\ell+1/2}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = [ 𝑫 ~ ℓ ] ⊤ ​ LN "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1", "title": "Training a Tokenizer", "snippet": "Training a Tokenizer To build a tokenizer, it amounts to building a vocabulary 𝒱 \\mathcal{V} caligraphic_V , which is a set of tokens and has some pre-specified size V V italic_V . There are several methods to do this. One popular algorithm is known as Byte Pair Encoding (BPE), w"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx2", "title": "Training a Language Model", "snippet": "Training a Language Model Once we have each document as a sequence of tokens 𝑿 ∈ [ V ] N ⊆ [ V ] ∗ = 𝒯 \\bm{X}\\in[V]^{N}\\subseteq[V]^{*}=\\mathcal{T} bold_italic_X ∈ [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ [ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPER"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#top", "title": "Chapter 8 Future Study of Intelligence", "snippet": ""}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1", "title": "8.1 Towards Autonomous Intelligence: Close the Loop?", "snippet": "8.1 Towards Autonomous Intelligence: Close the Loop? From the practice of machine intelligence in the past decade, it has become clear that, if there were sufficient data and computational resources, one could build a large enough model and pre-train it to learn the a priori dist"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2", "title": "8.2 Towards Intelligence of Nature: Beyond Back Propagation?", "snippet": "8.2 Towards Intelligence of Nature: Beyond Back Propagation? The practice of machine intelligence in the past few years has led many to believe that one needs to build a single large model to learn the distribution of all data and memorize all knowledge. Even if this might be tec"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3", "title": "8.3 Towards Artificial Intelligence of Human: Beyond the Turing Test?", "snippet": "8.3 Towards Artificial Intelligence of Human: Beyond the Turing Test? As we have discussed at the beginning of this book, Chapter 1 , intelligence in nature has evolved through multiple phases and manifested in four different forms: phylogentic ⟹ ontogenetic ⟹ societal ⟹ artifici"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.E1", "title": "p t ​ ( 𝒙 ) → p t + 1 ​ ( 𝒙 ) or p t ​ ( 𝒙 ∣ 𝒚 ) → p t + 1 ​ ( 𝒙 ∣ 𝒚 ) , p_{t}(\\bm{x})\\rightarrow p_{t+1}(\\bm{x})\\quad\\mbox{or}\\quad p_{t}(\\bm{x}\\mid\\bm{y})\\rightarrow p_{t+1}(\\bm{x}\\mid\\bm{y}), itali", "snippet": "p t ​ ( 𝒙 ) → p t + 1 ​ ( 𝒙 ) or p t ​ ( 𝒙 ∣ 𝒚 ) → p t + 1 ​ ( 𝒙 ∣ 𝒚 ) , p_{t}(\\bm{x})\\rightarrow p_{t+1}(\\bm{x})\\quad\\mbox{or}\\quad p_{t}(\\bm{x}\\mid\\bm{y})\\rightarrow p_{t+1}(\\bm{x}\\mid\\bm{y}), italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x ) → italic_p "}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.E2", "title": "open-ended models ⟹ closed-loop systems \\mbox{{open-ended} models}\\;\\Longrightarrow\\;\\mbox{{closed-loop} systems} bold_open-ended models ⟹ bold_closed-loop systems (8.1.2)", "snippet": "open-ended models ⟹ closed-loop systems \\mbox{{open-ended} models}\\;\\Longrightarrow\\;\\mbox{{closed-loop} systems} bold_open-ended models ⟹ bold_closed-loop systems (8.1.2)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.E1", "title": "incomputable ⟹ computable ⟹ tractable ⟹ scalable . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. incomputable ⟹ ", "snippet": "incomputable ⟹ computable ⟹ tractable ⟹ scalable . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. incomputable ⟹ computable ⟹ tractable ⟹ scalable . (8.2.1)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.E2", "title": "incomputable ⟹ computable ⟹ tractable ⟹ scalable ⟹ natural . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}\\;\\Long", "snippet": "incomputable ⟹ computable ⟹ tractable ⟹ scalable ⟹ natural . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}\\;\\Longrightarrow\\;\\mbox{{natural}}. incomputable ⟹ computable ⟹ tractable ⟹ scalable ⟹"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.E1", "title": "phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial int", "snippet": "phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence . (8"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.E2", "title": "low-level (animal) intelligence ⟹ high-level (human) intelligence. \\mbox{{low-level} (animal) intelligence}\\;\\Longrightarrow\\;\\mbox{{high-level} (human) intelligence.} bold_low-level (animal) intellig", "snippet": "low-level (animal) intelligence ⟹ high-level (human) intelligence. \\mbox{{low-level} (animal) intelligence}\\;\\Longrightarrow\\;\\mbox{{high-level} (human) intelligence.} bold_low-level (animal) intelligence ⟹ bold_high-level (human) intelligence. (8.3.2)"}, {"page": "Preface", "href": "Chx1.html#top", "title": "Preface", "snippet": ""}]}