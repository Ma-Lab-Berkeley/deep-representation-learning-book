<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 09:48:41 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Technical Setup and Outline of the Chapter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Simplified Contrastive Learning</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Task and Objective Function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS3" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Architecture: Vision Transformer</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS3.SSS0.Px1" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Embedding.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS3.SSS0.Px2" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Backbone.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS3.SSS0.Px3" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Feature extractor.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS3.SSS0.Px4" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Task-specific (“DINO”) head.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS4" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.4 </span>Optimization Strategy</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS4.SSS0.Px1" title="In 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Optimizing DINO.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS4.SSS0.Px2" title="In 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Optimizing SimDINO.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS5" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.5 </span>Evaluation Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS5.SSS0.Px1" title="In 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Linear probing.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS5.SSS0.Px2" title="In 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><math alttext="k" class="ltx_Math" display="inline"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbors.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS5.SSS0.Px3" title="In 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Fidelity of the attention maps.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS5.SSS0.Px4" title="In 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Object detection and segmentation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS6" title="In 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.6 </span>Experimental Setup and Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS6.SSS0.Px1" title="In 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Objective function.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS6.SSS0.Px2" title="In 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Model architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS6.SSS0.Px3" title="In 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Datasets and optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS6.SSS0.Px4" title="In 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Evaluation results.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Image Classification</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="In 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span>Task and Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="In 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.2 </span>The CRATE Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="In 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.3 </span>Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS4" title="In 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.4 </span>Evaluation Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS5" title="In 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.5 </span>Experimental Setup and Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS5.SSS0.Px1" title="In 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Model architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS5.SSS0.Px2" title="In 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Datasets and optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS5.SSS0.Px3" title="In 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Experiment results.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Causal Language Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.2 </span>Task and Objective</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSSx1" title="In 7.4.2 Task and Objective ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Training a Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSSx2" title="In 7.4.2 Task and Objective ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Training a Language Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.3 </span>Architecture: Causal CRATE</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px1" title="In 7.4.3 Architecture: Causal CRATE ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Embedding.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px2" title="In 7.4.3 Architecture: Causal CRATE ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Backbone.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px3" title="In 7.4.3 Architecture: Causal CRATE ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Feature extractor.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px4" title="In 7.4.3 Architecture: Causal CRATE ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Task-specific head.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS4" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.4 </span>Optimization Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS5" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.5 </span>Evaluation Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS6" title="In 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.6 </span>Experimental Setup and Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS6.SSS0.Px1" title="In 7.4.6 Experimental Setup and Results ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Model architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS6.SSS0.Px2" title="In 7.4.6 Experimental Setup and Results ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Datasets and optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS6.SSS0.Px3" title="In 7.4.6 Experimental Setup and Results ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Experiment results.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>Scaling White-Box Transformers</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="In 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.1 </span>Increasing Network Width: CRATE-<math alttext="\alpha" class="ltx_Math" display="inline"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS2" title="In 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.2 </span>Linear Time Complexity Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="In 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5.3 </span>Attention-Only Transformers</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Masked Autoencoding for Imagery Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS1" title="In 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.1 </span>Task and Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS2" title="In 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.2 </span>Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS2.SSS0.Px1" title="In 7.6.2 Architecture ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">The encoder.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS2.SSS0.Px2" title="In 7.6.2 Architecture ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">The decoder backbone.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS2.SSS0.Px3" title="In 7.6.2 Architecture ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">The un-embedding module.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS3" title="In 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.3 </span>Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS4" title="In 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.4 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS5" title="In 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6.5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS5.SSS0.Px1" title="In 7.6.5 Experiments ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Model architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS5.SSS0.Px2" title="In 7.6.5 Experiments ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Datasets and optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS5.SSS0.Px3" title="In 7.6.5 Experiments ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Experiment results.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.7 </span>Summary and Notes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S8" title="In Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.8 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 7 </span>Learning Representations for Real-World Data</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">Technical Setup and Outline of the Chapter</a></li><li><a href="#S2">Simplified Contrastive Learning</a><div class="mini-toc-sub"><a href="#S2.SS1">Data</a><a href="#S2.SS2">Task and Objective Function</a><a href="#S2.SS3">Architecture: Vision Transformer</a><a href="#S2.SS4">Optimization Strategy</a><a href="#S2.SS5">Evaluation Methodology</a><a href="#S2.SS6">Experimental Setup and Results</a></div></li><li><a href="#S3">Image Classification</a><div class="mini-toc-sub"><a href="#S3.SS1">Task and Objective</a><a href="#S3.SS2">The CRATE Architecture</a><a href="#S3.SS3">Optimization</a><a href="#S3.SS4">Evaluation Methodology</a><a href="#S3.SS5">Experimental Setup and Results</a></div></li><li><a href="#S4">Causal Language Modeling</a><div class="mini-toc-sub"><a href="#S4.SS1">Data</a><a href="#S4.SS2">Task and Objective</a><a href="#S4.SS3">Architecture: Causal CRATE</a><a href="#S4.SS4">Optimization Strategy</a><a href="#S4.SS5">Evaluation Methodology</a><a href="#S4.SS6">Experimental Setup and Results</a></div></li><li><a href="#S5">Scaling White-Box Transformers</a><div class="mini-toc-sub"><a href="#S5.SS1">Increasing Network Width: CRATE- α\alphaitalic_α</a><a href="#S5.SS2">Linear Time Complexity Transformers</a><a href="#S5.SS3">Attention-Only Transformers</a></div></li><li><a href="#S6">Masked Autoencoding for Imagery Data</a><div class="mini-toc-sub"><a href="#S6.SS1">Task and Objective</a><a href="#S6.SS2">Architecture</a><a href="#S6.SS3">Optimization</a><a href="#S6.SS4">Evaluation</a><a href="#S6.SS5">Experiments</a></div></li><li><a href="#S7">Summary and Notes</a></li><li><a href="#S8">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">The best theory is inspired by practice, and the best practice is inspired by theory</span>.”</p>
<p class="ltx_p">   — Donald Knuth</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">The previous chapters have presented a systematic introduction to mathematical problems, computational frameworks, and practical algorithms associated with learning low-dimensional distributions from high-dimensional data. Although most theoretical justifications of these methods have been established for idealistic models of data such as (mixtures of) subspaces and/or Gaussians, the principles and ideas behind these computational methods are nevertheless powerful and general, and they are in fact meant to be applicable for real-world datasets and tasks.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">To help readers understand the material in this book better and learn how to apply what you have learned so far to real-world data, towards the end of the book, we provide some demonstrations and vignettes of several representative applications. Each application proposes a solution to a real-world task with a real-world dataset (such as visual data and text data), using the methods we have introduced in this book. The results presented in this chapter are meant to serve the following two purposes:</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p">firstly, to provide additional experimental details and empirical evidence which validate the methods presented earlier in the book, and demonstrate their significant potential in real-world contexts;</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p">secondly, to introduce the reader to certain modern empirical methods and tasks in deep learning which are not well-documented outside of research or production codebases.</p>
</div>
</li>
</ul>
<p class="ltx_p">However, in our honest opinion, the solutions and results given here are designed simply to verify that the methodology works. As such, there is great room for future improvement, both in engineering and theoretical understanding, to potentially improve the state-of-the-art. We will discuss some future directions in <a class="ltx_ref" href="Ch8.html" title="Chapter 8 Future Study of Intelligence ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.1 </span>Technical Setup and Outline of the Chapter</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">In previous chapters, we alluded to different setups in which we used representation-learning techniques to process real data at scale. In this chapter, we will describe such setups in great detail. The objective of this section is to get you, the reader, to be able to reproduce any experiment discussed in this section (or indeed the book) using just the description we will give in the book, the principles introduced in previous chapters and expanded on in this chapter, and hyperparameters taken from a smattering of papers whose results are discussed in this chapter. To this end, we will <span class="ltx_text ltx_font_italic">precisely</span> describe all procedures in a detailed language, pseudocode, or mathematical notation that can be directly implemented in code. Wherever possible, we will discuss how the concrete implementations connect to the principles presented earlier in the book.</p>
</div>
<figure class="ltx_figure" id="F1"><img alt="Figure 7.1 : A diagram of the encoder pipeline. Data 𝑿 ∈ 𝒟 \bm{X}\in\mathcal{D} bold_italic_X ∈ caligraphic_D is fed through the embedding f θ emb f_{\theta}^{\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT to get a sequence in ( ℝ d ) ∗ (\mathbb{R}^{d})^{*} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT . The embedding is fed through a backbone f θ bb f_{\theta}^{\mathrm{bb}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT to get features 𝒁 θ ​ ( 𝑿 ) \bm{Z}_{\theta}(\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) for each token. We can extract an aggregate feature 𝒛 θ ​ ( 𝑿 ) \bm{z}_{\theta}(\bm{X}) bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) using the extraction map f θ ext f_{\theta}^{\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT . Finally, to use the aggregate feature in downstream tasks, we can use the task-specific head h θ h_{\theta} italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ." class="ltx_graphics" id="F1.g1" src="chapters/chapter7/figs/encoder_pipeline.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">A diagram of the encoder pipeline.<span class="ltx_text ltx_font_medium"> Data <math alttext="\bm{X}\in\mathcal{D}" class="ltx_Math" display="inline" id="F1.m9"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathcal{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ caligraphic_D</annotation></semantics></math> is fed through the embedding <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="F1.m10"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> to get a sequence in <math alttext="(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="F1.m11"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><annotation encoding="application/x-tex">(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. The embedding is fed through a backbone <math alttext="f_{\theta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="F1.m12"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> to get features <math alttext="\bm{Z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="F1.m13"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> for each token. We can extract an aggregate feature <math alttext="\bm{z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="F1.m14"><semantics><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> using the extraction map <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="F1.m15"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math>. Finally, to use the aggregate feature in downstream tasks, we can use the task-specific head <math alttext="h_{\theta}" class="ltx_Math" display="inline" id="F1.m16"><semantics><msub><mi>h</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">h_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="F2"><img alt="Figure 7.2 : A diagram of the autoencoder pipeline. Data 𝑿 ∈ 𝒟 \bm{X}\in\mathcal{D} bold_italic_X ∈ caligraphic_D is fed through the embedding f θ emb f_{\theta}^{\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT to get a sequence in ( ℝ d ) ∗ (\mathbb{R}^{d})^{*} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT . The embedding is fed through an encoder backbone f θ bb f_{\theta}^{\mathrm{bb}} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT to get features 𝒁 θ ​ ( 𝑿 ) \bm{Z}_{\theta}(\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) for each token. To decode 𝒁 θ ​ ( 𝑿 ) \bm{Z}_{\theta}(\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) , we pass it through a decoder backbone g η bb g_{\eta}^{\mathrm{bb}} italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT . To map the decoder backbone output back to data space 𝒟 \mathcal{D} caligraphic_D , we use an unembedding layer g η unemb g_{\eta}^{\mathrm{unemb}} italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT , overall obtaining a reconstruction 𝑿 ^ θ , η ​ ( 𝑿 ) \hat{\bm{X}}_{\theta,\eta}(\bm{X}) over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X ) (here stylized to be a pixelated reconstruction of the input)." class="ltx_graphics" id="F2.g1" src="chapters/chapter7/figs/autoencoder_pipeline.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">A diagram of the autoencoder pipeline.<span class="ltx_text ltx_font_medium"> Data <math alttext="\bm{X}\in\mathcal{D}" class="ltx_Math" display="inline" id="F2.m11"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathcal{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ caligraphic_D</annotation></semantics></math> is fed through the embedding <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="F2.m12"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> to get a sequence in <math alttext="(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="F2.m13"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><annotation encoding="application/x-tex">(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. The embedding is fed through an encoder backbone <math alttext="f_{\theta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="F2.m14"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> to get features <math alttext="\bm{Z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="F2.m15"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> for each token. To decode <math alttext="\bm{Z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="F2.m16"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>, we pass it through a decoder backbone <math alttext="g_{\eta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="F2.m17"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math>. To map the decoder backbone output back to data space <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="F2.m18"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math>, we use an <span class="ltx_text ltx_font_italic">unembedding</span> layer <math alttext="g_{\eta}^{\mathrm{unemb}}" class="ltx_Math" display="inline" id="F2.m19"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{unemb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT</annotation></semantics></math>, overall obtaining a reconstruction <math alttext="\hat{\bm{X}}_{\theta,\eta}(\bm{X})" class="ltx_Math" display="inline" id="F2.m20"><semantics><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{\theta,\eta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> (here stylized to be a pixelated reconstruction of the input).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Let us define the set of possible data as <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S1.p2.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> (eventually this will be the set of images <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S1.p2.m2"><semantics><mi class="ltx_font_mathcaligraphic">ℐ</mi><annotation encoding="application/x-tex">\mathcal{I}</annotation><annotation encoding="application/x-llamapun">caligraphic_I</annotation></semantics></math>, for example, or the set of text <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S1.p2.m3"><semantics><mi class="ltx_font_mathcaligraphic">𝒯</mi><annotation encoding="application/x-tex">\mathcal{T}</annotation><annotation encoding="application/x-llamapun">caligraphic_T</annotation></semantics></math>), and the set of finite sequences of tokens in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.p2.m4"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> (i.e., the set of matrices with <math alttext="d" class="ltx_Math" display="inline" id="S1.p2.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> rows) as <math alttext="(\mathbb{R}^{d})^{*}\doteq\bigcup_{T=1}^{\infty}\mathbb{R}^{d\times T}" class="ltx_Math" display="inline" id="S1.p2.m6"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo rspace="0.111em">≐</mo><mrow><msubsup><mo>⋃</mo><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>T</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">(\mathbb{R}^{d})^{*}\doteq\bigcup_{T=1}^{\infty}\mathbb{R}^{d\times T}</annotation><annotation encoding="application/x-llamapun">( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ≐ ⋃ start_POSTSUBSCRIPT italic_T = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_d × italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. In order to discuss the wealth of applications we introduce in this chapter, we first recall that in the rest of the book, we discuss two different types of model architectures.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">encoder</span> architecture, parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, which is composed of several components:</p>
<ul class="ltx_itemize" id="S1.I1.i1.I1">
<li class="ltx_item" id="S1.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i1.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">embedding</span> <math alttext="f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i1.p1.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, which converts the input data <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i1.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> into a series of <span class="ltx_text ltx_font_italic">tokens</span> which are mapped into, or <span class="ltx_text ltx_font_italic">embedded</span> in, <math alttext="D" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i1.p1.m3"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>-dimensional space. <span class="ltx_text ltx_font_italic">In the rest of the chapter, we will often identify tokens and embeddings with each other.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i2.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">encoder backbone</span> <math alttext="f_{\theta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i2.p1.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, which processes the series of embeddings using a sequence-to-sequence operation. This backbone is implemented by the network architectures discussed in the previous chapters, but we will give a more formal description as we go along.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i3.p1">
<p class="ltx_p">A <span class="ltx_text ltx_font_italic">aggregate feature extractor</span> <math alttext="f_{\theta}^{\mathrm{ext}}\colon(\mathbb{R}^{d})^{*}\to\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i3.p1.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}\colon(\mathbb{R}^{d})^{*}\to\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, which extracts an aggregate representation of the whole sequence. This is used to define a single feature for the entire data sample.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i1.I1.i4.p1">
<p class="ltx_p">A <span class="ltx_text ltx_font_italic">task-specific head</span> <math alttext="h_{\theta}\colon\mathbb{R}^{d}\to\mathbb{R}^{m}" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i4.p1.m1"><semantics><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow></mrow><annotation encoding="application/x-tex">h_{\theta}\colon\mathbb{R}^{d}\to\mathbb{R}^{m}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>, which extracts an <math alttext="m" class="ltx_Math" display="inline" id="S1.I1.i1.I1.i4.p1.m2"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math>-dimensional output for prediction.</p>
</div>
</li>
</ul>
<p class="ltx_p">We also define <math alttext="f_{\theta}\doteq f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m2"><semantics><mrow><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}\doteq f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. Given an input <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we write <math alttext="\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m4"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> and <math alttext="\bm{z}_{\theta}(\bm{X})\doteq f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}))" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m5"><semantics><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X})\doteq f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}))</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) )</annotation></semantics></math>. The overall pipeline is depicted in <a class="ltx_ref" href="#F1" title="In 7.1 Technical Setup and Outline of the Chapter ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.1</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">autoencoder</span> architecture, which is composed of several components:</p>
<ul class="ltx_itemize" id="S1.I1.i2.I1">
<li class="ltx_item" id="S1.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i1.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">embedding</span> <math alttext="f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i1.p1.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, which converts the input data <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i1.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> into a series of <span class="ltx_text ltx_font_italic">tokens</span> which are embedded in <math alttext="D" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i1.p1.m3"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>-dimensional space.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i2.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">encoder backbone</span> <math alttext="f_{\theta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i2.p1.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, which processes the series of embeddings using a sequence-to-sequence operation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i3.p1">
<p class="ltx_p">A <span class="ltx_text ltx_font_italic">decoder backbone</span> <math alttext="g_{\eta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i3.p1.m1"><semantics><mrow><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, which conceptually undoes the operation of the encoder backbone.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S1.I1.i2.I1.i4.p1">
<p class="ltx_p">An <span class="ltx_text ltx_font_italic">unembedding</span> <math alttext="g_{\eta}^{\mathrm{unemb}}\colon(\mathbb{R}^{d})^{*}\to\mathcal{D}" class="ltx_Math" display="inline" id="S1.I1.i2.I1.i4.p1.m1"><semantics><mrow><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{unemb}}\colon(\mathbb{R}^{d})^{*}\to\mathcal{D}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → caligraphic_D</annotation></semantics></math>, which acts as an inverse of the embedding.</p>
</div>
</li>
</ul>
<p class="ltx_p">We also define <math alttext="f_{\theta}\doteq f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1"><semantics><mrow><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}\doteq f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}\colon\mathcal{D}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="g_{\eta}\doteq g_{\eta}^{\mathrm{unemb}}\circ g_{\eta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to\mathcal{D}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m2"><semantics><mrow><mrow><msub><mi>g</mi><mi>η</mi></msub><mo>≐</mo><mrow><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}\doteq g_{\eta}^{\mathrm{unemb}}\circ g_{\eta}^{\mathrm{bb}}\colon(\mathbb{R}^{d})^{*}\to\mathcal{D}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ≐ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ∘ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → caligraphic_D</annotation></semantics></math>. Given an input <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we write <math alttext="\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m4"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> and <math alttext="\hat{\bm{X}}_{\theta,\eta}(\bm{X})\doteq g_{\eta}(\bm{Z}_{\theta}(\bm{X}))" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m5"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>g</mi><mi>η</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{\theta,\eta}(\bm{X})\doteq g_{\eta}(\bm{Z}_{\theta}(\bm{X}))</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) )</annotation></semantics></math>. The overall pipeline is depicted in <a class="ltx_ref" href="#F2" title="In 7.1 Technical Setup and Outline of the Chapter ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.2</span></a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">We will repeatedly use this notation many times in this chapter, so please feel free to refer back to it if something doesn’t make sense. This decomposition of our networks also closely mirrors most code implementations, and you can start your coding projects by defining these networks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">In this chapter, we will discuss applications of the book’s principles to contrastive learning in <a class="ltx_ref" href="#S2" title="7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a>. This will serve as both an introduction to imagery data, data augmentation techniques, and the common architecture known as the transformer, <span class="ltx_text ltx_font_italic">as well as</span> a first demonstration of the drastic kinds of simplifications we can make using the demonstrated principles. We will continue with modifications to the <span class="ltx_text ltx_font_italic">network architecture</span> in <a class="ltx_ref" href="#S3" title="7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">7.3</span></a> and <a class="ltx_ref" href="#S4" title="7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.4</span></a>, which demonstrate the capabilities of simplified architectures for <span class="ltx_text ltx_font_italic">encoding</span> within the image and text domains. We then demonstrate simplified architectures for <span class="ltx_text ltx_font_italic">autoencoding</span> in <a class="ltx_ref" href="#S6" title="7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.6</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.2 </span>Simplified Contrastive Learning</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">Learning high-quality and faithful representations of data is a fundamental problem in deep learning, known as <span class="ltx_text ltx_font_italic">self-supervised learning</span>. There have been many approaches proposed for this task, many of which do not evidently use the techniques and principles outlined in this manuscript. One such approach is called <span class="ltx_text ltx_font_italic">contrastive learning</span>, so named because the learning objective is (roughly speaking) about ensuring that features of “similar” data are similar, and features of “dis-similar” data are far apart. Contrastive learning solutions are often highly-engineered, empirically designed approaches. In this section, we will describe one such approach named DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite>, and use the principles described in the previous chapters to drastically simplify their design decisions while improving the learned representations.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.1 </span>Data</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">The data that we will use to explore and simplify the DINO methodology is all 2-dimensional image data. For <span class="ltx_text ltx_font_italic">training</span>, we will use the ImageNet-1K and ImageNet-21K datasets. Each sample in the dataset is an RGB image, of varying resolution, and a label indicating the object or scene that the image contains (i.e., the <span class="ltx_text ltx_font_italic">class</span> of the image). The ImageNet-1K dataset contains 1.28M training images and 50K validation images partitioned into 1K classes. The ImageNet-21K dataset contains 14.2M training images and 21.8K classes, but the classes are not disjoint (i.e., some classes are subsets of others). Since we are doing self-supervised learning, the labels will not be used during training, only during evaluation. For <span class="ltx_text ltx_font_italic">evaluation</span>, we will use a litany of datasets. Of these, the most common is CIFAR-10. CIFAR-10 is a dataset of 60K RGB 32 <math alttext="\times" class="ltx_Math" display="inline" id="S2.SS1.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation><annotation encoding="application/x-llamapun">×</annotation></semantics></math> 32 natural images partitioned into 10 classes, with a pre-established training set of 50K samples and a validation set of 10K samples. The purpose of using CIFAR-10 will be to ensure that models which train on one distribution of images (ImageNet) can generalize to another distribution of images (CIFAR-10). We also refer to other similar datasets, such as CIFAR-100 (disjoint from CIFAR-10), Oxford Flowers, and Oxford Pets. Exemplars of ImageNet-1K and CIFAR-10 data are shown in <a class="ltx_ref" href="#F3" title="In 7.2.1 Data ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.3</span></a>. In order to increase robustness of our model, we often apply <span class="ltx_text ltx_font_italic">small data augmentations</span> to each image during processing, such as flips, added small random noise, or random large crops; we do not include this in our notation, as each augmentation of a natural image is itself (very close to) a natural image in our dataset.</p>
</div>
<figure class="ltx_figure" id="F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F3.sf1"><img alt="(a) ImageNet-1K samples." class="ltx_graphics" id="F3.sf1.g1" src="chapters/chapter7/figs/imagenet.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">ImageNet-1K samples.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F3.sf2"><img alt="(a) ImageNet-1K samples." class="ltx_graphics" id="F3.sf2.g1" src="chapters/chapter7/figs/cifar10.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR10 samples.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel"><span class="ltx_text ltx_phantom"><span style="visibility:hidden"></span></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Images from ImageNet-1K <span class="ltx_text ltx_font_italic">(left)</span> and CIFAR-10 <span class="ltx_text ltx_font_italic">(right)</span>.<span class="ltx_text ltx_font_medium"> Notice that the CIFAR-10 images are much lower resolution, generally speaking, reducing the complexity of learning that distribution.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">On a slightly more formal level, our data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> will be images; we let <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS1.p2.m2"><semantics><mi class="ltx_font_mathcaligraphic">ℐ</mi><annotation encoding="application/x-tex">\mathcal{I}</annotation><annotation encoding="application/x-llamapun">caligraphic_I</annotation></semantics></math> be the set of all images. Since an image is a rectangular array of pixels, and each pixel has a color given by RGB, CMYK, or another color format, we say that an image is an element of <math alttext="\mathbb{R}^{c\times h\times w}" class="ltx_Math" display="inline" id="S2.SS1.p2.m3"><semantics><msup><mi>ℝ</mi><mrow><mi>c</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>w</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{c\times h\times w}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_c × italic_h × italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> — here <math alttext="c" class="ltx_Math" display="inline" id="S2.SS1.p2.m4"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation><annotation encoding="application/x-llamapun">italic_c</annotation></semantics></math> is the number of channels (i.e., <math alttext="3" class="ltx_Math" display="inline" id="S2.SS1.p2.m5"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation><annotation encoding="application/x-llamapun">3</annotation></semantics></math> for RGB and <math alttext="4" class="ltx_Math" display="inline" id="S2.SS1.p2.m6"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation><annotation encoding="application/x-llamapun">4</annotation></semantics></math> for CMYK), <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.p2.m7"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> is the image height, and <math alttext="w" class="ltx_Math" display="inline" id="S2.SS1.p2.m8"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation><annotation encoding="application/x-llamapun">italic_w</annotation></semantics></math> is the image width. Consequently, the set of all images <math alttext="\mathcal{I}\doteq\bigcup_{c,h,w=1}^{\infty}\mathbb{R}^{c\times h\times w}" class="ltx_Math" display="inline" id="S2.SS1.p2.m9"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo rspace="0.111em">≐</mo><mrow><msubsup><mo>⋃</mo><mrow><mrow><mi>c</mi><mo>,</mo><mi>h</mi><mo>,</mo><mi>w</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>ℝ</mi><mrow><mi>c</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>w</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{I}\doteq\bigcup_{c,h,w=1}^{\infty}\mathbb{R}^{c\times h\times w}</annotation><annotation encoding="application/x-llamapun">caligraphic_I ≐ ⋃ start_POSTSUBSCRIPT italic_c , italic_h , italic_w = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_c × italic_h × italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> is the set of all possible such data. Again, we will use this notation repeatedly.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.2 </span>Task and Objective Function</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">Our task is to learn a good representation of the data. Contrastive learning, by and large, does this by defining what properties of the input image we wish the features to reflect, construct images which share these properties but vary others, and set up a loss which promotes that the features of images with shared properties are close and images with different properties are different. The naturally optimal solution to this learning problem is that the learned features preserve the desired properties of the input. However, there are many practical and empirical complications that arise in the course of training contrastive models.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">In the case of DINO, the authors propose to use a methodology which produces a single feature vector for the whole image and desires the feature vector to contain “global” (i.e., image-level) information. Accordingly, the loss will promote that images with similar global information have similar features and images with different global information have different features.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">This seems intuitive, but as previously mentioned, there are several empirical considerations, even while setting up the loss. First and foremost, how should we promote similarities and differences? The answer of DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite> is<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In the author’s view, inexplicably…</span></span></span> to convert the output features into “logits” corresponding to some probability distribution and take their cross-entropy. More specifically, let <math alttext="\Delta_{m}\doteq\{\bm{x}\in\mathbb{R}^{m}\colon x_{i}\geq 0\ \forall i\in[m],\sum_{i=1}^{m}x_{i}=1\}" class="ltx_Math" display="inline" id="S2.SS2.p3.m1"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub><mo>≐</mo><mrow><mo stretchy="false">{</mo><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>≥</mo><mrow><mn>0</mn><mo lspace="0.667em" rspace="0em">​</mo><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0em">,</mo><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta_{m}\doteq\{\bm{x}\in\mathbb{R}^{m}\colon x_{i}\geq 0\ \forall i\in[m],\sum_{i=1}^{m}x_{i}=1\}</annotation><annotation encoding="application/x-llamapun">roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ≐ { bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≥ 0 ∀ italic_i ∈ [ italic_m ] , ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 }</annotation></semantics></math> be the space of probability vectors in <math alttext="\mathbb{R}^{m}" class="ltx_Math" display="inline" id="S2.SS2.p3.m2"><semantics><msup><mi>ℝ</mi><mi>m</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{m}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> and define the function <math alttext="d_{\operatorname{CE}}\colon\mathbb{R}^{m}\times\mathbb{R}^{m}\to\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.p3.m3"><semantics><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msup><mi>ℝ</mi><mi>m</mi></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">d_{\operatorname{CE}}\colon\mathbb{R}^{m}\times\mathbb{R}^{m}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math> by</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{\operatorname{CE}}(\bm{p},\bm{q})\doteq\operatorname{CE}(\bm{p},\bm{q}),\quad\forall\bm{p},\bm{q}\in\Delta_{m}" class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒑</mi><mo>,</mo><mi>𝒒</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒑</mi><mo>,</mo><mi>𝒒</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mo rspace="0.167em">∀</mo><mi>𝒑</mi></mrow></mrow></mrow><mo>,</mo><mrow><mi>𝒒</mi><mo>∈</mo><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub></mrow></mrow><annotation encoding="application/x-tex">d_{\operatorname{CE}}(\bm{p},\bm{q})\doteq\operatorname{CE}(\bm{p},\bm{q}),\quad\forall\bm{p},\bm{q}\in\Delta_{m}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p , bold_italic_q ) ≐ roman_CE ( bold_italic_p , bold_italic_q ) , ∀ bold_italic_p , bold_italic_q ∈ roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\operatorname{CE}\colon\Delta_{m}\times\Delta_{m}\to\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.p3.m4"><semantics><mrow><mi>CE</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">\operatorname{CE}\colon\Delta_{m}\times\Delta_{m}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">roman_CE : roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT × roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT → blackboard_R</annotation></semantics></math> is the cross-entropy, defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{CE}(\bm{p},\bm{q})\doteq-\sum_{i=1}^{m}p_{i}\log q_{i},\quad\forall\bm{p}=(p_{1},\dots,p_{m}),\bm{q}=(q_{1},\dots,q_{m})\in\Delta_{m}." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mrow><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒑</mi><mo>,</mo><mi>𝒒</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>p</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><mo rspace="0.167em">∀</mo><mi>𝒑</mi></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>p</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>𝒒</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>q</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{CE}(\bm{p},\bm{q})\doteq-\sum_{i=1}^{m}p_{i}\log q_{i},\quad\forall\bm{p}=(p_{1},\dots,p_{m}),\bm{q}=(q_{1},\dots,q_{m})\in\Delta_{m}.</annotation><annotation encoding="application/x-llamapun">roman_CE ( bold_italic_p , bold_italic_q ) ≐ - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ∀ bold_italic_p = ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) , bold_italic_q = ( italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) ∈ roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Before we continue our discussion, let us build some intuition about this distance function. We have, in particular,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx86">
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{CE}(\bm{p},\bm{q})" class="ltx_Math" display="inline" id="S2.E3.m1"><semantics><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒑</mi><mo>,</mo><mi>𝒒</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{CE}(\bm{p},\bm{q})</annotation><annotation encoding="application/x-llamapun">roman_CE ( bold_italic_p , bold_italic_q )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\sum_{i=1}^{m}p_{i}\log q_{i}=\sum_{i=1}^{m}p_{i}\log(p_{i}/q_{i})-\sum_{i=1}^{m}p_{i}\log p_{i}=\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})+H(\bm{p})" class="ltx_Math" display="inline" id="S2.E3.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msub><mi>p</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>q</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msub><mi>p</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>/</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msub><mi>p</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒑</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>𝒒</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒑</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=-\sum_{i=1}^{m}p_{i}\log q_{i}=\sum_{i=1}^{m}p_{i}\log(p_{i}/q_{i})-\sum_{i=1}^{m}p_{i}\log p_{i}=\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})+H(\bm{p})</annotation><annotation encoding="application/x-llamapun">= - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = sansserif_KL ( bold_italic_p ∥ bold_italic_q ) + italic_H ( bold_italic_p )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\operatorname{\mathsf{KL}}\colon\Delta_{m}\times\Delta_{m}\to\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.p3.m5"><semantics><mrow><mi>𝖪𝖫</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{KL}}\colon\Delta_{m}\times\Delta_{m}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">sansserif_KL : roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT × roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT → blackboard_R</annotation></semantics></math> is the KL divergence, defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})\doteq\sum_{i=1}^{m}p_{i}\log(p_{i}/q_{i})," class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒑</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>𝒒</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>p</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>/</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})\doteq\sum_{i=1}^{m}p_{i}\log(p_{i}/q_{i}),</annotation><annotation encoding="application/x-llamapun">sansserif_KL ( bold_italic_p ∥ bold_italic_q ) ≐ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="H\colon\Delta_{m}\to\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.p3.m6"><semantics><mrow><mi>H</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msub><mi mathvariant="normal">Δ</mi><mi>m</mi></msub><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">H\colon\Delta_{m}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_H : roman_Δ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT → blackboard_R</annotation></semantics></math> is the entropy of a random variable. Note
that <math alttext="\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})" class="ltx_Math" display="inline" id="S2.SS2.p3.m7"><semantics><mrow><mi>𝖪𝖫</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒑</mi><mo lspace="0.558em" rspace="0.558em">∥</mo><mi>𝒒</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathsf{KL}}(\bm{p}\;\|\;\bm{q})</annotation><annotation encoding="application/x-llamapun">sansserif_KL ( bold_italic_p ∥ bold_italic_q )</annotation></semantics></math> is minimized if and only if <math alttext="\bm{p}=\bm{q}" class="ltx_Math" display="inline" id="S2.SS2.p3.m8"><semantics><mrow><mi>𝒑</mi><mo>=</mo><mi>𝒒</mi></mrow><annotation encoding="application/x-tex">\bm{p}=\bm{q}</annotation><annotation encoding="application/x-llamapun">bold_italic_p = bold_italic_q</annotation></semantics></math>. So minimizing <math alttext="d_{\operatorname{CE}}" class="ltx_Math" display="inline" id="S2.SS2.p3.m9"><semantics><msub><mi>d</mi><mi>CE</mi></msub><annotation encoding="application/x-tex">d_{\operatorname{CE}}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT</annotation></semantics></math> does two things: it makes <math alttext="\bm{p}=\bm{q}" class="ltx_Math" display="inline" id="S2.SS2.p3.m10"><semantics><mrow><mi>𝒑</mi><mo>=</mo><mi>𝒒</mi></mrow><annotation encoding="application/x-tex">\bm{p}=\bm{q}</annotation><annotation encoding="application/x-llamapun">bold_italic_p = bold_italic_q</annotation></semantics></math>, and it makes <math alttext="\bm{p}" class="ltx_Math" display="inline" id="S2.SS2.p3.m11"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">\bm{p}</annotation><annotation encoding="application/x-llamapun">bold_italic_p</annotation></semantics></math> and <math alttext="\bm{q}" class="ltx_Math" display="inline" id="S2.SS2.p3.m12"><semantics><mi>𝒒</mi><annotation encoding="application/x-tex">\bm{q}</annotation><annotation encoding="application/x-llamapun">bold_italic_q</annotation></semantics></math> have minimal entropy (i.e., vectors with <math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.p3.m13"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> in one component and <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.p3.m14"><mn>0</mn></math> elsewhere — these are called <span class="ltx_text ltx_font_italic">one-hot vectors</span>). Overall, the goal of this objective is not <span class="ltx_text ltx_font_italic">just</span> to match <math alttext="\bm{p}" class="ltx_Math" display="inline" id="S2.SS2.p3.m15"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">\bm{p}</annotation><annotation encoding="application/x-llamapun">bold_italic_p</annotation></semantics></math> and <math alttext="\bm{q}" class="ltx_Math" display="inline" id="S2.SS2.p3.m16"><semantics><mi>𝒒</mi><annotation encoding="application/x-tex">\bm{q}</annotation><annotation encoding="application/x-llamapun">bold_italic_q</annotation></semantics></math> but also to shape them in a certain way to make them low-entropy. Keep this in mind when we discuss the formulation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p">The next question is, how should we obtain samples with similar global information? The answer of DINO (as well as nearly all contrastive learning) is <span class="ltx_text ltx_font_italic">data augmentation</span> — from each sample, make several correlated samples which share the desired properties. In the DINO case, we use different crops or <span class="ltx_text ltx_font_italic">views</span> of the input image. Recall that we model an image as an element of the set <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS2.p4.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℐ</mi><annotation encoding="application/x-tex">\mathcal{I}</annotation><annotation encoding="application/x-llamapun">caligraphic_I</annotation></semantics></math>. In this notation, a view is a function <math alttext="v\colon\mathcal{I}\to\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS2.p4.m2"><semantics><mrow><mi>v</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow></mrow><annotation encoding="application/x-tex">v\colon\mathcal{I}\to\mathcal{I}</annotation><annotation encoding="application/x-llamapun">italic_v : caligraphic_I → caligraphic_I</annotation></semantics></math>. In the DINO case, the view is a <span class="ltx_text ltx_font_italic">random resized crop</span>: it takes a randomly chosen rectangular crop of the image (which has a fixed percentage <math alttext="p_{v}\in[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p4.m3"><semantics><mrow><msub><mi>p</mi><mi>v</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{v}\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> of the total area of the image), resize it proportionally so that the <span class="ltx_text ltx_font_italic">shorter edge</span> is <math alttext="S_{\mathrm{rsz}}" class="ltx_Math" display="inline" id="S2.SS2.p4.m4"><semantics><msub><mi>S</mi><mi>rsz</mi></msub><annotation encoding="application/x-tex">S_{\mathrm{rsz}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT</annotation></semantics></math> pixels long, then resizes it to a fixed shape <math alttext="(C,S_{v},S_{v})" class="ltx_Math" display="inline" id="S2.SS2.p4.m5"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>v</mi></msub><mo>,</mo><msub><mi>S</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{v},S_{v})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )</annotation></semantics></math> where <math alttext="S_{v}\geq 1" class="ltx_Math" display="inline" id="S2.SS2.p4.m6"><semantics><mrow><msub><mi>S</mi><mi>v</mi></msub><mo>≥</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">S_{v}\geq 1</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ≥ 1</annotation></semantics></math> is the size of the view and <math alttext="C" class="ltx_Math" display="inline" id="S2.SS2.p4.m7"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math> is the number of channels in the original image.</p>
</div>
<figure class="ltx_figure" id="F4"><img alt="Figure 7.4 : Local and global views in DINO. Local views and global views take a rectangular crop of the input image and resize it to a square shape, which is then input into the network for processing." class="ltx_graphics" id="F4.g1" src="chapters/chapter7/figs/global_local_views.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Local and global views in DINO.<span class="ltx_text ltx_font_medium"> Local views and global views take a rectangular crop of the input image and resize it to a square shape, which is then input into the network for processing.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p">There are two types of views we want to use, depicted in <a class="ltx_ref" href="#F4" title="In 7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">global views</span>, which are random resized crops with area percentage parameter <math alttext="p_{\mathrm{glo}}\in[0,1]" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m1"><semantics><mrow><msub><mi>p</mi><mi>glo</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{glo}}\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> and output shape <math alttext="(C,S_{\mathrm{glo}},S_{\mathrm{glo}})" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>glo</mi></msub><mo>,</mo><msub><mi>S</mi><mi>glo</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{\mathrm{glo}},S_{\mathrm{glo}})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT )</annotation></semantics></math>;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">and <span class="ltx_text ltx_font_italic">local views</span>, which are random resized crops with area percentage parameter <math alttext="p_{\mathrm{loc}}\in[0,1]" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m1"><semantics><mrow><msub><mi>p</mi><mi>loc</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{loc}}\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> and output shape <math alttext="(C,S_{\mathrm{loc}},S_{\mathrm{loc}})" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>loc</mi></msub><mo>,</mo><msub><mi>S</mi><mi>loc</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{\mathrm{loc}},S_{\mathrm{loc}})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT )</annotation></semantics></math>. Here <math alttext="p_{\mathrm{loc}}&lt;p_{\mathrm{glo}}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m3"><semantics><mrow><msub><mi>p</mi><mi>loc</mi></msub><mo>&lt;</mo><msub><mi>p</mi><mi>glo</mi></msub></mrow><annotation encoding="application/x-tex">p_{\mathrm{loc}}&lt;p_{\mathrm{glo}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT &lt; italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="S_{\mathrm{loc}}&lt;S_{\mathrm{glo}}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m4"><semantics><mrow><msub><mi>S</mi><mi>loc</mi></msub><mo>&lt;</mo><msub><mi>S</mi><mi>glo</mi></msub></mrow><annotation encoding="application/x-tex">S_{\mathrm{loc}}&lt;S_{\mathrm{glo}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT &lt; italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p">DINO desires that the aggregate features <math alttext="\bm{z}_{\theta}(\bm{X}_{v})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{v})" class="ltx_Math" display="inline" id="S2.SS2.p6.m1"><semantics><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X}_{v})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{v})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )</annotation></semantics></math> of all views <math alttext="\bm{X}_{v}\doteq v(\bm{X})" class="ltx_Math" display="inline" id="S2.SS2.p6.m2"><semantics><mrow><msub><mi>𝑿</mi><mi>v</mi></msub><mo>≐</mo><mrow><mi>v</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{v}\doteq v(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ≐ italic_v ( bold_italic_X )</annotation></semantics></math> of an input image <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p6.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> are consistent with each other. DINO does this by using a <span class="ltx_text ltx_font_italic">“DINO head”<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">2</span></span><span class="ltx_text ltx_font_upright">Note that </span><math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="footnote2.m1"><semantics><msub><mi>h</mi><mrow><mi>𝐖</mi><mo>,</mo><mi>𝛍</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_upright"> is the task-specific head, which in </span><a class="ltx_ref ltx_font_upright" href="#S1" title="7.1 Technical Setup and Outline of the Chapter ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.1</span></a><span class="ltx_text ltx_font_upright"> is parameterized only by </span><math alttext="\theta" class="ltx_Math" display="inline" id="footnote2.m2"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math><span class="ltx_text ltx_font_upright"> as opposed to any specific parameters, but since we use two invocations of </span><math alttext="h" class="ltx_Math" display="inline" id="footnote2.m3"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math><span class="ltx_text ltx_font_upright"> with different values of the second parameter, we keep the specified notation.</span></span></span></span></span> <math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="S2.SS2.p6.m4"><semantics><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math>, parameterized by a matrix <math alttext="\bm{W}\in\mathbb{R}^{s\times d}" class="ltx_Math" display="inline" id="S2.SS2.p6.m5"><semantics><mrow><mi>𝑾</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{W}\in\mathbb{R}^{s\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_s × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and a vector <math alttext="\bm{\mu}\in\mathbb{R}^{s}" class="ltx_Math" display="inline" id="S2.SS2.p6.m6"><semantics><mrow><mi>𝝁</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\mu}\in\mathbb{R}^{s}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ ∈ blackboard_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT</annotation></semantics></math>, to extract a probability vector <math alttext="\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{v})\doteq h_{\bm{W},\bm{\mu}}(\bm{z}_{\theta}(\bm{X}_{v}))" class="ltx_Math" display="inline" id="S2.SS2.p6.m7"><semantics><mrow><mrow><msub><mi>𝒑</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{v})\doteq h_{\bm{W},\bm{\mu}}(\bm{z}_{\theta}(\bm{X}_{v}))</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ , bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) )</annotation></semantics></math> from the aggregate feature <math alttext="\bm{z}_{\theta}(\bm{X}_{v})" class="ltx_Math" display="inline" id="S2.SS2.p6.m8"><semantics><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X}_{v})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )</annotation></semantics></math>, using the following simple recipe:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h_{\bm{W},\bm{\mu}}(\bm{z})\doteq\operatorname{\mathrm{softmax}}([\bm{W}\bm{z}-\bm{\mu}]/\tau),\qquad\forall\bm{z}\in\mathbb{R}^{d}," class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><mrow><mrow><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝑾</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">]</mo></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>𝒛</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}(\bm{z})\doteq\operatorname{\mathrm{softmax}}([\bm{W}\bm{z}-\bm{\mu}]/\tau),\qquad\forall\bm{z}\in\mathbb{R}^{d},</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_z ) ≐ roman_softmax ( [ bold_italic_W bold_italic_z - bold_italic_μ ] / italic_τ ) , ∀ bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\operatorname{\mathrm{softmax}}\colon\mathbb{R}^{s}\to\Delta_{s}" class="ltx_Math" display="inline" id="S2.SS2.p6.m9"><semantics><mrow><mi>softmax</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>s</mi></msup><mo stretchy="false">→</mo><msub><mi mathvariant="normal">Δ</mi><mi>s</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathrm{softmax}}\colon\mathbb{R}^{s}\to\Delta_{s}</annotation><annotation encoding="application/x-llamapun">roman_softmax : blackboard_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT → roman_Δ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> function is defined by</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathrm{softmax}}\left(\begin{bmatrix}x_{1}\\
\vdots\\
x_{s}\end{bmatrix}\right)\doteq\frac{1}{\sum_{i=1}^{s}e^{x_{i}}}\begin{bmatrix}e^{x_{1}}\\
\vdots\\
e^{x_{s}}\end{bmatrix}" class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msub><mi>x</mi><mi>s</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msup><mi>e</mi><msub><mi>x</mi><mi>s</mi></msub></msup></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathrm{softmax}}\left(\begin{bmatrix}x_{1}\\
\vdots\\
x_{s}\end{bmatrix}\right)\doteq\frac{1}{\sum_{i=1}^{s}e^{x_{i}}}\begin{bmatrix}e^{x_{1}}\\
\vdots\\
e^{x_{s}}\end{bmatrix}</annotation><annotation encoding="application/x-llamapun">roman_softmax ( [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) ≐ divide start_ARG 1 end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG [ start_ARG start_ROW start_CELL italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="\tau&gt;0" class="ltx_Math" display="inline" id="S2.SS2.p6.m10"><semantics><mrow><mi>τ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\tau&gt;0</annotation><annotation encoding="application/x-llamapun">italic_τ &gt; 0</annotation></semantics></math> is a “temperature” parameter which controls the entropy of the softmax’s output.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p">In particular, DINO minimizes the difference between the probability vector <math alttext="\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{g})\doteq h_{\bm{W},\bm{\mu}}(\bm{z}_{\theta}(\bm{X}_{g}))" class="ltx_Math" display="inline" id="S2.SS2.p7.m1"><semantics><mrow><mrow><msub><mi>𝒑</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{g})\doteq h_{\bm{W},\bm{\mu}}(\bm{z}_{\theta}(\bm{X}_{g}))</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ , bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) )</annotation></semantics></math> for each global view <math alttext="\bm{X}_{g}\doteq v_{g}(\bm{X})" class="ltx_Math" display="inline" id="S2.SS2.p7.m2"><semantics><mrow><msub><mi>𝑿</mi><mi>g</mi></msub><mo>≐</mo><mrow><msub><mi>v</mi><mi>g</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{g}\doteq v_{g}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> and the probability vector <math alttext="\bm{p}_{\theta,\bm{W}}(\bm{X}_{c})\doteq h_{\bm{W},\bm{0}_{m}}(\bm{z}_{\theta}(\bm{X}_{c}))" class="ltx_Math" display="inline" id="S2.SS2.p7.m3"><semantics><mrow><mrow><msub><mi>𝒑</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><msub><mn>𝟎</mn><mi>m</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta,\bm{W}}(\bm{X}_{c})\doteq h_{\bm{W},\bm{0}_{m}}(\bm{z}_{\theta}(\bm{X}_{c}))</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ , bold_italic_W end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W , bold_0 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) )</annotation></semantics></math> for each view <math alttext="\bm{X}_{c}\doteq v_{c}(\bm{X})" class="ltx_Math" display="inline" id="S2.SS2.p7.m4"><semantics><mrow><msub><mi>𝑿</mi><mi>c</mi></msub><mo>≐</mo><mrow><msub><mi>v</mi><mi>c</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{c}\doteq v_{c}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>. Here, <math alttext="v_{c}" class="ltx_Math" display="inline" id="S2.SS2.p7.m5"><semantics><msub><mi>v</mi><mi>c</mi></msub><annotation encoding="application/x-tex">v_{c}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> can <span class="ltx_text ltx_font_italic">either</span> be a local view or a global view. We will discuss the implementation of <math alttext="f_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p7.m6"><semantics><msub><mi>f</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">f_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S2.SS2.p7.m7"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math> shortly in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>. Overall, DINO solves the problem</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta,\bm{W},\bm{\mu}}\mathcal{L}_{\mathrm{DINO}}(\theta,\bm{W},\bm{\mu})\qquad\text{where}\qquad\mathcal{L}_{\mathrm{DINO}}(\theta,\bm{W},\bm{\mu})\doteq\operatorname{\mathbb{E}}[d_{\operatorname{CE}}(\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{g}),\bm{p}_{\theta,\bm{W}}(\bm{X}_{c}))]," class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>DINO</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi><mo stretchy="false">)</mo></mrow></mrow><mspace width="2em"></mspace><mtext>where</mtext><mspace width="2em"></mspace><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>DINO</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒑</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒑</mi><mrow><mi>θ</mi><mo>,</mo><mi>𝑾</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\theta,\bm{W},\bm{\mu}}\mathcal{L}_{\mathrm{DINO}}(\theta,\bm{W},\bm{\mu})\qquad\text{where}\qquad\mathcal{L}_{\mathrm{DINO}}(\theta,\bm{W},\bm{\mu})\doteq\operatorname{\mathbb{E}}[d_{\operatorname{CE}}(\bm{p}_{\theta,\bm{W},\bm{\mu}}(\bm{X}_{g}),\bm{p}_{\theta,\bm{W}}(\bm{X}_{c}))],</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ , bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT roman_DINO end_POSTSUBSCRIPT ( italic_θ , bold_italic_W , bold_italic_μ ) where caligraphic_L start_POSTSUBSCRIPT roman_DINO end_POSTSUBSCRIPT ( italic_θ , bold_italic_W , bold_italic_μ ) ≐ blackboard_E [ italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p start_POSTSUBSCRIPT italic_θ , bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ , bold_italic_W end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the expectation is over data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p7.m8"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, global views <math alttext="v_{g}" class="ltx_Math" display="inline" id="S2.SS2.p7.m9"><semantics><msub><mi>v</mi><mi>g</mi></msub><annotation encoding="application/x-tex">v_{g}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, and other views <math alttext="v_{c}" class="ltx_Math" display="inline" id="S2.SS2.p7.m10"><semantics><msub><mi>v</mi><mi>c</mi></msub><annotation encoding="application/x-tex">v_{c}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p">In this specific case, however, if you try to implement (<a class="ltx_ref" href="#S2.E7" title="Equation 7.2.7 ‣ 7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.7</span></a>) and optimize it on a real network, it is very likely that you run into a problem: after running a few iterations of the learning algorithm, the feature mapping <math alttext="f_{\theta}^{\mathrm{ext}}\circ f_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p8.m1"><semantics><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}\circ f_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> will <span class="ltx_text ltx_font_italic">become the constant function</span>! This certainly optimizes the above loss since it minimizes the distance between features of different views of the same image. But we obviously do not want to learn this solution.</p>
</div>
<div class="ltx_para" id="S2.SS2.p9">
<p class="ltx_p">Actually avoiding collapse is a very common consideration in contrastive learning. So how to do it in this case? The solution of DINO, again, is empirically designed, and carefully tunes the optimization of the parameter <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.SS2.p9.m1"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> (which is updated using all samples in the batch) and a “temperature” hyperparameter <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS2.p9.m2"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math> which is part of the implementation of <math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="S2.SS2.p9.m3"><semantics><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math> and discussed in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>. Given a certain special set of hyperparameters that work well, this is indeed enough to ensure non-collapse of the representation. However, outside of this special configuration, training models to converge is difficult, and the training is highly unstable.</p>
</div>
<div class="ltx_para" id="S2.SS2.p10">
<p class="ltx_p">Towards amending this state of affairs, let us discuss simplifications to the formulation. First, instead of computing a probability vector using a learned transformation <math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="S2.SS2.p10.m1"><semantics><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math> of the aggregate features <math alttext="\bm{z}_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p10.m2"><semantics><msub><mi>𝒛</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\bm{z}_{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we can <span class="ltx_text ltx_font_italic">directly use the aggregate representation</span>, ignoring the task-specific head (or equivalently, setting it to the identity mapping). But now we need a way to compare the vectors directly. Using our hypothesis from <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a> that good representations should have Euclidean (subspace) geometry, a much more natural measure of difference is the <span class="ltx_text ltx_font_italic">squared <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S2.SS2.p10.m3"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> distance</span> <math alttext="d_{\ell^{2}}\colon\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.p10.m4"><semantics><mrow><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">d_{\ell^{2}}\colon\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math>, defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{\ell^{2}}(\bm{x},\bm{y})\doteq\frac{1}{2}\|\bm{x}-\bm{y}\|_{2}^{2},\qquad\forall\bm{x},\bm{y}\in\mathbb{R}^{d}." class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><mrow><mrow><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo rspace="2.167em">,</mo><mrow><mo rspace="0.167em">∀</mo><mi>𝒙</mi></mrow></mrow></mrow><mo>,</mo><mrow><mi>𝒚</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d_{\ell^{2}}(\bm{x},\bm{y})\doteq\frac{1}{2}\|\bm{x}-\bm{y}\|_{2}^{2},\qquad\forall\bm{x},\bm{y}\in\mathbb{R}^{d}.</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x , bold_italic_y ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_x - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ∀ bold_italic_x , bold_italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This distance-based score is even more efficient to compute than the cross-entropy score. Thus, <math alttext="d_{\ell^{2}}" class="ltx_Math" display="inline" id="S2.SS2.p10.m5"><semantics><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><annotation encoding="application/x-tex">d_{\ell^{2}}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> takes the place of <math alttext="d_{\operatorname{CE}}" class="ltx_Math" display="inline" id="S2.SS2.p10.m6"><semantics><msub><mi>d</mi><mi>CE</mi></msub><annotation encoding="application/x-tex">d_{\operatorname{CE}}</annotation><annotation encoding="application/x-llamapun">italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT</annotation></semantics></math> in our simplification.</p>
</div>
<div class="ltx_para" id="S2.SS2.p11">
<p class="ltx_p">Before, collapse was avoided by using tricks to update <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.SS2.p11.m1"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> and <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS2.p11.m2"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math>. In our simplification, if we compare the features within the representation space instead of converting them to probabilities, we do not have either of these parameters and so must consider a different way to avoid collapse. To do this, we return to the fundamentals. The basic idea of avoiding collapse is that in order to make sure that all samples do not return the same exact same features, we need different samples to have different features. In other words, we would like the <span class="ltx_text ltx_font_italic">covariance</span> of the features to be <span class="ltx_text ltx_font_italic">large</span> in some sense. But from <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapters</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, we already have a quantity which measures the size of the covariance matrix. Namely, we use the straightforward (population-level) <span class="ltx_text ltx_font_italic">Gaussian coding rate</span> <math alttext="R" class="ltx_Math" display="inline" id="S2.SS2.p11.m3"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation><annotation encoding="application/x-llamapun">italic_R</annotation></semantics></math> to ensure that the features of global views of different images, which have different global information, are well-separated and not collapsed (hence expanded). The overall modified loss <math alttext="\mathcal{L}_{\mathrm{SimDINO}}" class="ltx_Math" display="inline" id="S2.SS2.p11.m4"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>SimDINO</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{SimDINO}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_SimDINO end_POSTSUBSCRIPT</annotation></semantics></math> becomes:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{SimDINO}}(\theta)\doteq\operatorname{\mathbb{E}}[d_{\ell^{2}}(\bm{z}_{\theta}(\bm{X}_{g}),\bm{z}_{\theta}(\bm{X}_{c}))]-\frac{\gamma}{2}\log\det\left(\bm{I}+\frac{d}{\varepsilon^{2}}\operatorname{Cov}(\bm{z}_{\theta}(\bm{X}_{g}))\right)," class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>SimDINO</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mrow><mfrac><mi>γ</mi><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>d</mi><msup><mi>ε</mi><mn>2</mn></msup></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{SimDINO}}(\theta)\doteq\operatorname{\mathbb{E}}[d_{\ell^{2}}(\bm{z}_{\theta}(\bm{X}_{g}),\bm{z}_{\theta}(\bm{X}_{c}))]-\frac{\gamma}{2}\log\det\left(\bm{I}+\frac{d}{\varepsilon^{2}}\operatorname{Cov}(\bm{z}_{\theta}(\bm{X}_{g}))\right),</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_SimDINO end_POSTSUBSCRIPT ( italic_θ ) ≐ blackboard_E [ italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) , bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ) ] - divide start_ARG italic_γ end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_Cov ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\varepsilon&gt;0" class="ltx_Math" display="inline" id="S2.SS2.p11.m5"><semantics><mrow><mi>ε</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\varepsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ε &gt; 0</annotation></semantics></math> is fixed and the appropriate expectations are, as before, taken over data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p11.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, global view <math alttext="v_{g}" class="ltx_Math" display="inline" id="S2.SS2.p11.m7"><semantics><msub><mi>v</mi><mi>g</mi></msub><annotation encoding="application/x-tex">v_{g}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, and other (local or global) view <math alttext="v_{c}" class="ltx_Math" display="inline" id="S2.SS2.p11.m8"><semantics><msub><mi>v</mi><mi>c</mi></msub><annotation encoding="application/x-tex">v_{c}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>. The loss in (<a class="ltx_ref" href="#S2.E9" title="Equation 7.2.9 ‣ 7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.9</span></a>) is the loss used for the simplified DINO (“SimDINO”). As we will see, when properly implemented, it works at least as well as the original DINO.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.3 </span>Architecture: Vision Transformer</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">For the architecture, we use a standard vision transformer. Here is how such an architecture works, formally, in the context of image data. Recall from <a class="ltx_ref" href="#S1" title="7.1 Technical Setup and Outline of the Chapter ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.1</span></a> that there are four components to an encoder architecture, namely an embedding, a backbone, a feature extractor, and a task-specific head. We discuss these three parts presently.</p>
</div>
<figure class="ltx_figure" id="F5"><img alt="Figure 7.5 : An example of an image turned into 5 × 5 5\times 5 5 × 5 square patches, which are placed in raster order. Each patch is of the same size, and the grid of patches is of shape ( N H , N W ) = ( 5 , 5 ) (N_{H},N_{W})=(5,5) ( italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT ) = ( 5 , 5 ) . The grid of patches is then unrolled into a sequence of length 5 × 5 = 25 5\times 5=25 5 × 5 = 25 in raster order." class="ltx_graphics" id="F5.g1" src="chapters/chapter7/figs/patchify.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">An example of an image turned into <math alttext="5\times 5" class="ltx_Math" display="inline" id="F5.m4"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times 5</annotation><annotation encoding="application/x-llamapun">5 × 5</annotation></semantics></math> square patches, which are placed in raster order.<span class="ltx_text ltx_font_medium"> Each patch is of the same size, and the grid of patches is of shape <math alttext="(N_{H},N_{W})=(5,5)" class="ltx_Math" display="inline" id="F5.m5"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>N</mi><mi>H</mi></msub><mo>,</mo><msub><mi>N</mi><mi>W</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(N_{H},N_{W})=(5,5)</annotation><annotation encoding="application/x-llamapun">( italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT ) = ( 5 , 5 )</annotation></semantics></math>. The grid of patches is then unrolled into a sequence of length <math alttext="5\times 5=25" class="ltx_Math" display="inline" id="F5.m6"><semantics><mrow><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>5</mn></mrow><mo>=</mo><mn>25</mn></mrow><annotation encoding="application/x-tex">5\times 5=25</annotation><annotation encoding="application/x-llamapun">5 × 5 = 25</annotation></semantics></math> in raster order.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="F6"><img alt="Figure 7.6 : The transformer embedding pipeline. Given a sequence of unrolled patches in raster order 𝑿 patch \bm{X}^{\mathrm{patch}} bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT , each unrolled patch is linearly projected into the feature space, and equipped with an (additive) positional encoding and an additional token known as the class token. The output is the first-layer-input feature 𝒁 θ 1 ​ ( 𝑿 ) = f θ emb ​ ( 𝑿 ) \bm{Z}_{\theta}^{1}(\bm{X})=f_{\theta}^{\mathrm{emb}}(\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ." class="ltx_graphics" id="F6.g1" src="chapters/chapter7/figs/transformer_embedding.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The transformer embedding pipeline.<span class="ltx_text ltx_font_medium"> Given a sequence of unrolled patches in raster order <math alttext="\bm{X}^{\mathrm{patch}}" class="ltx_Math" display="inline" id="F6.m3"><semantics><msup><mi>𝑿</mi><mi>patch</mi></msup><annotation encoding="application/x-tex">\bm{X}^{\mathrm{patch}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT</annotation></semantics></math>, each unrolled patch is linearly projected into the feature space, and equipped with an (additive) positional encoding and an additional token known as the class token. The output is the first-layer-input feature <math alttext="\bm{Z}_{\theta}^{1}(\bm{X})=f_{\theta}^{\mathrm{emb}}(\bm{X})" class="ltx_Math" display="inline" id="F6.m4"><semantics><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{1}(\bm{X})=f_{\theta}^{\mathrm{emb}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math>.</span></span></figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Embedding.</h5>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p">Given imagery data <math alttext="\bm{X}\in\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathcal{I}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ caligraphic_I</annotation></semantics></math>, we embed it as a sequence of tokens in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m2"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> using the map <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m3"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math>, as follows. The first two steps are depicted in <a class="ltx_ref" href="#F5" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.5</span></a>, and the latter two are depicted in <a class="ltx_ref" href="#F6" title="In 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.6</span></a>.</p>
<ol class="ltx_enumerate" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p">First, we turn the image data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into a sequence of patches of shape <math alttext="(C,P_{H},P_{W})" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>P</mi><mi>H</mi></msub><mo>,</mo><msub><mi>P</mi><mi>W</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,P_{H},P_{W})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT )</annotation></semantics></math> where <math alttext="P_{H}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m3"><semantics><msub><mi>P</mi><mi>H</mi></msub><annotation encoding="application/x-tex">P_{H}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P_{W}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m4"><semantics><msub><mi>P</mi><mi>W</mi></msub><annotation encoding="application/x-tex">P_{W}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math> are the patch dimensions. We assume that <math alttext="P_{H}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m5"><semantics><msub><mi>P</mi><mi>H</mi></msub><annotation encoding="application/x-tex">P_{H}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P_{W}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m6"><semantics><msub><mi>P</mi><mi>W</mi></msub><annotation encoding="application/x-tex">P_{W}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math> evenly divide the height and width of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, respectively (in the notation of <a class="ltx_ref" href="#S2.SS2" title="7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.2</span></a> we assume that <math alttext="P_{H}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m8"><semantics><msub><mi>P</mi><mi>H</mi></msub><annotation encoding="application/x-tex">P_{H}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P_{W}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m9"><semantics><msub><mi>P</mi><mi>W</mi></msub><annotation encoding="application/x-tex">P_{W}</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math> evenly divide <math alttext="S_{\mathrm{loc}}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m10"><semantics><msub><mi>S</mi><mi>loc</mi></msub><annotation encoding="application/x-tex">S_{\mathrm{loc}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="S_{\mathrm{glo}}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m11"><semantics><msub><mi>S</mi><mi>glo</mi></msub><annotation encoding="application/x-tex">S_{\mathrm{glo}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT</annotation></semantics></math>). Let the resulting grid of patches have <math alttext="N_{H}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m12"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding="application/x-tex">N_{H}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT</annotation></semantics></math> rows and <math alttext="N_{W}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m13"><semantics><msub><mi>N</mi><mi>W</mi></msub><annotation encoding="application/x-tex">N_{W}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math> columns.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p">We unroll each patch into a vector of length <math alttext="D\doteq CP_{H}P_{W}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m1"><semantics><mrow><mi>D</mi><mo>≐</mo><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>P</mi><mi>H</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>P</mi><mi>W</mi></msub></mrow></mrow><annotation encoding="application/x-tex">D\doteq CP_{H}P_{W}</annotation><annotation encoding="application/x-llamapun">italic_D ≐ italic_C italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math>. There are <math alttext="N\doteq N_{H}N_{W}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m2"><semantics><mrow><mi>N</mi><mo>≐</mo><mrow><msub><mi>N</mi><mi>H</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>N</mi><mi>W</mi></msub></mrow></mrow><annotation encoding="application/x-tex">N\doteq N_{H}N_{W}</annotation><annotation encoding="application/x-llamapun">italic_N ≐ italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT</annotation></semantics></math> patch vectors, which we place in “raster order” (top left <math alttext="\to" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m3"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\to</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> top right <math alttext="\to" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m4"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\to</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> bottom left <math alttext="\to" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m5"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\to</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> bottom right) into a matrix <math alttext="\bm{X}^{\mathrm{patch}}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m6"><semantics><mrow><msup><mi>𝑿</mi><mi>patch</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}^{\mathrm{patch}}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\bm{X}^{\mathrm{patch}}\doteq f^{\mathrm{patch}}(\bm{X})" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m7"><semantics><mrow><msup><mi>𝑿</mi><mi>patch</mi></msup><mo>≐</mo><mrow><msup><mi>f</mi><mi>patch</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}^{\mathrm{patch}}\doteq f^{\mathrm{patch}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ≐ italic_f start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math>. Notice that <math alttext="D" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m8"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> only depends on the patch size and number of channels. Since the latter quantity is normally constant among samples in the same dataset, <math alttext="D" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m9"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> is the same for all images in the dataset, while <math alttext="N" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m10"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is different for larger and smaller images.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p">We then perform the following operation on <math alttext="\bm{X}^{\mathrm{patch}}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m1"><semantics><mrow><msup><mi>𝑿</mi><mi>patch</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}^{\mathrm{patch}}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> to project it to <math alttext="\mathbb{R}^{d\times n}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m2"><semantics><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{d\times n}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="n\doteq N+1" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m3"><semantics><mrow><mi>n</mi><mo>≐</mo><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">n\doteq N+1</annotation><annotation encoding="application/x-llamapun">italic_n ≐ italic_N + 1</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}^{\mathrm{patch}}\mapsto[\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}\bm{X}]+\bm{E}^{\mathrm{pos}}." class="ltx_Math" display="block" id="S2.E10.m1"><semantics><mrow><mrow><msup><mi>𝑿</mi><mi>patch</mi></msup><mo stretchy="false">↦</mo><mrow><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><mo>,</mo><mrow><msup><mi>𝑾</mi><mi>emb</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">]</mo></mrow><mo>+</mo><msup><mi>𝑬</mi><mi>pos</mi></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}^{\mathrm{patch}}\mapsto[\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}\bm{X}]+\bm{E}^{\mathrm{pos}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ↦ [ bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT bold_italic_X ] + bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here we have three trainable parameters <math alttext="\bm{W}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m4"><semantics><msup><mi>𝑾</mi><mi>emb</mi></msup><annotation encoding="application/x-tex">\bm{W}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{z}_{\mathrm{cls}}^{1}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m5"><semantics><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\bm{z}_{\mathrm{cls}}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\bm{E}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.m6"><semantics><msup><mi>𝑬</mi><mi>pos</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> whose purpose is as follows:</p>
<ul class="ltx_itemize" id="S2.I2.i3.I1">
<li class="ltx_item" id="S2.I2.i3.I0.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.I0.i1.p1">
<p class="ltx_p"><math alttext="\bm{W}^{\mathrm{emb}}\in\mathbb{R}^{d\times D}" class="ltx_Math" display="inline" id="S2.I2.i3.I0.i1.p1.m1"><semantics><mrow><msup><mi>𝑾</mi><mi>emb</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{W}^{\mathrm{emb}}\in\mathbb{R}^{d\times D}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is a matrix which projects each patch vector to a token feature.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3.I0.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.I0.i2.p1">
<p class="ltx_p"><math alttext="\bm{z}_{\mathrm{cls}}^{1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.I2.i3.I0.i2.p1.m1"><semantics><mrow><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{\mathrm{cls}}^{1}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is a so-called <span class="ltx_text ltx_font_italic">class token</span> or <span class="ltx_text ltx_font_italic">register token</span>. The class token heuristically holds global information of the whole data and is used for downstream tasks. In the framework of compressive deep networks from <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we expect that the class token is projected onto the same subspaces as the salient or semantically relevant tokens during the progression of the forward pass.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3.I0.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.I0.i3.p1">
<p class="ltx_p"><math alttext="\bm{E}^{\mathrm{pos}}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.I2.i3.I0.i3.p1.m1"><semantics><mrow><msup><mi>𝑬</mi><mi>pos</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is a so-called <span class="ltx_text ltx_font_italic">positional encoding</span> which distinguishes tokens of different patches from each other. That is, token features should have positional information, so that the overall map <math alttext="f^{\mathrm{pre}}" class="ltx_Math" display="inline" id="S2.I2.i3.I0.i3.p1.m2"><semantics><msup><mi>f</mi><mi>pre</mi></msup><annotation encoding="application/x-tex">f^{\mathrm{pre}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_pre end_POSTSUPERSCRIPT</annotation></semantics></math> is not invariant to permutations of the patches, and <math alttext="\bm{E}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="S2.I2.i3.I0.i3.p1.m3"><semantics><msup><mi>𝑬</mi><mi>pos</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> inserts this positional information.</p>
<ul class="ltx_itemize" id="S2.I2.i3.I0.i3.I1">
<li class="ltx_item" id="S2.I2.i3.I0.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S2.I2.i3.I0.i3.I1.i1.p1">
<p class="ltx_p">In this DINO case, where the transformer receives differently-sized images, we learn a positional encoding for the largest size received during training, and interpolate to get the positional encodings for smaller-sized inputs.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
<p class="ltx_p">Thus, in the end we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq\begin{bmatrix}\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}f^{\mathrm{patch}}(\bm{X})+\bm{E}^{\mathrm{pos}}\end{bmatrix}." class="ltx_Math" display="block" id="S2.E11.m1"><semantics><mrow><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>[</mo><mtable displaystyle="true"><mtr><mtd><mrow><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><mo>,</mo><mrow><mrow><msup><mi>𝑾</mi><mi>emb</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>f</mi><mi>patch</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝑬</mi><mi>pos</mi></msup></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq\begin{bmatrix}\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}f^{\mathrm{patch}}(\bm{X})+\bm{E}^{\mathrm{pos}}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ( bold_italic_X ) + bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">All parameters <math alttext="\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}},\bm{E}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m4"><semantics><mrow><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><mo>,</mo><msup><mi>𝑾</mi><mi>emb</mi></msup><mo>,</mo><msup><mi>𝑬</mi><mi>pos</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}},\bm{E}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT , bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> are contained in the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m5"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F7"><img alt="Figure 7.7 : One layer f θ ℓ f_{\theta}^{\ell} italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT of the transformer backbone. The input features go through layer-normalization, multi-head self-attention, and multi-layer perceptron blocks in sequence to form the output features of the layer." class="ltx_graphics" id="F7.g1" src="chapters/chapter7/figs/transformer_backbone.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer <math alttext="f_{\theta}^{\ell}" class="ltx_Math" display="inline" id="F7.m2"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> of the transformer backbone.<span class="ltx_text ltx_font_medium"> The input features go through layer-normalization, multi-head self-attention, and multi-layer perceptron blocks in sequence to form the output features of the layer.</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Backbone.</h5>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p">Given a sequence of embeddings <math alttext="\bm{Z}_{\theta}^{1}(\bm{X})\doteq f_{\theta}^{\mathrm{emb}}(\bm{X})\in(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m1"><semantics><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{1}(\bm{X})\doteq f_{\theta}^{\mathrm{emb}}(\bm{X})\in(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ∈ ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, we process it using the backbone map <math alttext="f_{\theta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m2"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> as follows and as depicted in <a class="ltx_ref" href="#F7" title="In Embedding. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.7</span></a>. The function <math alttext="f_{\theta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m3"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> is composed of <math alttext="L" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m4"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> <span class="ltx_text ltx_font_italic">layers</span> <math alttext="f_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m5"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{bb}}=f_{\theta}^{L}\circ\cdots\circ f_{\theta}^{1}." class="ltx_Math" display="block" id="S2.E12.m1"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>L</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mn>1</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}=f_{\theta}^{L}\circ\cdots\circ f_{\theta}^{1}.</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∘ ⋯ ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The layer <math alttext="f_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m6"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> has the following implementation:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx87">
<tbody id="S2.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})" class="ltx_Math" display="inline" id="S2.E13.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MHSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))" class="ltx_Math" display="inline" id="S2.E13.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>MHSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MHSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))</annotation><annotation encoding="application/x-llamapun">= bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.13)</span></td>
</tr></tbody>
<tbody id="S2.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S2.E14.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})+\operatorname{MLP}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})))" class="ltx_Math" display="inline" id="S2.E14.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>MLP</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>2</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})+\operatorname{MLP}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})))</annotation><annotation encoding="application/x-llamapun">= bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="f_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m7"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is defined such that <math alttext="f_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X}))\doteq\bm{Z}_{\theta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m8"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X}))\doteq\bm{Z}_{\theta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) ) ≐ bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math>. Here we have used some operators, such as <math alttext="\operatorname{MHSA}_{\theta}^{\ell},\operatorname{MLP}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m9"><semantics><mrow><msubsup><mi>MHSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msubsup><mi>MLP</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow><annotation encoding="application/x-tex">\operatorname{MHSA}_{\theta}^{\ell},\operatorname{MLP}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\operatorname{LN}_{\theta}^{i,\ell}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.m10"><semantics><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><annotation encoding="application/x-tex">\operatorname{LN}_{\theta}^{i,\ell}</annotation><annotation encoding="application/x-llamapun">roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> that are defined as follows:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p">The <math alttext="\operatorname{MHSA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m1"><semantics><msubsup><mi>MHSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{MHSA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> operator is multi-head-self-attention, the predecessor of the multi-head subspace self-attention (cf <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>). The formulation is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx88">
<tbody id="S2.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{MHSA}_{\theta}^{\ell}(\bm{Z})" class="ltx_Math" display="inline" id="S2.E15.m1"><semantics><mrow><msubsup><mi>MHSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{MHSA}_{\theta}^{\ell}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">roman_MHSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{1,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{1,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{K,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{K,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{K,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{n}^{\top}," class="ltx_Math" display="inline" id="S2.E15.m2"><semantics><mrow><mrow><mi></mi><mo>≐</mo><mrow><mrow><msubsup><mi>𝑼</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>val</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>val</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>𝒃</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{1,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{1,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{K,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{K,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{K,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{n}^{\top},</annotation><annotation encoding="application/x-llamapun">≐ bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW end_ARG ] + bold_italic_b start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.15)</span></td>
</tr></tbody>
<tbody id="S2.E16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\text{where}\qquad\operatorname{SA}(\bm{Q},\bm{K},\bm{V})" class="ltx_Math" display="inline" id="S2.E16.m1"><semantics><mrow><mtext>where</mtext><mspace width="2.167em"></mspace><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo>,</mo><mi>𝑽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\text{where}\qquad\operatorname{SA}(\bm{Q},\bm{K},\bm{V})</annotation><annotation encoding="application/x-llamapun">where roman_SA ( bold_italic_Q , bold_italic_K , bold_italic_V )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\bm{V}\underbrace{\operatorname{\mathrm{softmax}}\left(\frac{\bm{K}^{\top}\bm{Q}}{\sqrt{p}}\right)}_{\doteq\bm{A}(\bm{Q},\bm{K})}" class="ltx_Math" display="inline" id="S2.E16.m2"><semantics><mrow><mi></mi><mo>≐</mo><mrow><mi>𝑽</mi><mo lspace="0.167em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><msup><mi>𝑲</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><msqrt><mi>p</mi></msqrt></mfrac></mstyle><mo>)</mo></mrow></mrow><mo>⏟</mo></munder><mrow><mi></mi><mo>≐</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\bm{V}\underbrace{\operatorname{\mathrm{softmax}}\left(\frac{\bm{K}^{\top}\bm{Q}}{\sqrt{p}}\right)}_{\doteq\bm{A}(\bm{Q},\bm{K})}</annotation><annotation encoding="application/x-llamapun">≐ bold_italic_V under⏟ start_ARG roman_softmax ( divide start_ARG bold_italic_K start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q end_ARG start_ARG square-root start_ARG italic_p end_ARG end_ARG ) end_ARG start_POSTSUBSCRIPT ≐ bold_italic_A ( bold_italic_Q , bold_italic_K ) end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="p" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> is a positive integer, <math alttext="\bm{U}_{\mathrm{qry}}^{k,\ell},\bm{U}_{\mathrm{key}}^{k,\ell},\bm{U}_{\mathrm{val}}^{k,\ell}\in\mathbb{R}^{d\times p}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m3"><semantics><mrow><mrow><msubsup><mi>𝑼</mi><mi>qry</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>,</mo><msubsup><mi>𝑼</mi><mi>key</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>,</mo><msubsup><mi>𝑼</mi><mi>val</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{\mathrm{qry}}^{k,\ell},\bm{U}_{\mathrm{key}}^{k,\ell},\bm{U}_{\mathrm{val}}^{k,\ell}\in\mathbb{R}^{d\times p}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT , bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT , bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{U}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d\times Kp}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m4"><semantics><mrow><msubsup><mi>𝑼</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d\times Kp}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_K italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\bm{b}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m5"><semantics><mrow><msubsup><mi>𝒃</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{b}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_b start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are trainable parameters contained in the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m6"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, and the <math alttext="\operatorname{\mathrm{softmax}}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m7"><semantics><mi>softmax</mi><annotation encoding="application/x-tex">\operatorname{\mathrm{softmax}}</annotation><annotation encoding="application/x-llamapun">roman_softmax</annotation></semantics></math> is defined column-wise as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx89">
<tbody id="S2.E17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{\mathrm{softmax}}(\bm{M})" class="ltx_Math" display="inline" id="S2.E17.m1"><semantics><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{\mathrm{softmax}}(\bm{M})</annotation><annotation encoding="application/x-llamapun">roman_softmax ( bold_italic_M )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\begin{bmatrix}\operatorname{\mathrm{softmax}}(\bm{m}_{1})&amp;\cdots&amp;\operatorname{\mathrm{softmax}}(\bm{m}_{p})\end{bmatrix}," class="ltx_Math" display="inline" id="S2.E17.m2"><semantics><mrow><mrow><mi></mi><mo>≐</mo><mrow><mo>[</mo><mtable columnspacing="5pt"><mtr><mtd><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒎</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">⋯</mi></mtd><mtd><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒎</mi><mi>p</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\begin{bmatrix}\operatorname{\mathrm{softmax}}(\bm{m}_{1})&amp;\cdots&amp;\operatorname{\mathrm{softmax}}(\bm{m}_{p})\end{bmatrix},</annotation><annotation encoding="application/x-llamapun">≐ [ start_ARG start_ROW start_CELL roman_softmax ( bold_italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_CELL start_CELL ⋯ end_CELL start_CELL roman_softmax ( bold_italic_m start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.17)</span></td>
</tr></tbody>
<tbody id="S2.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\forall\bm{M}" class="ltx_Math" display="inline" id="S2.E18.m1"><semantics><mrow><mo rspace="0.167em">∀</mo><mi>𝑴</mi></mrow><annotation encoding="application/x-tex">\displaystyle\forall\bm{M}</annotation><annotation encoding="application/x-llamapun">∀ bold_italic_M</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\begin{bmatrix}\bm{m}_{1},\dots,\bm{m}_{p}\end{bmatrix}\in\mathbb{R}^{n\times p}." class="ltx_Math" display="inline" id="S2.E18.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒎</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒎</mi><mi>p</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>p</mi></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\begin{bmatrix}\bm{m}_{1},\dots,\bm{m}_{p}\end{bmatrix}\in\mathbb{R}^{n\times p}.</annotation><annotation encoding="application/x-llamapun">= [ start_ARG start_ROW start_CELL bold_italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_m start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_p end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In practice, the dimensions are usually picked is usually picked such that <math alttext="Kp=d" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m8"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">Kp=d</annotation><annotation encoding="application/x-llamapun">italic_K italic_p = italic_d</annotation></semantics></math>. The terms</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}_{\theta}^{k,\ell}(\bm{Z})\doteq\bm{A}([\bm{U}_{\mathrm{qry}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{k,\ell}]^{\top}\bm{Z}),\qquad\operatorname{SA}_{\theta}^{k,\ell}(\bm{Z})\doteq\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{k,\ell}]^{\top}\bm{Z})" class="ltx_Math" display="block" id="S2.E19.m1"><semantics><mrow><mrow><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msubsup><mi>SA</mi><mi>θ</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>val</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{A}_{\theta}^{k,\ell}(\bm{Z})\doteq\bm{A}([\bm{U}_{\mathrm{qry}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{k,\ell}]^{\top}\bm{Z}),\qquad\operatorname{SA}_{\theta}^{k,\ell}(\bm{Z})\doteq\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{key}}^{k,\ell}]^{\top}\bm{Z},[\bm{U}_{\mathrm{val}}^{k,\ell}]^{\top}\bm{Z})</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ bold_italic_A ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) , roman_SA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ roman_SA ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">are also known as the <math alttext="k^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m9"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> attention map</span> and <math alttext="k^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m10"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> attention head output</span> at layer <math alttext="\ell" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m11"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>, respectively. Furthermore, the operation <math alttext="\operatorname{SA}(\bm{Q},\bm{K},\bm{V})" class="ltx_Math" display="inline" id="S2.I3.i1.p1.m12"><semantics><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo>,</mo><mi>𝑽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{SA}(\bm{Q},\bm{K},\bm{V})</annotation><annotation encoding="application/x-llamapun">roman_SA ( bold_italic_Q , bold_italic_K , bold_italic_V )</annotation></semantics></math> can be computed extremely efficiently using specialized software such as FlashAttention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx241" title="">SBZ+25</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p">The <math alttext="\operatorname{MLP}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S2.I3.i2.p1.m1"><semantics><msubsup><mi>MLP</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{MLP}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is a two-layer perceptron, a regular nonlinearity used in deep networks, and has the form</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{MLP}_{\theta}^{\ell}(\bm{Z})\doteq\bm{W}_{\mathrm{down}}^{\ell}\operatorname{ReLU}(\bm{W}_{\mathrm{up}}^{\ell}\bm{Z}+\bm{b}_{\mathrm{up}}^{\ell}\bm{1}_{n}^{\top})+\bm{b}_{\mathrm{down}}^{\ell}\bm{1}_{n}^{\top}" class="ltx_Math" display="block" id="S2.E20.m1"><semantics><mrow><mrow><msubsup><mi>MLP</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msubsup><mi>𝑾</mi><mi>down</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>ReLU</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msubsup><mi>𝑾</mi><mi>up</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>+</mo><mrow><msubsup><mi>𝒃</mi><mi>up</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msubsup><mi>𝒃</mi><mi>down</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{MLP}_{\theta}^{\ell}(\bm{Z})\doteq\bm{W}_{\mathrm{down}}^{\ell}\operatorname{ReLU}(\bm{W}_{\mathrm{up}}^{\ell}\bm{Z}+\bm{b}_{\mathrm{up}}^{\ell}\bm{1}_{n}^{\top})+\bm{b}_{\mathrm{down}}^{\ell}\bm{1}_{n}^{\top}</annotation><annotation encoding="application/x-llamapun">roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ bold_italic_W start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT roman_ReLU ( bold_italic_W start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z + bold_italic_b start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) + bold_italic_b start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{W}_{\mathrm{up}}^{\ell}\in\mathbb{R}^{q\times d},\bm{W}_{\mathrm{down}}^{\ell}\in\mathbb{R}^{d\times q},\bm{b}_{\mathrm{up}}^{\ell}\in\mathbb{R}^{q},\bm{b}_{\mathrm{down}}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.I3.i2.p1.m2"><semantics><mrow><mrow><msubsup><mi>𝑾</mi><mi>up</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>q</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mrow><msubsup><mi>𝑾</mi><mi>down</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>q</mi></mrow></msup></mrow><mo>,</mo><mrow><mrow><msubsup><mi>𝒃</mi><mi>up</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>q</mi></msup></mrow><mo>,</mo><mrow><msubsup><mi>𝒃</mi><mi>down</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{W}_{\mathrm{up}}^{\ell}\in\mathbb{R}^{q\times d},\bm{W}_{\mathrm{down}}^{\ell}\in\mathbb{R}^{d\times q},\bm{b}_{\mathrm{up}}^{\ell}\in\mathbb{R}^{q},\bm{b}_{\mathrm{down}}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_q × italic_d end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_q end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are trainable parameters also contained in the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S2.I3.i2.p1.m3"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, and <math alttext="\operatorname{ReLU}" class="ltx_Math" display="inline" id="S2.I3.i2.p1.m4"><semantics><mi>ReLU</mi><annotation encoding="application/x-tex">\operatorname{ReLU}</annotation><annotation encoding="application/x-llamapun">roman_ReLU</annotation></semantics></math> is the element-wise ReLU nonlinearity, i.e., <math alttext="\operatorname{ReLU}(\bm{M})_{ij}=\max\{M_{ij},0\}" class="ltx_math_unparsed" display="inline" id="S2.I3.i2.p1.m5"><semantics><mrow><mi>ReLU</mi><msub><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>max</mi><mrow><mo stretchy="false">{</mo><msub><mi>M</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>,</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{ReLU}(\bm{M})_{ij}=\max\{M_{ij},0\}</annotation><annotation encoding="application/x-llamapun">roman_ReLU ( bold_italic_M ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = roman_max { italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , 0 }</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p">Each layer-norm <math alttext="\operatorname{LN}_{\theta}^{i,\ell}" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m1"><semantics><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><annotation encoding="application/x-tex">\operatorname{LN}_{\theta}^{i,\ell}</annotation><annotation encoding="application/x-llamapun">roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> for <math alttext="i\in\{1,2\}" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m2"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i\in\{1,2\}</annotation><annotation encoding="application/x-llamapun">italic_i ∈ { 1 , 2 }</annotation></semantics></math> is a standard normalization, which applies column-wise to each token feature independently:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{LN}_{\theta}^{i,\ell}(\bm{Z})=\operatorname{LN}_{\theta}^{i,\ell}(\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{n}\end{bmatrix})=\begin{bmatrix}\operatorname{LN}_{\theta}^{i,\ell}(\bm{z}_{1}),\dots,\operatorname{LN}_{\theta}^{i,\ell}(\bm{z}_{n})\end{bmatrix}" class="ltx_Math" display="block" id="S2.E21.m1"><semantics><mrow><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>[</mo><mtable displaystyle="true"><mtr><mtd><mrow><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>n</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable displaystyle="true"><mtr><mtd><mrow><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{LN}_{\theta}^{i,\ell}(\bm{Z})=\operatorname{LN}_{\theta}^{i,\ell}(\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{n}\end{bmatrix})=\begin{bmatrix}\operatorname{LN}_{\theta}^{i,\ell}(\bm{z}_{1}),\dots,\operatorname{LN}_{\theta}^{i,\ell}(\bm{z}_{n})\end{bmatrix}</annotation><annotation encoding="application/x-llamapun">roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) = roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ( [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) = [ start_ARG start_ROW start_CELL roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , … , roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and has the form</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{LN}_{\theta}^{i,\ell}(\bm{z})=\frac{\bm{z}-\operatorname{mean}(\bm{z})\bm{1}_{d}}{\|\bm{z}-\operatorname{mean}(\bm{z})\bm{1}_{d}\|_{2}}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{\alpha}^{i,\ell}+\bm{\beta}^{i,\ell}\qquad\text{where}\qquad\operatorname{mean}(\bm{z})=\frac{1}{d}\bm{1}_{d}^{\top}\bm{z}" class="ltx_Math" display="block" id="S2.E22.m1"><semantics><mrow><mrow><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mfrac><mrow><mi>𝒛</mi><mo>−</mo><mrow><mrow><mi>mean</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟏</mn><mi>d</mi></msub></mrow></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒛</mi><mo>−</mo><mrow><mrow><mi>mean</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟏</mn><mi>d</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><msup><mi>𝜶</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup></mrow><mo>+</mo><msup><mi>𝜷</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup></mrow><mspace width="2em"></mspace><mtext>where</mtext></mrow></mrow><mspace width="2.167em"></mspace><mrow><mrow><mi>mean</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>d</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{LN}_{\theta}^{i,\ell}(\bm{z})=\frac{\bm{z}-\operatorname{mean}(\bm{z})\bm{1}_{d}}{\|\bm{z}-\operatorname{mean}(\bm{z})\bm{1}_{d}\|_{2}}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{\alpha}^{i,\ell}+\bm{\beta}^{i,\ell}\qquad\text{where}\qquad\operatorname{mean}(\bm{z})=\frac{1}{d}\bm{1}_{d}^{\top}\bm{z}</annotation><annotation encoding="application/x-llamapun">roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z ) = divide start_ARG bold_italic_z - roman_mean ( bold_italic_z ) bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_z - roman_mean ( bold_italic_z ) bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ⊙ bold_italic_α start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT + bold_italic_β start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT where roman_mean ( bold_italic_z ) = divide start_ARG 1 end_ARG start_ARG italic_d end_ARG bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m3"><semantics><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><annotation encoding="application/x-tex">\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}</annotation><annotation encoding="application/x-llamapun">⊙</annotation></semantics></math> denotes element-wise multiplication, and <math alttext="\bm{\alpha}^{i,\ell},\bm{\beta}^{i,\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m4"><semantics><mrow><mrow><msup><mi>𝜶</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo>,</mo><msup><mi>𝜷</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\alpha}^{i,\ell},\bm{\beta}^{i,\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_α start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT , bold_italic_β start_POSTSUPERSCRIPT italic_i , roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are trainable parameters contained in the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S2.I3.i3.p1.m5"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>. The layer-norm operator serves as a sort of normalization on each token, where the scale of each token afterwards is learnable and shared amongst all tokens.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p2">
<p class="ltx_p">The transformer is one of the most popular neural network architectures in history, powering applications in almost all fields of deep learning.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Feature extractor.</h5>
<div class="ltx_para" id="S2.SS3.SSS0.Px3.p1">
<p class="ltx_p">We use a post-processing step <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px3.p1.m1"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math> which extracts the <span class="ltx_text ltx_font_italic">class token feature</span>, which (recall) is the feature meant to contain aggregate information about the input image, and applies an MLP and normalization to it. Namely, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}_{\theta}(\bm{X})\doteq f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}))=f_{\theta}^{\mathrm{ext}}([\bm{z}_{\theta}^{1}(\bm{X}),\dots,\bm{z}_{\theta}^{n}(\bm{X})])\doteq\frac{\operatorname{MLP}_{\theta}^{\mathrm{ext}}(\bm{z}_{\theta}^{1}(\bm{X}))}{\|\operatorname{MLP}_{\theta}^{\mathrm{ext}}(\bm{z}_{\theta}^{1}(\bm{X}))\|_{2}}." class="ltx_Math" display="block" id="S2.E23.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝒛</mi><mi>θ</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝒛</mi><mi>θ</mi><mi>n</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mfrac><mrow><msubsup><mi>MLP</mi><mi>θ</mi><mi>ext</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒛</mi><mi>θ</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>MLP</mi><mi>θ</mi><mi>ext</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒛</mi><mi>θ</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X})\doteq f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}))=f_{\theta}^{\mathrm{ext}}([\bm{z}_{\theta}^{1}(\bm{X}),\dots,\bm{z}_{\theta}^{n}(\bm{X})])\doteq\frac{\operatorname{MLP}_{\theta}^{\mathrm{ext}}(\bm{z}_{\theta}^{1}(\bm{X}))}{\|\operatorname{MLP}_{\theta}^{\mathrm{ext}}(\bm{z}_{\theta}^{1}(\bm{X}))\|_{2}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ) = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( [ bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) , … , bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( bold_italic_X ) ] ) ≐ divide start_ARG roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ) end_ARG start_ARG ∥ roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.23)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Task-specific (“DINO”) head.</h5>
<div class="ltx_para" id="S2.SS3.SSS0.Px4.p1">
<p class="ltx_p">For DINO, we use the task-specific DINO head <math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px4.p1.m1"><semantics><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math>. For SimDINO, we use <span class="ltx_text ltx_font_italic">no</span> task-specific head <span class="ltx_text ltx_font_italic">at all</span>, as previously described.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.4 </span>Optimization Strategy</h3>
<figure class="ltx_figure" id="F8"><img alt="Figure 7.8 : The DINO pipeline. Student features and teacher features are computed for each input. The objective attempts to align the student features with the teacher features by projecting both sets of features into a high-dimensional probability simplex and computing a cross-entropy loss. Notably, because of the “stop-grad”, the gradient is only computed w.r.t. the student parameters’ outputs ." class="ltx_graphics" id="F8.g1" src="chapters/chapter7/figs/dino_pipeline.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.8</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The DINO pipeline.<span class="ltx_text ltx_font_medium"> Student features and teacher features are computed for each input. The objective attempts to align the student features with the teacher features by projecting both sets of features into a high-dimensional probability simplex and computing a cross-entropy loss. Notably, because of the “stop-grad”, the gradient is only computed w.r.t. the <span class="ltx_text ltx_font_italic">student parameters’ outputs</span>.</span></span></figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Optimizing DINO.</h5>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p">We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses <span class="ltx_text ltx_font_italic">two sets of weights for the same architecture</span>: <span class="ltx_text ltx_font_italic">student</span> weights <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.m1"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> and <span class="ltx_text ltx_font_italic">teacher</span> weights <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.m2"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math>. These correspond to two different neural networks, called the teacher network and student network, with the same architecture. The teacher network encodes all global views, while the student network encodes all “other” views. The goal of the loss is to distill teacher outputs into the student model. Namely, we train on the loss <math alttext="\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.m3"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}(\theta_{\mathrm{s}},\theta_{\mathrm{t}},\bm{W}_{\mathrm{s}},\bm{W}_{\mathrm{t}},\bm{\mu})\doteq\operatorname{\mathbb{E}}[d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{g}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{c}))]." class="ltx_Math" display="block" id="S2.E24.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}(\theta_{\mathrm{s}},\theta_{\mathrm{t}},\bm{W}_{\mathrm{s}},\bm{W}_{\mathrm{t}},\bm{\mu})\doteq\operatorname{\mathbb{E}}[d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{g}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{c}))].</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ ) ≐ blackboard_E [ italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, we can fully describe the overall pipeline of DINO, depicted in <a class="ltx_ref" href="#F8" title="In 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.8</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p2">
<p class="ltx_p">While it is easy to reason about (<a class="ltx_ref" href="#S2.E24" title="Equation 7.2.24 ‣ Optimizing DINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.24</span></a>), it is impossible in practice to implement optimization algorithms such as gradient descent with a loss given by <math alttext="\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p2.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT</annotation></semantics></math>. This is because the expectations in the loss are impossible to evaluate, much less take the gradient of. In this extremely frequent case, we approximate the expectation via finite samples. That is, at each timestep <math alttext="k" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p2.m2"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> we:</p>
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p">Subsample <math alttext="B" class="ltx_Math" display="inline" id="S2.I4.i1.p1.m1"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation><annotation encoding="application/x-llamapun">italic_B</annotation></semantics></math> data points from our dataset <math alttext="\{\bm{X}_{1}^{(k)},\dots,\bm{X}_{B}^{(k)}\}\subset\mathcal{I}" class="ltx_Math" display="inline" id="S2.I4.i1.p1.m2"><semantics><mrow><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑿</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝑿</mi><mi>B</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><mo>⊂</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow><annotation encoding="application/x-tex">\{\bm{X}_{1}^{(k)},\dots,\bm{X}_{B}^{(k)}\}\subset\mathcal{I}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , … , bold_italic_X start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } ⊂ caligraphic_I</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p">For each data point <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m1"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, sample <math alttext="M_{\mathrm{glo}}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m2"><semantics><msub><mi>M</mi><mi>glo</mi></msub><annotation encoding="application/x-tex">M_{\mathrm{glo}}</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT</annotation></semantics></math> global views <math alttext="v_{b,g}^{(k),i}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m3"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">v_{b,g}^{(k),i}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="M_{\mathrm{loc}}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m4"><semantics><msub><mi>M</mi><mi>loc</mi></msub><annotation encoding="application/x-tex">M_{\mathrm{loc}}</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT</annotation></semantics></math> local views <math alttext="v_{b,\ell}^{(k),i}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m5"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">v_{b,\ell}^{(k),i}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>. Apply the views to <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m6"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> to obtain <math alttext="\bm{X}_{b,g}^{(k),i}\doteq v_{b,g}^{(k),i}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m7"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,g}^{(k),i}\doteq v_{b,g}^{(k),i}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="\bm{X}_{b,\ell}^{(k),i}\doteq v_{b,\ell}^{(k),i}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S2.I4.i2.p1.m8"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,\ell}^{(k),i}\doteq v_{b,\ell}^{(k),i}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i3.p1">
<p class="ltx_p">For each <span class="ltx_text ltx_font_italic">local</span> view <math alttext="\bm{X}_{b,\ell}^{(k),i}" class="ltx_Math" display="inline" id="S2.I4.i3.p1.m1"><semantics><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b,\ell}^{(k),i}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> compute the following quantities:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,\ell}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq h_{\bm{W}_{\mathrm{s}},\bm{0}_{m}}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i}(\theta)))" class="ltx_Math" display="block" id="S2.E25.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mn>𝟎</mn><mi>m</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,\ell}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq h_{\bm{W}_{\mathrm{s}},\bm{0}_{m}}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i}(\theta)))</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_0 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ( italic_θ ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and for each <span class="ltx_text ltx_font_italic">global</span> view <math alttext="\bm{X}_{b,g}^{(k),i}" class="ltx_Math" display="inline" id="S2.I4.i3.p1.m2"><semantics><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b,g}^{(k),i}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> compute the following quantities (by an abuse of notation):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx90">
<tbody id="S2.E26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,g}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq h_{\bm{W}_{\mathrm{s}},\bm{0}_{m}}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i}))," class="ltx_Math" display="inline" id="S2.E26.m1"><semantics><mrow><mrow><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mn>𝟎</mn><mi>m</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,g}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq h_{\bm{W}_{\mathrm{s}},\bm{0}_{m}}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})),</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_0 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.26)</span></td>
</tr></tbody>
<tbody id="S2.E27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{t}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{t}}})(\bm{X}_{b,g}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i})\doteq h_{\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{Z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}))." class="ltx_Math" display="inline" id="S2.E27.m1"><semantics><mrow><mrow><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mrow><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{t}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{t}}})(\bm{X}_{b,g}^{(k),i}),\qquad\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i})\doteq h_{\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{Z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})).</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.27)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S2.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i4.p1">
<p class="ltx_p">Compute the <span class="ltx_text ltx_font_italic">surrogate, approximate loss</span> <math alttext="\hat{\mathcal{L}}_{\mathrm{DINO}-\mathrm{s}\mathrm{t}}^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m1"><semantics><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\mathrm{DINO}-\mathrm{s}\mathrm{t}}^{(k)}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, defined as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx91">
<tbody id="S2.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\hat{\mathcal{L}}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}^{(k)}(\theta_{\mathrm{s}},\theta_{\mathrm{t}},\bm{W}_{\mathrm{s}},\bm{W}_{\mathrm{t}},\bm{\mu})\doteq\frac{1}{BM_{\mathrm{glo}}(M_{\mathrm{glo}}+M_{\mathrm{loc}}-1)}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}" class="ltx_Math" display="inline" id="S2.E28.m2"><semantics><mrow><mrow><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>M</mi><mi>glo</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>M</mi><mi>glo</mi></msub><mo>+</mo><msub><mi>M</mi><mi>loc</mi></msub></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>glo</mi></msub></munderover></mstyle></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{\mathcal{L}}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}^{(k)}(\theta_{\mathrm{s}},\theta_{\mathrm{t}},\bm{W}_{\mathrm{s}},\bm{W}_{\mathrm{t}},\bm{\mu})\doteq\frac{1}{BM_{\mathrm{glo}}(M_{\mathrm{glo}}+M_{\mathrm{loc}}-1)}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ ) ≐ divide start_ARG 1 end_ARG start_ARG italic_B italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ( italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT + italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT - 1 ) end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.28)</span></td>
</tr></tbody>
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\Bigg{[}\sum_{j=1}^{M_{\mathrm{loc}}}d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),j}))+\sum_{\begin{subarray}{c}j=1\\
j\neq i\end{subarray}}^{M_{\mathrm{glo}}}d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),j}))\Bigg{]}" class="ltx_Math" display="inline" id="S2.Ex1.m1"><semantics><mrow><mo maxsize="260%" minsize="260%">[</mo><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>loc</mi></msub></munderover></mstyle><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>j</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></mtd></mtr></mtable><msub><mi>M</mi><mi>glo</mi></msub></munderover></mstyle><mrow><msub><mi>d</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><mo>,</mo><mi>𝝁</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒑</mi><mrow><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>j</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo maxsize="260%" minsize="260%">]</mo></mrow><annotation encoding="application/x-tex">\displaystyle\Bigg{[}\sum_{j=1}^{M_{\mathrm{loc}}}d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),j}))+\sum_{\begin{subarray}{c}j=1\\
j\neq i\end{subarray}}^{M_{\mathrm{glo}}}d_{\operatorname{CE}}(\bm{p}_{\theta_{\mathrm{t}},\bm{W}_{\mathrm{t}},\bm{\mu}}(\bm{X}_{b,g}^{(k),i}),\bm{p}_{\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),j}))\Bigg{]}</annotation><annotation encoding="application/x-llamapun">[ ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_j end_POSTSUPERSCRIPT ) ) + ∑ start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_j = 1 end_CELL end_ROW start_ROW start_CELL italic_j ≠ italic_i end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , bold_italic_μ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_j end_POSTSUPERSCRIPT ) ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as well as its gradients with respect to <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m2"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{W}_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m3"><semantics><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\bm{W}_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math>, which should be computed under the assumption that <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m4"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\bm{W}_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m5"><semantics><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\bm{W}_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m6"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> are constants — namely that they are <span class="ltx_text ltx_font_italic">detached from the computational graph</span> and not dependent on <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m7"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{W}_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i4.p1.m8"><semantics><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\bm{W}_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i5.p1">
<p class="ltx_p">Update the student parameters <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m1"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{W}_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m2"><semantics><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\bm{W}_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> via an iterative gradient-based optimization algorithm, and update <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m3"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\bm{W}_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m4"><semantics><msub><mi>𝑾</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\bm{W}_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m5"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> via exponential moving averages with decay parameters <math alttext="\nu^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m6"><semantics><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\nu^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\nu^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m7"><semantics><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\nu^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\rho^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m8"><semantics><msup><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\rho^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_ρ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> respectively, i.e.,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx92">
<tbody id="S2.E29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle(\theta_{\mathrm{s}}^{(k+1)},\bm{W}_{\mathrm{s}}^{(k+1)})" class="ltx_Math" display="inline" id="S2.E29.m1"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝑾</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\displaystyle(\theta_{\mathrm{s}}^{(k+1)},\bm{W}_{\mathrm{s}}^{(k+1)})</annotation><annotation encoding="application/x-llamapun">( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\textsc{OptUpdate}^{(k)}(\theta_{\mathrm{s}}^{(k)},\bm{W}_{\mathrm{s}}^{(k)};\nabla_{(\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}})}\hat{\mathcal{L}}_{\mathrm{DINO}-\mathrm{s}\mathrm{t}}^{(k)})" class="ltx_Math" display="inline" id="S2.E29.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mtext class="ltx_font_smallcaps">OptUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝑾</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>;</mo><mrow><msub><mo rspace="0.167em">∇</mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>𝑾</mi><mi mathvariant="normal">s</mi></msub><mo stretchy="false">)</mo></mrow></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\textsc{OptUpdate}^{(k)}(\theta_{\mathrm{s}}^{(k)},\bm{W}_{\mathrm{s}}^{(k)};\nabla_{(\theta_{\mathrm{s}},\bm{W}_{\mathrm{s}})}\hat{\mathcal{L}}_{\mathrm{DINO}-\mathrm{s}\mathrm{t}}^{(k)})</annotation><annotation encoding="application/x-llamapun">= OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ∇ start_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.29)</span></td>
</tr></tbody>
<tbody id="S2.E30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta_{\mathrm{t}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E30.m1"><semantics><msubsup><mi>θ</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\displaystyle\theta_{\mathrm{t}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\nu^{(k)}\theta_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\theta_{\mathrm{s}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E30.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>θ</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\nu^{(k)}\theta_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\theta_{\mathrm{s}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">= italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT + ( 1 - italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.30)</span></td>
</tr></tbody>
<tbody id="S2.E31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{W}_{\mathrm{t}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E31.m1"><semantics><msubsup><mi>𝑾</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\displaystyle\bm{W}_{\mathrm{t}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\nu^{(k)}\bm{W}_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\bm{W}_{\mathrm{s}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E31.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑾</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑾</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\nu^{(k)}\bm{W}_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\bm{W}_{\mathrm{s}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">= italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT + ( 1 - italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.31)</span></td>
</tr></tbody>
<tbody id="S2.E32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{\mu}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E32.m1"><semantics><msup><mi>𝝁</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\displaystyle\bm{\mu}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\rho^{(k)}\bm{\mu}^{(k)}+(1-\rho^{(k)})\cdot\frac{1}{BM_{\mathrm{glo}}}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}\bm{W}^{(k)}\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})," class="ltx_Math" display="inline" id="S2.E32.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝝁</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>M</mi><mi>glo</mi></msub></mrow></mfrac></mstyle></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>glo</mi></msub></munderover></mstyle><mrow><msup><mi>𝑾</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\rho^{(k)}\bm{\mu}^{(k)}+(1-\rho^{(k)})\cdot\frac{1}{BM_{\mathrm{glo}}}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}\bm{W}^{(k)}\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}),</annotation><annotation encoding="application/x-llamapun">= italic_ρ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT bold_italic_μ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT + ( 1 - italic_ρ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ⋅ divide start_ARG 1 end_ARG start_ARG italic_B italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT bold_italic_W start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.32)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For example, if the chosen optimization algorithm were stochastic gradient descent, we would have the update <math alttext="\theta_{\mathrm{s}}^{(k+1)}\doteq\theta_{\mathrm{s}}^{(k)}-\delta^{(k)}\nabla_{\theta_{\mathrm{s}}}\hat{\mathcal{L}}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}^{(k)}" class="ltx_Math" display="inline" id="S2.I4.i5.p1.m9"><semantics><mrow><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mrow><msup><mi>δ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>DINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta_{\mathrm{s}}^{(k+1)}\doteq\theta_{\mathrm{s}}^{(k)}-\delta^{(k)}\nabla_{\theta_{\mathrm{s}}}\hat{\mathcal{L}}_{\mathrm{DINO}{}-\mathrm{s}\mathrm{t}}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ≐ italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT - italic_δ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, and so on.</p>
</div>
</li>
</ul>
<p class="ltx_p">Notice that the optimization procedure is rather irregular: although all four parameters change at each iteration, only two of them are directly updated from a gradient-based method. The other two are updated from exponential moving averages, and indeed treated as constants when computing any gradients. After training, we discard the student weights and use the teacher weights for our trained network <math alttext="f" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p2.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, as this exponentially moving average has been empirically shown to stabilize the resulting model (this idea is known as Polyak averaging or iterate averaging).</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p3">
<p class="ltx_p">The way that <math alttext="\nu" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m1"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation><annotation encoding="application/x-llamapun">italic_ν</annotation></semantics></math> and <math alttext="\rho" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m2"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation><annotation encoding="application/x-llamapun">italic_ρ</annotation></semantics></math> change over the optimization trajectory (i.e., the functions <math alttext="k\mapsto\nu^{(k)}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m3"><semantics><mrow><mi>k</mi><mo stretchy="false">↦</mo><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">k\mapsto\nu^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_k ↦ italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="k\mapsto\rho^{(k)}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m4"><semantics><mrow><mi>k</mi><mo stretchy="false">↦</mo><msup><mi>ρ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">k\mapsto\rho^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_k ↦ italic_ρ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>) are hyperparameters or design decisions, with <math alttext="\nu^{(1)}&lt;1" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m5"><semantics><mrow><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\nu^{(1)}&lt;1</annotation><annotation encoding="application/x-llamapun">italic_ν start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT &lt; 1</annotation></semantics></math> and <math alttext="\lim_{k\to\infty}\nu^{(k)}=1" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m6"><semantics><mrow><mrow><msub><mo>lim</mo><mrow><mi>k</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></msub><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lim_{k\to\infty}\nu^{(k)}=1</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_k → ∞ end_POSTSUBSCRIPT italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = 1</annotation></semantics></math> usually, and similar for <math alttext="\rho" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m7"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation><annotation encoding="application/x-llamapun">italic_ρ</annotation></semantics></math>. The temperature hyperparameter <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m8"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math>, used in the DINO head <math alttext="h_{\bm{W},\bm{\mu}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m9"><semantics><msub><mi>h</mi><mrow><mi>𝑾</mi><mo>,</mo><mi>𝝁</mi></mrow></msub><annotation encoding="application/x-tex">h_{\bm{W},\bm{\mu}}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_μ end_POSTSUBSCRIPT</annotation></semantics></math>, also changes over the optimization trajectory (though this dependence is not explicitly notated).</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p4">
<p class="ltx_p">Using the surrogate (“empirical”) loss transforms our intractable optimization problem, as in optimizing the loss in (<a class="ltx_ref" href="#S2.E24" title="Equation 7.2.24 ‣ Optimizing DINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.24</span></a>), into a tractable stochastic optimization problem which is run to train essentially every deep learning model in the world. This conversion is extremely natural once you have seen some examples, and we will hopefully give these examples throughout the chapter.</p>
</div>
<figure class="ltx_figure" id="F9"><img alt="Figure 7.9 : The SimDINO pipeline. Here, in contrast to the DINO pipeline in Figure 7.8 , the loss is computed directly on the features without need of further manipulation. This shaves off several large matrices’ worth of parameters and simplifies the pipeline, simultaneously making it more stable to train." class="ltx_graphics" id="F9.g1" src="chapters/chapter7/figs/simdino_pipeline.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.9</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The SimDINO pipeline.<span class="ltx_text ltx_font_medium"> Here, in contrast to the DINO pipeline in <a class="ltx_ref" href="#F8" title="In 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.8</span></a>, the loss is computed directly on the features without need of further manipulation. This shaves off several large matrices’ worth of parameters and simplifies the pipeline, simultaneously making it more stable to train.</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Optimizing SimDINO.</h5>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p">The simplified DINO population-level objective is very similar in spirit but much simpler in execution, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}(\theta_{\mathrm{s}},\theta_{\mathrm{t}})\doteq\operatorname{\mathbb{E}}\left[d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{g}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{c}))\right]-\frac{\gamma}{2}\log\det\left(\bm{I}+\frac{d}{\varepsilon^{2}}\operatorname{Cov}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{g})))\right)." class="ltx_math_unparsed" display="block" id="S2.E33.m1"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>SimDINO</mi><mo>−</mo><mi>st</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo stretchy="false">)</mo></mrow><mo>≐</mo><mi>𝔼</mi><mrow><mo>[</mo><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo>−</mo><mfrac><mi>γ</mi><mn>2</mn></mfrac><mi>log</mi><mo lspace="0.167em" movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mi>𝑰</mi><mo>+</mo><mfrac><mi>d</mi><msup><mi>ε</mi><mn>2</mn></msup></mfrac><mi>Cov</mi><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>g</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>)</mo><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}(\theta_{\mathrm{s}},\theta_{\mathrm{t}})\doteq\operatorname{\mathbb{E}}\left[d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{g}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{c}))\right]-\frac{\gamma}{2}\log\det\left(\bm{I}+\frac{d}{\varepsilon^{2}}\operatorname{Cov}(\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{g})))\right).</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT ) ≐ blackboard_E [ italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) , bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ) ] - divide start_ARG italic_γ end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_Cov ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, as elaborated in <a class="ltx_ref" href="#F9" title="In Optimizing DINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.9</span></a>, the SimDINO pipeline is strictly simpler than the DINO pipeline. We can use a simpler version of the DINO training pipeline to optimize SimDINO. At each timestep <math alttext="k" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>, we:</p>
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p">Subsample <math alttext="B" class="ltx_Math" display="inline" id="S2.I5.i1.p1.m1"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation><annotation encoding="application/x-llamapun">italic_B</annotation></semantics></math> data points from our dataset <math alttext="\{\bm{X}_{1}^{(k)},\dots,\bm{X}_{B}^{(k)}\}\subset\mathcal{I}" class="ltx_Math" display="inline" id="S2.I5.i1.p1.m2"><semantics><mrow><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑿</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝑿</mi><mi>B</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><mo>⊂</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow><annotation encoding="application/x-tex">\{\bm{X}_{1}^{(k)},\dots,\bm{X}_{B}^{(k)}\}\subset\mathcal{I}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , … , bold_italic_X start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } ⊂ caligraphic_I</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p">For each data point <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m1"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, sample <math alttext="M_{\mathrm{glo}}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m2"><semantics><msub><mi>M</mi><mi>glo</mi></msub><annotation encoding="application/x-tex">M_{\mathrm{glo}}</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT</annotation></semantics></math> global views <math alttext="v_{b,g}^{(k),i}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m3"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">v_{b,g}^{(k),i}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="M_{\mathrm{loc}}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m4"><semantics><msub><mi>M</mi><mi>loc</mi></msub><annotation encoding="application/x-tex">M_{\mathrm{loc}}</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT</annotation></semantics></math> local views <math alttext="v_{b,\ell}^{(k),i}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m5"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">v_{b,\ell}^{(k),i}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>. Apply the views to <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m6"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> to obtain <math alttext="\bm{X}_{b,g}^{(k),i}\doteq v_{b,g}^{(k),i}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m7"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,g}^{(k),i}\doteq v_{b,g}^{(k),i}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="\bm{X}_{b,\ell}^{(k),i}\doteq v_{b,\ell}^{(k),i}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S2.I5.i2.p1.m8"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,\ell}^{(k),i}\doteq v_{b,\ell}^{(k),i}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p">For each <span class="ltx_text ltx_font_italic">local</span> view <math alttext="\bm{X}_{b,\ell}^{(k),i}" class="ltx_Math" display="inline" id="S2.I5.i3.p1.m1"><semantics><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b,\ell}^{(k),i}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> compute <math alttext="\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,\ell}^{(k),i})" class="ltx_Math" display="inline" id="S2.I5.i3.p1.m2"><semantics><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,\ell}^{(k),i})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT )</annotation></semantics></math>. For each <span class="ltx_text ltx_font_italic">global</span> view <math alttext="\bm{X}_{b,g}^{(k),i}" class="ltx_Math" display="inline" id="S2.I5.i3.p1.m3"><semantics><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b,g}^{(k),i}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> compute <math alttext="\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,g}^{(k),i})" class="ltx_Math" display="inline" id="S2.I5.i3.p1.m4"><semantics><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{s}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{s}}})(\bm{X}_{b,g}^{(k),i})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{t}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{t}}})(\bm{X}_{b,g}^{(k),i})" class="ltx_Math" display="inline" id="S2.I5.i3.p1.m5"><semantics><mrow><mrow><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i})\doteq(f_{\theta_{\mathrm{t}}}^{\mathrm{ext}}\circ f_{\theta_{\mathrm{t}}})(\bm{X}_{b,g}^{(k),i})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i4.p1">
<p class="ltx_p">Compute the <span class="ltx_text ltx_font_italic">surrogate, approximate loss</span> <math alttext="\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)}" class="ltx_Math" display="inline" id="S2.I5.i4.p1.m1"><semantics><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>SimDINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, defined as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx93">
<tbody id="S2.E34"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\hat{\mathcal{L}}_{\mathrm{SimDINO}{}-\mathrm{s}\mathrm{t}}^{(k)}(\theta_{\mathrm{s}},\theta_{\mathrm{t}})\doteq\frac{1}{BM_{\mathrm{glo}}(M_{\mathrm{glo}}+M_{\mathrm{loc}}-1)}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}\Bigg{[}\sum_{j=1}^{M_{\mathrm{loc}}}d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),j}))" class="ltx_math_unparsed" display="inline" id="S2.E34.m2"><semantics><mrow><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>SimDINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><mo stretchy="false">)</mo></mrow><mo>≐</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>M</mi><mi>glo</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>M</mi><mi>glo</mi></msub><mo>+</mo><msub><mi>M</mi><mi>loc</mi></msub></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>glo</mi></msub></munderover></mstyle><mrow><mo maxsize="260%" minsize="260%">[</mo><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>loc</mi></msub></munderover></mstyle><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>j</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{\mathcal{L}}_{\mathrm{SimDINO}{}-\mathrm{s}\mathrm{t}}^{(k)}(\theta_{\mathrm{s}},\theta_{\mathrm{t}})\doteq\frac{1}{BM_{\mathrm{glo}}(M_{\mathrm{glo}}+M_{\mathrm{loc}}-1)}\sum_{b=1}^{B}\sum_{i=1}^{M_{\mathrm{glo}}}\Bigg{[}\sum_{j=1}^{M_{\mathrm{loc}}}d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,\ell}^{(k),j}))</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT ) ≐ divide start_ARG 1 end_ARG start_ARG italic_B italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ( italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT + italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT - 1 ) end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT [ ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_j end_POSTSUPERSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.34)</span></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad\qquad+\sum_{j=1}^{M_{\mathrm{glo}}}d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),j}))\Bigg{]}-\frac{\gamma}{M_{\mathrm{glo}}}\sum_{i=1}^{M_{\mathrm{glo}}}R_{\varepsilon}([\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{1,g}^{(k),i}),\dots,\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{B,g}^{(k),i})])" class="ltx_math_unparsed" display="inline" id="S2.Ex2.m1"><semantics><mrow><mo>+</mo><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>M</mi><mi>glo</mi></msub></munderover></mstyle><msub><mi>d</mi><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>j</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="260%" minsize="260%">]</mo><mo>−</mo><mstyle displaystyle="true"><mfrac><mi>γ</mi><msub><mi>M</mi><mi>glo</mi></msub></mfrac></mstyle><mstyle displaystyle="true"><mo movablelimits="false">∑</mo></mstyle><msub><mi></mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></msub><msup><mi></mi><msub><mi>M</mi><mi>glo</mi></msub></msup><mi>R</mi><msub><mi></mi><mi>ε</mi></msub><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mn>1</mn><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>B</mi><mo>,</mo><mi>g</mi></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>i</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\displaystyle\qquad\qquad+\sum_{j=1}^{M_{\mathrm{glo}}}d_{\ell^{2}}(\bm{z}_{\theta_{\mathrm{t}}}(\bm{X}_{b,g}^{(k),i}),\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{b,g}^{(k),j}))\Bigg{]}-\frac{\gamma}{M_{\mathrm{glo}}}\sum_{i=1}^{M_{\mathrm{glo}}}R_{\varepsilon}([\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{1,g}^{(k),i}),\dots,\bm{z}_{\theta_{\mathrm{s}}}(\bm{X}_{B,g}^{(k),i})])</annotation><annotation encoding="application/x-llamapun">+ ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_j end_POSTSUPERSCRIPT ) ) ] - divide start_ARG italic_γ end_ARG start_ARG italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( [ bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT 1 , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) , … , bold_italic_z start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_B , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT ) ] )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="R_{\varepsilon}" class="ltx_Math" display="inline" id="S2.I5.i4.p1.m2"><semantics><msub><mi>R</mi><mi>ε</mi></msub><annotation encoding="application/x-tex">R_{\varepsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT</annotation></semantics></math> is the Gaussian coding rate estimated on finite samples, described in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The gradient of <math alttext="\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)}" class="ltx_Math" display="inline" id="S2.I5.i4.p1.m3"><semantics><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>SimDINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> with respect to <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I5.i4.p1.m4"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> should (again) be computed, under the assumption that <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I5.i4.p1.m5"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math> is constant.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i5.p1">
<p class="ltx_p">Update the student parameters <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I5.i5.p1.m1"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> via an iterative gradient-based optimization algorithm, and update <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I5.i5.p1.m2"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math> via an exponential moving average with decay parameter <math alttext="\nu^{(k)}" class="ltx_Math" display="inline" id="S2.I5.i5.p1.m3"><semantics><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\nu^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx94">
<tbody id="S2.E35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta_{\mathrm{s}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E35.m1"><semantics><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\displaystyle\theta_{\mathrm{s}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\textsc{OptUpdate}^{(k)}(\theta_{\mathrm{s}}^{(k)};\nabla_{\theta_{\mathrm{s}}}\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)})" class="ltx_Math" display="inline" id="S2.E35.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mtext class="ltx_font_smallcaps">OptUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>;</mo><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mrow><mi>SimDINO</mi><mo>−</mo><mi>st</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\textsc{OptUpdate}^{(k)}(\theta_{\mathrm{s}}^{(k)};\nabla_{\theta_{\mathrm{s}}}\hat{\mathcal{L}}_{\mathrm{SimDINO}-\mathrm{s}\mathrm{t}}^{(k)})</annotation><annotation encoding="application/x-llamapun">= OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ∇ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.35)</span></td>
</tr></tbody>
<tbody id="S2.E36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta_{\mathrm{t}}^{(k+1)}" class="ltx_Math" display="inline" id="S2.E36.m1"><semantics><msubsup><mi>θ</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\displaystyle\theta_{\mathrm{t}}^{(k+1)}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\nu^{(k)}\theta_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\theta_{\mathrm{s}}^{(k+1)}." class="ltx_Math" display="inline" id="S2.E36.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>θ</mi><mi mathvariant="normal">t</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ν</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>θ</mi><mi mathvariant="normal">s</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\nu^{(k)}\theta_{\mathrm{t}}^{(k)}+(1-\nu^{(k)})\theta_{\mathrm{s}}^{(k+1)}.</annotation><annotation encoding="application/x-llamapun">= italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT + ( 1 - italic_ν start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.36)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
<p class="ltx_p">Again, we re-iterate that the gradient is only taken w.r.t. <math alttext="\theta_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m2"><semantics><msub><mi>θ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math>, treating <math alttext="\theta_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m3"><semantics><msub><mi>θ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math> as a constant. Here, note that while the choice of <math alttext="\nu" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m4"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation><annotation encoding="application/x-llamapun">italic_ν</annotation></semantics></math> is still a design decision, the hyperparameters <math alttext="\rho" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m5"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation><annotation encoding="application/x-llamapun">italic_ρ</annotation></semantics></math> and <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p1.m6"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math> is removed.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.5 </span>Evaluation Methodology</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p">There are several ways to evaluate a trained transformer model. We highlight two in this section. Let us define the <span class="ltx_text ltx_font_italic">center crop</span> view <math alttext="v_{\mathrm{cc}}\colon\mathcal{I}\to\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS5.p1.m1"><semantics><mrow><msub><mi>v</mi><mi>cc</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow></mrow><annotation encoding="application/x-tex">v_{\mathrm{cc}}\colon\mathcal{I}\to\mathcal{I}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT : caligraphic_I → caligraphic_I</annotation></semantics></math> which is a a <span class="ltx_text ltx_font_italic">deterministic resized crop</span>:</p>
<ul class="ltx_itemize" id="S2.I6">
<li class="ltx_item" id="S2.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I6.i1.p1">
<p class="ltx_p">it resizes the image so that the shortest edge is of size <math alttext="S_{\mathrm{rsz}}" class="ltx_Math" display="inline" id="S2.I6.i1.p1.m1"><semantics><msub><mi>S</mi><mi>rsz</mi></msub><annotation encoding="application/x-tex">S_{\mathrm{rsz}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT</annotation></semantics></math> (similar to random resized crops with area percentage parameter <math alttext="1" class="ltx_Math" display="inline" id="S2.I6.i1.p1.m2"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>);</p>
</div>
</li>
<li class="ltx_item" id="S2.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I6.i2.p1">
<p class="ltx_p">then it takes the <span class="ltx_text ltx_font_italic">center</span> <math alttext="S_{\mathrm{cc}}\times S_{\mathrm{cc}}" class="ltx_Math" display="inline" id="S2.I6.i2.p1.m1"><semantics><mrow><msub><mi>S</mi><mi>cc</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>S</mi><mi>cc</mi></msub></mrow><annotation encoding="application/x-tex">S_{\mathrm{cc}}\times S_{\mathrm{cc}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT × italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT</annotation></semantics></math> crop;</p>
</div>
</li>
</ul>
<p class="ltx_p">so that the final shape is <math alttext="(C,S_{\mathrm{cc}},S_{\mathrm{cc}})" class="ltx_Math" display="inline" id="S2.SS5.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>cc</mi></msub><mo>,</mo><msub><mi>S</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{\mathrm{cc}},S_{\mathrm{cc}})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT )</annotation></semantics></math>. Notice that the view <math alttext="v_{\mathrm{cc}}" class="ltx_Math" display="inline" id="S2.SS5.p1.m3"><semantics><msub><mi>v</mi><mi>cc</mi></msub><annotation encoding="application/x-tex">v_{\mathrm{cc}}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT</annotation></semantics></math> is completely deterministic given an input. For an input <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS5.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> we write <math alttext="\bm{X}_{\mathrm{cc}}\doteq v_{\mathrm{cc}}(\bm{X})" class="ltx_Math" display="inline" id="S2.SS5.p1.m5"><semantics><mrow><msub><mi>𝑿</mi><mi>cc</mi></msub><mo>≐</mo><mrow><msub><mi>v</mi><mi>cc</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{\mathrm{cc}}\doteq v_{\mathrm{cc}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ≐ italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>. Here <math alttext="S_{\mathrm{cc}}\leq S_{\mathrm{rsz}}" class="ltx_Math" display="inline" id="S2.SS5.p1.m6"><semantics><mrow><msub><mi>S</mi><mi>cc</mi></msub><mo>≤</mo><msub><mi>S</mi><mi>rsz</mi></msub></mrow><annotation encoding="application/x-tex">S_{\mathrm{cc}}\leq S_{\mathrm{rsz}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ≤ italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Linear probing.</h5>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p1">
<p class="ltx_p">The first, and most architecture-agnostic, way to evaluate an encoder model <math alttext="\bm{X}\mapsto\bm{z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo stretchy="false">↦</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\mapsto\bm{z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X ↦ bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> is to employ <span class="ltx_text ltx_font_italic">linear probing</span>. Linear probing is, in a sentence, running logistic regression on the aggregate features computed by the encoder. This tells us how much semantic information exists in the representations, as well as how easily extractable this information is. (That is: to what extent do the features of images with different semantics live on different subspaces of the feature space?)</p>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p2">
<p class="ltx_p">More formally, let us suppose that we want to evaluate the quality and faithfulness of the features of the encoder on image-label data <math alttext="(\bm{X},\bm{y})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_y )</annotation></semantics></math>, where there are <math alttext="N_{\mathrm{cls}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m2"><semantics><msub><mi>N</mi><mi>cls</mi></msub><annotation encoding="application/x-tex">N_{\mathrm{cls}}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT</annotation></semantics></math> classes and <math alttext="\bm{y}\in\{0,1\}^{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m3"><semantics><mrow><mi>𝒚</mi><mo>∈</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><msub><mi>N</mi><mi>cls</mi></msub></msup></mrow><annotation encoding="application/x-tex">\bm{y}\in\{0,1\}^{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">bold_italic_y ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> is a “one-hot encoding” (namely, zeros in all positions except a <math alttext="1" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m4"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> in the <math alttext="i^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m5"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> position if <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is in the <math alttext="i^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m7"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> class). One way to do this is to solve the logistic regression problem</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E37">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{W}\in\mathbb{R}^{N_{\mathrm{cls}}\times d}}\operatorname{\mathbb{E}}[\operatorname{CE}(\bm{y},\bm{W}\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))]." class="ltx_Math" display="block" id="S2.E37.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mi>𝑾</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>N</mi><mi>cls</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo>,</mo><mrow><mi>𝑾</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{W}\in\mathbb{R}^{N_{\mathrm{cls}}\times d}}\operatorname{\mathbb{E}}[\operatorname{CE}(\bm{y},\bm{W}\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))].</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E [ roman_CE ( bold_italic_y , bold_italic_W bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.37)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">More practically, if we have labeled data <math alttext="\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m8"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>b</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><annotation encoding="application/x-tex">\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B}</annotation><annotation encoding="application/x-llamapun">{ ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT</annotation></semantics></math>, we can solve the <span class="ltx_text ltx_font_italic">empirical</span> logistic regression problem (akin to (<a class="ltx_ref" href="#S2.E24" title="Equation 7.2.24 ‣ Optimizing DINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.24</span></a>) vs. (<a class="ltx_ref" href="#S2.E28" title="Equation 7.2.28 ‣ 4th item ‣ Optimizing DINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.28</span></a>)) given by</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E38">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{W}\in\mathbb{R}^{N_{\mathrm{cls}}\times d}}\frac{1}{B}\sum_{b=1}^{B}\operatorname{CE}(\bm{y}_{b},\bm{W}\bm{z}_{\theta}(\bm{X}_{b,\mathrm{cc}}))." class="ltx_Math" display="block" id="S2.E38.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>𝑾</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>N</mi><mi>cls</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></munder><mo lspace="0.167em">⁡</mo><mfrac><mn>1</mn><mi>B</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo>,</mo><mrow><mi>𝑾</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{W}\in\mathbb{R}^{N_{\mathrm{cls}}\times d}}\frac{1}{B}\sum_{b=1}^{B}\operatorname{CE}(\bm{y}_{b},\bm{W}\bm{z}_{\theta}(\bm{X}_{b,\mathrm{cc}})).</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_CE ( bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_W bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.38)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This problem is a convex optimization problem in <math alttext="\bm{W}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px1.p2.m9"><semantics><mi>𝑾</mi><annotation encoding="application/x-tex">\bm{W}</annotation><annotation encoding="application/x-llamapun">bold_italic_W</annotation></semantics></math>, and thus can be solved efficiently via (stochastic) gradient descent or a litany of other algorithms. This linear probe, together with the encoder, may be used as a classifier, and we can evaluate the classification accuracy. The usual practice is to train the model first on a large dataset (such as ImageNet-1K), then train the linear probe on a dataset (such as the training dataset of CIFAR-10), and evaluate it on a third (“holdout”) dataset which is drawn from the same distribution as the second one (such as the evaluation dataset of CIFAR-10).</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">
<math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbors.</h5>
<div class="ltx_para" id="S2.SS5.SSS0.Px2.p1">
<p class="ltx_p">We can also evaluate the performance of the features on classification tasks <span class="ltx_text ltx_font_italic">without needing to explicitly train a classifier</span> by using the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbor algorithm to get an average predicted label. Namely, given a dataset <math alttext="\{\bm{z}_{b}\}_{b=1}^{B}\subseteq\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>b</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{z}_{b}\}_{b=1}^{B}\subseteq\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, define the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbors of another point <math alttext="\bm{z}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m4"><semantics><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> as <math alttext="\operatorname{NN}_{k}(\bm{z},\{\bm{z}_{b}\}_{b=1}^{B})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m5"><semantics><mrow><msub><mi>NN</mi><mi>k</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>b</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{NN}_{k}(\bm{z},\{\bm{z}_{b}\}_{b=1}^{B})</annotation><annotation encoding="application/x-llamapun">roman_NN start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z , { bold_italic_z start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT )</annotation></semantics></math>. Using this notation, we can compute the predicted label <math alttext="\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m6"><semantics><mrow><msub><mover accent="true"><mi>𝒚</mi><mo>^</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>b</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ∣ { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT )</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E39">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})=\bm{1}(i^{\star})\quad\text{where}\quad i^{\star}\doteq\operatorname*{arg\ max}_{i\in[Q]}\sum_{b=1}^{B}\bm{y}_{b}\mathbf{1}[\bm{z}_{\theta}(\bm{X}_{\mathrm{cc},b})\in\operatorname{NN}_{k}(\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))]." class="ltx_Math" display="block" id="S2.E39.m1"><semantics><mrow><mrow><mrow><mrow><msub><mover accent="true"><mi>𝒚</mi><mo>^</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>b</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mspace width="1em"></mspace><mtext>where</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><msup><mi>i</mi><mo>⋆</mo></msup><mo>≐</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>Q</mi><mo stretchy="false">]</mo></mrow></mrow></munder><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><msub><mi>𝒚</mi><mi>b</mi></msub><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi>cc</mi><mo>,</mo><mi>b</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><msub><mi>NN</mi><mi>k</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})=\bm{1}(i^{\star})\quad\text{where}\quad i^{\star}\doteq\operatorname*{arg\ max}_{i\in[Q]}\sum_{b=1}^{B}\bm{y}_{b}\mathbf{1}[\bm{z}_{\theta}(\bm{X}_{\mathrm{cc},b})\in\operatorname{NN}_{k}(\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))].</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ∣ { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ) = bold_1 ( italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) where italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ≐ start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_i ∈ [ italic_Q ] end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT bold_1 [ bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc , italic_b end_POSTSUBSCRIPT ) ∈ roman_NN start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.39)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="\bm{1}(i)\in\Delta_{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m7"><semantics><mrow><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msub><mi mathvariant="normal">Δ</mi><msub><mi>N</mi><mi>cls</mi></msub></msub></mrow><annotation encoding="application/x-tex">\bm{1}(i)\in\Delta_{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">bold_1 ( italic_i ) ∈ roman_Δ start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is (by an abuse of notation, c.f. indicator variables) the one-hot probability vector supported at <math alttext="i" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m8"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>, i.e., <math alttext="1" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m9"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> in the <math alttext="i^{\mathrm{th}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m10"><semantics><msup><mi>i</mi><mi>th</mi></msup><annotation encoding="application/x-tex">i^{\mathrm{th}}</annotation><annotation encoding="application/x-llamapun">italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT</annotation></semantics></math> coordinate and <math alttext="0" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m11"><mn>0</mn></math> elsewhere. That is, this procedure takes the most common label among the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m12"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> nearest points in feature space. The <math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m13"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbor classification accuracy is just the accuracy of this predicted label, namely,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E40">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}_{\bm{X},\bm{y}}[\mathbf{1}(\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})=\bm{y})]" class="ltx_Math" display="block" id="S2.E40.m1"><semantics><mrow><msub><mi>𝔼</mi><mrow><mi>𝑿</mi><mo>,</mo><mi>𝒚</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mover accent="true"><mi>𝒚</mi><mo>^</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∣</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>b</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}_{\bm{X},\bm{y}}[\mathbf{1}(\hat{\bm{y}}_{\theta}(\bm{X}\mid\{(\bm{X}_{b},\bm{y}_{b})\}_{b=1}^{B})=\bm{y})]</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_X , bold_italic_y end_POSTSUBSCRIPT [ bold_1 ( over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ∣ { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ) = bold_italic_y ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.40)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">or more commonly its corresponding empirical version, where <math alttext="(\bm{X},\bm{y})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m14"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_y )</annotation></semantics></math> ranges over a finite dataset (<span class="ltx_text ltx_font_italic">not</span> the existing samples <math alttext="(\bm{X}_{b},\bm{y}_{b})" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m15"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>b</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>b</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X}_{b},\bm{y}_{b})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT )</annotation></semantics></math> which are used for the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px2.p1.m16"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> neighbors).</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Fidelity of the attention maps.</h5>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p1">
<p class="ltx_p">Another way to check the performance of the representations, for a transformer-based encoder, is to examine the fidelity of the attention maps <math alttext="\bm{A}^{L,k}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m1"><semantics><mrow><msup><mi>𝑨</mi><mrow><mi>L</mi><mo>,</mo><mi>k</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}^{L,k}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT italic_L , italic_k end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> as defined in <a class="ltx_ref" href="#S2.E19" title="In 1st item ‣ Backbone. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">7.2.19</span></a>, at the last layer <math alttext="L" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m2"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>, and given by the following pipeline:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx95">
<tbody id="S2.E41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{X}\mapsto\cdots\mapsto\bm{Z}^{L-1}=[\underbrace{\bm{z}_{1}^{L-1}}_{\text{class token}},\underbrace{\bm{z}_{2}^{L-1}\dots,\bm{z}_{n}^{L-1}}_{\text{patch tokens}}]\mapsto\bm{A}^{k,L}=\begin{bmatrix}\bm{A}_{1,1}^{k,L}&amp;\bm{A}_{1,2:}^{k,L}\\
\bm{A}_{2:,1}^{k,L}&amp;\bm{A}_{2:,2:}^{k,L}\end{bmatrix}." class="ltx_Math" display="inline" id="S2.E41.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo stretchy="false">↦</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">↦</mo><msup><mi>𝒁</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><munder><munder accentunder="true"><msubsup><mi>𝒛</mi><mn>1</mn><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>⏟</mo></munder><mtext>class token</mtext></munder><mo>,</mo><munder><munder accentunder="true"><mrow><mrow><msubsup><mi>𝒛</mi><mn>2</mn><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><msubsup><mi>𝒛</mi><mi>n</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><mo>⏟</mo></munder><mtext>patch tokens</mtext></munder><mo stretchy="false">]</mo></mrow><mo stretchy="false">↦</mo><msup><mi>𝑨</mi><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msup><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd><msubsup><mi>𝑨</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup></mtd><mtd><msubsup><mi>𝑨</mi><mrow><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi></mi></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝑨</mi><mrow><mn>2</mn><mo lspace="0.278em">⁣</mo><mrow><mo rspace="0em">:</mo><mo>,</mo><mn>1</mn></mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup></mtd><mtd><msubsup><mi>𝑨</mi><mrow><mn>2</mn><mo lspace="0.278em">⁣</mo><mrow><mrow><mo rspace="0em">:</mo><mo>,</mo><mn>2</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi></mi></mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{X}\mapsto\cdots\mapsto\bm{Z}^{L-1}=[\underbrace{\bm{z}_{1}^{L-1}}_{\text{class token}},\underbrace{\bm{z}_{2}^{L-1}\dots,\bm{z}_{n}^{L-1}}_{\text{patch tokens}}]\mapsto\bm{A}^{k,L}=\begin{bmatrix}\bm{A}_{1,1}^{k,L}&amp;\bm{A}_{1,2:}^{k,L}\\
\bm{A}_{2:,1}^{k,L}&amp;\bm{A}_{2:,2:}^{k,L}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X ↦ ⋯ ↦ bold_italic_Z start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT = [ under⏟ start_ARG bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT class token end_POSTSUBSCRIPT , under⏟ start_ARG bold_italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT … , bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT patch tokens end_POSTSUBSCRIPT ] ↦ bold_italic_A start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_italic_A start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT end_CELL start_CELL bold_italic_A start_POSTSUBSCRIPT 1 , 2 : end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_A start_POSTSUBSCRIPT 2 : , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT end_CELL start_CELL bold_italic_A start_POSTSUBSCRIPT 2 : , 2 : end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.2.41)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In particular, we examine what the attention maps for a given input reveal about the salient objects in the input image, i.e., which parts of the image provide the most globally-relevant information to the class token. One particular way to do this is to examine the component of the attention map where the class token is extracted as the query and removed from the value matrix, i.e., <math alttext="\bm{A}_{2:,1}^{k,L}\in\mathbb{R}^{1\times(n-1)}=\mathbb{R}^{1\times N}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m3"><semantics><mrow><msubsup><mi>𝑨</mi><mrow><mn>2</mn><mo lspace="0.278em">⁣</mo><mrow><mo rspace="0em">:</mo><mo>,</mo><mn>1</mn></mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></msup><mo>=</mo><msup><mi>ℝ</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}_{2:,1}^{k,L}\in\mathbb{R}^{1\times(n-1)}=\mathbb{R}^{1\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT 2 : , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 1 × ( italic_n - 1 ) end_POSTSUPERSCRIPT = blackboard_R start_POSTSUPERSCRIPT 1 × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> or its transpose <math alttext="\bm{a}^{k,L}=(\bm{A}_{2:,1}^{k,L})^{\top}\in\mathbb{R}^{N}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m4"><semantics><mrow><msup><mi>𝒂</mi><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msup><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑨</mi><mrow><mn>2</mn><mo lspace="0.278em">⁣</mo><mrow><mo rspace="0em">:</mo><mo>,</mo><mn>1</mn></mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\bm{a}^{k,L}=(\bm{A}_{2:,1}^{k,L})^{\top}\in\mathbb{R}^{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_a start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT = ( bold_italic_A start_POSTSUBSCRIPT 2 : , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. Notice that this vector <math alttext="\bm{a}^{k,L}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m5"><semantics><msup><mi>𝒂</mi><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msup><annotation encoding="application/x-tex">\bm{a}^{k,L}</annotation><annotation encoding="application/x-llamapun">bold_italic_a start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>, which we label as the “<span class="ltx_text ltx_font_italic">saliency vector</span> at the <math alttext="k^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m6"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> attention head at layer <math alttext="L" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m7"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>,” has a value for every patch, <math alttext="1,\dots,N" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m8"><semantics><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1,\dots,N</annotation><annotation encoding="application/x-llamapun">1 , … , italic_N</annotation></semantics></math>, and we use this value to describe how relevant each patch is towards the global information. In particular for visualization’s sake we create a new image where each patch is replaced by its corresponding value in the saliency vector, showcasing the contribution of each patch; we call this image the “<span class="ltx_text ltx_font_italic">saliency map</span> at the <math alttext="k^{\textnormal{th}}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m9"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> attention head at layer <math alttext="L" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m10"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>”. To visualize the total relevance of each patch towards the global information across all heads, we can average the saliency vector, i.e., <math alttext="\tilde{\bm{a}}^{L}\doteq\frac{1}{K}\sum_{k=1}^{K}\bm{a}^{k,L}" class="ltx_Math" display="inline" id="S2.SS5.SSS0.Px3.p1.m11"><semantics><mrow><msup><mover accent="true"><mi>𝒂</mi><mo>~</mo></mover><mi>L</mi></msup><mo>≐</mo><mrow><mfrac><mn>1</mn><mi>K</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msup><mi>𝒂</mi><mrow><mi>k</mi><mo>,</mo><mi>L</mi></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{\bm{a}}^{L}\doteq\frac{1}{K}\sum_{k=1}^{K}\bm{a}^{k,L}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_a end_ARG start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ≐ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_a start_POSTSUPERSCRIPT italic_k , italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> and expand into the <span class="ltx_text ltx_font_italic">average saliency map</span>. The average saliency maps should highlight the relevant parts of the input image.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Object detection and segmentation.</h5>
<div class="ltx_para" id="S2.SS5.SSS0.Px4.p1">
<p class="ltx_p">We can evaluate how the representations capture the fine-grained (i.e., smaller or more detailed) properties of the input by using them for <span class="ltx_text ltx_font_italic">semantic segmentation</span>. Roughly, this means that we use the features to construct bounding boxes for all objects in the input. There are several ways to do this, and several ways to score the resulting bounding boxes compared to a ground-truth. Each combination of methods corresponds to a particular segmentation metric. We do not formally describe them here as they are not particularly insightful, but the DINO paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite> and DINOv2 paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx204" title="">ODM+23</a>]</cite> contain references to all metrics that are used in practice.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2.6 </span>Experimental Setup and Results</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p">Since SimDINO is directly built upon DINO, we compare the optimal settings for DINO as given by their original paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite> with the same settings applied to SimDINO for fair comparison.</p>
</div>
<section class="ltx_paragraph" id="S2.SS6.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Objective function.</h5>
<div class="ltx_para" id="S2.SS6.SSS0.Px1.p1">
<p class="ltx_p">We use <math alttext="10" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m1"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation><annotation encoding="application/x-llamapun">10</annotation></semantics></math> local views (i.e., <math alttext="M_{\mathrm{loc}}=10" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m2"><semantics><mrow><msub><mi>M</mi><mi>loc</mi></msub><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">M_{\mathrm{loc}}=10</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10</annotation></semantics></math>) of resolution <math alttext="96\times 96" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m3"><semantics><mrow><mn>96</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>96</mn></mrow><annotation encoding="application/x-tex">96\times 96</annotation><annotation encoding="application/x-llamapun">96 × 96</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{loc}}=96" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m4"><semantics><mrow><msub><mi>S</mi><mi>loc</mi></msub><mo>=</mo><mn>96</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{loc}}=96</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 96</annotation></semantics></math>) and <math alttext="2" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m5"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation><annotation encoding="application/x-llamapun">2</annotation></semantics></math> global views (i.e., <math alttext="M_{\mathrm{glo}}=2" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m6"><semantics><mrow><msub><mi>M</mi><mi>glo</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">M_{\mathrm{glo}}=2</annotation><annotation encoding="application/x-llamapun">italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 2</annotation></semantics></math>) of resolution <math alttext="224\times 224" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m7"><semantics><mrow><mn>224</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation><annotation encoding="application/x-llamapun">224 × 224</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{glo}}=224" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m8"><semantics><mrow><msub><mi>S</mi><mi>glo</mi></msub><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{glo}}=224</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 224</annotation></semantics></math>) for all experiments. The corresponding portions of the original images cropped for local and global views are <math alttext="p_{\mathrm{loc}}\in[\frac{1}{20},\frac{3}{10}]" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m9"><semantics><mrow><msub><mi>p</mi><mi>loc</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mfrac><mn>1</mn><mn>20</mn></mfrac><mo>,</mo><mfrac><mn>3</mn><mn>10</mn></mfrac><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{loc}}\in[\frac{1}{20},\frac{3}{10}]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT ∈ [ divide start_ARG 1 end_ARG start_ARG 20 end_ARG , divide start_ARG 3 end_ARG start_ARG 10 end_ARG ]</annotation></semantics></math> and <math alttext="p_{\mathrm{glo}}\in[\frac{3}{10},1]" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m10"><semantics><mrow><msub><mi>p</mi><mi>glo</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mfrac><mn>3</mn><mn>10</mn></mfrac><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{glo}}\in[\frac{3}{10},1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ∈ [ divide start_ARG 3 end_ARG start_ARG 10 end_ARG , 1 ]</annotation></semantics></math> (chosen randomly per-view). The smaller edge size within the resized crops is <math alttext="S_{\mathrm{rsz}}=256" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m11"><semantics><mrow><msub><mi>S</mi><mi>rsz</mi></msub><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{rsz}}=256</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256</annotation></semantics></math>, and the center crop (evaluation) view edge size is <math alttext="S_{\mathrm{cc}}=224" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px1.p1.m12"><semantics><mrow><msub><mi>S</mi><mi>cc</mi></msub><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{cc}}=224</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = 224</annotation></semantics></math>. All of these settings apply to both DINO and SimDINO.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS6.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Model architecture.</h5>
<div class="ltx_para" id="S2.SS6.SSS0.Px2.p1">
<p class="ltx_p">For all inputs we set the patch size to be <math alttext="16\times 16" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m1"><semantics><mrow><mn>16</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16\times 16</annotation><annotation encoding="application/x-llamapun">16 × 16</annotation></semantics></math> (namely, <math alttext="P_{H}=P_{W}=16" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m2"><semantics><mrow><msub><mi>P</mi><mi>H</mi></msub><mo>=</mo><msub><mi>P</mi><mi>W</mi></msub><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">P_{H}=P_{W}=16</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16</annotation></semantics></math>). We use the small, base, and large models of the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx75" title="">DBK+21</a>]</cite> architecture as the embedding and backbone. The feature extractor is a three-layer MLP with a hidden size of <math alttext="2048" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m3"><semantics><mn>2048</mn><annotation encoding="application/x-tex">2048</annotation><annotation encoding="application/x-llamapun">2048</annotation></semantics></math> and an output dimension of <math alttext="256" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m4"><semantics><mn>256</mn><annotation encoding="application/x-tex">256</annotation><annotation encoding="application/x-llamapun">256</annotation></semantics></math>, followed by an <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m5"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-normalization, as specified in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>. For DINO architectures (i.e., not SimDINO architectures), the DINO head <math alttext="\bm{W}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m6"><semantics><mi>𝑾</mi><annotation encoding="application/x-tex">\bm{W}</annotation><annotation encoding="application/x-llamapun">bold_italic_W</annotation></semantics></math> is a matrix in <math alttext="\mathbb{R}^{65536\times 256}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m7"><semantics><msup><mi>ℝ</mi><mrow><mn>65536</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>256</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{65536\times 256}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 65536 × 256 end_POSTSUPERSCRIPT</annotation></semantics></math>, and the parameter <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m8"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math> is a vector in <math alttext="\mathbb{R}^{65536}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px2.p1.m9"><semantics><msup><mi>ℝ</mi><mn>65536</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{65536}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 65536 end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS6.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Datasets and optimization.</h5>
<div class="ltx_para" id="S2.SS6.SSS0.Px3.p1">
<p class="ltx_p">For pre-training, both our DINO reproduction and SimDINO use the ImageNet-1K dataset across all methods. We use AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx169" title="">LH17</a>]</cite> as the optimizer, which is a very standard choice. We follow the following hyperparameter recommendations:</p>
<ul class="ltx_itemize" id="S2.I7">
<li class="ltx_item" id="S2.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I7.i1.p1">
<p class="ltx_p">The batch size is <math alttext="B=1024" class="ltx_Math" display="inline" id="S2.I7.i1.p1.m1"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">B=1024</annotation><annotation encoding="application/x-llamapun">italic_B = 1024</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I7.i2.p1">
<p class="ltx_p">The learning rate (for AdamW and the student model) has “base” value <math alttext="2\times 10^{-3}" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m1"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-3}</annotation><annotation encoding="application/x-llamapun">2 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. In the first <math alttext="10" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m2"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation><annotation encoding="application/x-llamapun">10</annotation></semantics></math> epochs the learning rate linearly increases from <math alttext="0" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m3"><mn>0</mn></math> to the base value (i.e., at the <math alttext="i^{\mathrm{th}}" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m4"><semantics><msup><mi>i</mi><mi>th</mi></msup><annotation encoding="application/x-tex">i^{\mathrm{th}}</annotation><annotation encoding="application/x-llamapun">italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT</annotation></semantics></math> epoch the learning rate is <math alttext="(i/10)\cdot 2\times 10^{-3}" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m5"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>/</mo><mn>10</mn></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><mn>2</mn></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(i/10)\cdot 2\times 10^{-3}</annotation><annotation encoding="application/x-llamapun">( italic_i / 10 ) ⋅ 2 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, for <math alttext="1\leq i\leq 10" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m6"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">1\leq i\leq 10</annotation><annotation encoding="application/x-llamapun">1 ≤ italic_i ≤ 10</annotation></semantics></math>). Then over the next <math alttext="90" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m7"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation><annotation encoding="application/x-llamapun">90</annotation></semantics></math> epochs the learning rate decays via a so-called <span class="ltx_text ltx_font_italic">cosine schedule</span> back down to <math alttext="0" class="ltx_Math" display="inline" id="S2.I7.i2.p1.m8"><mn>0</mn></math>. The definition of a cosine schedule is given in many places, including <a class="ltx_ref ltx_href" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html" title="">PyTorch documentation</a>, and it is commonly used when training deep vision models.</p>
</div>
</li>
<li class="ltx_item" id="S2.I7.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I7.i3.p1">
<p class="ltx_p">The weight decay (the W in AdamW) follows a cosine schedule from 0.04 to <math alttext="0.4" class="ltx_Math" display="inline" id="S2.I7.i3.p1.m1"><semantics><mn>0.4</mn><annotation encoding="application/x-tex">0.4</annotation><annotation encoding="application/x-llamapun">0.4</annotation></semantics></math> over training.</p>
</div>
</li>
<li class="ltx_item" id="S2.I7.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I7.i4.p1">
<p class="ltx_p">The EMA rate <math alttext="\nu" class="ltx_Math" display="inline" id="S2.I7.i4.p1.m1"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation><annotation encoding="application/x-llamapun">italic_ν</annotation></semantics></math> follows a cosine schedule from <math alttext="0.996" class="ltx_Math" display="inline" id="S2.I7.i4.p1.m2"><semantics><mn>0.996</mn><annotation encoding="application/x-tex">0.996</annotation><annotation encoding="application/x-llamapun">0.996</annotation></semantics></math> to <math alttext="1.0" class="ltx_Math" display="inline" id="S2.I7.i4.p1.m3"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation><annotation encoding="application/x-llamapun">1.0</annotation></semantics></math> over training. Specifically for DINO, the centering EMA rate <math alttext="\rho" class="ltx_Math" display="inline" id="S2.I7.i4.p1.m4"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation><annotation encoding="application/x-llamapun">italic_ρ</annotation></semantics></math> is fixed at <math alttext="0.9" class="ltx_Math" display="inline" id="S2.I7.i4.p1.m5"><semantics><mn>0.9</mn><annotation encoding="application/x-tex">0.9</annotation><annotation encoding="application/x-llamapun">0.9</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I7.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I7.i5.p1">
<p class="ltx_p">Specifically for DINO, the teacher temperature <math alttext="\tau_{\mathrm{t}}" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m1"><semantics><msub><mi>τ</mi><mi mathvariant="normal">t</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{t}}</annotation><annotation encoding="application/x-llamapun">italic_τ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT</annotation></semantics></math> is fixed at <math alttext="0.1" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m2"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation><annotation encoding="application/x-llamapun">0.1</annotation></semantics></math>, while the student temperature <math alttext="\tau_{\mathrm{s}}" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m3"><semantics><msub><mi>τ</mi><mi mathvariant="normal">s</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{s}}</annotation><annotation encoding="application/x-llamapun">italic_τ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math> linearly increases from <math alttext="0.04" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m4"><semantics><mn>0.04</mn><annotation encoding="application/x-tex">0.04</annotation><annotation encoding="application/x-llamapun">0.04</annotation></semantics></math> to <math alttext="0.07" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m5"><semantics><mn>0.07</mn><annotation encoding="application/x-tex">0.07</annotation><annotation encoding="application/x-llamapun">0.07</annotation></semantics></math> during the first <math alttext="30" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m6"><semantics><mn>30</mn><annotation encoding="application/x-tex">30</annotation><annotation encoding="application/x-llamapun">30</annotation></semantics></math> epochs and is fixed at <math alttext="0.07" class="ltx_Math" display="inline" id="S2.I7.i5.p1.m7"><semantics><mn>0.07</mn><annotation encoding="application/x-tex">0.07</annotation><annotation encoding="application/x-llamapun">0.07</annotation></semantics></math> thereafter.</p>
</div>
</li>
</ul>
<p class="ltx_p">We use some (essentially information-preserving) data augmentations, such as flips, color jittering, Gaussian blur, and solarization, for each seen image during training, before taking the local and global views. The exact hyperparameters governing these are not listed here, but are referenced in the DINO paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS6.SSS0.Px3.p2">
<p class="ltx_p">For linear probing, the linear probe is usually trained using the AdamW optimizer with learning rate <math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px3.p2.m1"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation><annotation encoding="application/x-llamapun">2 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, weight decay <math alttext="0.01" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px3.p2.m2"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation><annotation encoding="application/x-llamapun">0.01</annotation></semantics></math>, and batch size <math alttext="512" class="ltx_Math" display="inline" id="S2.SS6.SSS0.Px3.p2.m3"><semantics><mn>512</mn><annotation encoding="application/x-tex">512</annotation><annotation encoding="application/x-llamapun">512</annotation></semantics></math>, but these are often modified on a case-by-case basis to minimize the loss.</p>
</div>
<figure class="ltx_table" id="T1">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Epochs</td>
<td class="ltx_td ltx_align_center ltx_border_tt">20-NN</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Linear Probing</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">DINO</td>
<td class="ltx_td ltx_align_center ltx_border_t">ViT-B</td>
<td class="ltx_td ltx_align_center ltx_border_t">100</td>
<td class="ltx_td ltx_align_center ltx_border_t">72.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">76.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SimDINO</td>
<td class="ltx_td ltx_align_center">ViT-B</td>
<td class="ltx_td ltx_align_center">100</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">74.9</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">77.3</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">DINO</td>
<td class="ltx_td ltx_align_center">ViT-L</td>
<td class="ltx_td ltx_align_center">100</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">SimDINO</td>
<td class="ltx_td ltx_align_center">ViT-L</td>
<td class="ltx_td ltx_align_center">100</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">75.6</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">77.4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text" style="color:#808080;">SwAV</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">ViT-S</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">800</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">66.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">73.5</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text" style="color:#808080;">MoCov3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">ViT-B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">300</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">–</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">76.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Classification performance<span class="ltx_text ltx_font_medium"> on hold-out test data for DINO and SimDINO, using both <math alttext="k" class="ltx_Math" display="inline" id="T1.m4"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-nearest neighbor accuracy (<math alttext="k=20" class="ltx_Math" display="inline" id="T1.m5"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k=20</annotation><annotation encoding="application/x-llamapun">italic_k = 20</annotation></semantics></math>) and linear probing. At the same number of iterations (<math alttext="100" class="ltx_Math" display="inline" id="T1.m6"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation><annotation encoding="application/x-llamapun">100</annotation></semantics></math>), SimDINO is clearly better in terms of performance, and is more stable (the DINO training running on ViT-L backbone with the provided settings has very unstable optimization and obtains NaN loss in short order). We also compare to other standout methods, namely SwAV and MoCov3, which DINO was built on.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="F10"><img alt="Figure 7.10 : A qualitative comparison of saliency maps generated by DINO (middle row) and by SimDINO (bottom row) . For each image, we compute and display the average saliency map in the last layer L L italic_L . The saliency maps are similar across models, meaning that all models converge to a similar notion of what objects are important. Note that although X eval X_{\mathrm{eval}} italic_X start_POSTSUBSCRIPT roman_eval end_POSTSUBSCRIPT is a square image, it is interpolated back into rectangular shape to make this visualization." class="ltx_graphics ltx_img_landscape" height="159" id="F10.g1" src="chapters/chapter7/figs/dino_attention_maps.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.10</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">A qualitative comparison of saliency maps<span class="ltx_text ltx_font_medium"> generated by DINO <span class="ltx_text ltx_font_italic">(middle row)</span> and by SimDINO <span class="ltx_text ltx_font_italic">(bottom row)</span>. For each image, we compute and display the average saliency map in the last layer <math alttext="L" class="ltx_Math" display="inline" id="F10.m3"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>. The saliency maps are similar across models, meaning that all models converge to a similar notion of what objects are important. Note that although <math alttext="X_{\mathrm{eval}}" class="ltx_Math" display="inline" id="F10.m4"><semantics><msub><mi>X</mi><mi>eval</mi></msub><annotation encoding="application/x-tex">X_{\mathrm{eval}}</annotation><annotation encoding="application/x-llamapun">italic_X start_POSTSUBSCRIPT roman_eval end_POSTSUBSCRIPT</annotation></semantics></math> is a square image, it is interpolated back into rectangular shape to make this visualization.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Detection <math alttext="\uparrow" class="ltx_Math" display="inline" id="T2.m1"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Segmentation <math alttext="\uparrow" class="ltx_Math" display="inline" id="T2.m2"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SimDINO</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViT-L/16</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">5.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">1.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">SimDINO</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ViT-B/16</th>
<td class="ltx_td ltx_align_center">5.2</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">4.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.0</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">DINO</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ViT-B/16</th>
<td class="ltx_td ltx_align_center">3.9</td>
<td class="ltx_td ltx_align_center">1.5</td>
<td class="ltx_td ltx_align_center">1.8</td>
<td class="ltx_td ltx_align_center">3.1</td>
<td class="ltx_td ltx_align_center">1.0</td>
<td class="ltx_td ltx_align_center">1.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">DINO</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">ViT-B/8</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">5.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">2.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">2.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">4.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">1.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text" style="color:#808080;">1.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Segmentation performance<span class="ltx_text ltx_font_medium"> of pre-trained DINO and SimDINO models on COCO val2017 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx163" title="">LMB+14</a>]</cite>, a segmentation dataset which contains object location metadata. We do not train on COCO, merely using the pre-trained embedding and backbone, and the bounding boxes are extracted from the features via a method called MaskCut <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx286" title="">WGY+23</a>]</cite>. Nevertheless, SimDINO surpasses DINO at object detection and segmentation at fair comparison, and even surpasses DINO with smaller patch size (side length <math alttext="8" class="ltx_Math" display="inline" id="T2.m9"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> instead of <math alttext="16" class="ltx_Math" display="inline" id="T2.m10"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation><annotation encoding="application/x-llamapun">16</annotation></semantics></math>). Smaller patch sizes are known to help performance, especially with detection and segmentation tasks, so this result is quite surprising and encouraging.</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS6.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Evaluation results.</h5>
<div class="ltx_para" id="S2.SS6.SSS0.Px4.p1">
<p class="ltx_p">In terms of downsream classification performance, we obtain the performance in <a class="ltx_ref" href="#T1" title="In Datasets and optimization. ‣ 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.1</span></a>. We observe that the performance of SimDINO is much higher than in DINO under fair comparison. Also, it is much more stable: the prescribed settings of DINO cannot train a ViT-L(arge) model. On the other hand, <a class="ltx_ref" href="#F10" title="In Datasets and optimization. ‣ 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.10</span></a> shows visualizations of the average saliency maps in DINO and our simplified DINO, observing that the saliency maps look quite similar across models, indicating that the models learn features which are at least as good at capturing fine-grained details. The segmentation and object detection performances in <a class="ltx_ref" href="#T2" title="In Datasets and optimization. ‣ 7.2.6 Experimental Setup and Results ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.2</span></a> confirm this claim quantitatively, where SimDINO features show substantive improvement over those of DINO.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.3 </span>Image Classification</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">In the previous section, we simplified an overly complex learning objective using our intuition of representation learning through the lens of compression. However, many of the most popular learning procedures are incredibly simple. In these cases, it is difficult to further simplify the objective. Thus, in this and future sections, we will focus on principled ways to modify the <span class="ltx_text ltx_font_italic">deep network architectures</span> for a variety of tasks.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">Let us first start with arguably the most classical task in machine learning: <span class="ltx_text ltx_font_italic">image classification</span>, which is often used as a standard task to evaluate pattern recognition algorithms or deep network architectures. From our discussion of white-box architectures in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we only need a semantically meaningful task to learn good representations with white-box architectures. We will validate this idea in this section.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p">First, the dataset stays largely the same as <a class="ltx_ref" href="#S2.SS1" title="7.2.1 Data ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.1</span></a>. Both the training and test data consist of labeled images, i.e., image-label pairs <math alttext="(\bm{X},\bm{y})\in\mathbb{R}^{C\times H\times W}\times\{0,1\}^{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S3.p3.m1"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><msub><mi>N</mi><mi>cls</mi></msub></msup></mrow></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{y})\in\mathbb{R}^{C\times H\times W}\times\{0,1\}^{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_y ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_H × italic_W end_POSTSUPERSCRIPT × { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>. We still apply various data augmentations (e.g., flips, Gaussian blurring, solarization, etc.) to each sample in each new batch.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3.1 </span>Task and Objective</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">Unlike before, our task is not just to learn a good representation of the data, but also simultaneously build a classifier. Formally, we have labeled data pairs <math alttext="(\bm{X},\bm{y})" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_y )</annotation></semantics></math>, where <math alttext="\bm{y}\in\{0,1\}^{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2"><semantics><mrow><mi>𝒚</mi><mo>∈</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><msub><mi>N</mi><mi>cls</mi></msub></msup></mrow><annotation encoding="application/x-tex">\bm{y}\in\{0,1\}^{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">bold_italic_y ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> is a one-hot vector denoting the class membership of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. We consider a deterministic <span class="ltx_text ltx_font_italic">center crop view</span> <math alttext="v_{\mathrm{cc}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m4"><semantics><msub><mi>v</mi><mi>cc</mi></msub><annotation encoding="application/x-tex">v_{\mathrm{cc}}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT</annotation></semantics></math> of the input data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p1.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> (cf <a class="ltx_ref" href="#S2.SS2" title="7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.2</span></a>) of the input data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. We want to jointly train a feature mapping <math alttext="(f_{\theta},f_{\theta}^{\mathrm{ext}})" class="ltx_Math" display="inline" id="S3.SS1.p1.m7"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo>,</mo><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f_{\theta},f_{\theta}^{\mathrm{ext}})</annotation><annotation encoding="application/x-llamapun">( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT )</annotation></semantics></math> and a <span class="ltx_text ltx_font_italic">classification head</span> <math alttext="h_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8"><semantics><msub><mi>h</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">h_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h_{\theta}(\bm{z})\doteq\operatorname{\mathrm{softmax}}(\bm{W}^{\mathrm{head}}\bm{z}+\bm{b}^{\mathrm{head}}),\qquad\forall\bm{z}\in\mathbb{R}^{d}" class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑾</mi><mi>head</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><msup><mi>𝒃</mi><mi>head</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>𝒛</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">h_{\theta}(\bm{z})\doteq\operatorname{\mathrm{softmax}}(\bm{W}^{\mathrm{head}}\bm{z}+\bm{b}^{\mathrm{head}}),\qquad\forall\bm{z}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) ≐ roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT ) , ∀ bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="(\bm{W}^{\mathrm{head}},\bm{b}^{\mathrm{head}})\in\mathbb{R}^{N_{\mathrm{cls}}\times d}\times\mathbb{R}^{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m9"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>𝑾</mi><mi>head</mi></msup><mo>,</mo><msup><mi>𝒃</mi><mi>head</mi></msup><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><msup><mi>ℝ</mi><mrow><msub><mi>N</mi><mi>cls</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>ℝ</mi><msub><mi>N</mi><mi>cls</mi></msub></msup></mrow></mrow><annotation encoding="application/x-tex">(\bm{W}^{\mathrm{head}},\bm{b}^{\mathrm{head}})\in\mathbb{R}^{N_{\mathrm{cls}}\times d}\times\mathbb{R}^{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">( bold_italic_W start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> are trainable parameters in the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.m10"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, such that the map <math alttext="\bm{X}_{\mathrm{cc}}\mapsto\bm{p}_{\theta}(\bm{X}_{\mathrm{cc}})\doteq h_{\theta}(\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))" class="ltx_Math" display="inline" id="S3.SS1.p1.m11"><semantics><mrow><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">↦</mo><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{\mathrm{cc}}\mapsto\bm{p}_{\theta}(\bm{X}_{\mathrm{cc}})\doteq h_{\theta}(\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}}))</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ↦ bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) )</annotation></semantics></math> predicts a smoothed label for the view <math alttext="\bm{X}_{\mathrm{cc}}=v_{\mathrm{cc}}(\bm{X})" class="ltx_Math" display="inline" id="S3.SS1.p1.m12"><semantics><mrow><msub><mi>𝑿</mi><mi>cc</mi></msub><mo>=</mo><mrow><msub><mi>v</mi><mi>cc</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{\mathrm{cc}}=v_{\mathrm{cc}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> of the input <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p1.m13"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. The learning problem attempts to minimize the distance between <math alttext="\bm{p}_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.m14"><semantics><msub><mi>𝒑</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\bm{p}_{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.m15"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> measured through cross-entropy:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\left\{\mathcal{L}_{\operatorname{CE}}(\theta)\doteq\operatorname{\mathbb{E}}[\operatorname{CE}(\bm{y},\bm{p}_{\theta}(\bm{X}_{\mathrm{cc}}))]\right\}." class="ltx_Math" display="block" id="S3.E2.m1"><semantics><mrow><mrow><munder><mi>min</mi><mi>θ</mi></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>CE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo>,</mo><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\theta}\left\{\mathcal{L}_{\operatorname{CE}}(\theta)\doteq\operatorname{\mathbb{E}}[\operatorname{CE}(\bm{y},\bm{p}_{\theta}(\bm{X}_{\mathrm{cc}}))]\right\}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( italic_θ ) ≐ blackboard_E [ roman_CE ( bold_italic_y , bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ) ] } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3.2 </span>The CRATE Architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">The architecture that we use is the CRATE architecture, described in some level of detail in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The overall setup is similar to that of the regular transformer in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>, with a few changes. While the embedding step is the same as both DINO and SimDINO in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>, the feature extraction step is the same as SimDINO in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a> as it just extracts the feature corresponding to the class token, and the classification head is described in <a class="ltx_ref" href="#S3.SS1" title="7.3.1 Task and Objective ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.3.1</span></a>, the backbone architecture is different. Each layer takes the form</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx96">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})" class="ltx_Math" display="inline" id="S3.E3.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))," class="ltx_Math" display="inline" id="S3.E3.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>MSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X}))),</annotation><annotation encoding="application/x-llamapun">= bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S3.E4.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{ISTA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})))," class="ltx_Math" display="inline" id="S3.E4.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>2</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{ISTA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X}))),</annotation><annotation encoding="application/x-llamapun">= roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\operatorname{MSSA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><msubsup><mi>MSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{MSSA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_MSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\operatorname{ISTA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{ISTA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> blocks are as described in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, namely:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">The <math alttext="\operatorname{MSSA}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m1"><semantics><mi>MSSA</mi><annotation encoding="application/x-tex">\operatorname{MSSA}</annotation><annotation encoding="application/x-llamapun">roman_MSSA</annotation></semantics></math> operator is multi-head-subspace-self-attention, defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{MSSA}_{\theta}^{\ell}(\bm{Z})\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{SA}([\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{SA}([\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{n}^{\top}" class="ltx_Math" display="block" id="S3.E5.m1"><semantics><mrow><mrow><msubsup><mi>MSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msubsup><mi>𝑼</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>𝒃</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{MSSA}_{\theta}^{\ell}(\bm{Z})\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{SA}([\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{SA}([\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{n}^{\top}</annotation><annotation encoding="application/x-llamapun">roman_MSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW end_ARG ] + bold_italic_b start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{U}^{k,\ell}\in\mathbb{R}^{d\times p}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m2"><semantics><mrow><msup><mi>𝑼</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}^{k,\ell}\in\mathbb{R}^{d\times p}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{U}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d\times Kp}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m3"><semantics><mrow><msubsup><mi>𝑼</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d\times Kp}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_K italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\bm{b}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m4"><semantics><mrow><msubsup><mi>𝒃</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{b}_{\mathrm{out}}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_b start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are trainable parameters belonging to the parameter set <math alttext="\theta" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m5"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, and (recall) the self-attention operator <math alttext="\operatorname{SA}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m6"><semantics><mi>SA</mi><annotation encoding="application/x-tex">\operatorname{SA}</annotation><annotation encoding="application/x-llamapun">roman_SA</annotation></semantics></math> is defined in (<a class="ltx_ref" href="#S2.E16" title="Equation 7.2.16 ‣ 1st item ‣ Backbone. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.16</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">The <math alttext="\operatorname{ISTA}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m1"><semantics><mi>ISTA</mi><annotation encoding="application/x-tex">\operatorname{ISTA}</annotation><annotation encoding="application/x-llamapun">roman_ISTA</annotation></semantics></math> operator is the iterative-shrinkage-thresholding-algorithm operator, defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z})\doteq\operatorname{ReLU}(\bm{Z}-\beta(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell}\bm{Z}-\bm{Z})+\beta\lambda\bm{1}_{d}\bm{1}_{n}^{\top})," class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><mrow><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>ReLU</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝒁</mi><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>−</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟏</mn><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z})\doteq\operatorname{ReLU}(\bm{Z}-\beta(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell}\bm{Z}-\bm{Z})+\beta\lambda\bm{1}_{d}\bm{1}_{n}^{\top}),</annotation><annotation encoding="application/x-llamapun">roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ roman_ReLU ( bold_italic_Z - italic_β ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z - bold_italic_Z ) + italic_β italic_λ bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">so named because the map <math alttext="\bm{X}\mapsto\operatorname{ReLU}(\bm{X}-\beta\bm{D}^{\top}(\bm{D}\bm{X}-\bm{Z})+\beta\lambda\bm{1}_{d}\bm{1}_{n}^{\top})" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m2"><semantics><mrow><mi>𝑿</mi><mo stretchy="false">↦</mo><mrow><mi>ReLU</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑿</mi><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑫</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>−</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟏</mn><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\mapsto\operatorname{ReLU}(\bm{X}-\beta\bm{D}^{\top}(\bm{D}\bm{X}-\bm{Z})+\beta\lambda\bm{1}_{d}\bm{1}_{n}^{\top})</annotation><annotation encoding="application/x-llamapun">bold_italic_X ↦ roman_ReLU ( bold_italic_X - italic_β bold_italic_D start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_D bold_italic_X - bold_italic_Z ) + italic_β italic_λ bold_1 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math> is one step of the well-established ISTA algorithm to find an element-wise non-negative sparse representation for <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> with respect to the complete dictionary <math alttext="\bm{D}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m4"><semantics><mi>𝑫</mi><annotation encoding="application/x-tex">\bm{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_D</annotation></semantics></math> (cf <a class="ltx_ref" href="Ch2.html#S3" title="2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">We call this architecture CRATE, and a layer of the backbone is depicted in <a class="ltx_ref" href="Ch4.html#F13" title="In MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.13</span></a>. CRATE models, on top of being interpretable, are generally also highly performant as well as parameter-efficient.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3.3 </span>Optimization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">We train our classifier using a simple end-to-end stochastic optimization procedure, where we sub-sample data and views, compute the average loss and its gradient over these samples, and use an optimization algorithm to change the parameters. At each timestep <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>, we:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p">Subsample <math alttext="B" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m1"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation><annotation encoding="application/x-llamapun">italic_B</annotation></semantics></math> different labeled samples <math alttext="\{(\bm{X}_{b}^{(k)},\bm{y}_{b}^{(k)})\}_{b=1}^{B}\subseteq\mathcal{I}\times\{0,1\}^{N_{\mathrm{cls}}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝒚</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mo>⊆</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><msub><mi>N</mi><mi>cls</mi></msub></msup></mrow></mrow><annotation encoding="application/x-tex">\{(\bm{X}_{b}^{(k)},\bm{y}_{b}^{(k)})\}_{b=1}^{B}\subseteq\mathcal{I}\times\{0,1\}^{N_{\mathrm{cls}}}</annotation><annotation encoding="application/x-llamapun">{ ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ⊆ caligraphic_I × { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p">For each labeled sample <math alttext="(\bm{X}_{b}^{(k)},\bm{y}_{b}^{(k)})" class="ltx_Math" display="inline" id="S3.I2.i2.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>𝒚</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X}_{b}^{(k)},\bm{y}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>, compute the central crop view <math alttext="v_{b,\mathrm{cc}}^{(k)}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.m2"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">v_{b,\mathrm{cc}}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> and apply it to <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.m3"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> to get <math alttext="\bm{X}_{b,\mathrm{cc}}^{(k)}\doteq v_{b,\mathrm{cc}}^{(k)}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S3.I2.i2.p1.m4"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,\mathrm{cc}}^{(k)}\doteq v_{b,\mathrm{cc}}^{(k)}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p">Compute the predictions <math alttext="\bm{p}_{\theta}(\bm{X}_{b,\mathrm{cc}}^{(k)})\doteq(h_{\theta}\circ f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{b,\mathrm{cc}}^{(k)})" class="ltx_Math" display="inline" id="S3.I2.i3.p1.m1"><semantics><mrow><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta}(\bm{X}_{b,\mathrm{cc}}^{(k)})\doteq(h_{\theta}\circ f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{b,\mathrm{cc}}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ≐ ( italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p">Form the surrogate stochastic loss</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{L}}_{\operatorname{CE}}^{(k)}(\theta)\doteq\frac{1}{B}\sum_{b=1}^{B}\operatorname{CE}(\bm{y}_{b}^{(k)},\bm{p}_{\theta}(\bm{X}_{b,\mathrm{cc}}^{(k)}))." class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><mrow><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>CE</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒚</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>cc</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\operatorname{CE}}^{(k)}(\theta)\doteq\frac{1}{B}\sum_{b=1}^{B}\operatorname{CE}(\bm{y}_{b}^{(k)},\bm{p}_{\theta}(\bm{X}_{b,\mathrm{cc}}^{(k)})).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ ) ≐ divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_CE ( bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.7)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p">Compute one step of an optimization algorithm on <math alttext="\theta" class="ltx_Math" display="inline" id="S3.I2.i5.p1.m1"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, giving the following iteration:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta^{(k+1)}\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)};\nabla_{\theta}\hat{\mathcal{L}}_{\operatorname{CE}}^{(k)})." class="ltx_Math" display="block" id="S3.E8.m1"><semantics><mrow><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>≐</mo><mrow><msup><mtext class="ltx_font_smallcaps">OptUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>;</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>CE</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\theta^{(k+1)}\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)};\nabla_{\theta}\hat{\mathcal{L}}_{\operatorname{CE}}^{(k)}).</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ≐ OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ∇ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.3.8)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3.4 </span>Evaluation Methodology</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">We use the same evaluation procedure as <a class="ltx_ref" href="#S2.SS5" title="7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.5</span></a>. To summarize, for all evaluations (as well as training) we use a center crop view <math alttext="v_{\mathrm{cc}}" class="ltx_Math" display="inline" id="S3.SS4.p1.m1"><semantics><msub><mi>v</mi><mi>cc</mi></msub><annotation encoding="application/x-tex">v_{\mathrm{cc}}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT</annotation></semantics></math> which reshapes the input image and takes a large central crop of size <math alttext="(C,S_{\mathrm{cc}},S_{\mathrm{cc}})" class="ltx_Math" display="inline" id="S3.SS4.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>cc</mi></msub><mo>,</mo><msub><mi>S</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{\mathrm{cc}},S_{\mathrm{cc}})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT )</annotation></semantics></math> where <math alttext="C" class="ltx_Math" display="inline" id="S3.SS4.p1.m3"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math> is the number of channels in the input image. We can then do linear probing, attention map visualization, and detection/segmentation benchmarks, given the output of this view.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3.5 </span>Experimental Setup and Results</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p">Since CRATE is directly based on the transformer, we compare the optimal settings for ViT as given by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx75" title="">DBK+21</a>, <a class="ltx_ref" href="bib.html#bibx265" title="">TCD+20</a>]</cite> with the same settings applied to CRATE for fair comparison.</p>
</div>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Model architecture.</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p">The center crop resizes the whole image so that the shorter edge is of size <math alttext="256" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m1"><semantics><mn>256</mn><annotation encoding="application/x-tex">256</annotation><annotation encoding="application/x-llamapun">256</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{rsz}}=256" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m2"><semantics><mrow><msub><mi>S</mi><mi>rsz</mi></msub><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{rsz}}=256</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256</annotation></semantics></math>) before taking a center crop of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m3"><semantics><mrow><mn>224</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation><annotation encoding="application/x-llamapun">224 × 224</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{cc}}=224" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m4"><semantics><mrow><msub><mi>S</mi><mi>cc</mi></msub><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{cc}}=224</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = 224</annotation></semantics></math>), both in evaluation and training. We take patch size <math alttext="16" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m5"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation><annotation encoding="application/x-llamapun">16</annotation></semantics></math> (i.e., <math alttext="P_{H}=P_{W}=16" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m6"><semantics><mrow><msub><mi>P</mi><mi>H</mi></msub><mo>=</mo><msub><mi>P</mi><mi>W</mi></msub><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">P_{H}=P_{W}=16</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16</annotation></semantics></math>). We use the tiny, small, base, and large models of the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx75" title="">DBK+21</a>]</cite> architecture as the embedding and backbone, swapping out the MHSA and MLP components for MSSA and ISTA respectively, using the same number of heads and head dimension in the case of MSSA, and therefore reducing the number of training parameters drastically. For CRATE, we set <math alttext="(\beta,\lambda)=(1,0.1)" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.m7"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>β</mi><mo>,</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\beta,\lambda)=(1,0.1)</annotation><annotation encoding="application/x-llamapun">( italic_β , italic_λ ) = ( 1 , 0.1 )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Datasets and optimization.</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p">For pre-training, we use the ImageNet-1K dataset. We use the LION optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx51" title="">CLH+24</a>]</cite> to pre-train both our ViT replication as well as CRATE. We set the base learning rate as <math alttext="2.4\times 10^{-4}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m1"><semantics><mrow><mn>2.4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2.4\times 10^{-4}</annotation><annotation encoding="application/x-llamapun">2.4 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, the weight decay as <math alttext="0.5" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m2"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation><annotation encoding="application/x-llamapun">0.5</annotation></semantics></math>, and batch size as <math alttext="B=2048" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m3"><semantics><mrow><mi>B</mi><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">B=2048</annotation><annotation encoding="application/x-llamapun">italic_B = 2048</annotation></semantics></math>. Our learning rate schedule increases the learning rate linearly to the base learning rate over the first <math alttext="5" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m4"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation><annotation encoding="application/x-llamapun">5</annotation></semantics></math> epochs, and decreases to <math alttext="0" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m5"><mn>0</mn></math> using a cosine schedule over the next <math alttext="145" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m6"><semantics><mn>145</mn><annotation encoding="application/x-tex">145</annotation><annotation encoding="application/x-llamapun">145</annotation></semantics></math> epochs (training all models for <math alttext="150" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p1.m7"><semantics><mn>150</mn><annotation encoding="application/x-tex">150</annotation><annotation encoding="application/x-llamapun">150</annotation></semantics></math> epochs each). For pre-training, we apply a usual regime of data augmentations (flips, Gaussian blurs, solarization, etc) to the image data, and also add small noise to the labels (this is called <span class="ltx_text ltx_font_italic">label smoothing</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx192" title="">MKH19</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p2">
<p class="ltx_p">For linear probing, we use several evaluation datasets such as CIFAR10, Oxford-Flowers, and Oxford-IIT-Pets. We use the AdamW optimizer to train the linear probe, using learning rate <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.m1"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun">5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, weight decay <math alttext="0.01" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.m2"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation><annotation encoding="application/x-llamapun">0.01</annotation></semantics></math>, and batch size <math alttext="B=256" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.m3"><semantics><mrow><mi>B</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">B=256</annotation><annotation encoding="application/x-llamapun">italic_B = 256</annotation></semantics></math>. We also apply the aforementioned data augmentations to the image data.</p>
</div>
<figure class="ltx_table" id="T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-T</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-S</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">CRATE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;"> ViT-T</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">ViT-S</span>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"># parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">6.09M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">13.12M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">22.80M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">77.64M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;"> 5.72M</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;"> 22.05M</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">ImageNet-1K</td>
<td class="ltx_td ltx_align_center ltx_border_t">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">70.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.3</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 71.5</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 72.4</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">ImageNet-1K ReaL</td>
<td class="ltx_td ltx_align_center">74.0</td>
<td class="ltx_td ltx_align_center">76.0</td>
<td class="ltx_td ltx_align_center">76.5</td>
<td class="ltx_td ltx_align_center ltx_border_r">77.4</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 78.3</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 78.4</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">CIFAR10</td>
<td class="ltx_td ltx_align_center">95.5</td>
<td class="ltx_td ltx_align_center">96.0</td>
<td class="ltx_td ltx_align_center">96.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">97.2</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 96.6</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 97.2</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">CIFAR100</td>
<td class="ltx_td ltx_align_center">78.9</td>
<td class="ltx_td ltx_align_center">81.0</td>
<td class="ltx_td ltx_align_center">82.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">83.6</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 81.8</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 83.2</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Oxford Flowers-102</td>
<td class="ltx_td ltx_align_center">84.6</td>
<td class="ltx_td ltx_align_center">87.1</td>
<td class="ltx_td ltx_align_center">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">88.3</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 85.1</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 88.5</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">Oxford-IIIT-Pets</td>
<td class="ltx_td ltx_align_center ltx_border_bb">81.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb">84.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb">85.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;"> 88.5</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;"> 88.6</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Linear probing classification accuracy of CRATE and ViT<span class="ltx_text ltx_font_medium"> on various datasets with different model sizes when the backbone is pre-trained for classification on ImageNet-1K. We observe that given the same model configuration, CRATE has comparable classification performance with a simpler, more principled, and more parameter-efficient design.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="F11"><img alt="Figure 7.11 : Interpretable saliency maps in CRATE with patch size 8 8 8 . When given images with similar properties (perhaps but not necessarily from the same class), the saliency maps corresponding to different attention heads in the last layer each highlight a specific property. One can observe that the average saliency map (not included) then highlights all relevant objects in the image, showing that it uses all fine-grained details of the input image for classification. This is the first machine learning system to do this, to the authors’ knowledge, much less automatically without training on any segmentation data." class="ltx_graphics" id="F11.g1" src="chapters/chapter7/figs/crate_semantic_heads.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.11</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Interpretable saliency maps in CRATE<span class="ltx_text ltx_font_medium"> with patch size <math alttext="8" class="ltx_Math" display="inline" id="F11.m2"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math>. When given images with similar properties (perhaps but not necessarily from the same class), the saliency maps corresponding to different attention heads in the last layer each highlight a specific property. One can observe that the average saliency map (not included) then highlights all relevant objects in the image, showing that it uses all fine-grained details of the input image for classification. This is the <span class="ltx_text ltx_font_italic">first</span> machine learning system to do this, to the authors’ knowledge, much less automatically without training on any segmentation data.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Detection (<math alttext="\uparrow" class="ltx_Math" display="inline" id="T4.m1"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Segmentation (<math alttext="\uparrow" class="ltx_Math" display="inline" id="T4.m2"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CRATE-S/8</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">2.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">1.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.8</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CRATE-B/8</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.0</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ViT-S/8</th>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
<td class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">ViT-B/8</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Object detection and fine-grained segmentation via MaskCut on COCO val2017 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx163" title="">LMB+14</a>]</cite><span class="ltx_text ltx_font_medium">. Here all models are trained with patch size <math alttext="8" class="ltx_Math" display="inline" id="T4.m9"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> instead of <math alttext="16" class="ltx_Math" display="inline" id="T4.m10"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation><annotation encoding="application/x-llamapun">16</annotation></semantics></math>. CRATE conclusively performs better than the ViT at detection and segmentation metrics when both are trained using supervised classification.</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Experiment results.</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px3.p1">
<p class="ltx_p"><a class="ltx_ref" href="#T3" title="In Datasets and optimization. ‣ 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.3</span></a> demonstrates that CRATE models achieve parity or improvement compared to the popular Vision Transformer (ViT) architecture at similar parameter counts, at least in terms of the linear separability of their features w.r.t. different classes. In terms of attention map fidelity, <a class="ltx_ref" href="#F11" title="In Datasets and optimization. ‣ 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.11</span></a> demonstrates a truly extraordinary result: without needing to train on any segmentation or object detection data, <span class="ltx_text ltx_font_italic">not only</span> do the saliency maps effectively capture all relevant parts of the input image, the saliency maps <span class="ltx_text ltx_font_italic">self-organize</span> to each correspond to a discrete set of concepts, even across samples and classes! This is the first system to do this, to the authors’ knowledge, and it can do this without using any extra data except for the image classification data. <a class="ltx_ref" href="#T4" title="In Datasets and optimization. ‣ 7.3.5 Experimental Setup and Results ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.4</span></a> confirms these qualitative insights quantitatively, showing significant improvement over ViTs trained in the same supervised classification setup.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.4 </span>Causal Language Modeling</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">We now study <span class="ltx_text ltx_font_italic">causal language modeling</span>, a method for training large language models (LLMs). This is the same setup used to train, among many others, GPT-2 and many more language models.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.1 </span>Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx92" title="">GC19</a>]</cite>, an open-source reproduction of the unreleased WebText dataset used by OpenAI to train GPT2. Each sample in OWT is a web document, typically sourced from high-quality web pages, blogs, articles, or online discussions, that is written in well-formed natural language. The OpenWebText dataset contains around 8.01M documents of varying lengths, totaling around 41.70GB of text. For evaluation, we will use several datasets, such as WikiText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx183" title="">MXB+16</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For WikiText2 and WikiText103 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx183" title="">MXB+16</a>]</cite>, the test splits are the same, so we merge them as a single dataset referred to as WikiText.</span></span></span>, LAMBADA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx207" title="">PKL+16</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>To obtain the accuracy on LAMBADA dataset, we use greedy decoding.</span></span></span>, and PTB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx177" title="">MSM93</a>]</cite>. PTB and OWT are generally easier compared to other datasets. PTB focuses on simpler journalistic text, ideal for traditional language modeling, while OWT is diverse and informal, covering various topics but with less complexity in language structure or long-range dependencies. WikiText, with its formal structure and domain-specific content, requires a more complex understanding than OWT but remains manageable. LAMBADA is the most challenging, as it involves long-range dependencies, requiring the model to grasp broader contextual information to complete sentences accurately.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">On a more formal level, our data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS1.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> will be text, or strings of characters; we let <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S4.SS1.p2.m2"><semantics><mi class="ltx_font_mathcaligraphic">𝒯</mi><annotation encoding="application/x-tex">\mathcal{T}</annotation><annotation encoding="application/x-llamapun">caligraphic_T</annotation></semantics></math> be the set of all strings.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.2 </span>Task and Objective</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">For causal language modeling pre-training, the idea is that we want to <span class="ltx_text ltx_font_italic">train the model to output human-like text</span>. The most popular way to do this by far is to use a two-stage training process:<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Modern language model training has several additional training steps which demand different data distributions and algorithm approaches. However, training a model to merely mimic human writing only requires these few presented steps.</span></span></span></p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">First</span>, we wish to <span class="ltx_text ltx_font_italic">learn</span> a way to optimally encode documents as a sequence of basic (“building block”) strings, called <span class="ltx_text ltx_font_italic">tokens</span>. This is called <span class="ltx_text ltx_font_italic">tokenization</span>, and we build a <span class="ltx_text ltx_font_italic">tokenizer</span>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Second</span>, we wish to <span class="ltx_text ltx_font_italic">learn</span> a way to <span class="ltx_text ltx_font_italic">predict the distribution of a token given all previous tokens</span>. This is called <span class="ltx_text ltx_font_italic">next-token prediction</span>, and we build a <span class="ltx_text ltx_font_italic">language model</span>.</p>
</div>
</li>
</ul>
<p class="ltx_p">This procedure actually dates back to Markov who first noticed that natural language could be modeled by the eponymous Markov chain structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx178" title="">Mar06</a>]</cite> given an appropriate tokenization, and then Shannon who proposed doing this exact language modeling setup with a character-level tokenizer (i.e., each character is a token) and so-called “<math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.p1.m1"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>-gram” (i.e., an explicit look-up table, calculated from training data, for the distribution of a token given the <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.p1.m2"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> previous tokens) in place of the language model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx242" title="">Sha48</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>A recent study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx165" title="">LMZ+24</a>]</cite> scaling up <math alttext="n" class="ltx_Math" display="inline" id="footnote6.m1"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>-gram models has shown that they are able to model text reasonably well for large <math alttext="n" class="ltx_Math" display="inline" id="footnote6.m2"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>, but of course the memory required to store such a lookup table is of order <math alttext="V^{n}" class="ltx_Math" display="inline" id="footnote6.m3"><semantics><msup><mi>V</mi><mi>n</mi></msup><annotation encoding="application/x-tex">V^{n}</annotation><annotation encoding="application/x-llamapun">italic_V start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> and hence completely intractable.</span></span></span></p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Training a Tokenizer</h4>
<div class="ltx_para" id="S4.SS2.SSSx1.p1">
<p class="ltx_p">To build a tokenizer, it amounts to building a vocabulary <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒱</mi><annotation encoding="application/x-tex">\mathcal{V}</annotation><annotation encoding="application/x-llamapun">caligraphic_V</annotation></semantics></math>, which is a set of tokens and has some pre-specified size <math alttext="V" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p1.m2"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math>. There are several methods to do this. One popular algorithm is known as Byte Pair Encoding (BPE), which can be described as:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p">Start with a list of all unique characters in your training data, and their frequencies. Ensure that there are fewer than <math alttext="V" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m1"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math> such characters, and add each character as a separate string (“token”) to the vocabulary along with its frequency.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p">Until there are <math alttext="V" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m1"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math> tokens in the vocabulary:</p>
<ul class="ltx_itemize" id="S4.I2.i2.I1">
<li class="ltx_item" id="S4.I2.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S4.I2.i2.I1.i1.p1">
<p class="ltx_p">Construct a token by taking the two most frequent existing tokens and merging them.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S4.I2.i2.I1.i2.p1">
<p class="ltx_p">Compute this token’s frequency in the dataset.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold">–</span></span>
<div class="ltx_para" id="S4.I2.i2.I1.i3.p1">
<p class="ltx_p">Add it to the vocabulary (along with its frequency).</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p">At this point, the frequency information is no longer needed and can be discarded.</p>
</div>
</li>
</ul>
<p class="ltx_p">The overall process of BPE is in <a class="ltx_ref" href="#F12" title="In Training a Tokenizer ‣ 7.4.2 Task and Objective ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.12</span></a>. Note that this procedure is a modification of a classical information-theoretic compression procedure for <span class="ltx_text ltx_font_italic">learning a lossless encoding</span> of bytestream data (such as text), and as such, one can interpret it as finding an optimal lossless compression of the data. Notice that this is possible because (unlike images), the data here are fundamentally discrete and noise-free hence.</p>
</div>
<figure class="ltx_figure" id="F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 7.12 : The process of tokenizing text data using BPE. (Image credit to https://huggingface.co/learn/nlp-course/chapter6/5 ). (Left) We begin by analyzing the given text corpus and constructing an initial vocabulary that consists of individual characters (or bytes in the case of byte-level BPE). Then, we compute the frequencies of adjacent character pairs in the corpus. This involves scanning the entire text and counting how often each two-character sequence (bigram) appears. (Right) After computing the frequencies of adjacent character pairs, we identify the most frequent pair in the corpus. This pair is then merged into a new subword unit, which is added to the vocabulary as a single token. This process is repeated iteratively until the predefined vocabulary size is reached." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="187" id="F12.g1" src="chapters/chapter7/figs/BPE1.png" width="269"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 7.12 : The process of tokenizing text data using BPE. (Image credit to https://huggingface.co/learn/nlp-course/chapter6/5 ). (Left) We begin by analyzing the given text corpus and constructing an initial vocabulary that consists of individual characters (or bytes in the case of byte-level BPE). Then, we compute the frequencies of adjacent character pairs in the corpus. This involves scanning the entire text and counting how often each two-character sequence (bigram) appears. (Right) After computing the frequencies of adjacent character pairs, we identify the most frequent pair in the corpus. This pair is then merged into a new subword unit, which is added to the vocabulary as a single token. This process is repeated iteratively until the predefined vocabulary size is reached." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="181" id="F12.g2" src="chapters/chapter7/figs/BPE2.png" width="269"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.12</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The process of tokenizing text data using BPE.<span class="ltx_text ltx_font_medium"> (Image credit to <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/learn/nlp-course/chapter6/5" title="">https://huggingface.co/learn/nlp-course/chapter6/5</a>). (Left) We begin by analyzing the given text corpus and constructing an initial vocabulary that consists of individual characters (or bytes in the case of byte-level BPE). Then, we compute the frequencies of adjacent character pairs in the corpus. This involves scanning the entire text and counting how often each two-character sequence (bigram) appears. (Right) After computing the frequencies of adjacent character pairs, we identify the most frequent pair in the corpus. This pair is then merged into a new subword unit, which is added to the vocabulary as a single token. This process is repeated iteratively until the predefined vocabulary size is reached. </span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSSx1.p2">
<p class="ltx_p">After such a vocabulary is built, a tokenizer can break down a document into tokens (i.e., “tokenize” it). BPE uses a similar procedure to tokenize data as in training:</p>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p">Separate the document into a long list of one-character-long tokens. That is, if the document is “Hello” then the initial list is ‘H’, ‘e’, ‘l’, ‘l’, ‘o’.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p">While any two adjacent tokens can be concatenated and their concatenation is another token, we do it, i.e., we replace this pair of tokens with the merged token. Namely, if ‘He’ is a token in the vocabulary, ‘H’, ‘e’, ‘l’, ‘l’, ‘o’ would become ‘He’, ‘l’, ‘l’, ‘o’.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p">Repeat the above process until no more merges can be done. At this point, the document is partitioned into the final list (sequence) of tokens.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.SSSx1.p3">
<p class="ltx_p">There are many practical and efficiency-based considerations to take into account during tokenization. The above algorithm, as presented, is <span class="ltx_text ltx_font_italic">very far from optimal</span> if naively implemented, for instance. We do not cover this topic in great detail; there are many resources online to learn more, such as <a class="ltx_ref ltx_href" href="https://huggingface.co/learn/nlp-course/en/chapter6/5" title="">HuggingFace tutorials</a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSSx1.p4">
<p class="ltx_p">For instance, each token has a corresponding <span class="ltx_text ltx_font_italic">index</span> which is just its index in the vocabulary (which after all is just a list of length <math alttext="V" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p4.m1"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math>). Thus, the output of most tokenizers is a list of indices, say an element of <math alttext="[V]^{*}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p4.m2"><semantics><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mo>∗</mo></msup><annotation encoding="application/x-tex">[V]^{*}</annotation><annotation encoding="application/x-llamapun">[ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. Keep in mind that they correspond to substrings of the original document, as shown above.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSSx1.p5">
<p class="ltx_p">Once a tokenizer is learned, it can be used as a black box by any language model. For instance, many models have the same (OpenAI-based) tokenizer based off of the <span class="ltx_text ltx_font_typewriter">tiktoken</span> library. In the remainder of this section, we will use such a fixed and pre-built tokenizer for everything, and thus identify each text document <math alttext="\bm{X}\in\mathcal{T}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p5.m1"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒯</mi></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathcal{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ caligraphic_T</annotation></semantics></math> with its tokenized version in <math alttext="[V]^{*}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p5.m2"><semantics><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mo>∗</mo></msup><annotation encoding="application/x-tex">[V]^{*}</annotation><annotation encoding="application/x-llamapun">[ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. Therefore, we may as well consider the text space <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p5.m3"><semantics><mi class="ltx_font_mathcaligraphic">𝒯</mi><annotation encoding="application/x-tex">\mathcal{T}</annotation><annotation encoding="application/x-llamapun">caligraphic_T</annotation></semantics></math> as <span class="ltx_text ltx_font_italic">equal</span> to the space of token sequences <math alttext="[V]^{*}" class="ltx_Math" display="inline" id="S4.SS2.SSSx1.p5.m4"><semantics><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mo>∗</mo></msup><annotation encoding="application/x-tex">[V]^{*}</annotation><annotation encoding="application/x-llamapun">[ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> (and lose nothing essential).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Training a Language Model</h4>
<div class="ltx_para" id="S4.SS2.SSSx2.p1">
<p class="ltx_p">Once we have each document as a sequence of tokens <math alttext="\bm{X}\in[V]^{N}\subseteq[V]^{*}=\mathcal{T}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mi>N</mi></msup><mo>⊆</mo><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mo>∗</mo></msup><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒯</mi></mrow><annotation encoding="application/x-tex">\bm{X}\in[V]^{N}\subseteq[V]^{*}=\mathcal{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ [ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = caligraphic_T</annotation></semantics></math>, we wish to perform next-token prediction. That is, given a <span class="ltx_text ltx_font_italic">context</span> <math alttext="\bm{X}_{:n}\in[V]^{n}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m2"><semantics><mrow><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\bm{X}_{:n}\in[V]^{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ∈ [ italic_V ] start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> (i.e., the first <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m3"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> tokens <math alttext="\bm{x}_{1},\dots,\bm{x}_{n}\in[V]" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m4"><semantics><mrow><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{1},\dots,\bm{x}_{n}\in[V]</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ [ italic_V ]</annotation></semantics></math> in the document)<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note the incongruity with Python notation: here the notation <span class="ltx_text ltx_font_italic">includes</span> index <math alttext="n" class="ltx_Math" display="inline" id="footnote7.m1"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>.</span></span></span>, we wish to predict the token <math alttext="\bm{x}_{n+1}\in[V]" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m5"><semantics><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}\in[V]</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ∈ [ italic_V ]</annotation></semantics></math> at position <math alttext="n+1" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m6"><semantics><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n+1</annotation><annotation encoding="application/x-llamapun">italic_n + 1</annotation></semantics></math>. To do this, we compute the aggregate feature of <math alttext="\bm{X}_{:n}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m7"><semantics><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\bm{X}_{:n}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT</annotation></semantics></math> via <math alttext="\bm{z}_{\theta}(\bm{X}_{:n})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{:n})\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m8"><semantics><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X}_{:n})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{:n})\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and use a classification head <math alttext="h_{\theta}\colon\mathbb{R}^{d}\to\Delta_{V}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m9"><semantics><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><msub><mi mathvariant="normal">Δ</mi><mi>V</mi></msub></mrow></mrow><annotation encoding="application/x-tex">h_{\theta}\colon\mathbb{R}^{d}\to\Delta_{V}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → roman_Δ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> (implemented as either a linear layer, MLP, or something slightly more complicated) to project this feature into the <math alttext="V" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m10"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math>-dimensional probability simplex <math alttext="\Delta_{V}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m11"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>V</mi></msub><annotation encoding="application/x-tex">\Delta_{V}</annotation><annotation encoding="application/x-llamapun">roman_Δ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>. This projection <math alttext="\bm{p}_{\theta}(\bm{X}_{:n})\doteq h_{\theta}(\bm{z}_{\theta}(\bm{X}_{:n}))" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m12"><semantics><mrow><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta}(\bm{X}_{:n})\doteq h_{\theta}(\bm{z}_{\theta}(\bm{X}_{:n}))</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ≐ italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) )</annotation></semantics></math> serves as an estimated probability distribution of the next token. Then, using the notation <math alttext="\bm{1}(\bm{x}_{n+1})\in\Delta_{V}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m13"><semantics><mrow><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msub><mi mathvariant="normal">Δ</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">\bm{1}(\bm{x}_{n+1})\in\Delta_{V}</annotation><annotation encoding="application/x-llamapun">bold_1 ( bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ) ∈ roman_Δ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> to be <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m14"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> in the <math alttext="\bm{x}_{n+1}" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m15"><semantics><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\bm{x}_{n+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math>-th component and <math alttext="0" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p1.m16"><mn>0</mn></math> elsewhere, the causal language modeling loss is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\left\{\mathcal{L}_{\mathrm{CLM}}(\theta)\doteq\operatorname{\mathbb{E}}_{\bm{X}}\left[\frac{1}{N-1}\sum_{n=1}^{N-1}\operatorname{CE}(\bm{1}(\bm{x}_{n+1}),\bm{p}_{\theta}(\bm{X}_{:n}))\right]\right\}" class="ltx_Math" display="block" id="S4.E1.m1"><semantics><mrow><munder><mi>min</mi><mi>θ</mi></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>CLM</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>𝔼</mi><mi>𝑿</mi></msub><mo>⁡</mo><mrow><mo>[</mo><mrow><mfrac><mn>1</mn><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mi>CE</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\min_{\theta}\left\{\mathcal{L}_{\mathrm{CLM}}(\theta)\doteq\operatorname{\mathbb{E}}_{\bm{X}}\left[\frac{1}{N-1}\sum_{n=1}^{N-1}\operatorname{CE}(\bm{1}(\bm{x}_{n+1}),\bm{p}_{\theta}(\bm{X}_{:n}))\right]\right\}</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT ( italic_θ ) ≐ blackboard_E start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG italic_N - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT roman_CE ( bold_1 ( bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ) ] }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note how similar this is to a classification loss (say for images); one uses the cross-entropy and tries to align a predicted probability vector with the ground truth. The major difference between these two losses is that in this one, we compute the loss on a whole sequence, where each prediction is correlated with each other (unlike in the i.i.d. classification case).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSSx2.p2">
<p class="ltx_p">Optimizing this loss is usually called “pre-training” in the language model community (contrast with “post-training” and, more recently, “mid-training”, which are methodologies to modify a next-token-predictor for useful tasks).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSSx2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Side note:</span> Why does the first term of (<a class="ltx_ref" href="#S4.E1" title="Equation 7.4.1 ‣ Training a Language Model ‣ 7.4.2 Task and Objective ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.4.1</span></a>) predict <math alttext="\bm{1}(\bm{x}_{2})" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p3.m1"><semantics><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{1}(\bm{x}_{2})</annotation><annotation encoding="application/x-llamapun">bold_1 ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math>, and there is no term which measures the loss to predict the first token? It’s because if we wanted to predict the first token, we would have the <span class="ltx_text ltx_font_italic">empty sequence</span> as context, and therefore make this first token prediction using a qualitatively different mechanism than that which applies to the other tokens. So actually this model is not trained to predict the <span class="ltx_text ltx_font_italic">very first</span> token of any document. The reason this is OK is due to an implementation detail of the tokenizer: often, after building the tokenizer, we insert a <span class="ltx_text ltx_font_italic">special</span> token into its vocabulary, called the <span class="ltx_text ltx_font_italic">beginning-of-string (or document)</span> token and labeled <span class="ltx_text ltx_font_typewriter">&lt;|bos|&gt;</span>.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>There are usually several special tokens for different purposes. Existing text containing the special tokens are specially processed.</span></span></span> Then, while processing each document, we add the <span class="ltx_text ltx_font_typewriter">&lt;|bos|&gt;</span> token at the beginning of the document’s token sequence, increasing the length of the tokenized sequence by <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.SSSx2.p3.m2"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>. Thus the above causal language modeling objective has a term which involves trying to predict the first token of the document given only the <span class="ltx_text ltx_font_typewriter">&lt;|bos|&gt;</span> token as context, and so it is a conceptually correct loss.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.3 </span>Architecture: Causal CRATE</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">For the architecture, we use a standard GPT-2 style transformer, substituting CRATE layers in for the transformer layers.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>In direct contravention of the conventions in this book and those of many other communities, the NLP community calls such GPT-2 style transformers (encompassing nearly all current LLMs) as “decoder-only” transformers. “Encoder-only” transformers have a different architecture, and “encoder-decoder” transformers concatenate an “encoder-only” transformer with a “decoder-only” transformer. This despite the fact that “decoder-only” transformers <span class="ltx_text ltx_font_italic">also</span> compute an encoding of the data!</span></span></span> For completeness, we specify the architecture here.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Embedding.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p">We first embed the token sequence <math alttext="\bm{X}\in[V]^{N}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\bm{X}\in[V]^{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> to Euclidean space. This is often done by associating each index in <math alttext="[V]" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m2"><semantics><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[V]</annotation><annotation encoding="application/x-llamapun">[ italic_V ]</annotation></semantics></math> with a vector in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m3"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> using a <span class="ltx_text ltx_font_italic">massive<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">10</span></span><span class="ltx_text ltx_font_upright">By “massive” we mean that such a structure is often a large fraction of the language model’s total size.</span></span></span></span></span> array <math alttext="\bm{E}\in\mathbb{R}^{V\times d}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m4"><semantics><mrow><mi>𝑬</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>V</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{E}\in\mathbb{R}^{V\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_E ∈ blackboard_R start_POSTSUPERSCRIPT italic_V × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and directly forming the sequence <math alttext="[\bm{E}_{\bm{x}_{1}},\dots,\bm{E}_{\bm{x}_{N}}]\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m5"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>𝑬</mi><msub><mi>𝒙</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑬</mi><msub><mi>𝒙</mi><mi>N</mi></msub></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">[\bm{E}_{\bm{x}_{1}},\dots,\bm{E}_{\bm{x}_{N}}]\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">[ bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , … , bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. The full embedding map <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m6"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> also applies a positional encoding <math alttext="\bm{E}^{\mathrm{pos}}\in\mathbb{R}^{d\times N_{\max}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m7"><semantics><mrow><msup><mi>𝑬</mi><mi>pos</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>max</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}\in\mathbb{R}^{d\times N_{\max}}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="N_{\max}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m8"><semantics><msub><mi>N</mi><mi>max</mi></msub><annotation encoding="application/x-tex">N_{\max}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT</annotation></semantics></math> is the maximum number of tokens which are possible to process,<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Modern positional encoding methods have since taken care of this issue and allowed for (in theory) infinite extrapolation, but such methods are more complex to develop, and for simplicity we only introduce the absolute additive positional encoding here.</span></span></span> which yields the embedding map</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq[\bm{E}_{\bm{x}_{1}},\dots,\bm{E}_{\bm{x}_{N}}]+\bm{E}_{:N}^{\mathrm{pos}}" class="ltx_Math" display="block" id="S4.E2.m1"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">[</mo><msub><mi>𝑬</mi><msub><mi>𝒙</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑬</mi><msub><mi>𝒙</mi><mi>N</mi></msub></msub><mo stretchy="false">]</mo></mrow><mo>+</mo><msubsup><mi>𝑬</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>N</mi></mrow><mi>pos</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq[\bm{E}_{\bm{x}_{1}},\dots,\bm{E}_{\bm{x}_{N}}]+\bm{E}_{:N}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ [ bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , … , bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] + bold_italic_E start_POSTSUBSCRIPT : italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The parameters <math alttext="\bm{E}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m9"><semantics><mi>𝑬</mi><annotation encoding="application/x-tex">\bm{E}</annotation><annotation encoding="application/x-llamapun">bold_italic_E</annotation></semantics></math> and <math alttext="\bm{E}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m10"><semantics><msup><mi>𝑬</mi><mi>pos</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> are directly trainable. Since <math alttext="\bm{E}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m11"><semantics><mi>𝑬</mi><annotation encoding="application/x-tex">\bm{E}</annotation><annotation encoding="application/x-llamapun">bold_italic_E</annotation></semantics></math> is so large (and the gradient update is very sparse w.r.t. it since only a small fraction of the vocabulary is used in each sample), specialized software is used to make sure the memory updates are not too onerous. Notice also that we do not use a class token like in the other sections; more on this later.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Backbone.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p">We process the embeddings using a CRATE-like backbone which uses causal masking. To motivate causal masking, consider the causal language modeling loss <math alttext="\mathcal{L}_{\mathrm{CLM}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>CLM</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{CLM}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT</annotation></semantics></math> defined in (<a class="ltx_ref" href="#S4.E1" title="Equation 7.4.1 ‣ Training a Language Model ‣ 7.4.2 Task and Objective ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.4.1</span></a>). The most naive implementation would require us to compute hte forward pass <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m2"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> times in order to backpropagate once. Obviously this is extremely inefficient, since <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m3"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> can often be in the thousands. In order to scale training with this loss efficiently, we impose a <span class="ltx_text ltx_font_italic">causal</span> constraint, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}(\bm{X}_{:n})=\bm{Z}_{\theta}(\bm{X})_{:n}" class="ltx_Math" display="block" id="S4.E3.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{:n})=\bm{Z}_{\theta}(\bm{X})_{:n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) = bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">i.e., the <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m4"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> columns of the token features <math alttext="\bm{Z}_{\theta}(\bm{X}_{:n})\in\mathbb{R}^{d\times n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m5"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{:n})\in\mathbb{R}^{d\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> should be the same as the first <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m6"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> columns of the token features <math alttext="\bm{Z}_{\theta}(\bm{X})\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m7"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> regardless of the positive values of <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m8"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> and <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m9"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> such that <math alttext="N\geq n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m10"><semantics><mrow><mi>N</mi><mo>≥</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">N\geq n</annotation><annotation encoding="application/x-llamapun">italic_N ≥ italic_n</annotation></semantics></math>. In effect, this means we can apply the backbone <span class="ltx_text ltx_font_italic">once</span> to the whole sequence and compute <math alttext="\bm{Z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m11"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>, then apply <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m12"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math> to each increasing subset <math alttext="\bm{Z}_{\theta}(\bm{X}_{:n})=\bm{Z}_{\theta}(\bm{X})_{:n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m13"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{:n})=\bm{Z}_{\theta}(\bm{X})_{:n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) = bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT</annotation></semantics></math> as <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m14"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> grows to the sequence length <math alttext="N" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.m15"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math>. Then we can use all of those to compute the loss.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p2">
<p class="ltx_p">So now that we want a causal architecture for the backbone, how can we get it? Since the MLP and layer normalizations inside each transformer layer affect each token individually, the only thing that matters for causality is the attention block (or <math alttext="\operatorname{MSSA}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m1"><semantics><mi>MSSA</mi><annotation encoding="application/x-tex">\operatorname{MSSA}</annotation><annotation encoding="application/x-llamapun">roman_MSSA</annotation></semantics></math> in terms of CRATE). In order to make <math alttext="\operatorname{MSSA}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m2"><semantics><mi>MSSA</mi><annotation encoding="application/x-tex">\operatorname{MSSA}</annotation><annotation encoding="application/x-llamapun">roman_MSSA</annotation></semantics></math> causal, we define the <math alttext="\mathrm{CausalMSSA}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m3"><semantics><mi>CausalMSSA</mi><annotation encoding="application/x-tex">\mathrm{CausalMSSA}</annotation><annotation encoding="application/x-llamapun">roman_CausalMSSA</annotation></semantics></math> block as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx97">
<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{CausalMSSA}_{\theta}^{\ell}(\bm{Z})\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{CausalSA}([\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{CausalSA}([\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{N}^{\top}" class="ltx_Math" display="inline" id="S4.E4.m1"><semantics><mrow><mrow><msubsup><mi>CausalMSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msubsup><mi>𝑼</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>CausalSA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>CausalSA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mi>K</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mi>𝑼</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>𝒃</mi><mi>out</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>N</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{CausalMSSA}_{\theta}^{\ell}(\bm{Z})\doteq\bm{U}_{\mathrm{out}}^{\ell}\begin{bmatrix}\operatorname{CausalSA}([\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\\
\vdots\\
\operatorname{CausalSA}([\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{K,\ell}]^{\top}\bm{Z},[\bm{U}^{1,\ell}]^{\top}\bm{Z})\end{bmatrix}+\bm{b}_{\mathrm{out}}^{\ell}\bm{1}_{N}^{\top}</annotation><annotation encoding="application/x-llamapun">roman_CausalMSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL roman_CausalSA ( [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_CausalSA ( [ bold_italic_U start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT italic_K , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z , [ bold_italic_U start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) end_CELL end_ROW end_ARG ] + bold_italic_b start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_1 start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.4)</span></td>
</tr></tbody>
<tbody id="S4.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath">where</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})\doteq\bm{V}\operatorname{\mathrm{softmax}}\left(\frac{\operatorname{CausalMask}(\bm{K}^{\top}\bm{Q})}{\sqrt{p}}\right)" class="ltx_Math" display="inline" id="S4.E5.m2"><semantics><mrow><mrow><mi>CausalSA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo>,</mo><mi>𝑽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝑽</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mi>CausalMask</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑲</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><msqrt><mi>p</mi></msqrt></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})\doteq\bm{V}\operatorname{\mathrm{softmax}}\left(\frac{\operatorname{CausalMask}(\bm{K}^{\top}\bm{Q})}{\sqrt{p}}\right)</annotation><annotation encoding="application/x-llamapun">roman_CausalSA ( bold_italic_Q , bold_italic_K , bold_italic_V ) ≐ bold_italic_V roman_softmax ( divide start_ARG roman_CausalMask ( bold_italic_K start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q ) end_ARG start_ARG square-root start_ARG italic_p end_ARG end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.5)</span></td>
</tr></tbody>
<tbody id="S4.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath">where</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{CausalMask}(\bm{M})_{ij}=\begin{cases}M_{ij},&amp;\text{if}\ i\geq j,\\
-\infty,&amp;\text{if}\ i&lt;j\end{cases}." class="ltx_math_unparsed" display="inline" id="S4.E6.m2"><semantics><mrow><mi>CausalMask</mi><msub><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext>if</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>i</mi></mrow><mo>≥</mo><mi>j</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>i</mi></mrow><mo>&lt;</mo><mi>j</mi></mrow></mtd></mtr></mtable></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{CausalMask}(\bm{M})_{ij}=\begin{cases}M_{ij},&amp;\text{if}\ i\geq j,\\
-\infty,&amp;\text{if}\ i&lt;j\end{cases}.</annotation><annotation encoding="application/x-llamapun">roman_CausalMask ( bold_italic_M ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , end_CELL start_CELL if italic_i ≥ italic_j , end_CELL end_ROW start_ROW start_CELL - ∞ , end_CELL start_CELL if italic_i &lt; italic_j end_CELL end_ROW .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, practitioners say that the causal mask <span class="ltx_text ltx_font_italic">allows future tokens <math alttext="i" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m4"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math> to attend to past tokens <math alttext="j" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m5"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math> but not vice versa</span>. To see why, let us write out the expression for the <math alttext="t^{\textnormal{th}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m6"><semantics><msup><mi>t</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">t^{\textnormal{th}}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> column of <math alttext="\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m7"><semantics><mrow><mi>CausalSA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo>,</mo><mi>𝑽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})</annotation><annotation encoding="application/x-llamapun">roman_CausalSA ( bold_italic_Q , bold_italic_K , bold_italic_V )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})_{t}=\sum_{i=1}^{t}\bm{V}_{i}\operatorname{\mathrm{softmax}}\left([\bm{K}_{:t}]^{\top}\bm{Q}_{t}\right)_{i}" class="ltx_math_unparsed" display="block" id="S4.E7.m1"><semantics><mrow><mi>CausalSA</mi><msub><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo>,</mo><mi>𝑲</mi><mo>,</mo><mi>𝑽</mi><mo stretchy="false">)</mo></mrow><mi>t</mi></msub><mo rspace="0.111em">=</mo><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msub><mi>𝑽</mi><mi>i</mi></msub><mi>softmax</mi><msub><mrow><mo>(</mo><msup><mrow><mo stretchy="false">[</mo><msub><mi>𝑲</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><msub><mi>𝑸</mi><mi>t</mi></msub><mo>)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\operatorname{CausalSA}(\bm{Q},\bm{K},\bm{V})_{t}=\sum_{i=1}^{t}\bm{V}_{i}\operatorname{\mathrm{softmax}}\left([\bm{K}_{:t}]^{\top}\bm{Q}_{t}\right)_{i}</annotation><annotation encoding="application/x-llamapun">roman_CausalSA ( bold_italic_Q , bold_italic_K , bold_italic_V ) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_softmax ( [ bold_italic_K start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">(where here the non-colon subscript denotes the column). This expression for the <math alttext="t" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m8"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>-th token uses no information about any token beyond index <math alttext="t" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m9"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>. Therefore <math alttext="\operatorname{CausalSA}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m10"><semantics><mi>CausalSA</mi><annotation encoding="application/x-tex">\operatorname{CausalSA}</annotation><annotation encoding="application/x-llamapun">roman_CausalSA</annotation></semantics></math>, hence <math alttext="\operatorname{CausalMSSA}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p2.m11"><semantics><mi>CausalMSSA</mi><annotation encoding="application/x-tex">\operatorname{CausalMSSA}</annotation><annotation encoding="application/x-llamapun">roman_CausalMSSA</annotation></semantics></math>, hence the whole causal CRATE backbone is causal in terms of the definition in (<a class="ltx_ref" href="#S4.E3" title="Equation 7.4.3 ‣ Backbone. ‣ 7.4.3 Architecture: Causal CRATE ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.4.3</span></a>), and we unlock the considerable efficiency gains that we were promised.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Feature extractor.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p">We use a post-processing step <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m1"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math> which extracts the <span class="ltx_text ltx_font_italic">feature vector of the last known token</span> so as to predict the next token. In theory this means that each token <math alttext="\bm{Z}_{\theta}(\bm{X})_{n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m2"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> should contain rich information about all tokens that come before or on index <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m3"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>, i.e., <math alttext="\bm{x}_{1},\dots,\bm{x}_{n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m4"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\bm{x}_{1},\dots,\bm{x}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, as all of this information should be available for predicting the next token at index <math alttext="n+1" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m5"><semantics><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n+1</annotation><annotation encoding="application/x-llamapun">italic_n + 1</annotation></semantics></math>. In practice, only a few of these tokens are really needed for each prediction task. Anyways, the equation for <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.m6"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math> is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}_{:n}))\doteq(\bm{Z}_{\theta}(\bm{X}))_{n}" class="ltx_Math" display="block" id="S4.E8.m1"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}(\bm{Z}_{\theta}(\bm{X}_{:n}))\doteq(\bm{Z}_{\theta}(\bm{X}))_{n}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ) ≐ ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ) start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where (again) the non-colon subscript is the column. In this case, as promised, we just directly extract the feature vector of the last token in the sequence.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Task-specific head.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px4.p1">
<p class="ltx_p">For our classification head <math alttext="h_{\theta}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px4.p1.m1"><semantics><msub><mi>h</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">h_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, the GPT-2 architecture uses a simple linear layer and a softmax to get the desired probability vectors:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h_{\theta}(\bm{z})\doteq\operatorname{\mathrm{softmax}}(\bm{W}^{\mathrm{out}}\bm{z}+\bm{b}^{\mathrm{out}})," class="ltx_Math" display="block" id="S4.E9.m1"><semantics><mrow><mrow><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑾</mi><mi>out</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><msup><mi>𝒃</mi><mi>out</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">h_{\theta}(\bm{z})\doteq\operatorname{\mathrm{softmax}}(\bm{W}^{\mathrm{out}}\bm{z}+\bm{b}^{\mathrm{out}}),</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) ≐ roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{W}^{\mathrm{out}}\in\mathbb{R}^{V\times d},\bm{b}^{\mathrm{out}}\in\mathbb{R}^{V}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px4.p1.m2"><semantics><mrow><mrow><msup><mi>𝑾</mi><mi>out</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>V</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><msup><mi>𝒃</mi><mi>out</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>V</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{W}^{\mathrm{out}}\in\mathbb{R}^{V\times d},\bm{b}^{\mathrm{out}}\in\mathbb{R}^{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_V × italic_d end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math>. Some other more modern architectures use small MLPs and layer normalizations, but the idea is very much the same. Note that this linear layers also have large memory usage (because <math alttext="V" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px4.p1.m3"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math> is very large), and form a bottleneck in training; there has been significant effort attempting to circumvent it.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px4.p2">
<p class="ltx_p">All these architectural choices mean that causal training is extremely efficient relative to non-causal training:</p>
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p">We only need <span class="ltx_text ltx_font_italic">one forward pass</span> through the backbone to compute the loss for the whole sequence.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p">The feature extraction is basically <span class="ltx_text ltx_font_italic">free</span>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p">All tokens can be pushed through the task-specific head <span class="ltx_text ltx_font_italic">in parallel</span>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.4 </span>Optimization Strategy</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p">We train our language model using end-to-end stochastic optimization. One remaining issue is that, in practice, different documents in a batch have different lengths (in terms of the number of tokens required for each sequence), but as of the time of writing this book, the main deep learning frameworks by-and-large allow only “rectangular” tensors, which do not accommodate this behavior. To try to resolve this issue, we just insert a special padding token <span class="ltx_text ltx_font_typewriter">&lt;|pad|&gt;</span> for all shorter samples in the batch, so that we can batch process everything using rectangular tensors. At each timestep <math alttext="k" class="ltx_Math" display="inline" id="S4.SS4.p1.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>, we:</p>
<ul class="ltx_itemize" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p">Subsample <math alttext="B" class="ltx_Math" display="inline" id="S4.I5.i1.p1.m1"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation><annotation encoding="application/x-llamapun">italic_B</annotation></semantics></math> different tokenized documents <math alttext="\{\bm{X}_{b}^{(k)}\}_{b=1}^{B}\subseteq\mathcal{T}=[V]^{*}" class="ltx_Math" display="inline" id="S4.I5.i1.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mo>⊆</mo><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>=</mo><msup><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\{\bm{X}_{b}^{(k)}\}_{b=1}^{B}\subseteq\mathcal{T}=[V]^{*}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ⊆ caligraphic_T = [ italic_V ] start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, each with length <math alttext="N_{b}^{(k)}" class="ltx_Math" display="inline" id="S4.I5.i1.p1.m3"><semantics><msubsup><mi>N</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">N_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i2.p1">
<p class="ltx_p">Compute <math alttext="N_{\max}^{(k)}\doteq\max_{b\in[B]}N_{b}^{(k)}" class="ltx_Math" display="inline" id="S4.I5.i2.p1.m1"><semantics><mrow><msubsup><mi>N</mi><mi>max</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><mrow><msub><mi>max</mi><mrow><mi>b</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>B</mi><mo stretchy="false">]</mo></mrow></mrow></msub><mo lspace="0.167em">⁡</mo><msubsup><mi>N</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">N_{\max}^{(k)}\doteq\max_{b\in[B]}N_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ≐ roman_max start_POSTSUBSCRIPT italic_b ∈ [ italic_B ] end_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> and pad each <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S4.I5.i2.p1.m2"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> to length <math alttext="N_{\max}^{(k)}" class="ltx_Math" display="inline" id="S4.I5.i2.p1.m3"><semantics><msubsup><mi>N</mi><mi>max</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">N_{\max}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> using a special padding token.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i3.p1">
<p class="ltx_p">Compute the features <math alttext="\bm{Z}_{\theta}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S4.I5.i3.p1.m1"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i4.p1">
<p class="ltx_p">Compute the predicted distributions <math alttext="\bm{p}_{\theta}(\bm{X}_{b,:n}^{(k)})\doteq(h_{\theta}\circ f_{\theta}^{\mathrm{ext}})(\bm{Z}_{\theta}(\bm{X}_{b}^{(k)})_{:n})" class="ltx_Math" display="inline" id="S4.I5.i4.p1.m1"><semantics><mrow><mrow><msub><mi>𝒑</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>h</mi><mi>θ</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{\theta}(\bm{X}_{b,:n}^{(k)})\doteq(h_{\theta}\circ f_{\theta}^{\mathrm{ext}})(\bm{Z}_{\theta}(\bm{X}_{b}^{(k)})_{:n})</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , : italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ≐ ( italic_h start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ) ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i5.p1">
<p class="ltx_p">Form the surrogate stochastic loss</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{L}}_{\mathrm{CLM}}^{(k)}(\theta)\doteq\frac{1}{B(N_{\max}^{(k)}-1)}\sum_{b=1}^{B}\sum_{n=1}^{N_{\max}^{(k)}-1}\operatorname{CE}(\bm{1}(\bm{x}_{b,n+1}^{(k)}),\bm{p}_{\theta}(\bm{X}_{b,:n}^{(k)})))." class="ltx_math_unparsed" display="block" id="S4.E10.m1"><semantics><mrow><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>CLM</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>≐</mo><mfrac><mn>1</mn><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>N</mi><mi>max</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo movablelimits="false">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><msubsup><mi>N</mi><mi>max</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><mn>1</mn></mrow></munderover><mi>CE</mi><mrow><mo stretchy="false">(</mo><mn>𝟏</mn><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒙</mi><mrow><mi>b</mi><mo>,</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>𝒑</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>n</mi></mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\mathrm{CLM}}^{(k)}(\theta)\doteq\frac{1}{B(N_{\max}^{(k)}-1)}\sum_{b=1}^{B}\sum_{n=1}^{N_{\max}^{(k)}-1}\operatorname{CE}(\bm{1}(\bm{x}_{b,n+1}^{(k)}),\bm{p}_{\theta}(\bm{X}_{b,:n}^{(k)}))).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ ) ≐ divide start_ARG 1 end_ARG start_ARG italic_B ( italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT - 1 ) end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_CE ( bold_1 ( bold_italic_x start_POSTSUBSCRIPT italic_b , italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) , bold_italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , : italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.10)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S4.I5.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i6.p1">
<p class="ltx_p">Compute one step of an optimization algorithm on <math alttext="\theta" class="ltx_Math" display="inline" id="S4.I5.i6.p1.m1"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math>, giving the following iteration:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta^{(k+1)}\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)};\nabla_{\theta}\hat{\mathcal{L}}_{\mathrm{CLM}}^{(k)})." class="ltx_Math" display="block" id="S4.E11.m1"><semantics><mrow><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>≐</mo><mrow><msup><mtext class="ltx_font_smallcaps">OptUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>;</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>CLM</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\theta^{(k+1)}\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)};\nabla_{\theta}\hat{\mathcal{L}}_{\mathrm{CLM}}^{(k)}).</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ≐ OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ∇ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.4.11)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.5 </span>Evaluation Methodology</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p">There are several ways to evaluate a trained transformer language model.</p>
<ul class="ltx_itemize" id="S4.I6">
<li class="ltx_item" id="S4.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i1.p1">
<p class="ltx_p">On a holdout dataset of arbitrary text, we can evaluate <math alttext="\mathcal{L}_{\mathrm{CLM}}" class="ltx_Math" display="inline" id="S4.I6.i1.p1.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>CLM</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{CLM}}</annotation><annotation encoding="application/x-llamapun">caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT</annotation></semantics></math> on it; lower losses are better since they mean the model’s sampling yields better performance.</p>
</div>
</li>
<li class="ltx_item" id="S4.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i2.p1">
<p class="ltx_p">On a multiple choice question dataset, for each question we can put it as the context and check the estimated probability of the correct answer being generated.</p>
</div>
</li>
<li class="ltx_item" id="S4.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i3.p1">
<p class="ltx_p">We can also test the <span class="ltx_text ltx_font_italic">text generation</span> capabilities. Namely, we can repeatedly sample from the model’s probability distribution over the next token given the context. Each time we sample we generate a new token, which we print and add to the context. This allows us to sample from the LLM, and judge the generated samples however we please.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Having to re-run the model on each token every time can become prohibitively expensive. Clever storages of different internal features of the language model (such as the so-called <math alttext="K" class="ltx_Math" display="inline" id="footnote12.m1"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math><span class="ltx_text ltx_font_italic">-<math alttext="V" class="ltx_Math" display="inline" id="footnote12.m2"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation><annotation encoding="application/x-llamapun">italic_V</annotation></semantics></math> cache</span>), along with the causality of the architecture, can dramatically reduce the cost of sampling.</span></span></span></p>
</div>
</li>
</ul>
<p class="ltx_p">In this section, we perform the first kind of evaluation exclusively.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4.6 </span>Experimental Setup and Results</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p">Since our causal CRATE architecture is directly built upon GPT-2, we compare the optimal settings for GPT-2 as given by the NanoGPT repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx132" title="">Kar22</a>]</cite> with the same settings applied to CRATE for fair comparison.</p>
</div>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Model architecture.</h5>
<div class="ltx_para" id="S4.SS6.SSS0.Px1.p1">
<p class="ltx_p">We use the GPT-2 tokenizer, which has vocabulary size <math alttext="V=50257" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px1.p1.m1"><semantics><mrow><mi>V</mi><mo>=</mo><mn>50257</mn></mrow><annotation encoding="application/x-tex">V=50257</annotation><annotation encoding="application/x-llamapun">italic_V = 50257</annotation></semantics></math>, including a special token for <span class="ltx_text ltx_font_typewriter">&lt;|pad|&gt;</span>.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>The <span class="ltx_text ltx_font_typewriter">&lt;|bos|&gt;</span> token is not included in this setup, although it is very common in modern language models.</span></span></span> The context length is <math alttext="N_{\max}=1024" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px1.p1.m2"><semantics><mrow><msub><mi>N</mi><mi>max</mi></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">N_{\max}=1024</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = 1024</annotation></semantics></math>. The backbone model follows the GPT2-Base architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx222" title="">RWC+19</a>]</cite> with the appropriate alterations to have causal CRATE layers, and we compare against GPT2-Small and GPT2-Base.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Datasets and optimization.</h5>
<div class="ltx_para" id="S4.SS6.SSS0.Px2.p1">
<p class="ltx_p">For training causal CRATE, we follow the implementations in the NanoGPT repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx132" title="">Kar22</a>]</cite>. Specifically, we use a batch size of 384 and train for 600,000 steps with the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx135" title="">KB14</a>]</cite>. For the Adam optimizer, we use <math alttext="(\beta_{1},\beta_{2})=(0.9,0.95)" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m1"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>0.9</mn><mo>,</mo><mn>0.95</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\beta_{1},\beta_{2})=(0.9,0.95)</annotation><annotation encoding="application/x-llamapun">( italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0.9 , 0.95 )</annotation></semantics></math> and weight decay of <math alttext="0.1" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m2"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation><annotation encoding="application/x-llamapun">0.1</annotation></semantics></math>. For the learning rate schedule, we apply a linear warm-up and cosine decay, with a peak value of <math alttext="\eta=6\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m3"><semantics><mrow><mi>η</mi><mo>=</mo><mrow><mn>6</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\eta=6\times 10^{-4}</annotation><annotation encoding="application/x-llamapun">italic_η = 6 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> at the <math alttext="2,000" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m4"><semantics><mrow><mn>2</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">2,000</annotation><annotation encoding="application/x-llamapun">2 , 000</annotation></semantics></math> iteration, and minimum value <math alttext="6\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m5"><semantics><mrow><mn>6</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">6\times 10^{-5}</annotation><annotation encoding="application/x-llamapun">6 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>. The training and validation losses over iterations are shown in <a class="ltx_ref" href="#F13" title="In Datasets and optimization. ‣ 7.4.6 Experimental Setup and Results ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.13</span></a>. The training/validation loss converges around <math alttext="3.37" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m6"><semantics><mn>3.37</mn><annotation encoding="application/x-tex">3.37</annotation><annotation encoding="application/x-llamapun">3.37</annotation></semantics></math> after training with a batch size of <math alttext="384" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m7"><semantics><mn>384</mn><annotation encoding="application/x-tex">384</annotation><annotation encoding="application/x-llamapun">384</annotation></semantics></math> and <math alttext="600,000" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m8"><semantics><mrow><mn>600</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">600,000</annotation><annotation encoding="application/x-llamapun">600 , 000</annotation></semantics></math> iterations. In comparison, the open GPT-2 implementation is pre-trained on OpenWebText with a batch size of <math alttext="512" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m9"><semantics><mn>512</mn><annotation encoding="application/x-tex">512</annotation><annotation encoding="application/x-llamapun">512</annotation></semantics></math> and <math alttext="600,000" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m10"><semantics><mrow><mn>600</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">600,000</annotation><annotation encoding="application/x-llamapun">600 , 000</annotation></semantics></math> steps and converges to a validation loss of <math alttext="2.85" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px2.p1.m11"><semantics><mn>2.85</mn><annotation encoding="application/x-tex">2.85</annotation><annotation encoding="application/x-llamapun">2.85</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx132" title="">Kar22</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F13"><img alt="Figure 7.13 : The loss curve of CRATE-GPT-Base trained on the OpenWebText dataset." class="ltx_graphics" id="F13.g1" src="chapters/chapter7/figs/gpt-loss.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.13</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The loss curve of CRATE-GPT-Base trained on the OpenWebText dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Experiment results.</h5>
<div class="ltx_para" id="S4.SS6.SSS0.Px3.p1">
<p class="ltx_p"><a class="ltx_ref" href="#T5" title="In Experiment results. ‣ 7.4.6 Experimental Setup and Results ‣ 7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.5</span></a> demonstrates that CRATE models achieve reasonable performance on the causal language modeling loss across a variety of datasets compared to GPT-2 models with similar parameter counts and similar architectures.</p>
</div>
<figure class="ltx_table" id="T5">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7.5: </span>Zero-shot cross-entropy loss of the CRATE-GPT2-Base model and GPT2-Small, GPT2-Base model evaluated on the test split of the datasets (<math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m2"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math> lower is better).
</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">#parameters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">OWT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">LAMBADA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">WikiText</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">PTB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">GPT2-Base</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">124M</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span class="ltx_text" style="font-size:90%;">2.85</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m3"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span class="ltx_text" style="font-size:90%;">4.12</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m4"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span class="ltx_text" style="font-size:90%;">3.89</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m5"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span class="ltx_text" style="font-size:90%;">4.63</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m6"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span class="ltx_text" style="font-size:90%;">3.87</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T5.m7"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">GPT2-Small</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">64M</span></th>
<td class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">3.04</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.49</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.31</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">5.15</span></td>
<td class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.25</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">Causal-CRATE-Base</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">60M</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">3.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">5.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span class="ltx_text" style="font-size:90%;">4.61</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.5 </span>Scaling White-Box Transformers</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">In this section, we will discuss three ways in which various parts of CRATE-type models can be scaled up or made more efficient while still remaining white-box. These developments mix both conceptual and empirical insights, and can be viewed as case studies about how to use white-box understanding to improve deep learning models in practice. The tasks that we use to evaluate the methods will be image classification and next-token-prediction, the data will be ImageNet and OpenWebText respectively, the optimization procedure will be the same backpropagation, and the only thing that changes is the architecture.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.1 </span>Increasing Network Width: CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.m1"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>
</h3>
<figure class="ltx_figure" id="F14"><img alt="Figure 7.14 : One layer of the CRATE- α \alpha italic_α backbone. The difference from CRATE is that the ISTA θ ℓ \operatorname{ISTA}_{\theta}^{\ell} roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT block is replaced by the ODL θ ℓ \operatorname{ODL}_{\theta}^{\ell} roman_ODL start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT block, which performs several ISTA \operatorname{ISTA} roman_ISTA steps with an overcomplete dictionary." class="ltx_graphics" id="F14.g1" src="chapters/chapter7/figs/crate_alpha_backbone.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.14</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer of the CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="F14.m5"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> backbone.<span class="ltx_text ltx_font_medium"> The difference from CRATE is that the <math alttext="\operatorname{ISTA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="F14.m6"><semantics><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{ISTA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> block is replaced by the <math alttext="\operatorname{ODL}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="F14.m7"><semantics><msubsup><mi>ODL</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{ODL}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_ODL start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> block, which performs several <math alttext="\operatorname{ISTA}" class="ltx_Math" display="inline" id="F14.m8"><semantics><mi>ISTA</mi><annotation encoding="application/x-tex">\operatorname{ISTA}</annotation><annotation encoding="application/x-llamapun">roman_ISTA</annotation></semantics></math> steps with an overcomplete dictionary.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">One design decision enforced by the CRATE framework is that the <span class="ltx_text ltx_font_italic">width</span> of the nonlinearity in the network. In a regular transformer, the width is usually set to <math alttext="4" class="ltx_Math" display="inline" id="S5.SS1.p1.m1"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation><annotation encoding="application/x-llamapun">4</annotation></semantics></math>, <math alttext="8" class="ltx_Math" display="inline" id="S5.SS1.p1.m2"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math>, or <math alttext="\frac{11}{3}" class="ltx_Math" display="inline" id="S5.SS1.p1.m3"><semantics><mfrac><mn>11</mn><mn>3</mn></mfrac><annotation encoding="application/x-tex">\frac{11}{3}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 11 end_ARG start_ARG 3 end_ARG</annotation></semantics></math> times the feature dimension. However, CRATE enforces that the width is exactly equal to the feature dimension, i.e., the dictionaries <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="S5.SS1.p1.m4"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> are square, which could lead to reduced performance. The fundamental reason that the CRATE framework constrains us to this choice is as follows:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p">The ISTA block takes a <span class="ltx_text ltx_font_italic">single</span> step of optimization for dictionary learning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p">Usually one step of any iterative optimization algorithm cannot effectively optimize the objective. So then why does this work?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p">Optimization algorithms usually converge very quickly if and only if they have good initializations, or <span class="ltx_text ltx_font_italic">warm starts</span>. The ISTA block has a warm start — it treats the input features as an initialization to the resulting sparse codes.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p">This enforces that the input features and sparse codes are have the same dimension. Namely, ISTA learns a complete sparsifying dictionary (cf <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</li>
</ul>
<p class="ltx_p">Thus if we want to use a wide dictionary, we need ISTA to perform <span class="ltx_text ltx_font_italic">overcomplete</span> dictionary learning. This means we cannot have the same warm start (as our sparse codes have larger dimension than our features), and need more iterations to converge to a sparse code. Hence the step from features <math alttext="\bm{Z}_{\theta}^{\ell+1/2}" class="ltx_Math" display="inline" id="S5.SS1.p1.m5"><semantics><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> to sparse codes <math alttext="\bm{Z}_{\theta}^{\ell+1}" class="ltx_Math" display="inline" id="S5.SS1.p1.m6"><semantics><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> would no longer be</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}^{\ell+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell+1/2}\mid\bm{Z}_{\theta}^{\ell+1/2})" class="ltx_Math" display="block" id="S5.E1.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo>∣</mo><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell+1/2}\mid\bm{Z}_{\theta}^{\ell+1/2})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ∣ bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\operatorname{ISTA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S5.SS1.p1.m7"><semantics><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{ISTA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> function is defined as (by an abuse of notation from earlier sections)</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z}\mid\bm{Y})\doteq\operatorname{ReLU}(\bm{Z}-\beta(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell}\bm{Z}-\bm{Y})+\beta\lambda\bm{1}_{s}\bm{1}_{n}^{\top})" class="ltx_Math" display="block" id="S5.E2.m1"><semantics><mrow><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝒀</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>ReLU</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝒁</mi><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>−</mo><mi>𝒀</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟏</mn><mi>s</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mn>𝟏</mn><mi>n</mi><mo>⊤</mo></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{ISTA}_{\theta}^{\ell}(\bm{Z}\mid\bm{Y})\doteq\operatorname{ReLU}(\bm{Z}-\beta(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell}\bm{Z}-\bm{Y})+\beta\lambda\bm{1}_{s}\bm{1}_{n}^{\top})</annotation><annotation encoding="application/x-llamapun">roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_Y ) ≐ roman_ReLU ( bold_italic_Z - italic_β ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z - bold_italic_Y ) + italic_β italic_λ bold_1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">but rather the following iteration:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}^{\ell+1}=\bm{A}_{\theta}^{\ell,T};\qquad\bm{A}_{\theta}^{\ell,t+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{A}_{\theta}^{\ell,t}\mid\bm{Z}_{\theta}^{\ell+1/2})\quad\forall 0\leq t&lt;T;\qquad\bm{A}_{\theta}^{\ell,0}=\bm{0}_{s\times n}," class="ltx_Math" display="block" id="S5.E3.m1"><semantics><mrow><mrow><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mi>T</mi></mrow></msubsup></mrow><mo rspace="2.167em">;</mo><mrow><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mi>t</mi></mrow></msubsup><mo>∣</mo><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1.167em"></mspace><mrow><mrow><mo rspace="0.167em">∀</mo><mn>0</mn></mrow><mo>≤</mo><mi>t</mi><mo>&lt;</mo><mi>T</mi></mrow></mrow></mrow><mo rspace="2.167em">;</mo><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mn>0</mn></mrow></msubsup><mo>=</mo><msub><mn>𝟎</mn><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}=\bm{A}_{\theta}^{\ell,T};\qquad\bm{A}_{\theta}^{\ell,t+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{A}_{\theta}^{\ell,t}\mid\bm{Z}_{\theta}^{\ell+1/2})\quad\forall 0\leq t&lt;T;\qquad\bm{A}_{\theta}^{\ell,0}=\bm{0}_{s\times n},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_T end_POSTSUPERSCRIPT ; bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_t + 1 end_POSTSUPERSCRIPT = roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_t end_POSTSUPERSCRIPT ∣ bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ) ∀ 0 ≤ italic_t &lt; italic_T ; bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , 0 end_POSTSUPERSCRIPT = bold_0 start_POSTSUBSCRIPT italic_s × italic_n end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">i.e., running proximal gradient on the LASSO objective for <math alttext="T\geq 1" class="ltx_Math" display="inline" id="S5.SS1.p1.m8"><semantics><mrow><mi>T</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T\geq 1</annotation><annotation encoding="application/x-llamapun">italic_T ≥ 1</annotation></semantics></math> steps in the forward pass at each layer, initialized at <math alttext="\bm{0}_{s\times n}" class="ltx_Math" display="inline" id="S5.SS1.p1.m9"><semantics><msub><mn>𝟎</mn><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msub><annotation encoding="application/x-tex">\bm{0}_{s\times n}</annotation><annotation encoding="application/x-llamapun">bold_0 start_POSTSUBSCRIPT italic_s × italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. In this circumstance, the dictionary can be as wide as needed, i.e., <math alttext="\bm{D}^{\ell}\in\mathbb{R}^{s\times d}" class="ltx_Math" display="inline" id="S5.SS1.p1.m10"><semantics><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{D}^{\ell}\in\mathbb{R}^{s\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_s × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="s\geq d" class="ltx_Math" display="inline" id="S5.SS1.p1.m11"><semantics><mrow><mi>s</mi><mo>≥</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">s\geq d</annotation><annotation encoding="application/x-llamapun">italic_s ≥ italic_d</annotation></semantics></math> (usually one takes <math alttext="s=4d" class="ltx_Math" display="inline" id="S5.SS1.p1.m12"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">s=4d</annotation><annotation encoding="application/x-llamapun">italic_s = 4 italic_d</annotation></semantics></math> in practice).</p>
</div>
<figure class="ltx_table" id="T6">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Detection</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Segmentation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<td class="ltx_td ltx_align_center">AP<math alttext="{}_{50}\uparrow" class="ltx_math_unparsed" display="inline" id="T6.m1"><semantics><mmultiscripts><mo stretchy="false">↑</mo><mprescripts></mprescripts><mn>50</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{50}\uparrow</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT ↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center">AP<math alttext="{}_{75}\uparrow" class="ltx_math_unparsed" display="inline" id="T6.m2"><semantics><mmultiscripts><mo stretchy="false">↑</mo><mprescripts></mprescripts><mn>75</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{75}\uparrow</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT ↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center">AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="T6.m3"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center">AP<math alttext="{}_{50}\uparrow" class="ltx_math_unparsed" display="inline" id="T6.m4"><semantics><mmultiscripts><mo stretchy="false">↑</mo><mprescripts></mprescripts><mn>50</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{50}\uparrow</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT ↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center">AP<math alttext="{}_{75}\uparrow" class="ltx_math_unparsed" display="inline" id="T6.m5"><semantics><mmultiscripts><mo stretchy="false">↑</mo><mprescripts></mprescripts><mn>75</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{75}\uparrow</annotation><annotation encoding="application/x-llamapun">start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT ↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center">AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="T6.m6"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T6.m7"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>-B/8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">3.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">2.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1.1</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T6.m8"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>-L/8</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">4.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">2.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.4</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span class="ltx_text" style="color:#808080;">CRATE-B/8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">2.9</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">1.0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">1.3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">2.2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">0.7</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text" style="color:#808080;">1.0</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text" style="color:#808080;">ViT-B/8</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">0.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Object detection and fine-grained segmentation via MaskCut on COCO val2017 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx163" title="">LMB+14</a>]</cite><span class="ltx_text ltx_font_medium">. Here all models are trained with patch size <math alttext="8" class="ltx_Math" display="inline" id="T6.m12"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> instead of <math alttext="16" class="ltx_Math" display="inline" id="T6.m13"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation><annotation encoding="application/x-llamapun">16</annotation></semantics></math>. Compared with existing models such as CRATE and ViT, the CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T6.m14"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> model family noticeably has improved performance as well as scalability.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p">However, this presents an empirical problem. Using the above configuration, if <math alttext="\bm{Z}^{\ell+1/2}\in\mathbb{R}^{d\times n}" class="ltx_Math" display="inline" id="S5.SS1.p2.m1"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}\in\mathbb{R}^{d\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> then <math alttext="\bm{Z}^{\ell+1}\in\mathbb{R}^{s\times n}" class="ltx_Math" display="inline" id="S5.SS1.p2.m2"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}\in\mathbb{R}^{s\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_s × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, which can have arbitrarily large feature dimension. In practice, we want the feature dimension at each layer to be the same. So this sets up a practical trichotomy for designing the network, namely, we <span class="ltx_text ltx_font_italic">cannot</span> have <span class="ltx_text ltx_font_italic">all</span> of the following desiderata:</p>
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p">The feature dimension at each layer is the same.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p">The dictionary is wide, i.e., overcomplete.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p">The output of the nonlinearity is the sparse codes of the input w.r.t. the dictionary.</p>
</div>
</li>
</ol>
<p class="ltx_p">In practice, giving (1) is less tractable for efficiency reasons. Giving up (2) leads to the usual CRATE framework. Giving up (3) leads to a wide version of CRATE, i.e., CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.p2.m3"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>, which has the following nonlinearity to get from <math alttext="\bm{Z}^{\ell+1/2}" class="ltx_Math" display="inline" id="S5.SS1.p2.m4"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S5.SS1.p2.m5"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}^{\ell+1}=\bm{D}^{\ell}\bm{A}_{\theta}^{\ell,T};\qquad\bm{A}_{\theta}^{\ell,t+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{A}_{\theta}^{\ell,t}\mid\bm{Z}_{\theta}^{\ell+1/2});\qquad\bm{A}_{\theta}^{\ell,0}=\bm{0}," class="ltx_Math" display="block" id="S5.E4.m1"><semantics><mrow><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mi>T</mi></mrow></msubsup></mrow></mrow><mo rspace="2.167em">;</mo><mrow><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>ISTA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mi>t</mi></mrow></msubsup><mo>∣</mo><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">;</mo><mrow><msubsup><mi>𝑨</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>,</mo><mn>0</mn></mrow></msubsup><mo>=</mo><mn>𝟎</mn></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}=\bm{D}^{\ell}\bm{A}_{\theta}^{\ell,T};\qquad\bm{A}_{\theta}^{\ell,t+1}=\operatorname{ISTA}_{\theta}^{\ell}(\bm{A}_{\theta}^{\ell,t}\mid\bm{Z}_{\theta}^{\ell+1/2});\qquad\bm{A}_{\theta}^{\ell,0}=\bm{0},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_T end_POSTSUPERSCRIPT ; bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_t + 1 end_POSTSUPERSCRIPT = roman_ISTA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , italic_t end_POSTSUPERSCRIPT ∣ bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ) ; bold_italic_A start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ , 0 end_POSTSUPERSCRIPT = bold_0 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">i.e., takes the sparse codes obtained via proximal gradient descent and multiplies by the dictionary, to get the denoised version of the input. Thus CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.p2.m6"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>’s nonlinearity computes a denoised version of the input which is amenable to sparse coding, not the actual sparse codes themselves. The map from <math alttext="\bm{Z}_{\theta}^{\ell+1/2}" class="ltx_Math" display="inline" id="S5.SS1.p2.m7"><semantics><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\bm{Z}_{\theta}^{\ell+1}" class="ltx_Math" display="inline" id="S5.SS1.p2.m8"><semantics><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> here is called the Overcomplete Dictionary Learning (ODL) block and denoted <math alttext="\operatorname{ODL}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S5.SS1.p2.m9"><semantics><msubsup><mi>ODL</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{ODL}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_ODL start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}^{\ell+1}(\bm{X})\doteq\operatorname{ODL}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X}))." class="ltx_Math" display="block" id="S5.E5.m1"><semantics><mrow><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>ODL</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}(\bm{X})\doteq\operatorname{ODL}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ roman_ODL start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.5)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="F15"><img alt="Figure 7.15 : Saliency maps from CRATE- α \alpha italic_α with patch size 8 8 8 . Each row is a different image and each column corresponds to a different attention head in the last layer. We observe that the saliency maps strongly correspond to the objects in the input image." class="ltx_graphics" id="F15.g1" src="chapters/chapter7/figs/crate_alpha_semantic_heads.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Saliency maps from CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="F15.m3"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> with patch size <math alttext="8" class="ltx_Math" display="inline" id="F15.m4"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math>.<span class="ltx_text ltx_font_medium"> Each row is a different image and each column corresponds to a different attention head in the last layer. We observe that the saliency maps strongly correspond to the objects in the input image.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T7">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GPT-2-B(ase)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T7.m1"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>-S(mall)</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T7.m2"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math>-B</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"># parameters</th>
<td class="ltx_td ltx_align_center ltx_border_tt">124M</td>
<td class="ltx_td ltx_align_center ltx_border_tt">60M</td>
<td class="ltx_td ltx_align_center ltx_border_tt">57M</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">120M</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">OWT val. loss</th>
<td class="ltx_td ltx_align_center ltx_border_bb">2.85</td>
<td class="ltx_td ltx_align_center ltx_border_bb">3.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb">3.28</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">3.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Validation loss in language modeling.<span class="ltx_text ltx_font_medium"> Here all models are pre-trained on most of OpenWebText, and the validation cross-entropy loss is measured on a hold-out subset of OpenWebText. CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="T7.m4"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> shows significant improvement over the CRATE design, though there still exists a gap with traditional transformers like GPT-2.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p">The CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.p3.m1"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> layer is shown in <a class="ltx_ref" href="#F14" title="In 7.5.1 Increasing Network Width: CRATE-𝛼 ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.14</span></a>. In practice this modification of CRATE performs very well at larger scales. For example, when we pre-train CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.p3.m2"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> models on ImageNet-21K, unsupervised tasks like segmentation (see <a class="ltx_ref" href="#F15" title="In 7.5.1 Increasing Network Width: CRATE-𝛼 ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.15</span></a> and <a class="ltx_ref" href="#T6" title="In 7.5.1 Increasing Network Width: CRATE-𝛼 ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.6</span></a>) generally have significantly improved performance compared to CRATE. Similar trends are present in language model training using causal self-attention (see <a class="ltx_ref" href="#T7" title="In 7.5.1 Increasing Network Width: CRATE-𝛼 ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.7</span></a>). Overall, it is a promising avenue to scaling up the performance to match black-box models such as transformers.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Note that the experimental results in this section use a slightly different model architecture, which add very slight empirical gains. The changes are: (1) an additional residual connection on the ODL block, (2) modifying <math alttext="\operatorname{ISTA}" class="ltx_Math" display="inline" id="footnote14.m1"><semantics><mi>ISTA</mi><annotation encoding="application/x-tex">\operatorname{ISTA}</annotation><annotation encoding="application/x-llamapun">roman_ISTA</annotation></semantics></math> to use two separate dictionaries instead of <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="footnote14.m2"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="(\bm{D}^{\ell})^{\top}" class="ltx_Math" display="inline" id="footnote14.m3"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><annotation encoding="application/x-tex">(\bm{D}^{\ell})^{\top}</annotation><annotation encoding="application/x-llamapun">( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.2 </span>Linear Time Complexity Transformers</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">In practice, deep learning models suffer bottlenecks to space and time complexity, representing problem sizes which they cannot scale beyond given fixed resources. One such bottleneck, particularly meaningful when dealing with data where each sample is itself high-dimensional and rich (such as long streams of text or videos), is the <span class="ltx_text ltx_font_italic">time complexity</span> of processing long sequences of data. In order to alleviate the time-complexity of processing data using transformers, in <a class="ltx_ref" href="Ch4.html#S3.SS2" title="4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.2</span></a> we proposed a <span class="ltx_text ltx_font_italic">token statistics self-attention</span> operator <math alttext="\operatorname{TSSA}_{\theta}^{\ell}" class="ltx_Math" display="inline" id="S5.SS2.p1.m1"><semantics><msubsup><mi>TSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{TSSA}_{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_TSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>. We now build a <span class="ltx_text ltx_font_italic">token statistics transformer</span>, called ToST, around it, which we can use for long-context tasks. In particular, we can use the following layer (depicted in <a class="ltx_ref" href="#F16" title="In 7.5.2 Linear Time Complexity Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.16</span></a>) as a drop-in replacement for a backbone layer in CRATE:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx98">
<tbody id="S5.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})" class="ltx_Math" display="inline" id="S5.E6.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{TSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))" class="ltx_Math" display="inline" id="S5.E6.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>TSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{TSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{1,\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))</annotation><annotation encoding="application/x-llamapun">= bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_TSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.6)</span></td>
</tr></tbody>
<tbody id="S5.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S5.E7.m1"><semantics><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{\theta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})+\operatorname{MLP}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})))" class="ltx_Math" display="inline" id="S5.E7.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>MLP</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mrow><mn>2</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})+\operatorname{MLP}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{2,\ell}(\bm{Z}_{\theta}^{\ell+1/2}(\bm{X})))</annotation><annotation encoding="application/x-llamapun">= bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MLP start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 , roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\operatorname{TSSA}" class="ltx_Math" display="inline" id="S5.SS2.p1.m2"><semantics><mi>TSSA</mi><annotation encoding="application/x-tex">\operatorname{TSSA}</annotation><annotation encoding="application/x-llamapun">roman_TSSA</annotation></semantics></math> block is defined as in <a class="ltx_ref" href="Ch4.html#S3.SS2" title="4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.2</span></a>. Notice that this is exactly the same as a vision transformer architecture as discussed in <a class="ltx_ref" href="#S2.SS3" title="7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.3</span></a>, except that <math alttext="\operatorname{TSSA}" class="ltx_Math" display="inline" id="S5.SS2.p1.m3"><semantics><mi>TSSA</mi><annotation encoding="application/x-tex">\operatorname{TSSA}</annotation><annotation encoding="application/x-llamapun">roman_TSSA</annotation></semantics></math> replaces the conventional multi-head self-attention block <math alttext="\operatorname{MHSA}" class="ltx_Math" display="inline" id="S5.SS2.p1.m4"><semantics><mi>MHSA</mi><annotation encoding="application/x-tex">\operatorname{MHSA}</annotation><annotation encoding="application/x-llamapun">roman_MHSA</annotation></semantics></math>. Regardless, the computational complexity of the forward pass of this layer is linear in all problem variables — sequence length, feature dimension, number of heads, and head dimension.</p>
</div>
<figure class="ltx_figure" id="F16"><img alt="Figure 7.16 : One layer of the ToST backbone . Token representations go through layer-norms, the token statistics self-attention (TSSA) operator, and an MLP, in order to form the layer’s output." class="ltx_graphics" id="F16.g1" src="chapters/chapter7/figs/tost_backbone.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.16</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer of the ToST backbone<span class="ltx_text ltx_font_medium">. Token representations go through layer-norms, the token statistics self-attention (TSSA) operator, and an MLP, in order to form the layer’s output.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T8">
<div class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:118.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.4pt,12.9pt) scale(0.820471843229609,0.820471843229609) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Datasets</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ToST-T(iny)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ToST-S(mall)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">ToST-M(edium)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;"> XCiT-S</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text" style="color:#808080;">XCiT-M</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">ViT-S</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">ViT-B(ase)</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"># parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">5.8M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">22.6M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">68.1M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">24.9M</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text" style="color:#808080;">80.2M</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;"> 22.1M</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">86.6 M</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">ImageNet</td>
<td class="ltx_td ltx_align_center ltx_border_t">67.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.3</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 80.5</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="color:#808080;"> 81.5</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 79.8</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 81.8</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">ImageNet ReaL</td>
<td class="ltx_td ltx_align_center">72.2</td>
<td class="ltx_td ltx_align_center">84.1</td>
<td class="ltx_td ltx_align_center ltx_border_r">85.6</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 85.6</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="color:#808080;"> 85.9</span>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 85.6</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 86.7</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">CIFAR10</td>
<td class="ltx_td ltx_align_center ltx_border_t">95.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">96.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.5</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 98.1</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="color:#808080;"> 98.3</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 98.6</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;"> 98.8</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">CIFAR100</td>
<td class="ltx_td ltx_align_center">78.3</td>
<td class="ltx_td ltx_align_center">82.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">84.5</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 86.1</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="color:#808080;"> 87.6</span>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 88.8</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 89.3</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Oxford Flowers-102</td>
<td class="ltx_td ltx_align_center">88.6</td>
<td class="ltx_td ltx_align_center">92.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">94.2</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 93.9</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="color:#808080;"> 94.0</span>
</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;"> 94.0</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;"> 95.7</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">Oxford-IIIT-Pets</td>
<td class="ltx_td ltx_align_center ltx_border_bb">85.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb">91.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">92.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;"> 92.9</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text" style="color:#808080;"> 94.0</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;"> 92.8</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;"> 94.1</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.8</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Linear probing classification accuracy of ToST<span class="ltx_text ltx_font_medium"> on various datasets with different model sizes when the backbone is pre-trained for ImageNet-1K classification. We observe that, compared to the XCiT (a empirically-designed transformer-like architecture specialized for efficient processing of long sequences) and the ViT, ToST maintains relatively similar performance, even while enjoying benefits like faster runtime and white-box design.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T9">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># params</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">OWT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Lambada</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Wikitext</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PTB</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg <math alttext="\downarrow" class="ltx_Math" display="inline" id="T9.m1"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-2-Base</th>
<td class="ltx_td ltx_align_center ltx_border_t">124M</td>
<td class="ltx_td ltx_align_center ltx_border_t">2.84</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.32</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.13</td>
<td class="ltx_td ltx_align_center ltx_border_t">5.75</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4.26</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ToST-Base</th>
<td class="ltx_td ltx_align_center">110M</td>
<td class="ltx_td ltx_align_center">3.20</td>
<td class="ltx_td ltx_align_center">4.98</td>
<td class="ltx_td ltx_align_center">4.77</td>
<td class="ltx_td ltx_align_center">6.39</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4.84</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ToST-Medium</th>
<td class="ltx_td ltx_align_center">304M</td>
<td class="ltx_td ltx_align_center">2.88</td>
<td class="ltx_td ltx_align_center">4.45</td>
<td class="ltx_td ltx_align_center">4.30</td>
<td class="ltx_td ltx_align_center">5.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4.32</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">ToST-Large</th>
<td class="ltx_td ltx_align_center ltx_border_bb">655M</td>
<td class="ltx_td ltx_align_center ltx_border_bb">2.72</td>
<td class="ltx_td ltx_align_center ltx_border_bb">4.32</td>
<td class="ltx_td ltx_align_center ltx_border_bb">3.99</td>
<td class="ltx_td ltx_align_center ltx_border_bb">5.03</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">4.02</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.9</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Language modeling validation loss<span class="ltx_text ltx_font_medium"> computed on (holdout sets of) a variety of natural language datasets, after pre-training the model on that dataset. We observe that ToST scales well, so that ToST-Large surpasses the baseline GPT-2-Base in causal language modeling, while enjoying superior efficiency in long contexts.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="T10">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ListOps</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Text</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Retrieval</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Pathfinder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Reformer</th>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">37.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt">56.10</td>
<td class="ltx_td ltx_align_center ltx_border_tt">53.40</td>
<td class="ltx_td ltx_align_center ltx_border_tt">38.07</td>
<td class="ltx_td ltx_align_center ltx_border_tt">68.50</td>
<td class="ltx_td ltx_align_center ltx_border_tt">50.56</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">BigBird</th>
<td class="ltx_td ltx_align_center">36.05</td>
<td class="ltx_td ltx_align_center">64.02</td>
<td class="ltx_td ltx_align_center">59.29</td>
<td class="ltx_td ltx_align_center">40.83</td>
<td class="ltx_td ltx_align_center">74.87</td>
<td class="ltx_td ltx_align_center">54.17</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">LinFormer</th>
<td class="ltx_td ltx_align_center">16.13</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_underline">65.90</span></td>
<td class="ltx_td ltx_align_center">53.09</td>
<td class="ltx_td ltx_align_center">42.34</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_underline">75.30</span></td>
<td class="ltx_td ltx_align_center">50.46</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Performer</th>
<td class="ltx_td ltx_align_center">18.01</td>
<td class="ltx_td ltx_align_center">65.40</td>
<td class="ltx_td ltx_align_center">53.82</td>
<td class="ltx_td ltx_align_center">42.77</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">77.05</span></td>
<td class="ltx_td ltx_align_center">51.18</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Transformer</th>
<td class="ltx_td ltx_align_center">37.11</td>
<td class="ltx_td ltx_align_center">65.21</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_underline">79.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_underline">42.94</span></td>
<td class="ltx_td ltx_align_center">71.83</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_underline">59.24</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">ToST</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline">37.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">66.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">79.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">46.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">69.41</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">59.90</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.10</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Long-Range Arena (LRA) performance comparison of ToST(-B) versus the top transformer variants optimized for long-context.<span class="ltx_text ltx_font_medium"> Long-Range Arena is a family of benchmarks that test the long sequence modeling capability of algorithms and architectures, by fixing the dataset and evaluation mechanism. ToST scores at the top of the leaderboard compared to all known transformer variants, including XCiT and the regular (ViT) transformer (cf <a class="ltx_ref" href="#T8" title="In 7.5.2 Linear Time Complexity Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.8</span></a>). Moreover, ToST has the lowest time- and space-complexity inference. (In this table, the best score for a particular benchmark is bolded, and the second-best score is underlined.)</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p">Moreover, the proposed architecture, named ToST (for “Token Statistics Transformer”) performs well at vision tasks (i.e., <a class="ltx_ref" href="#T8" title="In 7.5.2 Linear Time Complexity Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.8</span></a>) and language tasks (i.e., <a class="ltx_ref" href="#T9" title="In 7.5.2 Linear Time Complexity Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.9</span></a>). This is especially true for long-sequence-length tasks (cf <a class="ltx_ref" href="#T10" title="In 7.5.2 Linear Time Complexity Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.10</span></a>), where it is both more performant and much more efficient than conventional transformers and all other transformer-like architectures.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5.3 </span>Attention-Only Transformers</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p">Another bottleneck to remove from deep learning models, specifically transformer-like architectures, is the memory bottleneck which comes from massive matrix multiplications in MLPs, where the internal dimension is far greater than the feature dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.SS3.p1.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>. It thus is an interesting and important question to ask: do we <span class="ltx_text ltx_font_italic">really</span> need the MLP inside a transformer, and how good can the performance get without it? To explore this question, we use the attention-only-transformer (AoT) architecture (see <a class="ltx_ref" href="Ch4.html#S3.SS1" title="4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.1</span></a>), depicted in <a class="ltx_ref" href="#F17" title="In 7.5.3 Attention-Only Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.17</span></a>. Namely, each layer is simply of the form</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{\theta}^{\ell+1}(\bm{X})=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X})))." class="ltx_Math" display="block" id="S5.E8.m1"><semantics><mrow><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>MSSA</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒁</mi><mi>θ</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}^{\ell+1}(\bm{X})=\bm{Z}_{\theta}^{\ell}(\bm{X})+\operatorname{MSSA}_{\theta}^{\ell}(\operatorname{LN}_{\theta}^{\ell}(\bm{Z}_{\theta}^{\ell}(\bm{X}))).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X ) = bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.5.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In our implementation, we also experimented with using multi-head self-attention (MHSA) in place of MSSA. It turns out that this architecture is <span class="ltx_text ltx_font_italic">also</span> viable, though the depth of the network needs to be much deeper in order to achieve equivalent performance as the usual CRATE or transformer architecture.</p>
</div>
<figure class="ltx_figure" id="F17"><img alt="Figure 7.17 : One layer of the AoT backbone. Token representations merely go through a layer-norm and the multi-head (subspace) self-attention operator to form the layer’s output. Notice that there is no token-wise nonlinearity such as MLP or ISTA or ODL." class="ltx_graphics" id="F17.g1" src="chapters/chapter7/figs/aot_backbone.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.17</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer of the AoT backbone.<span class="ltx_text ltx_font_medium"> Token representations merely go through a layer-norm and the multi-head (subspace) self-attention operator to form the layer’s output. Notice that there is no token-wise nonlinearity such as MLP or ISTA or ODL.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p">We conduct the experiments using the proposed AoT architecture and demonstrate its potential. We pre-train the AoT-MSSA and AoT-MHSA models of different sizes, along with GPT-2, on OpenWebText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx92" title="">GC19</a>]</cite>. We plot the training loss and validation loss against the number of training iterations in <a class="ltx_ref" href="#F18" title="In 7.5.3 Attention-Only Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.18</span></a>(a) and (b), respectively. It is observed that medium- and large-sized AoT-based models achieve training and validation losses comparable to those of the GPT-2 base model. In addition, compared to the GPT-2 base model, the AoT-MHSA model is identical to the GPT-2 base model, except for the absence of MLP layers in the architecture. As shown in <a class="ltx_ref" href="#F18" title="In 7.5.3 Attention-Only Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.18</span></a>, incorporating MLP layers can accelerate the training process. Using the above pre-trained models, we compute the cross-entropy validation loss without training on different datasets in <a class="ltx_ref" href="#T11" title="In 7.5.3 Attention-Only Transformers ‣ 7.5 Scaling White-Box Transformers ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.11</span></a>. It is observed that the AoT models with medium and large parameter sizes can achieve comparable performance to the GPT-2 base model. Moreover, we found that adding MLP layers to AoT does not improve the zero-shot performance. These results highlight the potential of attention-only models to achieve competitive results while maintaining interpretability.</p>
</div>
<figure class="ltx_table" id="T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.11</span>: </span><span class="ltx_text" style="font-size:90%;">Zero-shot results on several language benchmark datasets and tasks: Evaluation of different sizes of AoT with the MSSA and MHSA operators and comparison to the GPT2 model.</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">LAMBADA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">PTB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">WikiText</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">LAMBADA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">CBT CN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">CBT NE</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span class="ltx_text" style="font-size:90%;"># of parameters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(val loss) </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T11.m1"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(val loss) </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T11.m2"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(val loss) </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="T11.m3"><semantics><mo mathsize="90%" stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation><annotation encoding="application/x-llamapun">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(acc) </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="T11.m4"><semantics><mo mathsize="90%" stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(acc) </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="T11.m5"><semantics><mo mathsize="90%" stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span class="ltx_text" style="font-size:90%;">(acc) </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="T11.m6"><semantics><mo mathsize="90%" stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation><annotation encoding="application/x-llamapun">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">AoT-MSSA Base (102M)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">4.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">6.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">4.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.74</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span class="ltx_text" style="font-size:90%;">AoT-MSSA Medium (182M)</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">4.47</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">5.08</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">4.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.29</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.84</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.77</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span class="ltx_text" style="font-size:90%;">AoT-MHSA Base (122M)</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">4.42</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">5.52</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">4.19</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.38</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.86</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">0.82</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span class="ltx_text" style="font-size:90%;">GPT-2 Base (124M)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">4.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">5.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">4.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">0.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">0.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">0.84</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 7.18 : Evaluating models on language tasks. We plot the training loss (left) and validation loss (right) of the AoT and GPT-2 models pretrained on OpenWebText." class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="178" id="F18.g1" src="chapters/chapter7/figs/training_loss.png" width="240"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 7.18 : Evaluating models on language tasks. We plot the training loss (left) and validation loss (right) of the AoT and GPT-2 models pretrained on OpenWebText." class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="178" id="F18.g2" src="chapters/chapter7/figs/val_loss.png" width="240"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.18</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluating models on language tasks.<span class="ltx_text ltx_font_medium"> We plot the training loss (left) and validation loss (right) of the AoT and GPT-2 models pretrained on OpenWebText.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.6 </span>Masked Autoencoding for Imagery Data</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">The second application we discuss is <span class="ltx_text ltx_font_italic">nonlinear image completion</span>, also known as <span class="ltx_text ltx_font_italic">masked autoencoding</span> (MAE), which is a direct generalization of the low-rank matrix completion problem discussed in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Masked autoencoding, since its introduction in the deep learning context by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx101" title="">HCX+22</a>]</cite> has been a staple and simple self-supervised representation learning method, which aims to endow each patch feature within <math alttext="\bm{Z}_{\theta}" class="ltx_Math" display="inline" id="S6.p1.m1"><semantics><msub><mi>𝒁</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> with aggregate information as well as information about its neighbors, such that both the patch feature and aggregate features are rich sources of information for the whole sample.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p">The dataset is kept to be the same imagery datasets as discussed in <a class="ltx_ref" href="#S2.SS1" title="7.2.1 Data ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.1</span></a>. As usual, we still apply data augmentations to each sample in each new batch.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.1 </span>Task and Objective</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p">As the name suggests, masked autoencoding involves a view <math alttext="v_{m}" class="ltx_Math" display="inline" id="S6.SS1.p1.m1"><semantics><msub><mi>v</mi><mi>m</mi></msub><annotation encoding="application/x-tex">v_{m}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> which, given an input, performs a random resized crop (cf <a class="ltx_ref" href="#S2.SS2" title="7.2.2 Task and Objective Function ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2.2</span></a>) to turn the input image into a square image of size <math alttext="(C,S_{\mathrm{mask}},S_{\mathrm{mask}})" class="ltx_Math" display="inline" id="S6.SS1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><msub><mi>S</mi><mi>mask</mi></msub><mo>,</mo><msub><mi>S</mi><mi>mask</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C,S_{\mathrm{mask}},S_{\mathrm{mask}})</annotation><annotation encoding="application/x-llamapun">( italic_C , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT )</annotation></semantics></math>, then <span class="ltx_text ltx_font_italic">masks</span> (i.e., sets to zero) a fixed percentage <math alttext="p_{\mathrm{mask}}\in[0,1]" class="ltx_Math" display="inline" id="S6.SS1.p1.m3"><semantics><mrow><msub><mi>p</mi><mi>mask</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{mask}}\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> of pixels in the input. For efficiency reasons<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>The original implementation of MAE by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx101" title="">HCX+22</a>]</cite> embeds the whole image, <span class="ltx_text ltx_font_italic">removes</span> the tokens that would be masked, feeds the resulting token set through the encoder, adds back learned placeholder tokens in the masked spots and adds back the appropriate positional encoding, and feeds the resulting token set through the decoder to get the autoencoding prediction. This is more efficient since the encoder has fewer tokens to go through, but conceptually is the same as the method discussed in the text, and the resulting models’ performance in the masked autoencoding task and downstream evaluations is very similar.</span></span></span>, the masking is done patch-wise, i.e., after embedding the whole image, <math alttext="p_{\mathrm{mask}}" class="ltx_Math" display="inline" id="S6.SS1.p1.m4"><semantics><msub><mi>p</mi><mi>mask</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{mask}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT</annotation></semantics></math> percentage of <span class="ltx_text ltx_font_italic">patches</span> are set to zero. The goal of MAE is to train an encoder <math alttext="f_{\theta}\colon\mathcal{I}\to(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S6.SS1.p1.m5"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}\colon\mathcal{I}\to(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : caligraphic_I → ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> and a decoder <math alttext="g_{\eta}\colon(\mathbb{R}^{d})^{*}\to\mathcal{I}" class="ltx_Math" display="inline" id="S6.SS1.p1.m6"><semantics><mrow><msub><mi>g</mi><mi>η</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}\colon(\mathbb{R}^{d})^{*}\to\mathcal{I}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT → caligraphic_I</annotation></semantics></math> which can reconstruct an input from its masking, i.e., writing <math alttext="\hat{\bm{X}}_{\theta,\eta}\doteq g_{\eta}\circ f_{\theta}" class="ltx_Math" display="inline" id="S6.SS1.p1.m7"><semantics><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo>≐</mo><mrow><msub><mi>g</mi><mi>η</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{\theta,\eta}\doteq g_{\eta}\circ f_{\theta}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ≐ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta,\eta}\left\{\mathcal{L}_{\mathrm{MAE}}(\theta,\eta)\doteq\operatorname{\mathbb{E}}\|\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{m})-\bm{X}\|_{F}^{2}\right\}" class="ltx_Math" display="block" id="S6.E1.m1"><semantics><mrow><munder><mi>min</mi><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>MAE</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\min_{\theta,\eta}\left\{\mathcal{L}_{\mathrm{MAE}}(\theta,\eta)\doteq\operatorname{\mathbb{E}}\|\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{m})-\bm{X}\|_{F}^{2}\right\}</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_θ , italic_η ) ≐ blackboard_E ∥ over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) - bold_italic_X ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Essentially this means that the features <math alttext="\bm{Z}_{\theta}(\bm{X}_{m})" class="ltx_Math" display="inline" id="S6.SS1.p1.m8"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{m})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> of the view <math alttext="\bm{X}_{m}\doteq v_{m}(\bm{X})" class="ltx_Math" display="inline" id="S6.SS1.p1.m9"><semantics><mrow><msub><mi>𝑿</mi><mi>m</mi></msub><mo>≐</mo><mrow><msub><mi>v</mi><mi>m</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{m}\doteq v_{m}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> must contain information about the <span class="ltx_text ltx_font_italic">masked patches</span> as well as the existing patches. From the perspective of the compression-based white-box models in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, if a white-box autoencoder <math alttext="(f_{\theta},g_{\eta})" class="ltx_Math" display="inline" id="S6.SS1.p1.m10"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo>,</mo><msub><mi>g</mi><mi>η</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f_{\theta},g_{\eta})</annotation><annotation encoding="application/x-llamapun">( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT )</annotation></semantics></math> succeeds at this task, it means that the learned subspaces and dictionaries perform a <span class="ltx_text ltx_font_italic">redundant</span> encoding of the data such that it can reconstruct missing parts of the data from encoded other parts of the data. This means that information about each patch is stored in other patches. Therefore, each patch feature should contain both information about the patch and information about the statistics of the whole image. Thus, again, we expect that the representations should contain both local and global semantically relevant information, and the therefore representations of different patches with similar local and global information should be related (i.e., on the same subspace or encoded together by a dictionary).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.2 </span>Architecture</h3>
<figure class="ltx_figure" id="F19"><img alt="Figure 7.19 : One layer of the encoder and decoder in a CRATE autoencoder backbone. The encoder and decoder layers both feed their inputs through multi-head subspace self-attention and a dictionary learning or dictionary encoding step. Note that the encoder and decoder layers are symmetrically designed; the conceptual goal of each decoder layer is to invert an encoder layer, so this symmetry is very much by design (see e.g., Chapter 5 )." class="ltx_graphics" id="F19.g1" src="chapters/chapter7/figs/crate_ae_backbone.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.19</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer of the encoder and decoder in a CRATE autoencoder backbone.<span class="ltx_text ltx_font_medium"> The encoder and decoder layers both feed their inputs through multi-head subspace self-attention and a dictionary learning or dictionary encoding step. Note that the encoder and decoder layers are symmetrically designed; the conceptual goal of each decoder layer is to invert an encoder layer, so this symmetry is very much by design (see e.g., <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</span></span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p">We use a CRATE encoder and decoder, depicted in <a class="ltx_ref" href="#F7" title="In Embedding. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.7</span></a>, though of course it is possible to use a regular transformer encoder and decoder. Details follow now.</p>
</div>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">The encoder.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px1.p1">
<p class="ltx_p">The encoder is the same as the CRATE encoder in <a class="ltx_ref" href="#S3.SS2" title="7.3.2 The CRATE Architecture ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.3.2</span></a>, with the caveat that there is no feature extractor <math alttext="f_{\theta}^{\mathrm{ext}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px1.p1.m1"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT</annotation></semantics></math>. However, both the embedding <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px1.p1.m2"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> and the backbone <math alttext="f_{\theta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px1.p1.m3"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> are the same.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">The decoder backbone.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px2.p1">
<p class="ltx_p">The decoder backbone is the CRATE decoder described in <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>. For completeness’ sake, we describe it now. Given a feature sequence <math alttext="\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})\in(\mathbb{R}^{d})^{*}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">)</mo></mrow><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X})\doteq f_{\theta}(\bm{X})\in(\mathbb{R}^{d})^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X ) ∈ ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, we can process it using the decoder backbone <math alttext="g_{\eta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m2"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> as follows. The function <math alttext="g_{\eta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m3"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math> is composed of <math alttext="L" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m4"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> layers <math alttext="g_{\eta}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m5"><semantics><msubsup><mi>g</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="g_{\eta}^{\mathrm{bb}}=g_{\eta}^{L}\circ\cdots\circ g_{\eta}^{1}." class="ltx_Math" display="block" id="S6.E2.m1"><semantics><mrow><mrow><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup><mo>=</mo><mrow><msubsup><mi>g</mi><mi>η</mi><mi>L</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>g</mi><mi>η</mi><mn>1</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{bb}}=g_{\eta}^{L}\circ\cdots\circ g_{\eta}^{1}.</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∘ ⋯ ∘ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The layer <math alttext="g_{\eta}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m6"><semantics><msubsup><mi>g</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> has the following implementation. First, define <math alttext="\tilde{\bm{Z}}_{\theta,\eta}^{1}(\bm{X})\doteq\bm{Z}_{\theta}(\bm{X})" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m7"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{\bm{Z}}_{\theta,\eta}^{1}(\bm{X})\doteq\bm{Z}_{\theta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>. Then, we obtain</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx99">
<tbody id="S6.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}(\bm{X})" class="ltx_Math" display="inline" id="S6.E3.m1"><semantics><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}(\bm{X})</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=[\tilde{\bm{D}}^{\ell}]^{\top}\operatorname{LN}_{\eta}^{1,\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell}(\bm{X}))" class="ltx_Math" display="inline" id="S6.E3.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msup><mover accent="true"><mi>𝑫</mi><mo>~</mo></mover><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><msubsup><mi>LN</mi><mi>η</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=[\tilde{\bm{D}}^{\ell}]^{\top}\operatorname{LN}_{\eta}^{1,\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell}(\bm{X}))</annotation><annotation encoding="application/x-llamapun">= [ over~ start_ARG bold_italic_D end_ARG start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT roman_LN start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 , roman_ℓ end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_X ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.3)</span></td>
</tr></tbody>
<tbody id="S6.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S6.E4.m1"><semantics><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}(\bm{X})-\operatorname{MSSA}_{\eta}^{\ell}(\operatorname{LN}_{\eta}^{2,\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}))" class="ltx_Math" display="inline" id="S6.E4.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>MSSA</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>LN</mi><mi>η</mi><mrow><mn>2</mn><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}(\bm{X})-\operatorname{MSSA}_{\eta}^{\ell}(\operatorname{LN}_{\eta}^{2,\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1/2}))</annotation><annotation encoding="application/x-llamapun">= over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) - roman_MSSA start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( roman_LN start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 , roman_ℓ end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="g_{\eta}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m8"><semantics><msubsup><mi>g</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is defined such that <math alttext="g_{\eta}^{\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell})\doteq\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1}(\bm{X})" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m9"><semantics><mrow><mrow><msubsup><mi>g</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}^{\ell}(\tilde{\bm{Z}}_{\theta,\eta}^{\ell})\doteq\tilde{\bm{Z}}_{\theta,\eta}^{\ell+1}(\bm{X})</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ≐ over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ( bold_italic_X )</annotation></semantics></math>. Here, the relevant concept is that <math alttext="g_{\eta}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m10"><semantics><msubsup><mi>g</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> should learn an approximate inverse of <math alttext="f_{\theta}^{L+1-\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m11"><semantics><msubsup><mi>f</mi><mi>θ</mi><mrow><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><annotation encoding="application/x-tex">f_{\theta}^{L+1-\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L + 1 - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, as discretizations of a forward- and reverse-time diffusion process, respectively. In particular, <math alttext="\tilde{\bm{D}}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m12"><semantics><msup><mover accent="true"><mi>𝑫</mi><mo>~</mo></mover><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\tilde{\bm{D}}^{\ell}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_D end_ARG start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> should approximate <math alttext="\bm{D}^{L+1-\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m13"><semantics><msup><mi>𝑫</mi><mrow><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><annotation encoding="application/x-tex">\bm{D}^{L+1-\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT italic_L + 1 - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, and similarly the <math alttext="\operatorname{MSSA}_{\eta}^{\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m14"><semantics><msubsup><mi>MSSA</mi><mi>η</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\operatorname{MSSA}_{\eta}^{\ell}</annotation><annotation encoding="application/x-llamapun">roman_MSSA start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> parameters should be similar to the parameters of <math alttext="\operatorname{MSSA}_{\theta}^{L+1-\ell}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m15"><semantics><msubsup><mi>MSSA</mi><mi>θ</mi><mrow><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><annotation encoding="application/x-tex">\operatorname{MSSA}_{\theta}^{L+1-\ell}</annotation><annotation encoding="application/x-llamapun">roman_MSSA start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L + 1 - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>. The output is <math alttext="\tilde{\bm{Z}}_{\theta,\eta}\doteq\tilde{\bm{Z}}_{\theta,\eta}^{L+1}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.p1.m16"><semantics><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo>≐</mo><msubsup><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tilde{\bm{Z}}_{\theta,\eta}\doteq\tilde{\bm{Z}}_{\theta,\eta}^{L+1}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ≐ over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">The un-embedding module.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px3.p1">
<p class="ltx_p">To transform <math alttext="\tilde{\bm{Z}}_{\theta,\eta}(\bm{X})" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\tilde{\bm{Z}}_{\theta,\eta}(\bm{X})</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> back into an estimate for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we need to undo the effect of the embedding module <math alttext="f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m3"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> using the unembedding module <math alttext="g_{\eta}^{\mathrm{unemb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m4"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{unemb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT</annotation></semantics></math>. As such, harkening back to the functional form of the embedding module in (<a class="ltx_ref" href="#S2.E11" title="Equation 7.2.11 ‣ Embedding. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.11</span></a>), i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq\begin{bmatrix}\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}f^{\mathrm{patch}}(\bm{X})+\bm{E}^{\mathrm{pos}}\end{bmatrix}" class="ltx_Math" display="block" id="S6.E5.m1"><semantics><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>[</mo><mtable displaystyle="true"><mtr><mtd><mrow><msubsup><mi>𝒛</mi><mi>cls</mi><mn>1</mn></msubsup><mo>,</mo><mrow><mrow><msup><mi>𝑾</mi><mi>emb</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>f</mi><mi>patch</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝑬</mi><mi>pos</mi></msup></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{emb}}(\bm{X})\doteq\begin{bmatrix}\bm{z}_{\mathrm{cls}}^{1},\bm{W}^{\mathrm{emb}}f^{\mathrm{patch}}(\bm{X})+\bm{E}^{\mathrm{pos}}\end{bmatrix}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ≐ [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ( bold_italic_X ) + bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">it implies that our inverse operation <math alttext="g_{\eta}^{\mathrm{unemb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m5"><semantics><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{unemb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT</annotation></semantics></math> looks like the following:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="g_{\eta}^{\mathrm{unemb}}(\tilde{\bm{Z}})\doteq g_{\eta}^{\mathrm{unemb}}(\begin{bmatrix}\tilde{\bm{z}}^{1},\dots,\tilde{\bm{z}}^{n}\end{bmatrix})=g^{\mathrm{unpatch}}(\bm{W}^{\mathrm{unemb}}([\tilde{\bm{z}}^{2},\dots,\tilde{\bm{z}}^{n}]-\tilde{\bm{E}}^{\mathrm{pos}}))," class="ltx_Math" display="block" id="S6.E6.m1"><semantics><mrow><mrow><mrow><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo>[</mo><mtable displaystyle="true"><mtr><mtd><mrow><msup><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>n</mi></msup></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>g</mi><mi>unpatch</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑾</mi><mi>unemb</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">[</mo><msup><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mn>2</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>n</mi></msup><mo stretchy="false">]</mo></mrow><mo>−</mo><msup><mover accent="true"><mi>𝑬</mi><mo>~</mo></mover><mi>pos</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">g_{\eta}^{\mathrm{unemb}}(\tilde{\bm{Z}})\doteq g_{\eta}^{\mathrm{unemb}}(\begin{bmatrix}\tilde{\bm{z}}^{1},\dots,\tilde{\bm{z}}^{n}\end{bmatrix})=g^{\mathrm{unpatch}}(\bm{W}^{\mathrm{unemb}}([\tilde{\bm{z}}^{2},\dots,\tilde{\bm{z}}^{n}]-\tilde{\bm{E}}^{\mathrm{pos}})),</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_Z end_ARG ) ≐ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ( [ start_ARG start_ROW start_CELL over~ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , over~ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] ) = italic_g start_POSTSUPERSCRIPT roman_unpatch end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ( [ over~ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , … , over~ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ] - over~ start_ARG bold_italic_E end_ARG start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="g^{\mathrm{unpatch}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m6"><semantics><msup><mi>g</mi><mi>unpatch</mi></msup><annotation encoding="application/x-tex">g^{\mathrm{unpatch}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUPERSCRIPT roman_unpatch end_POSTSUPERSCRIPT</annotation></semantics></math> does the inverse operation of the unrolling and flattening operation that <math alttext="f^{\mathrm{patch}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.m7"><semantics><msup><mi>f</mi><mi>patch</mi></msup><annotation encoding="application/x-tex">f^{\mathrm{patch}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT</annotation></semantics></math> does.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>Again, the “inverse positional encoding” <math alttext="\tilde{\bm{E}}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="footnote16.m1"><semantics><msup><mover accent="true"><mi>𝑬</mi><mo>~</mo></mover><mi>pos</mi></msup><annotation encoding="application/x-tex">\tilde{\bm{E}}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_E end_ARG start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> is learned for a large input, and for smaller inputs may be interpolated. It is even possible to directly set <math alttext="\tilde{\bm{E}}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="footnote16.m2"><semantics><msup><mover accent="true"><mi>𝑬</mi><mo>~</mo></mover><mi>pos</mi></msup><annotation encoding="application/x-tex">\tilde{\bm{E}}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_E end_ARG start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> equal to the positional encoding <math alttext="\bm{E}^{\mathrm{pos}}" class="ltx_Math" display="inline" id="footnote16.m3"><semantics><msup><mi>𝑬</mi><mi>pos</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\mathrm{pos}}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT</annotation></semantics></math> and use the same interpolated positional encodings for each input in both the encoder and decoder.</span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS2.SSS0.Px3.p2">
<p class="ltx_p">This architecture is a white-box autoencoder <math alttext="(f_{\theta},g_{\eta})" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p2.m1"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>f</mi><mi>θ</mi></msub><mo>,</mo><msub><mi>g</mi><mi>η</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f_{\theta},g_{\eta})</annotation><annotation encoding="application/x-llamapun">( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT )</annotation></semantics></math> where (recall) <math alttext="f_{\theta}=f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p2.m2"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>bb</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>f</mi><mi>θ</mi><mi>emb</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">f_{\theta}=f_{\theta}^{\mathrm{bb}}\circ f_{\theta}^{\mathrm{emb}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="g_{\eta}=g_{\eta}^{\mathrm{unemb}}\circ g_{\eta}^{\mathrm{bb}}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p2.m3"><semantics><mrow><msub><mi>g</mi><mi>η</mi></msub><mo>=</mo><mrow><msubsup><mi>g</mi><mi>η</mi><mi>unemb</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msubsup><mi>g</mi><mi>η</mi><mi>bb</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">g_{\eta}=g_{\eta}^{\mathrm{unemb}}\circ g_{\eta}^{\mathrm{bb}}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ∘ italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT</annotation></semantics></math>. In particular, we can use it to compute an estimate for a masked view <math alttext="\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{m})=(g_{\eta}\circ f_{\eta})(\bm{X}_{m})" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p2.m4"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>g</mi><mi>η</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>η</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{m})=(g_{\eta}\circ f_{\eta})(\bm{X}_{m})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) = ( italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> which should approximately equal <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p2.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> itself.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.3 </span>Optimization</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p">As in <a class="ltx_ref" href="#S3.SS3" title="7.3.3 Optimization ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.3.3</span></a>, we use a simple optimization setup: we sample images and masks, compute the loss on those samples and the gradients of this loss, and update the parameters using a generic optimization algorithm and the aforementioned gradients. For each timestep <math alttext="k" class="ltx_Math" display="inline" id="S6.SS3.p1.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>, we:</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p">Subsample <math alttext="B" class="ltx_Math" display="inline" id="S6.I1.i1.p1.m1"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation><annotation encoding="application/x-llamapun">italic_B</annotation></semantics></math> different samples <math alttext="\{\bm{X}_{b}^{(k)}\}_{b=1}^{B}\subseteq\mathcal{I}" class="ltx_Math" display="inline" id="S6.I1.i1.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></msubsup><mo>⊆</mo><mi class="ltx_font_mathcaligraphic">ℐ</mi></mrow><annotation encoding="application/x-tex">\{\bm{X}_{b}^{(k)}\}_{b=1}^{B}\subseteq\mathcal{I}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ⊆ caligraphic_I</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p">For each sample <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S6.I1.i2.p1.m1"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math>, compute a different randomized resized crop and mask <math alttext="v_{b,m}^{(k)}" class="ltx_Math" display="inline" id="S6.I1.i2.p1.m2"><semantics><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>m</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">v_{b,m}^{(k)}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_b , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> and apply it to <math alttext="\bm{X}_{b}^{(k)}" class="ltx_Math" display="inline" id="S6.I1.i2.p1.m3"><semantics><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{X}_{b}^{(k)}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT</annotation></semantics></math> to get <math alttext="\bm{X}_{b,m}^{(k)}\doteq v_{b,m}^{t}(\bm{X}_{b}^{(k)})" class="ltx_Math" display="inline" id="S6.I1.i2.p1.m4"><semantics><mrow><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>m</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≐</mo><mrow><msubsup><mi>v</mi><mrow><mi>b</mi><mo>,</mo><mi>m</mi></mrow><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{b,m}^{(k)}\doteq v_{b,m}^{t}(\bm{X}_{b}^{(k)})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_b , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ≐ italic_v start_POSTSUBSCRIPT italic_b , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p">Compute the estimated autoencoding <math alttext="\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{b,r}^{(k)})\doteq(g_{\eta}\circ f_{\theta})(\bm{X}_{b,r}^{(k)})" class="ltx_Math" display="inline" id="S6.I1.i3.p1.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>r</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>g</mi><mi>η</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>r</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{b,r}^{(k)})\doteq(g_{\eta}\circ f_{\theta})(\bm{X}_{b,r}^{(k)})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ≐ ( italic_g start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p">Form the surrogate stochastic loss</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathcal{L}}_{\mathrm{MAE}}^{(k)}(\theta,\eta)\doteq\frac{1}{B}\sum_{b=1}^{B}\|\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{b,r}^{(k)})-\bm{X}_{b}^{(k)}\|_{F}^{2}." class="ltx_Math" display="block" id="S6.E7.m1"><semantics><mrow><mrow><mrow><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>MAE</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mi>B</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>θ</mi><mo>,</mo><mi>η</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑿</mi><mrow><mi>b</mi><mo>,</mo><mi>r</mi></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msubsup><mi>𝑿</mi><mi>b</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\mathcal{L}}_{\mathrm{MAE}}^{(k)}(\theta,\eta)\doteq\frac{1}{B}\sum_{b=1}^{B}\|\hat{\bm{X}}_{\theta,\eta}(\bm{X}_{b,r}^{(k)})-\bm{X}_{b}^{(k)}\|_{F}^{2}.</annotation><annotation encoding="application/x-llamapun">over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ , italic_η ) ≐ divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ∑ start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ∥ over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_θ , italic_η end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) - bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.7)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p">Compute one step of an optimization algorithm on <math alttext="(\theta,\eta)" class="ltx_Math" display="inline" id="S6.I1.i5.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\theta,\eta)</annotation><annotation encoding="application/x-llamapun">( italic_θ , italic_η )</annotation></semantics></math>, giving the following iteration:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\theta^{(k+1)},\eta^{(k+1)})\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)},\eta^{(k)};\nabla_{(\theta,\eta)}\hat{\mathcal{L}}_{\mathrm{MAE}}^{(k)})." class="ltx_Math" display="block" id="S6.E8.m1"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>η</mi><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo>≐</mo><mrow><msup><mtext class="ltx_font_smallcaps">OptUpdate</mtext><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>η</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>;</mo><mrow><msub><mo rspace="0.167em">∇</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></msub><msubsup><mover accent="true"><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>^</mo></mover><mi>MAE</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\theta^{(k+1)},\eta^{(k+1)})\doteq\textsc{OptUpdate}^{(k)}(\theta^{(k)},\eta^{(k)};\nabla_{(\theta,\eta)}\hat{\mathcal{L}}_{\mathrm{MAE}}^{(k)}).</annotation><annotation encoding="application/x-llamapun">( italic_θ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , italic_η start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ) ≐ OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_θ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_η start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ∇ start_POSTSUBSCRIPT ( italic_θ , italic_η ) end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.8)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.4 </span>Evaluation</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p">This is the first autoencoder network we discuss in this chapter. We use the same center crop view <math alttext="v_{\mathrm{cc}}" class="ltx_Math" display="inline" id="S6.SS4.p1.m1"><semantics><msub><mi>v</mi><mi>cc</mi></msub><annotation encoding="application/x-tex">v_{\mathrm{cc}}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT</annotation></semantics></math> as in <a class="ltx_ref" href="#S2.SS5" title="7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">7.2.5</span></a> and <a class="ltx_ref" href="#S3.SS4" title="7.3.4 Evaluation Methodology ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.3.4</span></a>, resizing the final image to a square with side length <math alttext="S_{\mathrm{cc}}=S_{\mathrm{mask}}" class="ltx_Math" display="inline" id="S6.SS4.p1.m2"><semantics><mrow><msub><mi>S</mi><mi>cc</mi></msub><mo>=</mo><msub><mi>S</mi><mi>mask</mi></msub></mrow><annotation encoding="application/x-tex">S_{\mathrm{cc}}=S_{\mathrm{mask}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT</annotation></semantics></math> pixels so as to match the shapes of the input images seen during training.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p">On top of evaluating the masked autoencoding loss itself, it is also possible to evaluate the features <math alttext="\bm{Z}_{\theta}(\bm{X}_{\mathrm{cc}})" class="ltx_Math" display="inline" id="S6.SS4.p2.m1"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{\mathrm{cc}})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT )</annotation></semantics></math> of the view <math alttext="\bm{X}_{\mathrm{cc}}\doteq v_{\mathrm{cc}}(\bm{X})" class="ltx_Math" display="inline" id="S6.SS4.p2.m2"><semantics><mrow><msub><mi>𝑿</mi><mi>cc</mi></msub><mo>≐</mo><mrow><msub><mi>v</mi><mi>cc</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{\mathrm{cc}}\doteq v_{\mathrm{cc}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ≐ italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math> of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S6.SS4.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> directly. For the sake of attention map fidelity evaluation, just obtaining <math alttext="\bm{Z}_{\theta}(\bm{X}_{\mathrm{cc}})" class="ltx_Math" display="inline" id="S6.SS4.p2.m4"><semantics><mrow><msub><mi>𝒁</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{\theta}(\bm{X}_{\mathrm{cc}})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT )</annotation></semantics></math> is enough, but for linear probing we need to extract a summarized or aggregate feature from <math alttext="\bm{Z}_{\theta}" class="ltx_Math" display="inline" id="S6.SS4.p2.m5"><semantics><msub><mi>𝒁</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>. To do this, we can use a (parameter-free) feature extraction map which just returns the feature corresponding to the class token, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta}^{\mathrm{ext}}(\bm{Z})\doteq f_{\theta}^{\mathrm{ext}}([\bm{z}^{1},\dots,\bm{z}^{n}])=\bm{z}^{1}," class="ltx_Math" display="block" id="S6.E9.m1"><semantics><mrow><mrow><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>𝒛</mi><mi>n</mi></msup><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>𝒛</mi><mn>1</mn></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">f_{\theta}^{\mathrm{ext}}(\bm{Z})\doteq f_{\theta}^{\mathrm{ext}}([\bm{z}^{1},\dots,\bm{z}^{n}])=\bm{z}^{1},</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z ) ≐ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( [ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , bold_italic_z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ] ) = bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7.6.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">as in (for example) <a class="ltx_ref" href="#S3.SS1" title="7.3.1 Task and Objective ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">7.3.1</span></a> and <a class="ltx_ref" href="#S3.SS2" title="7.3.2 The CRATE Architecture ‣ 7.3 Image Classification ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.3.2</span></a>. With this, we have a way to obtain aggregate features <math alttext="\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{\mathrm{cc}})" class="ltx_Math" display="inline" id="S6.SS4.p2.m6"><semantics><mrow><mrow><msub><mi>𝒛</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>ext</mi></msubsup><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mi>θ</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>cc</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{\theta}(\bm{X}_{\mathrm{cc}})\doteq(f_{\theta}^{\mathrm{ext}}\circ f_{\theta})(\bm{X}_{\mathrm{cc}})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ≐ ( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT )</annotation></semantics></math>, at which point we can perform linear probing, segmentation evaluations, and so on.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6.5 </span>Experiments</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p">Since CRATE-MAE is directly based on the ViT-MAE, we compare the optimal settings for ViT-MAE as given by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx101" title="">HCX+22</a>]</cite> with the same settings applied to CRATE-MAE for fair comparison.</p>
</div>
<section class="ltx_paragraph" id="S6.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Model architecture.</h5>
<div class="ltx_para" id="S6.SS5.SSS0.Px1.p1">
<p class="ltx_p">During training, the masked crop <math alttext="v_{m}" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m1"><semantics><msub><mi>v</mi><mi>m</mi></msub><annotation encoding="application/x-tex">v_{m}</annotation><annotation encoding="application/x-llamapun">italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> resizes the whole image so that the shorter edge is of size <math alttext="256" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m2"><semantics><mn>256</mn><annotation encoding="application/x-tex">256</annotation><annotation encoding="application/x-llamapun">256</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{rsz}}=256" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m3"><semantics><mrow><msub><mi>S</mi><mi>rsz</mi></msub><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{rsz}}=256</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256</annotation></semantics></math>) before taking a random crop of size <math alttext="224\times 224" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m4"><semantics><mrow><mn>224</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation><annotation encoding="application/x-llamapun">224 × 224</annotation></semantics></math> (i.e., <math alttext="S_{\mathrm{mask}}=224" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m5"><semantics><mrow><msub><mi>S</mi><mi>mask</mi></msub><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">S_{\mathrm{mask}}=224</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = 224</annotation></semantics></math>), and masking <math alttext="p_{\mathrm{mask}}=\frac{3}{4}" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m6"><semantics><mrow><msub><mi>p</mi><mi>mask</mi></msub><mo>=</mo><mfrac><mn>3</mn><mn>4</mn></mfrac></mrow><annotation encoding="application/x-tex">p_{\mathrm{mask}}=\frac{3}{4}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = divide start_ARG 3 end_ARG start_ARG 4 end_ARG</annotation></semantics></math> of the patches. We take patch size <math alttext="16" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m7"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation><annotation encoding="application/x-llamapun">16</annotation></semantics></math> (i.e., <math alttext="P_{H}=P_{W}=16" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m8"><semantics><mrow><msub><mi>P</mi><mi>H</mi></msub><mo>=</mo><msub><mi>P</mi><mi>W</mi></msub><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">P_{H}=P_{W}=16</annotation><annotation encoding="application/x-llamapun">italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16</annotation></semantics></math>). We use the small and base variants of the ViT-MAE architecture as the embedding and backbone for both the encoder and decoder, swapping out the MHSA and MLP components for MSSA, ISTA, and linear layers respectively. We use the same number of heads and head dimension in the case of MSSA. However, the original ViT-MAE uses an encoder which uses nearly all the total layers and a decoder which only uses a few layers; we allocate half the total number of layers (which stays the same from ViT-MAE to CRATE-MAE) to our encoder and decoder, as suggested by the conceptual and theoretical framework in <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>. For CRATE-MAE we set <math alttext="(\beta,\lambda)=(1,0.1)" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px1.p1.m9"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>β</mi><mo>,</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\beta,\lambda)=(1,0.1)</annotation><annotation encoding="application/x-llamapun">( italic_β , italic_λ ) = ( 1 , 0.1 )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Datasets and optimization.</h5>
<div class="ltx_para" id="S6.SS5.SSS0.Px2.p1">
<p class="ltx_p">For pre-training, we use the ImageNet-1K dataset. We use the AdamW optimizer to pre-train both our ViT-MAE replication as well as CRATE-MAE. We set the base learning rate as <math alttext="3\times 10^{-5}" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m1"><semantics><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3\times 10^{-5}</annotation><annotation encoding="application/x-llamapun">3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, the weight decay as <math alttext="0.1" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m2"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation><annotation encoding="application/x-llamapun">0.1</annotation></semantics></math>, and batch size as <math alttext="B=4096" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m3"><semantics><mrow><mi>B</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">B=4096</annotation><annotation encoding="application/x-llamapun">italic_B = 4096</annotation></semantics></math>. Our learning rate schedule increases the learning rate linearly to the base learning rate over the first <math alttext="40" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m4"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation><annotation encoding="application/x-llamapun">40</annotation></semantics></math> epochs, and decreases to <math alttext="0" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m5"><mn>0</mn></math> using a cosine schedule over the next <math alttext="760" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m6"><semantics><mn>760</mn><annotation encoding="application/x-tex">760</annotation><annotation encoding="application/x-llamapun">760</annotation></semantics></math> epochs (training all models for <math alttext="800" class="ltx_Math" display="inline" id="S6.SS5.SSS0.Px2.p1.m7"><semantics><mn>800</mn><annotation encoding="application/x-tex">800</annotation><annotation encoding="application/x-llamapun">800</annotation></semantics></math> epochs each). For pre-training, we apply the usual regime of data augmentations (flips, Gaussian blurs, solarization, etc) to the image data.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS0.Px2.p2">
<p class="ltx_p">For linear probing, we use several evaluation datasets such as CIFAR10, CIFAR100, Oxford-Flowers, and Oxford-IIT-Pets. For linear probing, we precompute the feature of all samples in the target dataset and apply a fast linear regression solver, e.g., from a standard package such as Scikit-Learn.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Experiment results.</h5>
<div class="ltx_para" id="S6.SS5.SSS0.Px3.p1">
<p class="ltx_p"><a class="ltx_ref" href="#T12" title="In Experiment results. ‣ 7.6.5 Experiments ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7.12</span></a> demonstrates that CRATE-MAE models achieve, roughly speaking, parity compared to the popular ViT-MAE architecture at similar parameter counts, and also that the feature learning performance (as measured by performance on downstream classification tasks) increases with scale. Meanwhile, <a class="ltx_ref" href="#F20" title="In Experiment results. ‣ 7.6.5 Experiments ‣ 7.6 Masked Autoencoding for Imagery Data ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7.20</span></a> demonstrates that the encoder saliency maps (and therefore the fine-grained features learned by the encoder) indeed isolate and highlight the key parts of the input image.</p>
</div>
<figure class="ltx_table" id="T12">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CRATE-MAE-S(mall)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">CRATE-MAE-B(ase)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">ViT-MAE-S</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">ViT-MAE-B</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"># parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">25.4M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">44.6M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">47.6M</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="color:#808080;">143.8M</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CIFAR10</th>
<td class="ltx_td ltx_align_center ltx_border_t">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.9</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">79.9</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span class="ltx_text" style="color:#808080;">87.9</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CIFAR100</th>
<td class="ltx_td ltx_align_center">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_r">60.1</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;">62.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;">68.0</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Oxford Flowers-102</th>
<td class="ltx_td ltx_align_center">57.7</td>
<td class="ltx_td ltx_align_center ltx_border_r">61.8</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="color:#808080;">66.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text" style="color:#808080;">66.4</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Oxford-IIIT-Pets</th>
<td class="ltx_td ltx_align_center ltx_border_bb">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">51.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text" style="color:#808080;">80.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 7.12</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Linear probing classification accuracy of CRATE-MAE and ViT-MAE<span class="ltx_text ltx_font_medium"> on various datasets with different model sizes when the backbone is pre-trained for masked autoencoding on ImageNet-1K. Given the same parameter count, CRATE-MAE achieves roughly similar performance, while simultaneously enjoying a simpler and more principled architecture design.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="F20"><img alt="Figure 7.20 : Saliency maps of CRATE-MAE. Each pair of images consists of the original image (left) and a selected saliency map (right) corresponding to an attention head in the last layer. As is usual for CRATE models, but unusual for general transformer-like models, the saliency maps correspond to the objects in the input image." class="ltx_graphics" id="F20.g1" src="chapters/chapter7/figs/crate_mae_semantic_heads.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7.20</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Saliency maps of CRATE-MAE.<span class="ltx_text ltx_font_medium"> Each pair of images consists of the original image (left) and a selected saliency map (right) corresponding to an attention head in the last layer. As is usual for CRATE models, but unusual for general transformer-like models, the saliency maps correspond to the objects in the input image.</span></span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.7 </span>Summary and Notes</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p">All work in this chapter is downstream of the Transformer architecture, which was introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite>. The Transformer architecture is formally described in <a class="ltx_ref" href="#S2" title="7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a>. A main empirical innovation in recent years, spurred by the prevalence and performance of the transformer architecture, is to formulate a given learning problem as a sequence-to-sequence problem and apply the transformer architecture. This has enabled the transformer architecture to be essentially ubiquitous in (almost) all deep learning applications. As such, direct improvements to the transformer can propagate to become solutions to many problems and have considerable impact; similarly, we can apply our white-box understanding of transformer-like architectures to many modern problems. The material covered in this Chapter is merely a subset of the work which has already been done; other work includes masked completion for text data (i.e., BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx70" title="">DCL+19</a>, <a class="ltx_ref" href="bib.html#bibx311" title="">YBP+24</a>]</cite>, (mechanistic) interpretability of language and vision models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx11" title="">BM24</a>]</cite>, and error correcting coding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx323" title="">ZLG+</a>]</cite>. There is much more to do.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p">There is also much more theory specifically about the practice of scaling neural networks, which is enormously practically viable, and we at least remark on it here. This line of work was popularized by the “Tensor Programs” line of work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx305" title="">YHB+22</a>]</cite>. The basic prescription is that we want the initial gradient updates in a transformer to be constant size, and by working through the backpropagation equations (<a class="ltx_ref" href="A1.html" title="Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>) carefully, we can determine the scale of the initialization and learning rates (chosen layer-wise) that are required to achieve this. In practice, such prescriptions greatly increase the stability and convergence of training at large scales; they also prescribe a way to find the “optimal”<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>The word “optimal” is used in quotes because the work on this merely uses some desiderata about the weight size, feature size, and gradient size at initialization to determine “optimality”, as opposed to, say, the test loss at convergence.</span></span></span> hyperparameters for large-scale training using only small-scale training. Follow-ups to this work attempt to accommodate the feature geometry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx20" title="">BN24</a>]</cite>, which could be informed by the work in this book about representation learning. Other follow-ups incorporate this weight-wise information into the optimizer itself to obtain these scaling benefits automatically, obtaining optimizers such as Muon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx127" title="">JJB+</a>]</cite>, which have recently been used for training trillion-parameter models very stably <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx258" title="">Tea</a>]</cite>. Overall, the two approaches to deep learning theory are orthogonal or complementary.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7.8 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 7.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<p class="ltx_p">Read the DINO paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx39" title="">CTM+21</a>]</cite>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 7.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<p class="ltx_p">DINO v2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx204" title="">ODM+23</a>]</cite> uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of images with similar local information are similar. Formulate an optimization problem which promotes this in the encoder, and implement it.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 7.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise3.p1">
<p class="ltx_p">This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations.</p>
<ol class="ltx_enumerate" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p">Propose an alternative to the term involving <math alttext="R_{\varepsilon}" class="ltx_Math" display="inline" id="S8.I1.i1.p1.m1"><semantics><msub><mi>R</mi><mi>ε</mi></msub><annotation encoding="application/x-tex">R_{\varepsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT</annotation></semantics></math> in (<a class="ltx_ref" href="#S2.E34" title="Equation 7.2.34 ‣ 4th item ‣ Optimizing SimDINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.34</span></a>) for approximating the covariance regularization term in (<a class="ltx_ref" href="#S2.E33" title="Equation 7.2.33 ‣ Optimizing SimDINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.33</span></a>). Evaluate the time complexity required to compute your proposed term and its gradient. Include analysis for computing it on a single compute node vs. multiple nodes.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p">Evaluate the time complexity required to compute the existing term in (<a class="ltx_ref" href="#S2.E34" title="Equation 7.2.34 ‣ 4th item ‣ Optimizing SimDINO. ‣ 7.2.4 Optimization Strategy ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.34</span></a>) and its gradient.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 7.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise4.p1">
<p class="ltx_p">Prove that (<a class="ltx_ref" href="#S2.E37" title="Equation 7.2.37 ‣ Linear probing. ‣ 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.37</span></a>) and (<a class="ltx_ref" href="#S2.E38" title="Equation 7.2.38 ‣ Linear probing. ‣ 7.2.5 Evaluation Methodology ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.38</span></a>) are convex optimization problems.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 7.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise5.p1">
<p class="ltx_p"><span class="ltx_text ltx_phantom"><span style="visibility:hidden"></span></span></p>
<ol class="ltx_enumerate" id="S8.I2">
<li class="ltx_item" id="S8.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S8.I2.i1.p1">
<p class="ltx_p">Implement the CRATE and CRATE-<math alttext="\alpha" class="ltx_Math" display="inline" id="S8.I2.i1.p1.m1"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation><annotation encoding="application/x-llamapun">italic_α</annotation></semantics></math> models.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S8.I2.i2.p1">
<p class="ltx_p">Compare their performance and efficiency on the CIFAR-10 dataset.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S8.I2.i3.p1">
<p class="ltx_p">Compare their interpretability in two ways:</p>
<ul class="ltx_itemize" id="S8.I2.i3.I1">
<li class="ltx_item" id="S8.I2.i3.I0.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I2.i3.I0.i1.p1">
<p class="ltx_p">The sparsity <math alttext="\|\bm{Z}\|_{0}" class="ltx_Math" display="inline" id="S8.I2.i3.I0.i1.p1.m1"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><annotation encoding="application/x-tex">\|\bm{Z}\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> of the representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S8.I2.i3.I0.i1.p1.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i3.I0.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I2.i3.I0.i2.p1">
<p class="ltx_p">The attention maps <math alttext="\bm{a}_{\theta}^{k,\ell}" class="ltx_Math" display="inline" id="S8.I2.i3.I0.i2.p1.m1"><semantics><msubsup><mi>𝒂</mi><mi>θ</mi><mrow><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi></mrow></msubsup><annotation encoding="application/x-tex">\bm{a}_{\theta}^{k,\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_a start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 09:48:41 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
