<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 09:16:00 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>White-Box Deep Networks via Unrolled Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS1" title="In 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Deep Networks from Unrolled Gradient Descent</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px1" title="In 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Gradient Ascent for Coding Rate Reduction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px2" title="In 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Gradient-Guided Feature Map Increment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px3" title="In 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Deep Network for Optimizing Rate Reduction.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS2" title="In 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Convolutional Networks from Invariant Rate Reduction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px1" title="In 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">1D Serial Data and Shift Invariance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px2" title="In 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">A Fundamental Trade-off between Invariance and Sparsity.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px3" title="In 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Overall Network Architecture and Comparison.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>White-Box Transformers from Unrolled Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS1" title="In 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Unrolled Optimization for Sparse Rate Reduction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px1" title="In 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Objective for Learning a Structured and Compact Representation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px2" title="In 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Sparse Rate Reduction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px3" title="In 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">White-Box Network Architecture via Unrolled Optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px4" title="In 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Self-Attention as Gradient Descent on Coding Rate of Token Representations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px5" title="In 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">MLP as Proximal Gradient Descent for Sparse Coding of Token Representations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="In 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Overall White-Box Transformer Architecture: CRATE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Variants of Deep Architectures by Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS1" title="In 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Attention-Only Transformer Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px1" title="In 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Denoising Operator for Token Representations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px2" title="In 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Attention-Only Transformer.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="In 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Linear-Time Attention: Token Statistics Transformer</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px1" title="In 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">A New Variational Form for Coding Rates.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px2" title="In 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Model interpretation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px3" title="In 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Practical Implementation Details.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S4" title="In Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Summary and Notes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="In Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 4 </span>Deep Representations from Unrolled Optimization</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">White-Box Deep Networks via Unrolled Optimization</a><div class="mini-toc-sub"><a href="#S1.SS1">Deep Networks from Unrolled Gradient Descent</a><a href="#S1.SS2">Convolutional Networks from Invariant Rate Reduction</a></div></li><li><a href="#S2">White-Box Transformers from Unrolled Optimization</a><div class="mini-toc-sub"><a href="#S2.SS1">Unrolled Optimization for Sparse Rate Reduction</a><a href="#S2.SS2">Overall White-Box Transformer Architecture: CRATE</a></div></li><li><a href="#S3">Variants of Deep Architectures by Design</a><div class="mini-toc-sub"><a href="#S3.SS1">Attention-Only Transformer Architecture</a><a href="#S3.SS2">Linear-Time Attention: Token Statistics Transformer</a></div></li><li><a href="#S4">Summary and Notes</a></li><li><a href="#S5">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">What I cannot create, I do not understand</span>.”</p>
<p class="ltx_p">   — Richard Feynman</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">In previous chapters, we have shown how to identify low-dimensional structures in high-dimensional spaces, <span class="ltx_text ltx_font_italic">mainly focusing on linear structures</span>.
For example, we introduced principal component analysis (PCA) to learn the linear denoiser <math alttext="\hat{\bm{U}}^{\top}" class="ltx_Math" display="inline" id="p2.m1"><semantics><msup><mover accent="true"><mi>𝑼</mi><mo>^</mo></mover><mo>⊤</mo></msup><annotation encoding="application/x-tex">\hat{\bm{U}}^{\top}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> when the observed data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> follow the statistical model <math alttext="\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}" class="ltx_Math" display="inline" id="p2.m3"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε</annotation></semantics></math>.
In this setting, the learned representations are linearly transformed input data <math alttext="\hat{\bm{U}}^{\top}\bm{x}" class="ltx_Math" display="inline" id="p2.m4"><semantics><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>^</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><annotation encoding="application/x-tex">\hat{\bm{U}}^{\top}\bm{x}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x</annotation></semantics></math>.
Under the linear model assumption, one can learn the low-dimensional linear structure with efficient optimization algorithms and strong theoretical guarantees.
Moreover, the linear model assumption covers a wide range of applications and problems, including face recognition, magnetic resonance image recovery, and structure texture recovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">On the other hand, the linear model can be limited when dealing with real-world applications, especially when the input data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is complex, such as speech and natural languages, images and videos, and robotic motions. The low-dimensional distributions of such data are typically nonlinear.
How to deal with nonlinearity has a long history across different disciplines such as control theory, signal processing, and pattern recognition. There have been considerable efforts that try to extend methods and solutions for linear models to handle nonlinearity, including early effort to extend PCA to nonlinear PCA (as we will study in more detail in <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>). In most cases, the methods are designed based on certain assumptions about the data distributions and tailored to specific problems.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">More recently, deep neural networks have achieved remarkable success across a wide range of data and applications. A neural network</p>
<table class="ltx_equation ltx_eqn_table" id="S0.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f(\cdot,\bm{\theta})\colon\bm{x}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{z}^{0}\rightarrow\cdots\rightarrow\bm{z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{z}^{\ell+1}\rightarrow\cdots\to\bm{z}^{L}=\bm{z}." class="ltx_Math" display="block" id="S0.E1.m1"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mn>0</mn></msup></mover><msup><mi>𝒛</mi><mn>0</mn></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">→</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup></mover><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">→</mo><msup><mi>𝒛</mi><mi>L</mi></msup><mo>=</mo><mi>𝒛</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})\colon\bm{x}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{z}^{0}\rightarrow\cdots\rightarrow\bm{z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{z}^{\ell+1}\rightarrow\cdots\to\bm{z}^{L}=\bm{z}.</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ ) : bold_italic_x start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT → ⋯ → bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT → ⋯ → bold_italic_z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = bold_italic_z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.0.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">can learn effective features/representations for downstream applications. For example, a trained deep neural network <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="p4.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> can be applied to map images to feature vectors, that is, <math alttext="\bm{z}_{i}=f(\bm{x}_{i},\bm{\theta})" class="ltx_Math" display="inline" id="p4.m2"><semantics><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{i}=f(\bm{x}_{i},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_θ )</annotation></semantics></math>, while a linear classifier can be learned on top of such representations <math alttext="\{\bm{z}_{i}\}" class="ltx_Math" display="inline" id="p4.m3"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{z}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math>. One notable breakthrough is AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>, a deep convolutional neural network trained with more than a million natural images, outperforming all previous approaches that were based on hand-crafted features. One of the key differences between AlexNet and previous approaches is that the former <span class="ltx_text ltx_font_italic">learns parameters of the nonlinear transformation from massive amounts of data</span> trained with back-propagation (BP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx236" title="">RHW86a</a>]</cite>, as detailed in <a class="ltx_ref" href="A1.html#S2.SS3" title="A.2.3 Back Propagation ‣ A.2 Computing Gradients via Automatic Differentiation ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2.3</span></a> of <a class="ltx_ref" href="A1.html" title="Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p">Subsequent popular practice models the mapping <math alttext="f" class="ltx_Math" display="inline" id="p5.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> with other empirically designed artificial deep neural networks and learns the parameters <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="p5.m2"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math> from random initialization via BP. Starting with the AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>, the architectures of modern deep networks continue to be empirically revised and improved. Network architectures such as VGG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx245" title="">SZ14</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx103" title="">HZR+16a</a>]</cite>, DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx111" title="">HLV+17</a>]</cite>, CNN, RNN or LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx108" title="">HS97</a>]</cite>, Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite>, and a mixture of experts (MoE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx244" title="">SMM+17</a>, <a class="ltx_ref" href="bib.html#bibx83" title="">FZS22</a>]</cite>, etc. have continued to push the performance envelope. As part of the effort to improve the performance of deep networks, almost every component of the networks has been empirically scrutinized, and various revisions and improvements have been proposed. They are not limited to nonlinear activation functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx175" title="">MHN13</a>, <a class="ltx_ref" href="bib.html#bibx139" title="">KUM+17</a>, <a class="ltx_ref" href="bib.html#bibx304" title="">XWC+15</a>, <a class="ltx_ref" href="bib.html#bibx199" title="">NIG+18</a>]</cite>, skip connections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx232" title="">RFB15</a>, <a class="ltx_ref" href="bib.html#bibx103" title="">HZR+16a</a>]</cite>, normalizations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx121" title="">IS15</a>, <a class="ltx_ref" href="bib.html#bibx10" title="">BKH16</a>, <a class="ltx_ref" href="bib.html#bibx269" title="">UVL16</a>, <a class="ltx_ref" href="bib.html#bibx301" title="">WH18</a>, <a class="ltx_ref" href="bib.html#bibx189" title="">MKK+18</a>]</cite>, up/down sampling or pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx240" title="">SMB10</a>]</cite>, convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx151" title="">LBB+98a</a>, <a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>, etc.
However, almost all such modifications have been developed through years of empirical trial and error or ablation studies. Some recent practices even take to the extreme by searching for effective network structures and training strategies through extensive random search techniques, such as Neural Architecture Search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx328" title="">ZL17</a>, <a class="ltx_ref" href="bib.html#bibx12" title="">BGN+17</a>]</cite>, AutoML <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx115" title="">HKV19</a>]</cite>, and Learning to Learn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx5" title="">ADG+16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">Despite the wide application of deep neural networks, it is not clear what the underlying design principles of such a constructed network are. In particular, it is not clear what mathematical function each layer of the network performs. In this chapter, based on the results from previous chapters, we develop a principled framework that will provide a fully rigorous mathematical interpretation of the role of a deep network, including its individual layers and the network as a whole.</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p">To understand deep networks and how they should be better designed, we must start with the objective of representation learning. In previous chapters, we have argued that the objective is to identify the intrinsically low-dimensional data distribution and then transform it to a compact and structured (say piecewise linear) representation. As we have seen in the previous chapter, the general approach to identifying a low-dimensional data distribution is through a compression process that progressively minimizes the entropy or coding rate of the distribution. However, up to this point, we have been using empirically designed deep networks to model or approximate the operations that aim to optimize these objectives, such as the score function for denoising (in <a class="ltx_ref" href="Ch1.html#S3.SS1.SSSx3" title="General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1.3.1</span></a>) or the transformation that maximizes the rate reduction (in <a class="ltx_ref" href="Ch3.html#S4.SS3" title="3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4.3</span></a>).</p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p">As we have argued in the previous chapter, <a class="ltx_ref" href="Ch3.html#S4" title="3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a> in particular, one can measure the goodness of the resulting representation by the information of the representation gained from a “lazy” representation which models all data as one big Gaussian.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>that we have seen in the previous chapter as one particular choice of interpretation of the sampled dataset.</span></span></span> In particular, if <span class="ltx_text ltx_font_italic">we use a mixture of Gaussians (subspaces)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright">2</span></span><span class="ltx_text ltx_font_upright">which we have studied thoroughly in the previous chapter.</span></span></span></span> as prototypical distributions to approximate the non-linear distribution of interest</span>, then we can efficiently measure the coding rate of such a representation using the (sum of) rate distortion functions of the associated Gaussians. Then the amount of <span class="ltx_text ltx_font_italic">information gained</span> or (relative) entropy reduced with such a modeling can be measured by the difference between the coding rate for the lazy representation and that for the more refined representation. Then, the objective of representation learning is to maximize this information gain, also known as the rate reduction objective.</p>
</div>
<div class="ltx_para" id="p9">
<p class="ltx_p">As we will see in this chapter, once the objective of representation learning is clear, the role of a deep neural network is precisely to help optimize the objective iteratively. Each layer of a deep neural network can be naturally derived as an iterative optimization step to incrementally maximize the information gain, including the popular architectures of ResNet, CNN, and Transformer, and other more advanced variants. In particular, this chapter aims to answer the following questions about deep networks:</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p"><a class="ltx_ref" href="#S1" title="4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> — given a measure of goodness for a learned representation, how to construct the nonlinear mapping from the data to the optimal representation via unrolled optimization for the objective?</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p"><a class="ltx_ref" href="#S2" title="4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> — how would the above unrolling approach provide a principled interpretation of the popular transformer architectures; if so, what are the associated objective and optimization mechanisms?</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i3.p1">
<p class="ltx_p"><a class="ltx_ref" href="#S3" title="4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a> — how would this framework guide us to design more efficient or more parsimonious deep architectures?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.1 </span>White-Box Deep Networks via Unrolled Optimization</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Now, if we agree that maximizing the rate reduction or information gain leads to the desired representation as discussed in <a class="ltx_ref" href="Ch3.html#S4" title="3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, the remaining question is how to construct and learn a (nonlinear) mapping from the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to the optimal representation <math alttext="\bm{Z}^{*}" class="ltx_Math" display="inline" id="S1.p1.m2"><semantics><msup><mi>𝒁</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. This involves designing a network architecture and learning algorithm that can effectively capture the underlying structures in the data and faithfully realize the optimal representation.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1.1 </span>Deep Networks from Unrolled Gradient Descent</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p">In the previous chapter, we presented the rate reduction objective (<a class="ltx_ref" href="Ch3.html#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) as a principled objective for learning linear discriminative representations of the data. We have, however, not specified the architecture of the feature mapping <math alttext="\bm{z}=f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.p1.m1"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}=f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z = italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math> for extracting such representations from input data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
A straightforward choice is to use a conventional deep network, such as ResNet, for implementing <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.p1.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math>. As we have seen in Example <a class="ltx_ref" href="Ch3.html#F24" title="Figure 3.24 ‣ Example 3.13 (Classification of Images on CIFAR-10). ‣ 3.4.3 Optimization Properties of Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.24</span></a>, such a choice often leads to decent performance empirically. Nonetheless, there remain several unanswered problems with adopting an arbitrary deep network. Although the learned feature representation is now more interpretable, the network itself is still <span class="ltx_text ltx_font_italic">not</span>. It is unclear why any chosen “black-box” network is able to optimize the desired MCR<sup class="ltx_sup">2</sup> objective at all. The good empirical results (say with a ResNet) do not necessarily justify the particular choice in architectures and operators of the network: Why is a deep layered model even necessary; what do additional layers try to improve or simplify; how wide and deep is adequate; or is there any rigorous justification for the convolutions (in a popular multi-channel form) and nonlinear operators (e.g. ReLU or softmax) used?</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p">In this chapter, we show that using gradient ascent to maximize the rate reduction <math alttext="\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})" class="ltx_Math" display="inline" id="S1.SS1.p2.m1"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π )</annotation></semantics></math> as defined in (<a class="ltx_ref" href="Ch3.html#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) naturally leads to a “white-box” deep network that realizes the desired mapping. All network layers, linear/nonlinear operators, and parameters are <span class="ltx_text ltx_font_italic">explicitly constructed in a purely forward propagation fashion</span>. Moreover, such network architectures resemble existing empirically-designed deep networks, providing principled justifications for their design.</p>
</div>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Gradient Ascent for Coding Rate Reduction.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p1">
<p class="ltx_p">From the previous chapter, we see that to seek a linear discriminative representation (LDR), mathematically, we are essentially seeking a continuous mapping <math alttext="f(\cdot):\bm{x}\mapsto\bm{z}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">f(\cdot):\bm{x}\mapsto\bm{z}</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ ) : bold_italic_x ↦ bold_italic_z</annotation></semantics></math> from the data <math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m2"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> (or initial features extracted from the data<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>As we will see the necessity of such a feature extraction in the next section.</span></span></span>) to an optimal representation <math alttext="\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m3"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> that maximizes the following coding rate reduction objective:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})\doteq\underbrace{\frac{1}{2}\log\det\Big{(}\bm{I}+{\alpha}\bm{Z}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}(\bm{Z})}\;-\;\underbrace{\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\log\det\Big{(}\bm{I}+{\alpha_{k}}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})},\end{split}" class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mtable displaystyle="true"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><munder><munder accentunder="true"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></munder><mo rspace="0.335em">−</mo><munder><munder accentunder="true"><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><msub><mi>γ</mi><mi>k</mi></msub><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></munder></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})\doteq\underbrace{\frac{1}{2}\log\det\Big{(}\bm{I}+{\alpha}\bm{Z}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}(\bm{Z})}\;-\;\underbrace{\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\log\det\Big{(}\bm{I}+{\alpha_{k}}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}^{c}(\bm{Z}\mid\bm{\Pi})},\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π ) ≐ under⏟ start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG start_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) end_POSTSUBSCRIPT - under⏟ start_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG start_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_Π ) end_POSTSUBSCRIPT , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m4"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math> is a prescribed quantization error and for simplicity we denote<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Notice our use of slightly simplified notation compared to <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</span></span></span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx40">
<tbody id="S1.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\alpha\doteq\frac{d}{N\epsilon^{2}},\qquad\alpha_{k}\doteq\frac{d}{\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}},\qquad\gamma_{k}\doteq\frac{\mathrm{tr}(\bm{\Pi}_{k})}{N},\qquad\text{for}\ k=1,\ldots,K." class="ltx_Math" display="inline" id="S1.Ex1.m1"><semantics><mrow><mrow><mrow><mi>α</mi><mo>≐</mo><mstyle displaystyle="true"><mfrac><mi>d</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>≐</mo><mstyle displaystyle="true"><mfrac><mi>d</mi><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo>≐</mo><mstyle displaystyle="true"><mfrac><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mi>N</mi></mfrac></mstyle></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mtext>for</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>k</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\alpha\doteq\frac{d}{N\epsilon^{2}},\qquad\alpha_{k}\doteq\frac{d}{\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}},\qquad\gamma_{k}\doteq\frac{\mathrm{tr}(\bm{\Pi}_{k})}{N},\qquad\text{for}\ k=1,\ldots,K.</annotation><annotation encoding="application/x-llamapun">italic_α ≐ divide start_ARG italic_d end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≐ divide start_ARG italic_d end_ARG start_ARG roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≐ divide start_ARG roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG italic_N end_ARG , for italic_k = 1 , … , italic_K .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p2">
<p class="ltx_p">The question really boils down to whether there is a <span class="ltx_text ltx_font_italic">constructive</span> way of finding such a continuous mapping <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> from <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>? To this end, let us consider incrementally maximizing the objective <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m4"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> as a function of <math alttext="\bm{Z}\subseteq\mathbb{S}^{d-1}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m5"><semantics><mrow><mi>𝒁</mi><mo>⊆</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\subseteq\mathbb{S}^{d-1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z ⊆ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>. Although there might be many optimization schemes to choose from, for simplicity we first consider the arguably simplest projected <span class="ltx_text ltx_font_italic">gradient ascent</span> (PGA) scheme:<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Notice that we use subscript <math alttext="j" class="ltx_Math" display="inline" id="footnote5.m1"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math> on <math alttext="\bm{Z}_{j}" class="ltx_Math" display="inline" id="footnote5.m2"><semantics><msub><mi>𝒁</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> to indicate features in the <math alttext="j" class="ltx_Math" display="inline" id="footnote5.m3"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math>-th class and superscript <math alttext="\ell" class="ltx_Math" display="inline" id="footnote5.m4"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> on <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="footnote5.m5"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> to indicate all features at <math alttext="\ell" class="ltx_Math" display="inline" id="footnote5.m6"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th iteration or layer.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1}\;\propto\;\bm{Z}^{\ell}+\eta\cdot\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})\quad\mbox{s.t.}\quad\bm{Z}^{\ell+1}\subseteq\mathbb{S}^{d-1},\quad\ell=1,2,\ldots," class="ltx_Math" display="block" id="S1.E2.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo lspace="0.558em" rspace="0.558em">∝</mo><mrow><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1em"></mspace><mtext>s.t.</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>⊆</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo rspace="1.167em">,</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}\;\propto\;\bm{Z}^{\ell}+\eta\cdot\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})\quad\mbox{s.t.}\quad\bm{Z}^{\ell+1}\subseteq\mathbb{S}^{d-1},\quad\ell=1,2,\ldots,</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) s.t. bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ⊆ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT , roman_ℓ = 1 , 2 , … ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some step size <math alttext="\eta&gt;0" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m6"><semantics><mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\eta&gt;0</annotation><annotation encoding="application/x-llamapun">italic_η &gt; 0</annotation></semantics></math> and the iterate starts with the given data <math alttext="\bm{Z}^{0}=\bm{X}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m7"><semantics><mrow><msup><mi>𝒁</mi><mn>0</mn></msup><mo>=</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">\bm{Z}^{0}=\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = bold_italic_X</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Again, for simplicity, we here first assume the initial features <math alttext="\bm{Z}^{1}" class="ltx_Math" display="inline" id="footnote6.m1"><semantics><msup><mi>𝒁</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> are the data themselves. Note that here <math alttext="\ell" class="ltx_Math" display="inline" id="footnote6.m2"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> denotes the number of iterations. Hence, the data and the features have the same dimension <math alttext="d" class="ltx_Math" display="inline" id="footnote6.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>. This needs not to be the case though. As we will see in the next section, the initial features can be some (lifted) features of the data to begin with and could in principle have a different (much higher) dimension. All subsequent iterates have the same dimension.</span></span></span>
This scheme can be interpreted as how one should incrementally adjust locations of the current features <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m8"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, initialized as the input data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m9"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, in order for the resulting <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m10"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> to improve the rate reduction <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m11"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math>, as illustrated in Figure <a class="ltx_ref" href="#F1" title="Figure 4.1 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="F1"><img alt="Figure 4.1 : Incremental deformation via gradient flow to both flatten data of each class into a subspace and push different classes apart." class="ltx_graphics ltx_img_landscape" height="214" id="F1.g1" src="chapters/chapter4/figs/redu_gradient_diagram.png" width="509"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.1</span>: </span><span class="ltx_text" style="font-size:90%;">Incremental deformation via gradient flow to both flatten data of each class into a subspace and push different classes apart.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p3">
<p class="ltx_p">Simple calculation shows that the gradient <math alttext="{\partial\Delta R_{\epsilon}}/{\partial\bm{Z}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p3.m1"><semantics><mrow><mo rspace="0em">∂</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo>/</mo><mrow><mo lspace="0em" rspace="0em">∂</mo><mi>𝒁</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">{\partial\Delta R_{\epsilon}}/{\partial\bm{Z}}</annotation><annotation encoding="application/x-llamapun">∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT / ∂ bold_italic_Z</annotation></semantics></math> entails evaluating the following derivatives of the two terms in <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p3.m2"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{1}{2}\frac{\partial\log\det(\bm{I}\!+\!\alpha\bm{Z}\bm{Z}^{\top})}{\partial\bm{Z}}(\bm{Z}^{\ell})=\underbrace{\alpha(\bm{I}\!+\!\alpha\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top})^{-1}}_{\bm{E}^{\ell}\;\in\mathbb{R}^{d\times d}}\bm{Z}^{\ell}," class="ltx_Math" display="block" id="S1.E3.m1"><semantics><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mo rspace="0.167em">∂</mo><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>⏟</mo></munder><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0.558em">∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></munder><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{1}{2}\frac{\partial\log\det(\bm{I}\!+\!\alpha\bm{Z}\bm{Z}^{\top})}{\partial\bm{Z}}(\bm{Z}^{\ell})=\underbrace{\alpha(\bm{I}\!+\!\alpha\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top})^{-1}}_{\bm{E}^{\ell}\;\in\mathbb{R}^{d\times d}}\bm{Z}^{\ell},</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG ∂ roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) = under⏟ start_ARG italic_α ( bold_italic_I + italic_α bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p4">
<table class="ltx_equation ltx_eqn_table" id="S1.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{1}{2}\frac{\partial\left(\gamma_{k}\log\det(\bm{I}+\alpha_{k}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top})\right)}{\partial\bm{Z}}(\bm{Z}^{\ell})=\gamma_{k}\underbrace{\alpha_{k}(\bm{I}+\alpha_{k}\bm{Z}^{\ell}\bm{\Pi}_{k}(\bm{Z}^{\ell})^{\top})^{-1}}_{\bm{C}^{\ell}_{k}\;\in\mathbb{R}^{d\times d}}\bm{Z}^{\ell}\bm{\Pi}_{k}." class="ltx_Math" display="block" id="S1.E4.m1"><semantics><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mo>(</mo><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>⏟</mo></munder><mrow><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0.558em">∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></munder><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\frac{1}{2}\frac{\partial\left(\gamma_{k}\log\det(\bm{I}+\alpha_{k}\bm{Z}\bm{\Pi}_{k}\bm{Z}^{\top})\right)}{\partial\bm{Z}}(\bm{Z}^{\ell})=\gamma_{k}\underbrace{\alpha_{k}(\bm{I}+\alpha_{k}\bm{Z}^{\ell}\bm{\Pi}_{k}(\bm{Z}^{\ell})^{\top})^{-1}}_{\bm{C}^{\ell}_{k}\;\in\mathbb{R}^{d\times d}}\bm{Z}^{\ell}\bm{\Pi}_{k}.</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG ∂ ( italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log roman_det ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ) end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) = italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT under⏟ start_ARG italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that in the above, the matrix <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p4.m1"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> only depends on <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p4.m2"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and it aims to <span class="ltx_text ltx_font_italic">expand</span> all the features to increase the overall coding rate; the matrix <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p4.m3"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> depends on features from the <math alttext="k" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p4.m4"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-class and aims to <span class="ltx_text ltx_font_italic">compress</span> them to reduce the coding rate of each class.
Then the complete gradient <math alttext="\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p4.m5"><semantics><mrow><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is of the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})=\underbrace{\bm{E}^{\ell}}_{\text{Expansion}}\bm{Z}^{\ell}\;-\;\sum_{k=1}^{K}\gamma_{k}\underbrace{\bm{C}_{k}^{\ell}}_{\text{Compression}}\bm{Z}^{\ell}\bm{\Pi}_{k}." class="ltx_Math" display="block" id="S1.E5.m1"><semantics><mrow><mrow><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><munder accentunder="true"><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo>⏟</mo></munder><mtext>Expansion</mtext></munder><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo rspace="0.335em">−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>⏟</mo></munder><mtext>Compression</mtext></munder><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})=\underbrace{\bm{E}^{\ell}}_{\text{Expansion}}\bm{Z}^{\ell}\;-\;\sum_{k=1}^{K}\gamma_{k}\underbrace{\bm{C}_{k}^{\ell}}_{\text{Compression}}\bm{Z}^{\ell}\bm{\Pi}_{k}.</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) = under⏟ start_ARG bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT Expansion end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT under⏟ start_ARG bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT Compression end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Interpretation of <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.m1"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}_{j}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.m2"><semantics><msubsup><mi>𝑪</mi><mi>j</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}_{j}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> as linear operators)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark1.p1">
<p class="ltx_p">For any <math alttext="\bm{z}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmremark1.p1.m1"><semantics><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>,</p>
<table class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table" id="A2.S3.EGx41">
<tbody id="S1.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\displaystyle\bm{E}^{\ell}\bm{z}^{\ell}=\alpha(\bm{z}^{\ell}-\bm{Z}^{\ell}\bm{q}^{\ell}_{\star}),\qquad\mbox{where}\qquad\bm{q}^{\ell}_{\star}\doteq\operatorname*{arg\ min}_{\bm{q}^{\ell}}\big{\{}\alpha\|\bm{z}^{\ell}-\bm{Z}^{\ell}\bm{q}^{\ell}\|_{2}^{2}+\|\bm{q}^{\ell}\|_{2}^{2}\big{\}}." class="ltx_Math" display="block" id="S1.E6.m1"><semantics><mrow><mrow><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒒</mi><mo>⋆</mo><mi mathvariant="normal">ℓ</mi></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="2.167em">,</mo><mtext>where</mtext></mrow></mrow><mspace width="2em"></mspace><mrow><msubsup><mi>𝒒</mi><mo>⋆</mo><mi mathvariant="normal">ℓ</mi></msubsup><mo>≐</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><msup><mi>𝒒</mi><mi mathvariant="normal">ℓ</mi></msup></munder><mo>⁡</mo><mrow><mo maxsize="120%" minsize="120%">{</mo><mrow><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒒</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mrow><mo stretchy="false">‖</mo><msup><mi>𝒒</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo maxsize="120%" minsize="120%">}</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{E}^{\ell}\bm{z}^{\ell}=\alpha(\bm{z}^{\ell}-\bm{Z}^{\ell}\bm{q}^{\ell}_{\star}),\qquad\mbox{where}\qquad\bm{q}^{\ell}_{\star}\doteq\operatorname*{arg\ min}_{\bm{q}^{\ell}}\big{\{}\alpha\|\bm{z}^{\ell}-\bm{Z}^{\ell}\bm{q}^{\ell}\|_{2}^{2}+\|\bm{q}^{\ell}\|_{2}^{2}\big{\}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = italic_α ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ⋆ end_POSTSUBSCRIPT ) , where bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ⋆ end_POSTSUBSCRIPT ≐ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { italic_α ∥ bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ∥ bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that <math alttext="\bm{q}^{\ell}_{\star}" class="ltx_Math" display="inline" id="Thmremark1.p1.m2"><semantics><msubsup><mi>𝒒</mi><mo>⋆</mo><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{q}^{\ell}_{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_q start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ⋆ end_POSTSUBSCRIPT</annotation></semantics></math> is exactly the solution to the ridge regression by all the data points <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m3"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> concerned. Therefore, <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m4"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> (similarly for <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="Thmremark1.p1.m5"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) is approximately (i.e., when <math alttext="N" class="ltx_Math" display="inline" id="Thmremark1.p1.m6"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is large enough) the projection onto the orthogonal complement of the subspace spanned by columns of <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m7"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>. Another way to interpret the matrix <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m8"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is through eigenvalue decomposition of the covariance matrix <math alttext="\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}" class="ltx_Math" display="inline" id="Thmremark1.p1.m9"><semantics><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>. Assuming that <math alttext="\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}\doteq\bm{U}^{\ell}\bm{\Lambda}^{\ell}(\bm{U}^{\ell})^{\top}" class="ltx_Math" display="inline" id="Thmremark1.p1.m10"><semantics><mrow><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>≐</mo><mrow><msup><mi>𝑼</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚲</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}\doteq\bm{U}^{\ell}\bm{\Lambda}^{\ell}(\bm{U}^{\ell})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ≐ bold_italic_U start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_Λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="\bm{\Lambda}^{\ell}\doteq\operatorname{diag}\left(\lambda^{\ell}_{1},\ldots,\lambda^{\ell}_{d}\right)" class="ltx_Math" display="inline" id="Thmremark1.p1.m11"><semantics><mrow><msup><mi>𝚲</mi><mi mathvariant="normal">ℓ</mi></msup><mo>≐</mo><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo>(</mo><msubsup><mi>λ</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>λ</mi><mi>d</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\Lambda}^{\ell}\doteq\operatorname{diag}\left(\lambda^{\ell}_{1},\ldots,\lambda^{\ell}_{d}\right)</annotation><annotation encoding="application/x-llamapun">bold_Λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ≐ roman_diag ( italic_λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )</annotation></semantics></math>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{E}^{\ell}=\alpha\bm{U}^{\ell}\,\operatorname{diag}\left(\frac{1}{1+\alpha\lambda^{\ell}_{1}},\ldots,\frac{1}{1+\alpha\lambda^{\ell}_{d}}\right)\left(\bm{U}^{\ell}\right)^{\top}." class="ltx_Math" display="block" id="S1.E7.m1"><semantics><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo>=</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>λ</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup></mrow></mrow></mfrac><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>λ</mi><mi>d</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow></mrow></mfrac><mo>)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><msup><mi>𝑼</mi><mi mathvariant="normal">ℓ</mi></msup><mo>)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{E}^{\ell}=\alpha\bm{U}^{\ell}\,\operatorname{diag}\left(\frac{1}{1+\alpha\lambda^{\ell}_{1}},\ldots,\frac{1}{1+\alpha\lambda^{\ell}_{d}}\right)\left(\bm{U}^{\ell}\right)^{\top}.</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = italic_α bold_italic_U start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT roman_diag ( divide start_ARG 1 end_ARG start_ARG 1 + italic_α italic_λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , … , divide start_ARG 1 end_ARG start_ARG 1 + italic_α italic_λ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ) ( bold_italic_U start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, the matrix <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m12"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> operates on a vector <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p1.m13"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> by stretching in a way that directions of large variance are shrunk while directions of vanishing variance are kept. These are exactly the directions (<a class="ltx_ref" href="#S1.E3" title="Equation 4.1.3 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>) in which we move the features so that the overall volume expands and the coding rate will increase, hence the positive sign. To the opposite effect, the directions associated with (<a class="ltx_ref" href="#S1.E4" title="Equation 4.1.4 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.4</span></a>) are “residuals” of features of each class deviate from the subspace to which they are supposed to belong. These are exactly the directions in which the features need to be compressed back onto their respective subspace, hence the negative sign (see Figure <a class="ltx_ref" href="#F2" title="Figure 4.2 ‣ Remark 4.1 (Interpretation of 𝑬^ℓ and 𝑪_𝑗^ℓ as linear operators). ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<figure class="ltx_figure" id="F2"><img alt="Figure 4.2 : Interpretation of 𝑪 k ℓ \bm{C}^{\ell}_{k} bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and 𝑬 ℓ \bm{E}^{\ell} bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT : 𝑪 k ℓ \bm{C}^{\ell}_{k} bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT compresses each class by contracting the features to a low-dimensional subspace; 𝑬 ℓ \bm{E}^{\ell} bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT expands all features by contrasting and repelling features across different classes." class="ltx_graphics ltx_img_landscape" height="206" id="F2.g1" src="chapters/chapter4/figs/expand_compress.png" width="389"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.2</span>: </span><span class="ltx_text" style="font-size:90%;">Interpretation of <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="F2.m5"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="F2.m6"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>: <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="F2.m7"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> compresses each class by contracting the features to a low-dimensional subspace; <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="F2.m8"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> expands all features by contrasting and repelling features across different classes.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmremark1.p2">
<p class="ltx_p">Essentially, the linear operations <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p2.m1"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}_{k}^{\ell}" class="ltx_Math" display="inline" id="Thmremark1.p2.m2"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}_{k}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> in gradient ascend for rate reduction are determined by training data conducting “auto-regressions”. The recent renewed understanding about ridge regression in an over-parameterized setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx308" title="">YYY+20</a>, <a class="ltx_ref" href="bib.html#bibx298" title="">WX20</a>]</cite> indicates that using seemingly redundantly sampled data (from each subspace) as regressors does not lead to overfitting.</p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Gradient-Guided Feature Map Increment.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p1">
<p class="ltx_p">Notice that in the above, the gradient ascent considers all the features <math alttext="\bm{Z}^{\ell}=[\bm{z}^{\ell}_{1},\dots,\bm{z}^{\ell}_{N}]" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m1"><semantics><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒛</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒛</mi><mi>N</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell}=[\bm{z}^{\ell}_{1},\dots,\bm{z}^{\ell}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = [ bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> as free variables. The increment <math alttext="\bm{Z}^{\ell+1}-\bm{Z}^{\ell}=\eta\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m2"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>−</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>=</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}-\bm{Z}^{\ell}=\eta\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT - bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = italic_η divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> does not yet give a transformation on the entire feature domain <math alttext="\bm{z}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m3"><semantics><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. According to equation (<a class="ltx_ref" href="#S1.E5" title="Equation 4.1.5 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.5</span></a>), the gradient cannot be evaluated at a point whose membership is not known, as illustrated in Figure <a class="ltx_ref" href="#F1" title="Figure 4.1 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Hence, in order to find the optimal <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m4"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math> explicitly, we may consider constructing a small increment transform <math alttext="g(\cdot,\bm{\theta}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m5"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\bm{\theta}^{\ell})</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> on the <math alttext="\ell" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m6"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th layer feature <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m7"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> to emulate the above (projected) gradient scheme:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}^{\ell+1}\;\propto\;\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})\quad\mbox{subject to}\quad\bm{z}^{\ell+1}\in\mathbb{S}^{d-1}" class="ltx_Math" display="block" id="S1.E8.m1"><semantics><mrow><mrow><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo lspace="0.558em" rspace="0.558em">∝</mo><mrow><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1em"></mspace><mtext>subject to</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>∈</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{z}^{\ell+1}\;\propto\;\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})\quad\mbox{subject to}\quad\bm{z}^{\ell+1}\in\mathbb{S}^{d-1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∝ bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) subject to bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∈ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">such that <math alttext="\big{[}g(\bm{z}_{1}^{\ell},\bm{\theta}^{\ell}),\ldots,g(\bm{z}_{N}^{\ell},\bm{\theta}^{\ell})\big{]}\approx\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})." class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m8"><semantics><mrow><mrow><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>N</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow><mo>≈</mo><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\big{[}g(\bm{z}_{1}^{\ell},\bm{\theta}^{\ell}),\ldots,g(\bm{z}_{N}^{\ell},\bm{\theta}^{\ell})\big{]}\approx\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell}).</annotation><annotation encoding="application/x-llamapun">[ italic_g ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) , … , italic_g ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ] ≈ divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) .</annotation></semantics></math> That is, we need to approximate the gradient flow <math alttext="\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m9"><semantics><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG</annotation></semantics></math> that locally deforms all (training) features <math alttext="\{\bm{z}_{i}^{\ell}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m10"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}^{\ell}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> with a continuous mapping <math alttext="g(\bm{z},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m11"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z , bold_italic_θ )</annotation></semantics></math> defined on the entire feature space <math alttext="\bm{z}^{\ell}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m12"><semantics><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{\ell}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
Notice that one may interpret the increment (<a class="ltx_ref" href="#S1.E8" title="Equation 4.1.8 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.8</span></a>) as a discretized version of a continuous differential equation:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\dot{\bm{z}}=g(\bm{z},\theta)." class="ltx_Math" display="block" id="S1.E9.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒛</mi><mo>˙</mo></mover><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\dot{\bm{z}}=g(\bm{z},\theta).</annotation><annotation encoding="application/x-llamapun">over˙ start_ARG bold_italic_z end_ARG = italic_g ( bold_italic_z , italic_θ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence the (deep) network so constructed can be interpreted as certain neural ODE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx46" title="">CRB+18</a>]</cite>. Nevertheless, unlike neural ODE where the flow <math alttext="g" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m13"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> is chosen to be some generic structures, here our <math alttext="g(\bm{z},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m14"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z , bold_italic_θ )</annotation></semantics></math> is to emulate the gradient flow of the rate reduction on the feature set (as shown in Figure <a class="ltx_ref" href="#F1" title="Figure 4.1 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\dot{\bm{Z}}=\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}," class="ltx_Math" display="block" id="S1.Ex2.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒁</mi><mo>˙</mo></mover><mo>=</mo><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\dot{\bm{Z}}=\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}},</annotation><annotation encoding="application/x-llamapun">over˙ start_ARG bold_italic_Z end_ARG = divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and its structure is entirely derived and fully determined from this objective, without any other priors or heuristics.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p2">
<p class="ltx_p">By inspecting the structure of the gradient (<a class="ltx_ref" href="#S1.E5" title="Equation 4.1.5 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.5</span></a>), it suggests that a natural candidate for the increment transform <math alttext="g(\bm{z}^{\ell},\bm{\theta}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m1"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z}^{\ell},\bm{\theta}^{\ell})</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> is of the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="g(\bm{z}^{\ell},\bm{\theta}^{\ell})\;\doteq\;\bm{E}^{\ell}\bm{z}^{\ell}-\sum_{k=1}^{K}\gamma_{k}\pi_{k}(\bm{z}^{\ell})\bm{C}_{k}^{\ell}\bm{z}^{\ell}\in\mathbb{R}^{d}," class="ltx_Math" display="block" id="S1.E10.m1"><semantics><mrow><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">≐</mo><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo rspace="0.055em">−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">g(\bm{z}^{\ell},\bm{\theta}^{\ell})\;\doteq\;\bm{E}^{\ell}\bm{z}^{\ell}-\sum_{k=1}^{K}\gamma_{k}\pi_{k}(\bm{z}^{\ell})\bm{C}_{k}^{\ell}\bm{z}^{\ell}\in\mathbb{R}^{d},</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ≐ bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\pi_{k}(\bm{z}^{\ell})\in[0,1]" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m2"><semantics><mrow><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{k}(\bm{z}^{\ell})\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ∈ [ 0 , 1 ]</annotation></semantics></math> indicates the probability of <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m3"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> belonging to the <math alttext="k" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m4"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th class. The increment map parameters <math alttext="\bm{\theta}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m5"><semantics><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> depend on: First, a set of linear maps represented by <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m6"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m7"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> that depend only on statistics of features of the training <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m8"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>; Second, the membership <math alttext="\{\pi_{k}(\bm{z}^{\ell})\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m9"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\pi_{k}(\bm{z}^{\ell})\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> of any feature <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m10"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.
Notice that on the training samples <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m11"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, for which the memberships <math alttext="\bm{\Pi}_{k}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m12"><semantics><msub><mi>𝚷</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{\Pi}_{k}</annotation><annotation encoding="application/x-llamapun">bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are known, the so defined <math alttext="g(\bm{z}^{\ell},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m13"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z}^{\ell},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ )</annotation></semantics></math> gives exactly the values for the gradient <math alttext="\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m14"><semantics><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}(\bm{Z}^{\ell})</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p3">
<p class="ltx_p">Since we only have the membership for the training samples, the function <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m1"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ )</annotation></semantics></math> defined in (<a class="ltx_ref" href="#S1.E10" title="Equation 4.1.10 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.10</span></a>) can only be evaluated on the training. To extrapolate <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ )</annotation></semantics></math> to the entire feature space, we need to estimate <math alttext="\pi_{k}(\bm{z}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m3"><semantics><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{k}(\bm{z}^{\ell})</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> in its second term. In conventional deep learning, this map is typically modeled as a deep network and learned from the training data, say via <span class="ltx_text ltx_font_italic">back propagation</span>. Nevertheless, our goal here is not to learn a precise classifier <math alttext="\pi_{k}(\bm{z}^{\ell})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m4"><semantics><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{k}(\bm{z}^{\ell})</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> already. Instead, we only need a good enough estimate of the class information in order for <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m5"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ )</annotation></semantics></math> to approximate the gradient <math alttext="\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m6"><semantics><mfrac><mrow><mo rspace="0em">∂</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒁</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial\Delta R_{\epsilon}}{\partial\bm{Z}}</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT end_ARG start_ARG ∂ bold_italic_Z end_ARG</annotation></semantics></math> well.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p4">
<p class="ltx_p">From the geometric interpretation of the linear maps <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m1"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}_{k}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m2"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}_{k}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> given by Remark <a class="ltx_ref" href="#Thmremark1" title="Remark 4.1 (Interpretation of 𝑬^ℓ and 𝑪_𝑗^ℓ as linear operators). ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>, the term <math alttext="\bm{p}_{k}^{\ell}\doteq\bm{C}^{\ell}_{k}\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m3"><semantics><mrow><msubsup><mi>𝒑</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>≐</mo><mrow><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{p}_{k}^{\ell}\doteq\bm{C}^{\ell}_{k}\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ≐ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> can be viewed as (approximately) the projection of <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m4"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> onto the orthogonal complement of each class <math alttext="j" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m5"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math>. Therefore, <math alttext="\|\bm{p}_{j}^{\ell}\|_{2}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m6"><semantics><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>𝒑</mi><mi>j</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|\bm{p}_{j}^{\ell}\|_{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is small if <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m7"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is in class <math alttext="j" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m8"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math> and large otherwise. This motivates us to estimate its membership based on the following softmax function:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\widehat{\bm{\pi}}(\bm{z}^{\ell})\doteq\operatorname{\mathrm{softmax}}\left(-\lambda\begin{bmatrix}\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2}\\
\vdots\\
\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2}\end{bmatrix}\right)=\frac{1}{\sum_{k=1}^{K}\exp(-\lambda\|\bm{C}^{\ell}_{k}\bm{z}^{\ell}\|_{2})}\begin{bmatrix}\exp(-\lambda\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2})\\
\vdots\\
\exp(-\lambda\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2})\end{bmatrix}\in[0,1]^{K}." class="ltx_Math" display="block" id="S1.E11.m1"><semantics><mrow><mrow><mrow><mover accent="true"><mi>𝝅</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>K</mi></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\widehat{\bm{\pi}}(\bm{z}^{\ell})\doteq\operatorname{\mathrm{softmax}}\left(-\lambda\begin{bmatrix}\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2}\\
\vdots\\
\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2}\end{bmatrix}\right)=\frac{1}{\sum_{k=1}^{K}\exp(-\lambda\|\bm{C}^{\ell}_{k}\bm{z}^{\ell}\|_{2})}\begin{bmatrix}\exp(-\lambda\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2})\\
\vdots\\
\exp(-\lambda\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2})\end{bmatrix}\in[0,1]^{K}.</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_π end_ARG ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ≐ roman_softmax ( - italic_λ [ start_ARG start_ROW start_CELL ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) = divide start_ARG 1 end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( - italic_λ ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_ARG [ start_ARG start_ROW start_CELL roman_exp ( - italic_λ ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_exp ( - italic_λ ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ] ∈ [ 0 , 1 ] start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, the second term of (<a class="ltx_ref" href="#S1.E10" title="Equation 4.1.10 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.10</span></a>) can be approximated by this estimated membership:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx42">
<tbody id="S1.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\sum_{k=1}^{K}\gamma_{k}\pi_{k}(\bm{z}^{\ell})\bm{C}_{k}^{\ell}\bm{z}^{\ell}\;\approx\;\sum_{k=1}^{K}\gamma_{k}\widehat{\pi}_{k}(\bm{z}^{\ell})\bm{C}^{\ell}_{k}\bm{z}^{\ell}\;\doteq\;\bm{\sigma}\Big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\Big{)}," class="ltx_Math" display="inline" id="S1.E12.m1"><semantics><mrow><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><mo rspace="0.558em">≈</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>π</mi><mo>^</mo></mover><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><mo rspace="0.558em">≐</mo><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">]</mo></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{k=1}^{K}\gamma_{k}\pi_{k}(\bm{z}^{\ell})\bm{C}_{k}^{\ell}\bm{z}^{\ell}\;\approx\;\sum_{k=1}^{K}\gamma_{k}\widehat{\pi}_{k}(\bm{z}^{\ell})\bm{C}^{\ell}_{k}\bm{z}^{\ell}\;\doteq\;\bm{\sigma}\Big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\Big{)},</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ≈ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ≐ bold_italic_σ ( [ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , … , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is denoted as a nonlinear operator <math alttext="\bm{\sigma}(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m9"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\sigma}(\cdot)</annotation><annotation encoding="application/x-llamapun">bold_italic_σ ( ⋅ )</annotation></semantics></math> on outputs of the feature <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m10"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> through <math alttext="K" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m11"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> groups of filters: <math alttext="[\bm{C}^{\ell}_{1},\dots,\bm{C}^{\ell}_{K}]" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m12"><semantics><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\bm{C}^{\ell}_{1},\dots,\bm{C}^{\ell}_{K}]</annotation><annotation encoding="application/x-llamapun">[ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ]</annotation></semantics></math>. Notice that the nonlinearality arises due to a “soft” assignment of class membership based on the feature responses from those filters.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p5">
<p class="ltx_p">Overall, combining (<a class="ltx_ref" href="#S1.E8" title="Equation 4.1.8 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.8</span></a>), (<a class="ltx_ref" href="#S1.E10" title="Equation 4.1.10 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.10</span></a>), and (<a class="ltx_ref" href="#S1.E12" title="Equation 4.1.12 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.12</span></a>),
the increment feature transform from <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m1"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\bm{z}^{\ell+1}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m2"><semantics><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> now becomes</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S1.E13">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E13X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{z}^{\ell+1}" class="ltx_Math" display="inline" id="S1.E13X.m2"><semantics><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\displaystyle\bm{z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\propto\;\bm{z}^{\ell}+\eta\cdot\bm{E}^{\ell}\bm{z}^{\ell}-\eta\cdot\bm{\sigma}\big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\big{)}" class="ltx_Math" display="inline" id="S1.E13X.m3"><semantics><mrow><mi></mi><mo rspace="0.558em">∝</mo><mrow><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow><mo>−</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>𝝈</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">]</mo></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\propto\;\bm{z}^{\ell}+\eta\cdot\bm{E}^{\ell}\bm{z}^{\ell}-\eta\cdot\bm{\sigma}\big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\big{)}</annotation><annotation encoding="application/x-llamapun">∝ bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - italic_η ⋅ bold_italic_σ ( [ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , … , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ] )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4.1.13)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E13Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\;\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})\qquad\mbox{s.t.}\quad\bm{z}^{\ell+1}\in\mathbb{S}^{d-1}," class="ltx_Math" display="inline" id="S1.E13Xa.m2"><semantics><mrow><mrow><mrow><mi></mi><mo rspace="0.558em">=</mo><mrow><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="2em"></mspace><mtext>s.t.</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>∈</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\;\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})\qquad\mbox{s.t.}\quad\bm{z}^{\ell+1}\in\mathbb{S}^{d-1},</annotation><annotation encoding="application/x-llamapun">= bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) s.t. bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ∈ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">with the nonlinear function <math alttext="\bm{\sigma}(\cdot)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m3"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\sigma}(\cdot)</annotation><annotation encoding="application/x-llamapun">bold_italic_σ ( ⋅ )</annotation></semantics></math> defined above and <math alttext="\bm{\theta}^{\ell}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m4"><semantics><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{\theta}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> collecting all the layer-wise parameters. That is <math alttext="\bm{\theta}^{\ell}=\left\{\bm{E}^{\ell},\bm{C}^{\ell}_{1},\dots,\bm{C}^{\ell}_{K},\gamma_{k},\lambda\right\}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m5"><semantics><mrow><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo>=</mo><mrow><mo>{</mo><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msub><mi>γ</mi><mi>k</mi></msub><mo>,</mo><mi>λ</mi><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}^{\ell}=\left\{\bm{E}^{\ell},\bm{C}^{\ell}_{1},\dots,\bm{C}^{\ell}_{K},\gamma_{k},\lambda\right\}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT = { bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_λ }</annotation></semantics></math>. Note features at each layer are always “normalized” by projecting onto the unit sphere <math alttext="\mathbb{S}^{d-1}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m6"><semantics><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\mathbb{S}^{d-1}</annotation><annotation encoding="application/x-llamapun">blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, denoted as <math alttext="\mathcal{P}_{\mathbb{S}^{d-1}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m7"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></msub><annotation encoding="application/x-tex">\mathcal{P}_{\mathbb{S}^{d-1}}</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>. The form of increment in (<a class="ltx_ref" href="#S1.E13" title="Equation 4.1.13 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.13</span></a>) can be illustrated by a diagram in Figure <a class="ltx_ref" href="#F3" title="Figure 4.3 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3</span></a>(a).</p>
</div>
<figure class="ltx_figure" id="F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F3.sf1"><img alt="(a) ReduNet" class="ltx_graphics ltx_img_landscape" height="438" id="F3.sf1.g1" src="chapters/chapter4/figs/redunet_layer.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">ReduNet</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F3.sf2"><img alt="(a) ReduNet" class="ltx_graphics" id="F3.sf2.g1" src="chapters/chapter4/figs/resnet_resnext.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">ResNet<span class="ltx_text ltx_font_medium"> and </span>ResNeXt<span class="ltx_text ltx_font_medium">.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.3</span>: </span><span class="ltx_text" style="font-size:90%;">Network Architectures of the ReduNet and comparison with others. <span class="ltx_text ltx_font_bold">(a)</span>: Layer structure of the <span class="ltx_text ltx_font_bold">ReduNet</span> derived from one iteration of gradient ascent for optimizing rate reduction. <span class="ltx_text ltx_font_bold">(b)</span> (left): A layer of ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx103" title="">HZR+16a</a>]</cite>; and <span class="ltx_text ltx_font_bold">(b)</span> (right): A layer of ResNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx303" title="">XGD+17</a>]</cite>. As we will see in Section <a class="ltx_ref" href="#S1.SS2" title="4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>, the linear operators <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="F3.m3"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}_{k}^{\ell}" class="ltx_Math" display="inline" id="F3.m4"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}_{k}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> of the ReduNet naturally become (multi-channel) convolutions when shift-invariance is imposed.</span></figcaption>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 4.1</span> </span> Training algorithm for ReduNet</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span><math alttext="\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="alg1.l1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\ldots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{\Pi}=\{\bm{\Pi}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="alg1.l1.m2"><semantics><mrow><mi>𝚷</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\bm{\Pi}=\{\bm{\Pi}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">bold_Π = { bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="alg1.l1.m3"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math>, <math alttext="\lambda" class="ltx_Math" display="inline" id="alg1.l1.m4"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_λ</annotation></semantics></math>, and a learning rate <math alttext="\eta" class="ltx_Math" display="inline" id="alg1.l1.m5"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation><annotation encoding="application/x-llamapun">italic_η</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>The learned parameters <math alttext="\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{k}" class="ltx_Math" display="inline" id="alg1.l2.m1"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>γ</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{k}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { { bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span><span class="ltx_text ltx_font_bold">procedure</span> <span class="ltx_text ltx_font_smallcaps">ReduNetTraining</span>(<math alttext="\bm{X},\bm{\Pi},\epsilon,\lambda,\eta" class="ltx_Math" display="inline" id="alg1.l3.m1"><semantics><mrow><mi>𝑿</mi><mo>,</mo><mi>𝚷</mi><mo>,</mo><mi>ϵ</mi><mo>,</mo><mi>λ</mi><mo>,</mo><mi>η</mi></mrow><annotation encoding="application/x-tex">\bm{X},\bm{\Pi},\epsilon,\lambda,\eta</annotation><annotation encoding="application/x-llamapun">bold_italic_X , bold_Π , italic_ϵ , italic_λ , italic_η</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>  <span class="ltx_text ltx_font_typewriter"># Define constants</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>     <math alttext="\alpha\leftarrow d/(N\epsilon^{2})" class="ltx_Math" display="inline" id="alg1.l5.m1"><semantics><mrow><mi>α</mi><mo stretchy="false">←</mo><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha\leftarrow d/(N\epsilon^{2})</annotation><annotation encoding="application/x-llamapun">italic_α ← italic_d / ( italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="k\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="alg1.l6.m1"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_k ∈ { 1 , … , italic_K }</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span>         <math alttext="\alpha_{k}\leftarrow D/(\operatorname{tr}(\bm{\Pi}_{k})\epsilon^{2})" class="ltx_Math" display="inline" id="alg1.l7.m1"><semantics><mrow><msub><mi>α</mi><mi>k</mi></msub><mo stretchy="false">←</mo><mrow><mi>D</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>tr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha_{k}\leftarrow D/(\operatorname{tr}(\bm{\Pi}_{k})\epsilon^{2})</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ← italic_D / ( roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>         <math alttext="\gamma_{k}\leftarrow\operatorname{tr}(\bm{\Pi}_{k})/D" class="ltx_Math" display="inline" id="alg1.l8.m1"><semantics><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo stretchy="false">←</mo><mrow><mrow><mi>tr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mi>D</mi></mrow></mrow><annotation encoding="application/x-tex">\gamma_{k}\leftarrow\operatorname{tr}(\bm{\Pi}_{k})/D</annotation><annotation encoding="application/x-llamapun">italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ← roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) / italic_D</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span>  <span class="ltx_text ltx_font_typewriter"># ReduNet layer-by-layer iteration</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span>     <math alttext="\bm{Z}^{1}=\begin{bmatrix}\bm{z}_{1}^{1},\dots,\bm{z}_{N}^{1}\end{bmatrix}\leftarrow\bm{X}" class="ltx_Math" display="inline" id="alg1.l12.m1"><semantics><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo stretchy="false">←</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">\bm{Z}^{1}=\begin{bmatrix}\bm{z}_{1}^{1},\dots,\bm{z}_{N}^{1}\end{bmatrix}\leftarrow\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] ← bold_italic_X</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l12.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Initialize the ReduNet per-layer iteration
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="\ell\in\{1,\dots,L\}" class="ltx_Math" display="inline" id="alg1.l13.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\ell\in\{1,\dots,L\}</annotation><annotation encoding="application/x-llamapun">roman_ℓ ∈ { 1 , … , italic_L }</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">14:</span></span>  <span class="ltx_text ltx_font_typewriter"># Step 1: Compute network parameters</span> <math alttext="\bm{E}^{\ell},\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="alg1.l14.m1"><semantics><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\bm{E}^{\ell},\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , { bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">15:</span></span>         <math alttext="\bm{E}^{\ell}\leftarrow\alpha\left(\bm{I}+\alpha\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}\right)^{-1}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="alg1.l15.m1"><semantics><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">←</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{E}^{\ell}\leftarrow\alpha\left(\bm{I}+\alpha\bm{Z}^{\ell}(\bm{Z}^{\ell})^{\top}\right)^{-1}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ← italic_α ( bold_italic_I + italic_α bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">16:</span></span>         <span class="ltx_text ltx_font_bold">for</span> <math alttext="k\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="alg1.l16.m1"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_k ∈ { 1 , … , italic_K }</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">17:</span></span>              <math alttext="\bm{C}^{\ell}_{k}\leftarrow\alpha_{k}\left(\bm{I}+\alpha_{k}\bm{Z}^{\ell}\bm{\Pi}_{k}(\bm{Z}^{\ell})^{\top}\right)^{-1}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="alg1.l17.m1"><semantics><mrow><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">←</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}\leftarrow\alpha_{k}\left(\bm{I}+\alpha_{k}\bm{Z}^{\ell}\bm{\Pi}_{k}(\bm{Z}^{\ell})^{\top}\right)^{-1}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ← italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">18:</span></span>         <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">19:</span></span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">20:</span></span>   <span class="ltx_text ltx_font_typewriter"># Step 2: Update features <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="alg1.l20.m1"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math></span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">21:</span></span>         <span class="ltx_text ltx_font_bold">for</span> <math alttext="i\in\{1,\dots,N\}" class="ltx_Math" display="inline" id="alg1.l21.m1"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>N</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i\in\{1,\dots,N\}</annotation><annotation encoding="application/x-llamapun">italic_i ∈ { 1 , … , italic_N }</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">22:</span></span>              <math alttext="\hat{\bm{\pi}}(\bm{z}^{\ell}_{i})\leftarrow\displaystyle\operatorname{\mathrm{softmax}}(-\lambda[\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}_{i}\|_{2},\dots,\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}_{i}\|_{2}])\in[0,1]^{K}" class="ltx_Math" display="inline" id="alg1.l22.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝝅</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">←</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\hat{\bm{\pi}}(\bm{z}^{\ell}_{i})\leftarrow\displaystyle\operatorname{\mathrm{softmax}}(-\lambda[\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}_{i}\|_{2},\dots,\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}_{i}\|_{2}])\in[0,1]^{K}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_π end_ARG ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ← roman_softmax ( - italic_λ [ ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] ) ∈ [ 0 , 1 ] start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l22.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Compute soft assignments <math alttext="\hat{\bm{\pi}}(\bm{z}^{\ell}_{i})" class="ltx_Math" display="inline" id="alg1.l22.m3"><semantics><mrow><mover accent="true"><mi>𝝅</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{\pi}}(\bm{z}^{\ell}_{i})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_π end_ARG ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="alg1.l23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">23:</span></span>              <math alttext="\displaystyle\bm{z}^{\ell+1}_{i}\leftarrow\mathcal{P}_{\mathbb{S}^{d-1}}\left(\bm{z}^{\ell}_{i}+\eta\left(\bm{E}^{\ell}\bm{z}^{\ell}_{i}-\sum_{k=1}^{K}\gamma_{k}\hat{\pi}_{k}(\bm{z}^{\ell}_{i})\bm{C}^{\ell}_{k}\bm{z}^{\ell}_{i}\right)\right)\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="alg1.l23.m1"><semantics><mrow><msubsup><mi>𝒛</mi><mi>i</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">←</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>π</mi><mo>^</mo></mover><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\displaystyle\bm{z}^{\ell+1}_{i}\leftarrow\mathcal{P}_{\mathbb{S}^{d-1}}\left(\bm{z}^{\ell}_{i}+\eta\left(\bm{E}^{\ell}\bm{z}^{\ell}_{i}-\sum_{k=1}^{K}\gamma_{k}\hat{\pi}_{k}(\bm{z}^{\ell}_{i})\bm{C}^{\ell}_{k}\bm{z}^{\ell}_{i}\right)\right)\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ← caligraphic_P start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_η ( bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l23.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Update features <math alttext="\bm{z}^{\ell+1}_{i}" class="ltx_Math" display="inline" id="alg1.l23.m3"><semantics><msubsup><mi>𝒛</mi><mi>i</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\bm{z}^{\ell+1}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from <math alttext="\bm{z}^{\ell}_{i}" class="ltx_Math" display="inline" id="alg1.l23.m4"><semantics><msubsup><mi>𝒛</mi><mi>i</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{z}^{\ell}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="alg1.l24">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">24:</span></span>         <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l25">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">25:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l26">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">26:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="alg1.l26.m1"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>γ</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { { bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l26.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Return all network parameters.
</span>
</div>
<div class="ltx_listingline" id="alg1.l27">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">27:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Deep Network for Optimizing Rate Reduction.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px3.p1">
<p class="ltx_p">Notice that the increment is constructed to emulate the gradient ascent for the rate reduction <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m1"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math>. Hence by transforming the features iteratively via the above process, we expect the rate reduction to increase, as we will see in the experimental section. This iterative process, once converged say after <math alttext="L" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m2"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math> iterations, gives the desired feature map <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math> on the input <math alttext="\bm{x}=\bm{z}……0" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m4"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{z}……0</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_z … … 0</annotation></semantics></math>, precisely in the form of a <span class="ltx_text ltx_font_italic">deep network</span>, in which each layer has the structure shown in Figure <a class="ltx_ref" href="#F3" title="Figure 4.3 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3</span></a> left:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S1.E14">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E14X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f(\bm{x},\bm{\theta})\;=" class="ltx_Math" display="inline" id="S1.E14X.m2"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle f(\bm{x},\bm{\theta})\;=</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ ) =</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\;\;f^{L}\circ f^{L-1}\circ\cdots\circ f^{1}\circ f^{0}(\bm{z}^{0})," class="ltx_Math" display="inline" id="S1.E14X.m3"><semantics><mrow><mrow><mrow><msup><mi>f</mi><mi>L</mi></msup><mo lspace="0.222em" rspace="0.222em">∘</mo><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0.222em" rspace="0.222em">∘</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msup><mi>f</mi><mn>1</mn></msup><mo lspace="0.222em" rspace="0.222em">∘</mo><msup><mi>f</mi><mn>0</mn></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>0</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\;\;f^{L}\circ f^{L-1}\circ\cdots\circ f^{1}\circ f^{0}(\bm{z}^{0}),</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT ∘ ⋯ ∘ italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∘ italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4.1.14)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E14Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f^{\ell}(\bm{z}^{\ell},\bm{\theta}^{\ell})\;\doteq" class="ltx_Math" display="inline" id="S1.E14Xa.m2"><semantics><mrow><mrow><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle f^{\ell}(\bm{z}^{\ell},\bm{\theta}^{\ell})\;\doteq</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ≐</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\;\;\bm{z}^{\ell+1}=\mathcal{P}_{\mathbb{S}^{n-1}}[\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})]," class="ltx_Math" display="inline" id="S1.E14Xa.m3"><semantics><mrow><mrow><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><msup><mi>𝕊</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\;\;\bm{z}^{\ell+1}=\mathcal{P}_{\mathbb{S}^{n-1}}[\bm{z}^{\ell}+\eta\cdot g(\bm{z}^{\ell},\bm{\theta}^{\ell})],</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = caligraphic_P start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ⋅ italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E14Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle g(\bm{z}^{\ell},\bm{\theta}^{\ell})\;=" class="ltx_Math" display="inline" id="S1.E14Xb.m2"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝜽</mi><mi mathvariant="normal">ℓ</mi></msup><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle g(\bm{z}^{\ell},\bm{\theta}^{\ell})\;=</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) =</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\;\;\bm{E}^{\ell}\bm{z}^{\ell}-\bm{\sigma}\big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\big{)}." class="ltx_Math" display="inline" id="S1.E14Xb.m3"><semantics><mrow><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>−</mo><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">]</mo></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\;\;\bm{E}^{\ell}\bm{z}^{\ell}-\bm{\sigma}\big{(}[\bm{C}^{\ell}_{1}\bm{z}^{\ell},\dots,\bm{C}^{\ell}_{K}\bm{z}^{\ell}]\big{)}.</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_σ ( [ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , … , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">As this deep network is derived from maximizing the rate <span class="ltx_text ltx_font_bold">redu</span>cation, we call it the <span class="ltx_text ltx_font_bold">ReduNet</span>. By comparing the architecture of ReduNet with those of popular empirically designed networks, <span class="ltx_text ltx_font_bold">ResNet</span> and <span class="ltx_text ltx_font_bold">NesNeXt</span> shown in Figure <a class="ltx_ref" href="#F3" title="Figure 4.3 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3</span></a>, the similarity is somewhat uncanny. Conceptually, ReduNet could also be used to justify the popular mixture of experts (<span class="ltx_text ltx_font_bold">MoE</span>) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx244" title="">SMM+17</a>]</cite> as each parallel channel, <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m5"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, can be viewed as an “expert” trained for each class of objects.</p>
</div>
<figure class="ltx_figure" id="F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 4.4 : Left: a mixture of experts (MoE) deep network [ SMM+17 ] . Right: a sparsity-promoting Switch Transformer [ FZS22 ] , used to implement MoE with 1.7 trillion parameters." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="136" id="F4.g1" src="chapters/chapter4/figs/MoE.png" width="269"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 4.4 : Left: a mixture of experts (MoE) deep network [ SMM+17 ] . Right: a sparsity-promoting Switch Transformer [ FZS22 ] , used to implement MoE with 1.7 trillion parameters." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="137" id="F4.g2" src="chapters/chapter4/figs/switched-transformer.png" width="269"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.4</span>: </span><span class="ltx_text" style="font-size:90%;">Left: a mixture of experts (MoE) deep network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx244" title="">SMM+17</a>]</cite>. Right: a sparsity-promoting Switch Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx83" title="">FZS22</a>]</cite>, used to implement MoE with 1.7 trillion parameters.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.SSS0.Px3.p2">
<p class="ltx_p">We summarize the training and evaluation of ReduNet in <a class="ltx_ref" href="#alg1" title="In Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> and <a class="ltx_ref" href="#alg2" title="In Deep Network for Optimizing Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>, respectively. Notice that all parameters of the network are explicitly constructed layer by layer in a <span class="ltx_text ltx_font_italic">forward propagation</span> fashion. The construction does not need any back propagation! The so-learned features can be directly used for classification, say via a nearest subspace classifier.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 4.2</span> </span> Evaluation algorithm for ReduNet</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span>Input <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg2.l1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, network parameters <math alttext="\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="alg2.l1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>γ</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\bm{E}^{\ell}\}_{\ell=1}^{L},\{\{\bm{C}^{\ell}_{k}\}_{k=1}^{K}\}_{\ell=1}^{L},\{\gamma_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { { bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , { italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>, learning rate <math alttext="\lambda" class="ltx_Math" display="inline" id="alg2.l1.m3"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_λ</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>feature <math alttext="\bm{z}^{L+1}" class="ltx_Math" display="inline" id="alg2.l2.m1"><semantics><msup><mi>𝒛</mi><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{z}^{L+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span><span class="ltx_text ltx_font_bold">procedure</span> <span class="ltx_text ltx_font_smallcaps">ReduNetEvaluation</span>(<math alttext="\bm{x}" class="ltx_Math" display="inline" id="alg2.l3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>     <math alttext="\bm{z}^{1}\leftarrow\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg2.l4.m1"><semantics><mrow><msup><mi>𝒛</mi><mn>1</mn></msup><mo stretchy="false">←</mo><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{1}\leftarrow\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ← bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l4.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Initialize the ReduNet per-layer iteration
</span>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="\ell\in\{1,\dots,L\}" class="ltx_Math" display="inline" id="alg2.l5.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\ell\in\{1,\dots,L\}</annotation><annotation encoding="application/x-llamapun">roman_ℓ ∈ { 1 , … , italic_L }</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>         <math alttext="\hat{\bm{\pi}}(\bm{z}^{\ell})\leftarrow\operatorname{\mathrm{softmax}}(-\lambda\begin{bmatrix}\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2},\dots,\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2}\end{bmatrix})\in[0,1]^{K}" class="ltx_Math" display="inline" id="alg2.l6.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝝅</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">←</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\hat{\bm{\pi}}(\bm{z}^{\ell})\leftarrow\operatorname{\mathrm{softmax}}(-\lambda\begin{bmatrix}\|\bm{C}^{\ell}_{1}\bm{z}^{\ell}\|_{2},\dots,\|\bm{C}^{\ell}_{K}\bm{z}^{\ell}\|_{2}\end{bmatrix})\in[0,1]^{K}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_π end_ARG ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ← roman_softmax ( - italic_λ [ start_ARG start_ROW start_CELL ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , ∥ bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) ∈ [ 0 , 1 ] start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l6.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Compute soft assignments <math alttext="\hat{\bm{\pi}}(\bm{z}^{\ell})" class="ltx_Math" display="inline" id="alg2.l6.m3"><semantics><mrow><mover accent="true"><mi>𝝅</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{\pi}}(\bm{z}^{\ell})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_π end_ARG ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span>         <math alttext="\bm{z}^{\ell+1}\leftarrow\mathcal{P}_{\mathbb{S}^{d-1}}\left(\bm{z}^{\ell}+\eta\left(\bm{E}^{\ell}\bm{z}^{\ell}-\sum_{k=1}^{K}\gamma_{k}\hat{\pi}_{k}(\bm{z}^{\ell})\bm{C}^{\ell}_{k}\bm{z}^{\ell}\right)\right)\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="alg2.l7.m1"><semantics><mrow><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">←</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo rspace="0.055em">−</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>π</mi><mo>^</mo></mover><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{\ell+1}\leftarrow\mathcal{P}_{\mathbb{S}^{d-1}}\left(\bm{z}^{\ell}+\eta\left(\bm{E}^{\ell}\bm{z}^{\ell}-\sum_{k=1}^{K}\gamma_{k}\hat{\pi}_{k}(\bm{z}^{\ell})\bm{C}^{\ell}_{k}\bm{z}^{\ell}\right)\right)\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ← caligraphic_P start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + italic_η ( bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l7.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Update feature <math alttext="\bm{z}^{\ell+1}" class="ltx_Math" display="inline" id="alg2.l7.m3"><semantics><msup><mi>𝒛</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> using <math alttext="\bm{z}^{\ell}" class="ltx_Math" display="inline" id="alg2.l7.m4"><semantics><msup><mi>𝒛</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\bm{z}^{L+1}" class="ltx_Math" display="inline" id="alg2.l9.m1"><semantics><msup><mi>𝒛</mi><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{z}^{L+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l9.m2"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> Return the output features
</span>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 4.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">To provide some intuition on how ReduNet transforms the features, we provide a simple example with mixed 3D Gaussians and visualize how the features are transformed in Figure <a class="ltx_ref" href="#F5" title="Figure 4.5 ‣ Example 4.1. ‣ Deep Network for Optimizing Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.5</span></a>.
Consider a mixture of three Gaussian distributions in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> that is projected onto <math alttext="\mathbb{S}^{2}" class="ltx_Math" display="inline" id="Thmexample1.p1.m2"><semantics><msup><mi>𝕊</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{S}^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. We first generate data points for 3 classes: for <math alttext="k=1,2,3" class="ltx_Math" display="inline" id="Thmexample1.p1.m3"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mrow><annotation encoding="application/x-tex">k=1,2,3</annotation><annotation encoding="application/x-llamapun">italic_k = 1 , 2 , 3</annotation></semantics></math>, <math alttext="\bm{X}_{k}=[\bm{x}_{k,1},\ldots,\bm{x}_{k,m}]\in\mathbb{R}^{3\times m}" class="ltx_Math" display="inline" id="Thmexample1.p1.m4"><semantics><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mi>m</mi></mrow></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}_{k}=[\bm{x}_{k,1},\ldots,\bm{x}_{k,m}]\in\mathbb{R}^{3\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = [ bold_italic_x start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_k , italic_m end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{x}_{k,i}\sim\mathcal{N}(\bm{\mu}_{k},\sigma_{k}^{2}\bm{I})" class="ltx_Math" display="inline" id="Thmexample1.p1.m5"><semantics><mrow><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝁</mi><mi>k</mi></msub><mo>,</mo><mrow><msubsup><mi>σ</mi><mi>k</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{k,i}\sim\mathcal{N}(\bm{\mu}_{k},\sigma_{k}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_k , italic_i end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_italic_μ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>, and <math alttext="{\pi}(\bm{x}_{k,i})=k" class="ltx_Math" display="inline" id="Thmexample1.p1.m6"><semantics><mrow><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">{\pi}(\bm{x}_{k,i})=k</annotation><annotation encoding="application/x-llamapun">italic_π ( bold_italic_x start_POSTSUBSCRIPT italic_k , italic_i end_POSTSUBSCRIPT ) = italic_k</annotation></semantics></math>.
We set <math alttext="m=500,\sigma_{1}=\sigma_{2}=\sigma_{3}=0.1" class="ltx_Math" display="inline" id="Thmexample1.p1.m7"><semantics><mrow><mrow><mi>m</mi><mo>=</mo><mn>500</mn></mrow><mo>,</mo><mrow><msub><mi>σ</mi><mn>1</mn></msub><mo>=</mo><msub><mi>σ</mi><mn>2</mn></msub><mo>=</mo><msub><mi>σ</mi><mn>3</mn></msub><mo>=</mo><mn>0.1</mn></mrow></mrow><annotation encoding="application/x-tex">m=500,\sigma_{1}=\sigma_{2}=\sigma_{3}=0.1</annotation><annotation encoding="application/x-llamapun">italic_m = 500 , italic_σ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 0.1</annotation></semantics></math>, and <math alttext="\bm{\mu}_{1},\bm{\mu}_{2},\bm{\mu}_{3}\in\mathbb{S}^{2}" class="ltx_Math" display="inline" id="Thmexample1.p1.m8"><semantics><mrow><mrow><msub><mi>𝝁</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝝁</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝝁</mi><mn>3</mn></msub></mrow><mo>∈</mo><msup><mi>𝕊</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\bm{\mu}_{1},\bm{\mu}_{2},\bm{\mu}_{3}\in\mathbb{S}^{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_μ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_italic_μ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ∈ blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Then we project all the data points onto <math alttext="\mathbb{S}^{2}" class="ltx_Math" display="inline" id="Thmexample1.p1.m9"><semantics><msup><mi>𝕊</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{S}^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="\bm{x}_{k,i}/\|\bm{x}_{k,i}\|_{2}" class="ltx_Math" display="inline" id="Thmexample1.p1.m10"><semantics><mrow><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>/</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\bm{x}_{k,i}/\|\bm{x}_{k,i}\|_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_k , italic_i end_POSTSUBSCRIPT / ∥ bold_italic_x start_POSTSUBSCRIPT italic_k , italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.
To construct the network (computing <math alttext="\bm{E}^{\ell},\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="Thmexample1.p1.m11"><semantics><mrow><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup></mrow><annotation encoding="application/x-tex">\bm{E}^{\ell},\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for the <math alttext="\ell" class="ltx_Math" display="inline" id="Thmexample1.p1.m12"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th layer), we set the number of iterations/layers <math alttext="L=2,000" class="ltx_Math" display="inline" id="Thmexample1.p1.m13"><semantics><mrow><mi>L</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">L=2,000</annotation><annotation encoding="application/x-llamapun">italic_L = 2 , 000</annotation></semantics></math>, step size <math alttext="\eta=0.5" class="ltx_Math" display="inline" id="Thmexample1.p1.m14"><semantics><mrow><mi>η</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\eta=0.5</annotation><annotation encoding="application/x-llamapun">italic_η = 0.5</annotation></semantics></math>, and precision <math alttext="\epsilon=0.1" class="ltx_Math" display="inline" id="Thmexample1.p1.m15"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.1</annotation><annotation encoding="application/x-llamapun">italic_ϵ = 0.1</annotation></semantics></math>.
We do this only to demonstrate that our framework leads to stable deep networks even with thousands of layers.
In practice, thousands of layers may not be necessary and one can stop whenever adding new layers gives diminishing returns.
For this example, a couple of hundred layers is sufficient. Hence, the clear optimization objective gives a natural criterion for the depth of the network needed.</p>
</div>
<div class="ltx_para" id="Thmexample1.p2">
<p class="ltx_p">As shown in <a class="ltx_ref" href="#F5" title="In Example 4.1. ‣ Deep Network for Optimizing Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>, we can observe that after the mapping <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="Thmexample1.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math>, samples from the same class are highly compressed and converge to a single cluster and the angle between two different clusters is approximately <math alttext="\pi/2" class="ltx_Math" display="inline" id="Thmexample1.p2.m2"><semantics><mrow><mi>π</mi><mo>/</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\pi/2</annotation><annotation encoding="application/x-llamapun">italic_π / 2</annotation></semantics></math>, which is well aligned with the optimal solution <math alttext="\bm{Z}^{\star}" class="ltx_Math" display="inline" id="Thmexample1.p2.m3"><semantics><msup><mi>𝒁</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> of the MCR<sup class="ltx_sup">2</sup> loss in <math alttext="\mathbb{S}^{2}" class="ltx_Math" display="inline" id="Thmexample1.p2.m5"><semantics><msup><mi>𝕊</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{S}^{2}</annotation><annotation encoding="application/x-llamapun">blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
MCR<sup class="ltx_sup">2</sup> loss of features on different layers can be found in Figure <a class="ltx_ref" href="#F5" title="Figure 4.5 ‣ Example 4.1. ‣ Deep Network for Optimizing Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.5</span></a>(<span class="ltx_text ltx_font_bold">c</span>). Empirically, we find that the constructed ReduNet is able to maximize MCR<sup class="ltx_sup">2</sup> loss and converges stably and samples from the same class converge to one cluster and different clusters are orthogonal to each other.
Moreover, when sampling new data points from the same distributions, we find that new samples from the same class consistently converge to the same cluster center as the training samples.</p>
</div>
<figure class="ltx_figure" id="F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F5.sf1"><img alt="(a) 𝑿 train \bm{X}_{\text{train}} bold_italic_X start_POSTSUBSCRIPT train end_POSTSUBSCRIPT" class="ltx_graphics" id="F5.sf1.g1" src="chapters/chapter4/figs/scatter3d-X_train.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><math alttext="\bm{X}_{\text{train}}" class="ltx_Math" display="inline" id="F5.sf1.m2"><semantics><msub><mi mathsize="90%">𝑿</mi><mtext mathsize="90%">train</mtext></msub><annotation encoding="application/x-tex">\bm{X}_{\text{train}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT train end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F5.sf2"><img alt="(a) 𝑿 train \bm{X}_{\text{train}} bold_italic_X start_POSTSUBSCRIPT train end_POSTSUBSCRIPT" class="ltx_graphics" id="F5.sf2.g1" src="chapters/chapter4/figs/scatter3d-Z_train.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><math alttext="\bm{Z}_{\text{train}}" class="ltx_Math" display="inline" id="F5.sf2.m2"><semantics><msub><mi mathsize="90%">𝒁</mi><mtext mathsize="90%">train</mtext></msub><annotation encoding="application/x-tex">\bm{Z}_{\text{train}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT train end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F5.sf3"><img alt="(a) 𝑿 train \bm{X}_{\text{train}} bold_italic_X start_POSTSUBSCRIPT train end_POSTSUBSCRIPT" class="ltx_graphics" id="F5.sf3.g1" src="chapters/chapter4/figs/scatter3d-loss-traintest.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Loss (train/val)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.5</span>: </span><span class="ltx_text" style="font-size:90%;">Original samples and learned representations for 3D Mixture of Gaussians. We visualize data points <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F5.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> (before mapping <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="F5.m6"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math>) in (<span class="ltx_text ltx_font_bold">a</span>) and learned features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="F5.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> (after mapping <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="F5.m8"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math>) in (<span class="ltx_text ltx_font_bold">b</span>) by scatter plot. In each scatter plot, each color represents one class of samples. In (<span class="ltx_text ltx_font_bold">c</span>), we also show the plots for the progression of values of the objective functions.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample1.p3">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p3.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1.2 </span>Convolutional Networks from Invariant Rate Reduction</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p">In the previous section, we derived the layer-wise architecture of a deep network, the ReduNet, using unrolled optimization for the rate reduction objective.
Specifically, the compression term <math alttext="R^{c}_{\epsilon}(\bm{Z}\mid\bm{\Pi})" class="ltx_Math" display="inline" id="S1.SS2.p1.m1"><semantics><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R^{c}_{\epsilon}(\bm{Z}\mid\bm{\Pi})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π )</annotation></semantics></math> in (<a class="ltx_ref" href="#S1.E1" title="Equation 4.1.1 ‣ Gradient Ascent for Coding Rate Reduction. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>) is designed to compress representations from the same class. However, this formulation does not account for possible domain transformation or deformation of the input data. For instance, shifting an object slightly to the right does not change the semantic label of an image. In this section, we will demonstrate how convolutional layers can be derived by maximizing a rate reduction objective that is invariant to certain domain deformations, such as image rotations and translations.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p">For many clustering or classification tasks (such as object detection in images), we consider two samples as <span class="ltx_text ltx_font_italic">equivalent</span> if they differ by certain classes of domain deformations or augmentations <math alttext="\mathcal{T}=\{\tau\}" class="ltx_Math" display="inline" id="S1.SS2.p2.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>τ</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}=\{\tau\}</annotation><annotation encoding="application/x-llamapun">caligraphic_T = { italic_τ }</annotation></semantics></math>. Hence, we are only interested in low-dimensional structures that are <span class="ltx_text ltx_font_italic">invariant</span> to such deformations (i.e., <math alttext="\bm{x}\in\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS2.p2.m2"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathcal{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ caligraphic_M</annotation></semantics></math> iff <math alttext="\tau(\bm{x})\in\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS2.p2.m3"><semantics><mrow><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><annotation encoding="application/x-tex">\tau(\bm{x})\in\mathcal{M}</annotation><annotation encoding="application/x-llamapun">italic_τ ( bold_italic_x ) ∈ caligraphic_M</annotation></semantics></math> for all <math alttext="\tau\in\mathcal{T}" class="ltx_Math" display="inline" id="S1.SS2.p2.m4"><semantics><mrow><mi>τ</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒯</mi></mrow><annotation encoding="application/x-tex">\tau\in\mathcal{T}</annotation><annotation encoding="application/x-llamapun">italic_τ ∈ caligraphic_T</annotation></semantics></math> ),
which are known to have sophisticated geometric and topological structures and can be difficult to learn precisely in practice, even with rigorously designed CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx54" title="">CW16</a>]</cite>.
In this framework, this can be formulated in a very natural way: all equivariant instances are to be embedded into the same subspace, so that the subspace itself is invariant to the transformations under consideration.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p">In many applications, such as serial data or imagery data, the semantic meaning (labels) of the data are <span class="ltx_text ltx_font_italic">invariant</span> to certain transformations <math alttext="\mathfrak{g}\in\mathbb{G}" class="ltx_Math" display="inline" id="S1.SS2.p3.m1"><semantics><mrow><mi>𝔤</mi><mo>∈</mo><mi>𝔾</mi></mrow><annotation encoding="application/x-tex">\mathfrak{g}\in\mathbb{G}</annotation><annotation encoding="application/x-llamapun">fraktur_g ∈ blackboard_G</annotation></semantics></math> (for some group <math alttext="\mathbb{G}" class="ltx_Math" display="inline" id="S1.SS2.p3.m2"><semantics><mi>𝔾</mi><annotation encoding="application/x-tex">\mathbb{G}</annotation><annotation encoding="application/x-llamapun">blackboard_G</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx56" title="">CW16b</a>, <a class="ltx_ref" href="bib.html#bibx315" title="">ZKR+17</a>]</cite>. For example, the meaning of an audio signal is invariant to shift in time; and the identity of an object in an image is invariant to translation in the image plane. Hence, we prefer the feature mapping <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS2.p3.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math> is rigorously invariant to such transformations:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{\em Group Invariance:}\;f(\bm{x}\circ\mathfrak{g},\bm{\theta})\sim f(\bm{x},\bm{\theta}),\ \forall\mathfrak{g}\in\mathbb{G}," class="ltx_Math" display="block" id="S1.E15.m1"><semantics><mrow><mrow><mrow><mrow><mtext class="ltx_mathvariant_italic">Group Invariance:</mtext><mo lspace="0.280em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>𝔤</mi></mrow><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∼</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>𝔤</mi></mrow><mo>∈</mo><mi>𝔾</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mbox{\em Group Invariance:}\;f(\bm{x}\circ\mathfrak{g},\bm{\theta})\sim f(\bm{x},\bm{\theta}),\ \forall\mathfrak{g}\in\mathbb{G},</annotation><annotation encoding="application/x-llamapun">Group Invariance: italic_f ( bold_italic_x ∘ fraktur_g , bold_italic_θ ) ∼ italic_f ( bold_italic_x , bold_italic_θ ) , ∀ fraktur_g ∈ blackboard_G ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where “<math alttext="\sim" class="ltx_Math" display="inline" id="S1.SS2.p3.m4"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation><annotation encoding="application/x-llamapun">∼</annotation></semantics></math>” indicates two features belonging to the same equivalent class. Although to ensure invariance or equivarience, convolutional operators have been common practice in deep networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx56" title="">CW16b</a>]</cite>, it remains challenging in practice to train an (empirically designed) convolution network from scratch that can <span class="ltx_text ltx_font_italic">guarantee</span> invariance even to simple transformations such as translation and rotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx8" title="">AW18</a>, <a class="ltx_ref" href="bib.html#bibx81" title="">ETT+17</a>]</cite>. An alternative approach is to carefully design convolution filters of each layer so as to ensure translational invariance for a wide range of signals, say using wavelets as in ScatteringNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx33" title="">BM13</a>]</cite> and followup works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx289" title="">WB18</a>]</cite>. However, in order to ensure invariance to generic signals, the number of convolutions needed usually grows exponentially with network depth. That is the reason why this type of network cannot be constructed so deep, usually only several layers.</p>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p">Now, we show that the MCR<sup class="ltx_sup">2</sup> principle is compatible with invariance in a natural and precise way: we only need to assign all transformed versions <math alttext="\{\bm{x}\circ\mathfrak{g}\mid\mathfrak{g}\in\mathbb{G}\}" class="ltx_Math" display="inline" id="S1.SS2.p4.m2"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>𝒙</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>𝔤</mi></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mi>𝔤</mi><mo>∈</mo><mi>𝔾</mi></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}\circ\mathfrak{g}\mid\mathfrak{g}\in\mathbb{G}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x ∘ fraktur_g ∣ fraktur_g ∈ blackboard_G }</annotation></semantics></math> into the same class as the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS2.p4.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and map their features <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.p4.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> all to the same subspace <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S1.SS2.p4.m5"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation><annotation encoding="application/x-llamapun">caligraphic_S</annotation></semantics></math>. Hence, all group equivariant information is encoded only inside the subspace, and any classifier defined on the resulting set of subspaces will be automatically invariant to such group transformations. See Figure <a class="ltx_ref" href="#F6" title="Figure 4.6 ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.6</span></a> for an illustration of the examples of 1D rotation and 2D translation. Next, we will rigorously show that when the group <math alttext="\mathbb{G}" class="ltx_Math" display="inline" id="S1.SS2.p4.m6"><semantics><mi>𝔾</mi><annotation encoding="application/x-tex">\mathbb{G}</annotation><annotation encoding="application/x-llamapun">blackboard_G</annotation></semantics></math> is circular 1D shifting, the resulting deep network naturally becomes a <span class="ltx_text ltx_font_italic">multi-channel convolution network</span>.
Because the so-constructed network only needs to ensure invariance for the given data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS2.p4.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> or their features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.SS2.p4.m8"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, the number of convolutions needed actually remains constant through a very deep network, as opposed to the ScatteringNet.</p>
</div>
<figure class="ltx_figure" id="F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F6.fig1"><img alt="Figure 4.6 : Illustration of the sought representation that is equivariant/invariant to image rotation (left) or translation (right): all transformed images of each class are mapped into the same subspace that is incoherent to other subspaces. The features embedded in each subspace are equivariant to the transformation group whereas each subspace is invariant to such transformations." class="ltx_graphics" id="F6.g1" src="chapters/chapter4/figs/ortho_diagram_1d.png"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F6.fig2"><img alt="Figure 4.6 : Illustration of the sought representation that is equivariant/invariant to image rotation (left) or translation (right): all transformed images of each class are mapped into the same subspace that is incoherent to other subspaces. The features embedded in each subspace are equivariant to the transformation group whereas each subspace is invariant to such transformations." class="ltx_graphics" id="F6.g2" src="chapters/chapter4/figs/ortho_diagram_2d.png"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.6</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of the sought representation that is equivariant/invariant to image rotation (left) or translation (right): all transformed images of each class are mapped into the same subspace that is incoherent to other subspaces. The features embedded in each subspace are equivariant to the transformation group whereas each subspace is invariant to such transformations.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">1D Serial Data and Shift Invariance</h4>
<div class="ltx_para" id="S1.SS2.SSS0.Px1.p1">
<p class="ltx_p">To classify one-dimensional data <math alttext="\bm{x}=[x(0),x(1),\ldots,x(D-1)]\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}=[x(0),x(1),\ldots,x(D-1)]\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = [ italic_x ( 0 ) , italic_x ( 1 ) , … , italic_x ( italic_D - 1 ) ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> invariant under shifting, we take <math alttext="\mathbb{G}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m2"><semantics><mi>𝔾</mi><annotation encoding="application/x-tex">\mathbb{G}</annotation><annotation encoding="application/x-llamapun">blackboard_G</annotation></semantics></math> to be the group of all circular shifts. Each observation <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m3"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> generates a family <math alttext="\{\bm{x}_{i}\circ\mathfrak{g}\,|\,\mathfrak{g}\in\mathbb{G}\}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m4"><semantics><mrow><mo stretchy="false">{</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>𝔤</mi></mrow><mo lspace="0.170em" rspace="0.170em">|</mo><mrow><mi>𝔤</mi><mo>∈</mo><mi>𝔾</mi></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\circ\mathfrak{g}\,|\,\mathfrak{g}\in\mathbb{G}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∘ fraktur_g | fraktur_g ∈ blackboard_G }</annotation></semantics></math> of shifted copies, which are the columns of the circulant matrix <math alttext="\mathsf{circ}(\bm{x}_{i})\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m5"><semantics><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{x}_{i})\in\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> given by</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathsf{circ}(\bm{x})\,\doteq\,\left[\begin{array}[]{ccccc}x(0)&amp;x(D-1)&amp;\dots&amp;x(2)&amp;x(1)\\
x(1)&amp;x(0)&amp;x(D-1)&amp;\cdots&amp;x(2)\\
\vdots&amp;x(1)&amp;x(0)&amp;\ddots&amp;\vdots\\
x(D-2)&amp;\vdots&amp;\ddots&amp;\ddots&amp;x(D-1)\\
x(D-1)&amp;x(D-2)&amp;\dots&amp;x(1)&amp;x(0)\end{array}\right]\in\mathbb{R}^{D\times D}." class="ltx_Math" display="block" id="S1.E16.m1"><semantics><mrow><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.170em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.448em">≐</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">⋯</mi></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">⋱</mi></mtd><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">⋮</mi></mtd><mtd><mi mathvariant="normal">⋱</mi></mtd><mtd><mi mathvariant="normal">⋱</mi></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{x})\,\doteq\,\left[\begin{array}[]{ccccc}x(0)&amp;x(D-1)&amp;\dots&amp;x(2)&amp;x(1)\\
x(1)&amp;x(0)&amp;x(D-1)&amp;\cdots&amp;x(2)\\
\vdots&amp;x(1)&amp;x(0)&amp;\ddots&amp;\vdots\\
x(D-2)&amp;\vdots&amp;\ddots&amp;\ddots&amp;x(D-1)\\
x(D-1)&amp;x(D-2)&amp;\dots&amp;x(1)&amp;x(0)\end{array}\right]\in\mathbb{R}^{D\times D}.</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_x ) ≐ [ start_ARRAY start_ROW start_CELL italic_x ( 0 ) end_CELL start_CELL italic_x ( italic_D - 1 ) end_CELL start_CELL … end_CELL start_CELL italic_x ( 2 ) end_CELL start_CELL italic_x ( 1 ) end_CELL end_ROW start_ROW start_CELL italic_x ( 1 ) end_CELL start_CELL italic_x ( 0 ) end_CELL start_CELL italic_x ( italic_D - 1 ) end_CELL start_CELL ⋯ end_CELL start_CELL italic_x ( 2 ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL start_CELL italic_x ( 1 ) end_CELL start_CELL italic_x ( 0 ) end_CELL start_CELL ⋱ end_CELL start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL italic_x ( italic_D - 2 ) end_CELL start_CELL ⋮ end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL start_CELL italic_x ( italic_D - 1 ) end_CELL end_ROW start_ROW start_CELL italic_x ( italic_D - 1 ) end_CELL start_CELL italic_x ( italic_D - 2 ) end_CELL start_CELL … end_CELL start_CELL italic_x ( 1 ) end_CELL start_CELL italic_x ( 0 ) end_CELL end_ROW end_ARRAY ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We refer the reader to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx142" title="">KS12</a>]</cite> for properties of circulant matrices. For simplicity, let <math alttext="\bm{Z}^{1}\doteq[\bm{z}_{1}^{1},\dots,\bm{z}_{N}^{1}]=\bm{X}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m6"><semantics><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>≐</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo stretchy="false">]</mo></mrow><mo>=</mo><mi>𝑿</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}^{1}\doteq[\bm{z}_{1}^{1},\dots,\bm{z}_{N}^{1}]=\bm{X}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ≐ [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] = bold_italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Again, to simplify discussion, we assume for now that the initial features <math alttext="\bm{Z}^{1}" class="ltx_Math" display="inline" id="footnote7.m1"><semantics><msup><mi>𝒁</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> are <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote7.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> themselves hence have the same dimension <math alttext="d" class="ltx_Math" display="inline" id="footnote7.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, i.e., <math alttext="D=d" class="ltx_Math" display="inline" id="footnote7.m4"><semantics><mrow><mi>D</mi><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">D=d</annotation><annotation encoding="application/x-llamapun">italic_D = italic_d</annotation></semantics></math>. But that does not need to be the case as we will soon see that we need to lift <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote7.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to a higher dimension.</span></span></span> Then what happens if we construct the ReduNet from their circulant families <math alttext="\mathsf{circ}(\bm{Z}^{1})=\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]\in\mathbb{R}^{d\times(dN)}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m7"><semantics><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{Z}^{1})=\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]\in\mathbb{R}^{d\times(dN)}</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) = [ sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , … , sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × ( italic_d italic_N ) end_POSTSUPERSCRIPT</annotation></semantics></math>? That is, we want to compress and map all these into the same subspace by the ReduNet.</p>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px1.p2">
<p class="ltx_p">Notice that now the data covariance matrix:</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx43">
<tbody id="S1.E17">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathsf{circ}(\bm{Z}^{1})\mathsf{circ}(\bm{Z}^{1})^{\top}" class="ltx_Math" display="inline" id="S1.Ex3.m1"><semantics><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\displaystyle\mathsf{circ}(\bm{Z}^{1})\mathsf{circ}(\bm{Z}^{1})^{\top}</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S1.Ex3.m2"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]^{\top}" class="ltx_Math" display="inline" id="S1.Ex3.m3"><semantics><mrow><mrow><mo>[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\displaystyle\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]\left[\mathsf{circ}(\bm{z}_{1}^{1}),\dots,\mathsf{circ}(\bm{z}_{N}^{1})\right]^{\top}</annotation><annotation encoding="application/x-llamapun">[ sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , … , sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ] [ sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , … , sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.17)</span></td>
</tr>
<tr class="ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S1.E17.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sum_{i=1}^{N}\mathsf{circ}(\bm{z}_{i}^{1})\mathsf{circ}(\bm{z}_{i}^{1})^{\top}\;\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S1.E17.m2"><semantics><mrow><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>i</mi><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>i</mi><mn>1</mn></msubsup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i=1}^{N}\mathsf{circ}(\bm{z}_{i}^{1})\mathsf{circ}(\bm{z}_{i}^{1})^{\top}\;\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">associated with this family of samples is <span class="ltx_text ltx_font_italic">automatically</span> a (symmetric) circulant matrix. Moreover, because the circulant property is preserved under sums, inverses, and products, the matrices <math alttext="\bm{E}^{1}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p2.m1"><semantics><msup><mi>𝑬</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{E}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}^{1}_{k}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p2.m2"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\bm{C}^{1}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are also automatically circulant matrices, whose application to a feature vector <math alttext="\bm{z}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p2.m3"><semantics><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> can be implemented using circular convolution “<math alttext="\circledast" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p2.m4"><semantics><mo>⊛</mo><annotation encoding="application/x-tex">\circledast</annotation><annotation encoding="application/x-llamapun">⊛</annotation></semantics></math>”.
Specifically, we have the following proposition.</p>
</div>
<div class="ltx_theorem ltx_theorem_proposition" id="Thmproposition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Proposition 4.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Convolution structures of <math alttext="\bm{E}^{1}" class="ltx_Math" display="inline" id="Thmproposition1.m1"><semantics><msup><mi>𝑬</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{E}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}^{1}_{k}" class="ltx_Math" display="inline" id="Thmproposition1.m2"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\bm{C}^{1}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmproposition1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The matrix</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{E}^{1}=\alpha\big{(}\bm{I}+\alpha\mathsf{circ}(\bm{Z}^{1})\mathsf{circ}(\bm{Z}^{1})^{\top}\big{)}^{-1}" class="ltx_Math" display="block" id="S1.E18.m1"><semantics><mrow><msup><mi>𝑬</mi><mn>1</mn></msup><mo>=</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{E}^{1}=\alpha\big{(}\bm{I}+\alpha\mathsf{circ}(\bm{Z}^{1})\mathsf{circ}(\bm{Z}^{1})^{\top}\big{)}^{-1}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_α ( bold_italic_I + italic_α sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">is a circulant matrix and represents a circular convolution:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{E}^{1}\bm{z}=\bm{e}_{1}\circledast\bm{z}," class="ltx_Math" display="block" id="S1.Ex4.m1"><semantics><mrow><mrow><mrow><msup><mi>𝑬</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>=</mo><mrow><msub><mi>𝒆</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒛</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{E}^{1}\bm{z}=\bm{e}_{1}\circledast\bm{z},</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_z = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\bm{e}_{1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmproposition1.p1.m1"><semantics><mrow><msub><mi>𝐞</mi><mn>1</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{e}_{1}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the first column vector of <math alttext="\bm{E}^{1}" class="ltx_Math" display="inline" id="Thmproposition1.p1.m2"><semantics><msup><mi>𝐄</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{E}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> and “<math alttext="\circledast" class="ltx_Math" display="inline" id="Thmproposition1.p1.m3"><semantics><mo>⊛</mo><annotation encoding="application/x-tex">\circledast</annotation><annotation encoding="application/x-llamapun">⊛</annotation></semantics></math>” is circular convolution defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{e}_{1}\circledast\bm{z})_{i}\doteq\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\,\,\textsf{mod}\,\,d)." class="ltx_Math" display="block" id="S1.Ex5.m1"><semantics><mrow><mrow><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒆</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒛</mi></mrow><mo stretchy="false">)</mo></mrow><mi>i</mi></msub><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>e</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>i</mi><mo>+</mo><mi>d</mi></mrow><mo>−</mo><mrow><mi>j</mi><mo lspace="0.330em" rspace="0em">​</mo><mtext class="ltx_mathvariant_sans-serif-italic">mod</mtext><mo lspace="0.330em" rspace="0em">​</mo><mi>d</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\bm{e}_{1}\circledast\bm{z})_{i}\doteq\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\,\,\textsf{mod}\,\,d).</annotation><annotation encoding="application/x-llamapun">( bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_z ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≐ ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_j ) italic_x ( italic_i + italic_d - italic_j mod italic_d ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Similarly, the matrices <math alttext="\bm{C}^{1}_{k}" class="ltx_Math" display="inline" id="Thmproposition1.p1.m4"><semantics><msubsup><mi>𝐂</mi><mi>k</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\bm{C}^{1}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> associated with any subsets of <math alttext="\bm{Z}^{1}" class="ltx_Math" display="inline" id="Thmproposition1.p1.m5"><semantics><msup><mi>𝐙</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> are also circular convolutions.</span></p>
</div>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px1.p3">
<p class="ltx_p">Not only do the first-layer parameters <math alttext="\bm{E}^{1}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m1"><semantics><msup><mi>𝑬</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{E}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}^{1}_{k}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m2"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mn>1</mn></msubsup><annotation encoding="application/x-tex">\bm{C}^{1}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> of the ReduNet become circulant convolutions but also the next-layer features remain circulant matrices.
That is, the incremental feature transform in (<a class="ltx_ref" href="#S1.E13" title="Equation 4.1.13 ‣ Gradient-Guided Feature Map Increment. ‣ 4.1.1 Deep Networks from Unrolled Gradient Descent ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.13</span></a>) applied to all shifted versions of a <math alttext="\bm{z}^{1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m3"><semantics><mrow><msup><mi>𝒛</mi><mn>1</mn></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{1}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, given by</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathsf{circ}(\bm{z}^{1})+\eta\cdot\bm{E}^{1}\mathsf{circ}(\bm{z}^{1})-\eta\cdot\bm{\sigma}\Big{(}[\bm{C}_{1}^{1}\mathsf{circ}(\bm{z}^{1}),\ldots,\bm{C}^{1}_{K}\mathsf{circ}(\bm{z}^{1})]\Big{)}," class="ltx_Math" display="block" id="S1.E19.m1"><semantics><mrow><mrow><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>𝑬</mi><mn>1</mn></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>𝝈</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝑪</mi><mn>1</mn><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝑪</mi><mi>K</mi><mn>1</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{z}^{1})+\eta\cdot\bm{E}^{1}\mathsf{circ}(\bm{z}^{1})-\eta\cdot\bm{\sigma}\Big{(}[\bm{C}_{1}^{1}\mathsf{circ}(\bm{z}^{1}),\ldots,\bm{C}^{1}_{K}\mathsf{circ}(\bm{z}^{1})]\Big{)},</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) + italic_η ⋅ bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) - italic_η ⋅ bold_italic_σ ( [ bold_italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , … , bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is a circulant matrix. This implies that there is no need to construct circulant families from the second layer features as we did for the first layer.
By denoting</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}^{2}\propto\bm{z}^{1}+\eta\cdot g(\bm{z}^{1},\bm{\theta}^{1})=\bm{z}^{1}+\eta\cdot\bm{e}_{1}\circledast\bm{z}^{1}-\eta\cdot\bm{\sigma}\Big{(}[\bm{c}_{1}^{1}\circledast\bm{z}^{1},\dots,\bm{c}^{1}_{K}\circledast\bm{z}^{1}]\Big{)}," class="ltx_Math" display="block" id="S1.E20.m1"><semantics><mrow><mrow><msup><mi>𝒛</mi><mn>2</mn></msup><mo>∝</mo><mrow><msup><mi>𝒛</mi><mn>1</mn></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mn>1</mn></msup><mo>,</mo><msup><mi>𝜽</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>𝒛</mi><mn>1</mn></msup><mo>+</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝒆</mi><mn>1</mn></msub></mrow><mo lspace="0.222em" rspace="0.222em">⊛</mo><msup><mi>𝒛</mi><mn>1</mn></msup></mrow></mrow><mo>−</mo><mrow><mrow><mi>η</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>𝝈</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>𝒄</mi><mn>1</mn><mn>1</mn></msubsup><mo lspace="0.222em" rspace="0.222em">⊛</mo><msup><mi>𝒛</mi><mn>1</mn></msup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>𝒄</mi><mi>K</mi><mn>1</mn></msubsup><mo lspace="0.222em" rspace="0.222em">⊛</mo><msup><mi>𝒛</mi><mn>1</mn></msup></mrow><mo stretchy="false">]</mo></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{z}^{2}\propto\bm{z}^{1}+\eta\cdot g(\bm{z}^{1},\bm{\theta}^{1})=\bm{z}^{1}+\eta\cdot\bm{e}_{1}\circledast\bm{z}^{1}-\eta\cdot\bm{\sigma}\Big{(}[\bm{c}_{1}^{1}\circledast\bm{z}^{1},\dots,\bm{c}^{1}_{K}\circledast\bm{z}^{1}]\Big{)},</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∝ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_η ⋅ italic_g ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) = bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_η ⋅ bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - italic_η ⋅ bold_italic_σ ( [ bold_italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ⊛ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , bold_italic_c start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ⊛ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">the features at the next level can be written as</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathsf{circ}(\bm{Z}^{2})=\big{[}\mathsf{circ}(\bm{z}_{1}^{1}+\eta g(\bm{z}_{1}^{1},\bm{\theta}^{1})),\dots,\mathsf{circ}(\bm{z}_{N}^{1}+\eta g(\bm{z}_{N}^{1},\bm{\theta}^{1}))\big{]}." class="ltx_Math" display="block" id="S1.Ex6.m1"><semantics><mrow><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msup><mi>𝜽</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒛</mi><mi>N</mi><mn>1</mn></msubsup><mo>,</mo><msup><mi>𝜽</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{Z}^{2})=\big{[}\mathsf{circ}(\bm{z}_{1}^{1}+\eta g(\bm{z}_{1}^{1},\bm{\theta}^{1})),\dots,\mathsf{circ}(\bm{z}_{N}^{1}+\eta g(\bm{z}_{N}^{1},\bm{\theta}^{1}))\big{]}.</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = [ sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_η italic_g ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ) , … , sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_η italic_g ( bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_θ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Continuing inductively, we see that all matrices <math alttext="\bm{E}^{\ell}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m4"><semantics><msup><mi>𝑬</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{E}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_E start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{C}^{\ell}_{k}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m5"><semantics><msubsup><mi>𝑪</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{C}^{\ell}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_C start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> based on such <math alttext="\mathsf{circ}(\bm{Z}^{\ell})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p3.m6"><semantics><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{Z}^{\ell})</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math> are circulant, and so are all features.
By virtue of the properties of the data, ReduNet has taken the form of a convolutional network, <span class="ltx_text ltx_font_italic">with no need to explicitly choose this structure!</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">A Fundamental Trade-off between Invariance and Sparsity.</h4>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p1">
<p class="ltx_p">There is one problem though: In general, the set of all circular permutations of a vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> gives a full-rank matrix. That is, the <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> “augmented” features associated with each sample (hence each class) typically already span the entire space <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m3"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. For instance, all shifted versions of a delta function <math alttext="\delta(d)" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m4"><semantics><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta(d)</annotation><annotation encoding="application/x-llamapun">italic_δ ( italic_d )</annotation></semantics></math> can generate any other signal as their (dense) weighted superposition. The MCR<sup class="ltx_sup">2</sup> objective (<a class="ltx_ref" href="Ch3.html#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) will not be able to distinguish classes as different subspaces.</p>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p2">
<p class="ltx_p">One natural remedy is to improve the separability of the data by “lifting” the original signal to a higher dimensional space, e.g., by taking their responses to multiple, filters <math alttext="\bm{k}_{1},\ldots,\bm{k}_{C}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m1"><semantics><mrow><mrow><msub><mi>𝒌</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒌</mi><mi>C</mi></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{k}_{1},\ldots,\bm{k}_{C}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}[c]=\bm{k}_{c}\circledast\bm{x}=\mathsf{circ}(\bm{k}_{c})\bm{x}\in\mathbb{R}^{d},\quad c=1,\ldots,C." class="ltx_Math" display="block" id="S1.E21.m1"><semantics><mrow><mrow><mrow><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝒌</mi><mi>c</mi></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒙</mi></mrow><mo>=</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒌</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="1.167em">,</mo><mrow><mi>c</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>C</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}[c]=\bm{k}_{c}\circledast\bm{x}=\mathsf{circ}(\bm{k}_{c})\bm{x}\in\mathbb{R}^{d},\quad c=1,\ldots,C.</annotation><annotation encoding="application/x-llamapun">bold_italic_z [ italic_c ] = bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ⊛ bold_italic_x = sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_c = 1 , … , italic_C .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The filters can be pre-designed invariance-promoting filters,<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>For 1D signals like audio, one may consider the conventional short-time Fourier transform (STFT); for 2D images, one may consider 2D wavelets as in the ScatteringNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx33" title="">BM13</a>]</cite>.</span></span></span> or adaptively learned from the data,<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>For learned filters, one can learn filters as the principal components of samples as in the PCANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx42" title="">CJG+15</a>]</cite> or from convolution dictionary learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx161" title="">LB19</a>, <a class="ltx_ref" href="bib.html#bibx215" title="">QLZ19</a>]</cite>.</span></span></span> or randomly selected as we do in our experiments. This operation lifts each original signal <math alttext="\bm{x}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m2"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> to a <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m3"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math>-channel feature, denoted as <math alttext="\bar{\bm{z}}\doteq[\bm{z}[1],\ldots,\bm{z}[C]]^{\top}\in\mathbb{R}^{C\times d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m4"><semantics><mrow><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo>≐</mo><msup><mrow><mo stretchy="false">[</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{z}}\doteq[\bm{z}[1],\ldots,\bm{z}[C]]^{\top}\in\mathbb{R}^{C\times d}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG ≐ [ bold_italic_z [ 1 ] , … , bold_italic_z [ italic_C ] ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
Then, we may construct the ReduNet on vector representations of <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m5"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math>, denoted as
<math alttext="\vec{(}\bar{\bm{z}})\doteq[\bm{z}[1]^{\top},\ldots,\bm{z}[C]^{\top}]\in\mathbb{R}^{dC}" class="ltx_math_unparsed" display="inline" id="S1.SS2.SSS0.Px2.p2.m6"><semantics><mrow><mover accent="true"><mo stretchy="false">(</mo><mo stretchy="false">→</mo></mover><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo><mo>≐</mo><mo stretchy="false">[</mo><mi>𝒛</mi><msup><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>𝒛</mi><msup><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo stretchy="false">]</mo><mo>∈</mo><mi>ℝ</mi><msup><mi></mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\vec{(}\bar{\bm{z}})\doteq[\bm{z}[1]^{\top},\ldots,\bm{z}[C]^{\top}]\in\mathbb{R}^{dC}</annotation><annotation encoding="application/x-llamapun">over→ start_ARG ( end_ARG over¯ start_ARG bold_italic_z end_ARG ) ≐ [ bold_italic_z [ 1 ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , … , bold_italic_z [ italic_C ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>.
The associated circulant version <math alttext="\mathsf{circ}(\bar{\bm{z}})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m7"><semantics><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bar{\bm{z}})</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( over¯ start_ARG bold_italic_z end_ARG )</annotation></semantics></math> and its data covariance matrix, denoted as <math alttext="\bar{\bm{\Sigma}}(\bar{\bm{z}})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m8"><semantics><mrow><mover accent="true"><mi>𝚺</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{\Sigma}}(\bar{\bm{z}})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_Σ end_ARG ( over¯ start_ARG bold_italic_z end_ARG )</annotation></semantics></math>, for all its shifted versions are given as:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S1.E22">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S1.E22X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathsf{circ}(\bar{\bm{z}})\doteq\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])\\
\vdots\\
\mathsf{circ}(\bm{z}[C])\end{matrix}\right]\in\mathbb{R}^{dC\times d},\quad\bar{\bm{\Sigma}}(\bar{\bm{z}})\doteq\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])\\
\vdots\\
\mathsf{circ}(\bm{z}[C])\end{matrix}\right]\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])^{\top},\ldots,\mathsf{circ}(\bm{z}[C])^{\top}\end{matrix}\right]\in\mathbb{R}^{dC\times dC}," class="ltx_Math" display="inline" id="S1.E22X.m2"><semantics><mrow><mrow><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mover accent="true"><mi>𝚺</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathsf{circ}(\bar{\bm{z}})\doteq\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])\\
\vdots\\
\mathsf{circ}(\bm{z}[C])\end{matrix}\right]\in\mathbb{R}^{dC\times d},\quad\bar{\bm{\Sigma}}(\bar{\bm{z}})\doteq\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])\\
\vdots\\
\mathsf{circ}(\bm{z}[C])\end{matrix}\right]\left[\begin{matrix}\mathsf{circ}(\bm{z}[1])^{\top},\ldots,\mathsf{circ}(\bm{z}[C])^{\top}\end{matrix}\right]\in\mathbb{R}^{dC\times dC},</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( over¯ start_ARG bold_italic_z end_ARG ) ≐ [ start_ARG start_ROW start_CELL sansserif_circ ( bold_italic_z [ 1 ] ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL sansserif_circ ( bold_italic_z [ italic_C ] ) end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C × italic_d end_POSTSUPERSCRIPT , over¯ start_ARG bold_Σ end_ARG ( over¯ start_ARG bold_italic_z end_ARG ) ≐ [ start_ARG start_ROW start_CELL sansserif_circ ( bold_italic_z [ 1 ] ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL sansserif_circ ( bold_italic_z [ italic_C ] ) end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL sansserif_circ ( bold_italic_z [ 1 ] ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , … , sansserif_circ ( bold_italic_z [ italic_C ] ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C × italic_d italic_C end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4.1.22)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\mathsf{circ}(\bm{z}[c])\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m9"><semantics><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bm{z}[c])\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( bold_italic_z [ italic_c ] ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="c\in[C]" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m10"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">c\in[C]</annotation><annotation encoding="application/x-llamapun">italic_c ∈ [ italic_C ]</annotation></semantics></math> is the circulant version of the <math alttext="c" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m11"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation><annotation encoding="application/x-llamapun">italic_c</annotation></semantics></math>-th channel of the feature <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m12"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math>. Then the columns of <math alttext="\mathsf{circ}(\bar{\bm{z}})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m13"><semantics><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bar{\bm{z}})</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( over¯ start_ARG bold_italic_z end_ARG )</annotation></semantics></math> will only span at most a <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m14"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-dimensional proper subspace in <math alttext="\mathbb{R}^{dC}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m15"><semantics><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{dC}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>. However, this simple lifting operation (if linear) is not sufficient to render the classes separable yet—features associated with other classes will span the <span class="ltx_text ltx_font_italic">same</span> <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m16"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-dimensional subspace. This reflects a fundamental conflict between invariance and linear (subspace) modeling: <span class="ltx_text ltx_font_italic">one cannot hope for arbitrarily shifted and superposed signals to belong to the same class.</span></p>
</div>
<figure class="ltx_figure" id="F7">
<p class="ltx_p ltx_align_center">
<img alt="Figure 4.7 : Each input signal 𝒙 \bm{x} bold_italic_x (an image here) can be represented as a superposition of sparse convolutions with multiple kernels 𝒅 c \bm{d}_{c} bold_italic_d start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT in a dictionary 𝑫 \bm{D} bold_italic_D ." class="ltx_graphics" id="F7.g1" src="chapters/chapter4/figs/sparse-representation.png"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.7</span>: </span><span class="ltx_text" style="font-size:90%;">Each input signal <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F7.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (an image here) can be represented as a superposition of sparse convolutions with multiple kernels <math alttext="\bm{d}_{c}" class="ltx_Math" display="inline" id="F7.m5"><semantics><msub><mi>𝒅</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\bm{d}_{c}</annotation><annotation encoding="application/x-llamapun">bold_italic_d start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> in a dictionary <math alttext="\bm{D}" class="ltx_Math" display="inline" id="F7.m6"><semantics><mi>𝑫</mi><annotation encoding="application/x-tex">\bm{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_D</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p3">
<p class="ltx_p">One way of resolving this conflict is to leverage additional structure within each class, in the form of <span class="ltx_text ltx_font_italic">sparsity</span>: signals within each class are not generated as an arbitrary linear superposition of some base atoms (or motifs), but only <span class="ltx_text ltx_font_italic">sparse</span> combinations of them and their shifted versions, as shown in Figure <a class="ltx_ref" href="#F7" title="Figure 4.7 ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.7</span></a>. More precisely, let <math alttext="\bm{D}_{k}=[\bm{d}_{k,1},\ldots,\bm{d}_{k,c}]" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m1"><semantics><mrow><msub><mi>𝑫</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒅</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒅</mi><mrow><mi>k</mi><mo>,</mo><mi>c</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{D}_{k}=[\bm{d}_{k,1},\ldots,\bm{d}_{k,c}]</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = [ bold_italic_d start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT , … , bold_italic_d start_POSTSUBSCRIPT italic_k , italic_c end_POSTSUBSCRIPT ]</annotation></semantics></math> denote a matrix with a collection of atoms associated for class <math alttext="k" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m2"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>, also known as a dictionary, then each signal <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in this class is sparsely generated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{d}_{k,1}\circledast z_{1}+\ldots+\bm{d}_{k,c}\circledast z_{c}=\mathsf{circ}(\bm{D}_{k})\bm{z}," class="ltx_Math" display="block" id="S1.E23.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><msub><mi>𝒅</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><msub><mi>z</mi><mn>1</mn></msub></mrow><mo>+</mo><mi mathvariant="normal">…</mi><mo>+</mo><mrow><msub><mi>𝒅</mi><mrow><mi>k</mi><mo>,</mo><mi>c</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><msub><mi>z</mi><mi>c</mi></msub></mrow></mrow><mo>=</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑫</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{d}_{k,1}\circledast z_{1}+\ldots+\bm{d}_{k,c}\circledast z_{c}=\mathsf{circ}(\bm{D}_{k})\bm{z},</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_d start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT ⊛ italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + … + bold_italic_d start_POSTSUBSCRIPT italic_k , italic_c end_POSTSUBSCRIPT ⊛ italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some sparse vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. Signals in different classes are then generated by different dictionaries whose atoms (or motifs) are incoherent from one another. Due to incoherence, signals in one class are unlikely to be sparsely represented by atoms in any other class. Hence all signals can be represented as</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\big{[}\mathsf{circ}(\bm{D}_{1}),\mathsf{circ}(\bm{D}_{2}),\ldots,\mathsf{circ}(\bm{D}_{K})\big{]}\bar{\bm{z}}," class="ltx_Math" display="block" id="S1.E24.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑫</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑫</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑫</mi><mi>K</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\big{[}\mathsf{circ}(\bm{D}_{1}),\mathsf{circ}(\bm{D}_{2}),\ldots,\mathsf{circ}(\bm{D}_{K})\big{]}\bar{\bm{z}},</annotation><annotation encoding="application/x-llamapun">bold_italic_x = [ sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) ] over¯ start_ARG bold_italic_z end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m5"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math> is sparse.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Notice that similar sparse representation models have long been proposed and used for classification purposes in applications such a face recognition, demonstrating excellent effectiveness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx297" title="">WYG+09</a>, <a class="ltx_ref" href="bib.html#bibx279" title="">WWG+12</a>]</cite>. Recently, the convolution sparse coding model has been proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx209" title="">PRE17</a>]</cite> as a framework for interpreting the structures of deep convolution networks.</span></span></span> There is a vast literature on how to learn the most compact and optimal sparsifying dictionaries from sample data, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx161" title="">LB19</a>, <a class="ltx_ref" href="bib.html#bibx215" title="">QLZ19</a>]</cite> and subsequently solve the inverse problem and compute the associated sparse code <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> or <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p3.m7"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math>. Recent studies of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx216" title="">QLZ20</a>, <a class="ltx_ref" href="bib.html#bibx217" title="">QZL+20</a>]</cite> even show that under broad conditions the convolution dictionary learning problem can be solved effectively and efficiently.</p>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p4">
<p class="ltx_p">Nevertheless, for tasks such as classification, we are not necessarily interested in the precise optimal dictionary nor the precise sparse code for each individual signal. We are mainly interested if collectively the set of sparse codes for each class are adequately separable from those of other classes. Under the assumption of the sparse generative model, if the convolution kernels <math alttext="\{\bm{k}_{c}\}_{c=1}^{C}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p4.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒌</mi><mi>c</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><annotation encoding="application/x-tex">\{\bm{k}_{c}\}_{c=1}^{C}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> match well with the “transpose” or “inverse” of the above sparsifying dictionaries <math alttext="\bm{D}=[\bm{D}_{1},\ldots,\bm{D}_{K}]" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p4.m2"><semantics><mrow><mi>𝑫</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝑫</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑫</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{D}=[\bm{D}_{1},\ldots,\bm{D}_{K}]</annotation><annotation encoding="application/x-llamapun">bold_italic_D = [ bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ]</annotation></semantics></math>, also known as the <span class="ltx_text ltx_font_italic">analysis filters</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx196" title="">NDE+13</a>, <a class="ltx_ref" href="bib.html#bibx234" title="">RE14</a>]</cite>, signals in one class will only have high responses to a small subset of those filters and low responses to others (due to the incoherence assumption). Nevertheless, in practice, often a sufficiently large number of, say <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p4.m3"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math>, random filters <math alttext="\{\bm{k}_{c}\}_{c=1}^{C}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p4.m4"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒌</mi><mi>c</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><annotation encoding="application/x-tex">\{\bm{k}_{c}\}_{c=1}^{C}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> suffices to ensure that the extracted <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p4.m5"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math>-channel features</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\big{[}\bm{k}_{1}\circledast\bm{x},\bm{k}_{2}\circledast\bm{x},\ldots,\bm{k}_{C}\circledast\bm{x}\big{]}^{\top}=\big{[}\mathsf{circ}(\bm{k}_{1})\bm{x},\ldots,\mathsf{circ}(\bm{k}_{C})\bm{x}\big{]}^{\top}\in\mathbb{R}^{C\times d}" class="ltx_Math" display="block" id="S1.E25.m1"><semantics><mrow><msup><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><msub><mi>𝒌</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒙</mi></mrow><mo>,</mo><mrow><msub><mi>𝒌</mi><mn>2</mn></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒙</mi></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msub><mi>𝒌</mi><mi>C</mi></msub><mo lspace="0.222em" rspace="0.222em">⊛</mo><mi>𝒙</mi></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow><mo>⊤</mo></msup><mo>=</mo><msup><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒌</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒌</mi><mi>C</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\big{[}\bm{k}_{1}\circledast\bm{x},\bm{k}_{2}\circledast\bm{x},\ldots,\bm{k}_{C}\circledast\bm{x}\big{]}^{\top}=\big{[}\mathsf{circ}(\bm{k}_{1})\bm{x},\ldots,\mathsf{circ}(\bm{k}_{C})\bm{x}\big{]}^{\top}\in\mathbb{R}^{C\times d}</annotation><annotation encoding="application/x-llamapun">[ bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⊛ bold_italic_x , bold_italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⊛ bold_italic_x , … , bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ⊛ bold_italic_x ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = [ sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) bold_italic_x , … , sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) bold_italic_x ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for different classes have different response patterns to different filters hence make different classes separable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx42" title="">CJG+15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p5">
<p class="ltx_p">Therefore, in our framework, to a large extent the number of channels (or the width of the network) truly plays the role as the <span class="ltx_text ltx_font_italic">statistical resource</span> whereas the number of layers (the depth of the network) plays the role as the <span class="ltx_text ltx_font_italic">computational resource</span>. The theory of compressive sensing precisely characterizes how many measurements are needed in order to preserve the intrinsic low-dimensional structures (including separability) of the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx294" title="">WM21</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F8">
<p class="ltx_p ltx_align_center">
<img alt="Figure 4.8 : Estimate the sparse code 𝒛 ¯ \bar{\bm{z}} over¯ start_ARG bold_italic_z end_ARG of an input signal 𝒙 \bm{x} bold_italic_x (an image here) by taking convolutions with multiple kernels 𝒌 c \bm{k}_{c} bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and then sparsifying." class="ltx_graphics" id="F8.g1" src="chapters/chapter4/figs/sparse-lifting.png"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.8</span>: </span><span class="ltx_text" style="font-size:90%;">Estimate the sparse code <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="F8.m4"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math> of an input signal <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F8.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (an image here) by taking convolutions with multiple kernels <math alttext="\bm{k}_{c}" class="ltx_Math" display="inline" id="F8.m6"><semantics><msub><mi>𝒌</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\bm{k}_{c}</annotation><annotation encoding="application/x-llamapun">bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and then sparsifying.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p6">
<p class="ltx_p">The multi-channel responses <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m1"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math> should be sparse. So to approximate the sparse code <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m2"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math>, we may take an entry-wise <span class="ltx_text ltx_font_italic">sparsity-promoting nonlinear thresholding</span>, say <math alttext="\bm{\tau}(\cdot)" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m3"><semantics><mrow><mi>𝝉</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\tau}(\cdot)</annotation><annotation encoding="application/x-llamapun">bold_italic_τ ( ⋅ )</annotation></semantics></math>, on the above filter outputs by setting low (say absolute value below <math alttext="\epsilon" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>) or negative responses to be zero:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{z}}\doteq\bm{\tau}\left(\big{[}\mathsf{circ}(\bm{k}_{1})\bm{x},\ldots,\mathsf{circ}(\bm{k}_{C})\bm{x}\big{]}^{\top}\right)\in\mathbb{R}^{C\times d}." class="ltx_Math" display="block" id="S1.E26.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo>≐</mo><mrow><mi>𝝉</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒌</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒌</mi><mi>C</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow><mo>⊤</mo></msup><mo>)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{z}}\doteq\bm{\tau}\left(\big{[}\mathsf{circ}(\bm{k}_{1})\bm{x},\ldots,\mathsf{circ}(\bm{k}_{C})\bm{x}\big{]}^{\top}\right)\in\mathbb{R}^{C\times d}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG ≐ bold_italic_τ ( [ sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) bold_italic_x , … , sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) bold_italic_x ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Figure <a class="ltx_ref" href="#F8" title="Figure 4.8 ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.8</span></a> illustrates the basic ideas. One may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx234" title="">RE14</a>]</cite> for a more systematical study on the design of the sparsifying thresholding operator. Nevertheless, here we are not so interested in obtaining the best sparse codes as long as the codes are sufficiently separable. Hence the nonlinear operator <math alttext="\bm{\tau}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m5"><semantics><mi>𝝉</mi><annotation encoding="application/x-tex">\bm{\tau}</annotation><annotation encoding="application/x-llamapun">bold_italic_τ</annotation></semantics></math> can be simply chosen to be a soft thresholding or a ReLU.
These presumably sparse features <math alttext="\bar{\bm{z}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m6"><semantics><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG</annotation></semantics></math> can be assumed to lie on a lower-dimensional (nonlinear) submanifold of <math alttext="\mathbb{R}^{dC}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p6.m7"><semantics><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{dC}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, which can be linearized and separated from the other classes by subsequent ReduNet layers, as illustrated later in Figure <a class="ltx_ref" href="#F9" title="Figure 4.9 ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.9</span></a>.</p>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p7">
<p class="ltx_p">The ReduNet constructed from circulant version of these multi-channel features <math alttext="\bar{\bm{Z}}\doteq[\bar{\bm{z}}_{1},\ldots,\bar{\bm{z}}_{N}]\in\mathbb{R}^{C\times d\times N}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p7.m1"><semantics><mrow><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mo>≐</mo><mrow><mo stretchy="false">[</mo><msub><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{Z}}\doteq[\bar{\bm{z}}_{1},\ldots,\bar{\bm{z}}_{N}]\in\mathbb{R}^{C\times d\times N}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_Z end_ARG ≐ [ over¯ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , over¯ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="\mathsf{circ}(\bar{\bm{Z}})\doteq[\mathsf{circ}(\bar{\bm{z}}_{1}),\dots,\mathsf{circ}(\bar{\bm{z}}_{N})]\in\mathbb{R}^{dC\times dN}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p7.m2"><semantics><mrow><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mi>N</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathsf{circ}(\bar{\bm{Z}})\doteq[\mathsf{circ}(\bar{\bm{z}}_{1}),\dots,\mathsf{circ}(\bar{\bm{z}}_{N})]\in\mathbb{R}^{dC\times dN}</annotation><annotation encoding="application/x-llamapun">sansserif_circ ( over¯ start_ARG bold_italic_Z end_ARG ) ≐ [ sansserif_circ ( over¯ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , … , sansserif_circ ( over¯ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C × italic_d italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, retains the good invariance properties described above: the linear operators, now denoted as <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p7.m3"><semantics><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> and <math alttext="\bar{\bm{C}}_{k}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p7.m4"><semantics><msub><mover accent="true"><mi>𝑪</mi><mo>¯</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\bar{\bm{C}}_{k}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, remain block circulant, and represent <span class="ltx_text ltx_font_italic">multi-channel 1D circular convolutions. </span>
Specifically, we have the following result.</p>
</div>
<div class="ltx_theorem ltx_theorem_proposition" id="Thmproposition2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Proposition 4.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Multi-channel convolution structures of <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="Thmproposition2.m1"><semantics><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> and <math alttext="\bar{\bm{C}}_{k}" class="ltx_Math" display="inline" id="Thmproposition2.m2"><semantics><msub><mover accent="true"><mi>𝑪</mi><mo>¯</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\bar{\bm{C}}_{k}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmproposition2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The matrix</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{E}}\doteq\alpha\left(\bm{I}+\alpha\,\mathsf{circ}(\bar{\bm{Z}})\mathsf{circ}(\bar{\bm{Z}})^{\top}\right)^{-1}" class="ltx_Math" display="block" id="S1.E27.m1"><semantics><mrow><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mo>≐</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝖼𝗂𝗋𝖼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{E}}\doteq\alpha\left(\bm{I}+\alpha\,\mathsf{circ}(\bar{\bm{Z}})\mathsf{circ}(\bar{\bm{Z}})^{\top}\right)^{-1}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG ≐ italic_α ( bold_italic_I + italic_α sansserif_circ ( over¯ start_ARG bold_italic_Z end_ARG ) sansserif_circ ( over¯ start_ARG bold_italic_Z end_ARG ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">is block circulant, i.e.,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{E}}=\left[\begin{matrix}\bar{\bm{E}}_{1,1}&amp;\cdots&amp;\bar{\bm{E}}_{1,C}\\
\vdots&amp;\ddots&amp;\vdots\\
\bar{\bm{E}}_{C,1}&amp;\cdots&amp;\bar{\bm{E}}_{C,C}\\
\end{matrix}\right]\in\mathbb{R}^{dC\times dC}," class="ltx_Math" display="block" id="S1.Ex7.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi mathvariant="normal">⋯</mi></mtd><mtd><msub><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mrow><mn>1</mn><mo>,</mo><mi>C</mi></mrow></msub></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd><mtd><mi mathvariant="normal">⋱</mi></mtd><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msub><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mrow><mi>C</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi mathvariant="normal">⋯</mi></mtd><mtd><msub><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mrow><mi>C</mi><mo>,</mo><mi>C</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{E}}=\left[\begin{matrix}\bar{\bm{E}}_{1,1}&amp;\cdots&amp;\bar{\bm{E}}_{1,C}\\
\vdots&amp;\ddots&amp;\vdots\\
\bar{\bm{E}}_{C,1}&amp;\cdots&amp;\bar{\bm{E}}_{C,C}\\
\end{matrix}\right]\in\mathbb{R}^{dC\times dC},</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG = [ start_ARG start_ROW start_CELL over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT end_CELL start_CELL ⋯ end_CELL start_CELL over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT 1 , italic_C end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL start_CELL ⋱ end_CELL start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT italic_C , 1 end_POSTSUBSCRIPT end_CELL start_CELL ⋯ end_CELL start_CELL over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT italic_C , italic_C end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C × italic_d italic_C end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where each <math alttext="\bar{\bm{E}}_{c,c^{\prime}}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝐄</mi><mo>¯</mo></mover><mrow><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>′</mo></msup></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{E}}_{c,c^{\prime}}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT italic_c , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is a circulant matrix. Moreover, <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m2"><semantics><mover accent="true"><mi>𝐄</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> represents a multi-channel circular convolution, i.e., for any multi-channel signal <math alttext="\bar{\bm{z}}\in\mathbb{R}^{C\times n}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m3"><semantics><mrow><mover accent="true"><mi>𝐳</mi><mo>¯</mo></mover><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{z}}\in\mathbb{R}^{C\times n}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{E}}\cdot\textsf{vec}(\bar{\bm{z}})=\textsf{vec}(\bar{\bm{e}}\circledast\bar{\bm{z}})." class="ltx_Math" display="block" id="S1.Ex8.m1"><semantics><mrow><mrow><mrow><mrow><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><mo lspace="0.222em" rspace="0.222em">⋅</mo><mtext class="ltx_mathvariant_sans-serif-italic">vec</mtext></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mtext class="ltx_mathvariant_sans-serif-italic">vec</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>𝒆</mi><mo>¯</mo></mover><mo lspace="0.222em" rspace="0.222em">⊛</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{E}}\cdot\textsf{vec}(\bar{\bm{z}})=\textsf{vec}(\bar{\bm{e}}\circledast\bar{\bm{z}}).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG ⋅ vec ( over¯ start_ARG bold_italic_z end_ARG ) = vec ( over¯ start_ARG bold_italic_e end_ARG ⊛ over¯ start_ARG bold_italic_z end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In above, <math alttext="\bar{\bm{e}}\in\mathbb{R}^{C\times C\times d}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m4"><semantics><mrow><mover accent="true"><mi>𝐞</mi><mo>¯</mo></mover><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{e}}\in\mathbb{R}^{C\times C\times d}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_e end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_C × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is a multi-channel convolutional kernel with <math alttext="\bar{\bm{e}}[c,c^{\prime}]\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m5"><semantics><mrow><mrow><mover accent="true"><mi>𝐞</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>′</mo></msup><mo stretchy="false">]</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{e}}[c,c^{\prime}]\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_e end_ARG [ italic_c , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> being the first column vector of <math alttext="\bar{\bm{E}}_{c,c^{\prime}}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m6"><semantics><msub><mover accent="true"><mi>𝐄</mi><mo>¯</mo></mover><mrow><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>′</mo></msup></mrow></msub><annotation encoding="application/x-tex">\bar{\bm{E}}_{c,c^{\prime}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT italic_c , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\bar{\bm{e}}\circledast\bar{\bm{z}}\in\mathbb{R}^{C\times d}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m7"><semantics><mrow><mrow><mover accent="true"><mi>𝐞</mi><mo>¯</mo></mover><mo lspace="0.222em" rspace="0.222em">⊛</mo><mover accent="true"><mi>𝐳</mi><mo>¯</mo></mover></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{e}}\circledast\bar{\bm{z}}\in\mathbb{R}^{C\times d}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_e end_ARG ⊛ over¯ start_ARG bold_italic_z end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the multi-channel circular convolution defined as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bar{\bm{e}}\circledast\bar{\bm{z}})[c]\doteq\sum_{c^{\prime}=1}^{C}\bar{\bm{e}}[c,c^{\prime}]\circledast\bar{\bm{z}}[c^{\prime}],\quad\forall c=1,\ldots,C." class="ltx_Math" display="block" id="S1.Ex9.m1"><semantics><mrow><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>𝒆</mi><mo>¯</mo></mover><mo lspace="0.222em" rspace="0.222em">⊛</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><msup><mi>c</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow><mrow><mrow><mover accent="true"><mi>𝒆</mi><mo>¯</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>′</mo></msup><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">⊛</mo><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msup><mi>c</mi><mo>′</mo></msup><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>c</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>C</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\bar{\bm{e}}\circledast\bar{\bm{z}})[c]\doteq\sum_{c^{\prime}=1}^{C}\bar{\bm{e}}[c,c^{\prime}]\circledast\bar{\bm{z}}[c^{\prime}],\quad\forall c=1,\ldots,C.</annotation><annotation encoding="application/x-llamapun">( over¯ start_ARG bold_italic_e end_ARG ⊛ over¯ start_ARG bold_italic_z end_ARG ) [ italic_c ] ≐ ∑ start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT over¯ start_ARG bold_italic_e end_ARG [ italic_c , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] ⊛ over¯ start_ARG bold_italic_z end_ARG [ italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] , ∀ italic_c = 1 , … , italic_C .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Similarly, the matrices <math alttext="\bar{\bm{C}}_{k}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m8"><semantics><msub><mover accent="true"><mi>𝐂</mi><mo>¯</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\bar{\bm{C}}_{k}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> associated with any subsets of <math alttext="\bar{\bm{Z}}" class="ltx_Math" display="inline" id="Thmproposition2.p1.m9"><semantics><mover accent="true"><mi>𝐙</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> are also multi-channel circular convolutions.</span></p>
</div>
</div>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p8">
<p class="ltx_p">From Proposition <a class="ltx_ref" href="#Thmproposition2" title="Proposition 4.2 (Multi-channel convolution structures of 𝑬̄ and 𝑪̄_𝑘). ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2</span></a>, shift invariant ReduNet is a deep convolutional network for multi-channel 1D signals by construction. Notice that even if the initial lifting kernels are separated (<a class="ltx_ref" href="#S1.E26" title="Equation 4.1.26 ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.26</span></a>), the matrix inverse in (<a class="ltx_ref" href="#S1.E27" title="Equation 4.1.27 ‣ Proposition 4.2 (Multi-channel convolution structures of 𝑬̄ and 𝑪̄_𝑘). ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.27</span></a>) for computing <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p8.m1"><semantics><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> (similarly for <math alttext="\bar{\bm{C}_{k}}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p8.m2"><semantics><mover accent="true"><msub><mi>𝑪</mi><mi>k</mi></msub><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{C}_{k}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>) introduces “cross talk” among all <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p8.m3"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation><annotation encoding="application/x-llamapun">italic_C</annotation></semantics></math> channels. Hence, these multi-channel convolutions in general are <span class="ltx_text ltx_font_italic">not</span> depth-wise separable, unlike the Xception nets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx52" title="">Cho17</a>]</cite> that were once suggested to simplify multi-channel convolutional neural networks.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>It remains open what additional structures on the data would lead to depth-wise separable convolutions.</span></span></span></p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Reducing Computational Complexity in the Frequency Domain)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark2.p1">
<p class="ltx_p">The calculation of <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="Thmremark2.p1.m1"><semantics><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> in (<a class="ltx_ref" href="#S1.E27" title="Equation 4.1.27 ‣ Proposition 4.2 (Multi-channel convolution structures of 𝑬̄ and 𝑪̄_𝑘). ‣ A Fundamental Trade-off between Invariance and Sparsity. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1.27</span></a>) requires inverting a matrix of size <math alttext="dC\times dC" class="ltx_Math" display="inline" id="Thmremark2.p1.m2"><semantics><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">dC\times dC</annotation><annotation encoding="application/x-llamapun">italic_d italic_C × italic_d italic_C</annotation></semantics></math>, which in general has complexity <math alttext="O(d^{3}C^{3})" class="ltx_Math" display="inline" id="Thmremark2.p1.m3"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>d</mi><mn>3</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>C</mi><mn>3</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^{3}C^{3})</annotation><annotation encoding="application/x-llamapun">italic_O ( italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_C start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT )</annotation></semantics></math>. Nevertheless, by using the fact that a circulant matrix can be diagonalized by the Discrete Fourier Transform (DFT) matrix, the complexity can be significantly reduced. As shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx41" title="">CYY+22</a>]</cite>, to compute <math alttext="\bar{\bm{E}}" class="ltx_Math" display="inline" id="Thmremark2.p1.m4"><semantics><mover accent="true"><mi>𝑬</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{\bm{E}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_E end_ARG</annotation></semantics></math> and <math alttext="\bar{\bm{C}}_{k}\in\mathbb{R}^{dC\times dC}" class="ltx_Math" display="inline" id="Thmremark2.p1.m5"><semantics><mrow><msub><mover accent="true"><mi>𝑪</mi><mo>¯</mo></mover><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{\bm{C}}_{k}\in\mathbb{R}^{dC\times dC}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_C × italic_d italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, we only need to compute in the frequency domain the inverse of <math alttext="C\times C" class="ltx_Math" display="inline" id="Thmremark2.p1.m6"><semantics><mrow><mi>C</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">C\times C</annotation><annotation encoding="application/x-llamapun">italic_C × italic_C</annotation></semantics></math> blocks for <math alttext="d" class="ltx_Math" display="inline" id="Thmremark2.p1.m7"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> times hence the overall complexity becomes <math alttext="O(dC^{3})" class="ltx_Math" display="inline" id="Thmremark2.p1.m8"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>C</mi><mn>3</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(dC^{3})</annotation><annotation encoding="application/x-llamapun">italic_O ( italic_d italic_C start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Overall Network Architecture and Comparison.</h4>
<div class="ltx_para" id="S1.SS2.SSS0.Px3.p1">
<p class="ltx_p">Following the above derivation, we see that in order to find a linear discriminative representation (LDR) for multiple classes of signals/images that is invariant to translation, sparse coding, a multi-layer architecture with multi-channel convolutions, different nonlinear activation, and spectrum computing all become <span class="ltx_text ltx_font_italic">necessary</span> components for achieving the objective effectively and efficiently. Figure <a class="ltx_ref" href="#F9" title="Figure 4.9 ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.9</span></a> illustrates the overall process of learning such a representation via invariant rate reduction on the input sparse codes.</p>
</div>
<figure class="ltx_figure" id="F9"><img alt="Figure 4.9 : The overall process for classifying multi-class signals with shift invariance: Multi-channel lifting, sparse coding, followed by a multi-channel convolution ReduNet for invariant rate reduction. These components are necessary in order to map shift-invariant multi-class signals to incoherent (linear) subspaces as an LDR. Note that the architectures of most modern deep neural networks resemble this process. The so-learned LDR facilitates subsequent tasks such as classification." class="ltx_graphics ltx_img_landscape" height="185" id="F9.g1" src="chapters/chapter4/figs/learn_to_classify_diagram_updated.png" width="586"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.9</span>: </span><span class="ltx_text" style="font-size:90%;">The overall process for classifying multi-class signals with shift invariance: Multi-channel lifting, sparse coding, followed by a multi-channel convolution ReduNet for invariant rate reduction. These components are <span class="ltx_text ltx_font_italic">necessary</span> in order to map shift-invariant multi-class signals to incoherent (linear) subspaces as an LDR. Note that the architectures of most modern deep neural networks resemble this process. The so-learned LDR facilitates subsequent tasks such as classification.</span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 4.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Invariant Classification of Digits)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample2.p1">
<p class="ltx_p">We next provide an empirical performance of the ReduNet on learning <span class="ltx_text ltx_font_italic">rotation</span> invariant features on the real 10-class MNIST dataset.
We impose a polar grid on the image <math alttext="\bm{x}\in\mathbb{R}^{H\times W}" class="ltx_Math" display="inline" id="Thmexample2.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{H\times W}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, with its geometric center being the center of the 2D polar grid (as illustrated in Figure <a class="ltx_ref" href="#F10" title="Figure 4.10 ‣ Example 4.2 (Invariant Classification of Digits). ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.10</span></a>).
For each radius <math alttext="r_{i}" class="ltx_Math" display="inline" id="Thmexample2.p1.m2"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="i\in[C]" class="ltx_Math" display="inline" id="Thmexample2.p1.m3"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>C</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">i\in[C]</annotation><annotation encoding="application/x-llamapun">italic_i ∈ [ italic_C ]</annotation></semantics></math>, we can sample <math alttext="\Gamma" class="ltx_Math" display="inline" id="Thmexample2.p1.m4"><semantics><mi mathvariant="normal">Γ</mi><annotation encoding="application/x-tex">\Gamma</annotation><annotation encoding="application/x-llamapun">roman_Γ</annotation></semantics></math> pixels with respect to each angle <math alttext="\gamma_{l}=l\cdot({2\pi}/\Gamma)" class="ltx_Math" display="inline" id="Thmexample2.p1.m5"><semantics><mrow><msub><mi>γ</mi><mi>l</mi></msub><mo>=</mo><mrow><mi>l</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>π</mi></mrow><mo>/</mo><mi mathvariant="normal">Γ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\gamma_{l}=l\cdot({2\pi}/\Gamma)</annotation><annotation encoding="application/x-llamapun">italic_γ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_l ⋅ ( 2 italic_π / roman_Γ )</annotation></semantics></math> with <math alttext="l\in[\Gamma]" class="ltx_Math" display="inline" id="Thmexample2.p1.m6"><semantics><mrow><mi>l</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="normal">Γ</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">l\in[\Gamma]</annotation><annotation encoding="application/x-llamapun">italic_l ∈ [ roman_Γ ]</annotation></semantics></math>.
Then given a sample image <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from the dataset, we represent the image in the (sampled) polar coordinate as a multi-channel signal <math alttext="\bm{x}_{p}\in\mathbb{R}^{\Gamma\times C}" class="ltx_Math" display="inline" id="Thmexample2.p1.m8"><semantics><mrow><msub><mi>𝒙</mi><mi>p</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi mathvariant="normal">Γ</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{p}\in\mathbb{R}^{\Gamma\times C}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT roman_Γ × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>.
The goal here is to learn a rotation invariant representation, i.e., we expect to learn <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="Thmexample2.p1.m9"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> such that <math alttext="\{f(\bm{x}_{p}\circ\mathfrak{g},\bm{\theta})\}_{\mathfrak{g}\in\mathbb{G}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m10"><semantics><msub><mrow><mo stretchy="false">{</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mi>p</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>𝔤</mi></mrow><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>𝔤</mi><mo>∈</mo><mi>𝔾</mi></mrow></msub><annotation encoding="application/x-tex">\{f(\bm{x}_{p}\circ\mathfrak{g},\bm{\theta})\}_{\mathfrak{g}\in\mathbb{G}}</annotation><annotation encoding="application/x-llamapun">{ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∘ fraktur_g , bold_italic_θ ) } start_POSTSUBSCRIPT fraktur_g ∈ blackboard_G end_POSTSUBSCRIPT</annotation></semantics></math> lie in the same subspace, where <math alttext="\mathfrak{g}" class="ltx_Math" display="inline" id="Thmexample2.p1.m11"><semantics><mi>𝔤</mi><annotation encoding="application/x-tex">\mathfrak{g}</annotation><annotation encoding="application/x-llamapun">fraktur_g</annotation></semantics></math> is the cyclic-shift in polar angle.
We use <math alttext="N=100" class="ltx_Math" display="inline" id="Thmexample2.p1.m12"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N=100</annotation><annotation encoding="application/x-llamapun">italic_N = 100</annotation></semantics></math> training samples (<math alttext="10" class="ltx_Math" display="inline" id="Thmexample2.p1.m13"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation><annotation encoding="application/x-llamapun">10</annotation></semantics></math> from each class) and set <math alttext="\Gamma=200" class="ltx_Math" display="inline" id="Thmexample2.p1.m14"><semantics><mrow><mi mathvariant="normal">Γ</mi><mo>=</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">\Gamma=200</annotation><annotation encoding="application/x-llamapun">roman_Γ = 200</annotation></semantics></math>, <math alttext="C=15" class="ltx_Math" display="inline" id="Thmexample2.p1.m15"><semantics><mrow><mi>C</mi><mo>=</mo><mn>15</mn></mrow><annotation encoding="application/x-tex">C=15</annotation><annotation encoding="application/x-llamapun">italic_C = 15</annotation></semantics></math> for polar sampling.
By performing the above sampling in polar coordinate, we can obtain the data matrix <math alttext="\bm{X}_{p}\in\mathbb{R}^{(\Gamma\cdot C)\times N}" class="ltx_Math" display="inline" id="Thmexample2.p1.m16"><semantics><mrow><msub><mi>𝑿</mi><mi>p</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">Γ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>C</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}_{p}\in\mathbb{R}^{(\Gamma\cdot C)\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT ( roman_Γ ⋅ italic_C ) × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>.
For the ReduNet, we set the number of layers/iterations <math alttext="L=40" class="ltx_Math" display="inline" id="Thmexample2.p1.m17"><semantics><mrow><mi>L</mi><mo>=</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">L=40</annotation><annotation encoding="application/x-llamapun">italic_L = 40</annotation></semantics></math>, precision <math alttext="\epsilon=0.1" class="ltx_Math" display="inline" id="Thmexample2.p1.m18"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.1</annotation><annotation encoding="application/x-llamapun">italic_ϵ = 0.1</annotation></semantics></math>, step size <math alttext="\eta=0.5" class="ltx_Math" display="inline" id="Thmexample2.p1.m19"><semantics><mrow><mi>η</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\eta=0.5</annotation><annotation encoding="application/x-llamapun">italic_η = 0.5</annotation></semantics></math>. Before the first layer, we perform lifting of the input by 1D circulant-convolution with 20 random Gaussian kernels of size 5.</p>
</div>
<figure class="ltx_figure" id="F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F10.sf1"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F10.sf1.g1" src="chapters/chapter4/figs/1d-rotation.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><math alttext="\bm{X}_{\text{rotation}}" class="ltx_Math" display="inline" id="F10.sf1.m2"><semantics><msub><mi mathsize="90%">𝑿</mi><mtext mathsize="90%">rotation</mtext></msub><annotation encoding="application/x-tex">\bm{X}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F10.sf2"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F10.sf2.g1" src="chapters/chapter4/figs/mnist1d_img0.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><math alttext="\bm{Z}_{\text{rotation}}" class="ltx_Math" display="inline" id="F10.sf2.m2"><semantics><msub><mi mathsize="90%">𝒁</mi><mtext mathsize="90%">rotation</mtext></msub><annotation encoding="application/x-tex">\bm{Z}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F10.sf3"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F10.sf3.g1" src="chapters/chapter4/figs/mnist1d_img1.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Loss</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.10</span>: </span><span class="ltx_text" style="font-size:90%;">Examples of rotated images of MNIST digits, each by 18<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∘</span></sup>. (<span class="ltx_text ltx_font_bold">Left</span>) Diagram for polar coordinate representation; (<span class="ltx_text ltx_font_bold">Right</span>) Rotated images of digit ‘0’ and ‘1’.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample2.p2">
<p class="ltx_p">To evaluate the learned representation, each training sample is augmented by 20 of its rotated version, each shifted with stride=10. We compute the cosine similarities among the <math alttext="m\times 20" class="ltx_Math" display="inline" id="Thmexample2.p2.m1"><semantics><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">m\times 20</annotation><annotation encoding="application/x-llamapun">italic_m × 20</annotation></semantics></math> augmented training inputs <math alttext="\bm{X}_{\text{rotation}}" class="ltx_Math" display="inline" id="Thmexample2.p2.m2"><semantics><msub><mi>𝑿</mi><mtext>rotation</mtext></msub><annotation encoding="application/x-tex">\bm{X}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math> and the results are shown in Figure <a class="ltx_ref" href="#F11" title="Figure 4.11 ‣ Example 4.2 (Invariant Classification of Digits). ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.11</span></a> (<span class="ltx_text ltx_font_bold">a</span>).
We compare the cosine similarities among the learned features of all the augmented versions, i.e., <math alttext="\bar{\bm{Z}}_{\text{rotation}}" class="ltx_Math" display="inline" id="Thmexample2.p2.m3"><semantics><msub><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mtext>rotation</mtext></msub><annotation encoding="application/x-tex">\bar{\bm{Z}}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math> and summarize the results in Figure <a class="ltx_ref" href="#F11" title="Figure 4.11 ‣ Example 4.2 (Invariant Classification of Digits). ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.11</span></a> (<span class="ltx_text ltx_font_bold">b</span>).
As we see, the so constructed rotation-invariant ReduNet is able to map the training data (as well as all its rotated versions) from the 10 different classes into 10 nearly orthogonal subspaces. That is, the learnt subspaces are truly invariant to shift transformation in polar angle. Next, we randomly draw another <math alttext="100" class="ltx_Math" display="inline" id="Thmexample2.p2.m4"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation><annotation encoding="application/x-llamapun">100</annotation></semantics></math> test samples followed by the same augmentation procedure.
In Figure <a class="ltx_ref" href="#F11" title="Figure 4.11 ‣ Example 4.2 (Invariant Classification of Digits). ‣ Overall Network Architecture and Comparison. ‣ 4.1.2 Convolutional Networks from Invariant Rate Reduction ‣ 4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.11</span></a> (<span class="ltx_text ltx_font_bold">c</span>), we visualize the MCR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup> loss on the <math alttext="\ell" class="ltx_Math" display="inline" id="Thmexample2.p2.m6"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th layer representation of the ReduNet on the training and test dataset. From these results, we can find that the constructed ReduNet is indeed able to maximize the MCR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup> loss as well as generalize to the test data.</p>
</div>
<figure class="ltx_figure" id="F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf1"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F11.sf1.g1" src="chapters/chapter4/figs/mnist1d-heatmap-X_translate_train_all.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><math alttext="\bm{X}_{\text{rotation}}" class="ltx_Math" display="inline" id="F11.sf1.m2"><semantics><msub><mi mathsize="90%">𝑿</mi><mtext mathsize="90%">rotation</mtext></msub><annotation encoding="application/x-tex">\bm{X}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf2"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F11.sf2.g1" src="chapters/chapter4/figs/mnist1d-heatmap-Z_translate_train_all.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><math alttext="\bm{Z}_{\text{rotation}}" class="ltx_Math" display="inline" id="F11.sf2.m2"><semantics><msub><mi mathsize="90%">𝒁</mi><mtext mathsize="90%">rotation</mtext></msub><annotation encoding="application/x-tex">\bm{Z}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf3"><img alt="(a) 𝑿 rotation \bm{X}_{\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT" class="ltx_graphics" id="F11.sf3.g1" src="chapters/chapter4/figs/mnist1d-loss-traintest.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Loss</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.11</span>: </span><span class="ltx_text" style="font-size:90%;">(a)(b) are heatmaps of cosine similarity among rotated training data <math alttext="\bm{X}_{\text{rotation}}" class="ltx_Math" display="inline" id="F11.m4"><semantics><msub><mi>𝑿</mi><mtext>rotation</mtext></msub><annotation encoding="application/x-tex">\bm{X}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math> and learned features <math alttext="\bar{\bm{Z}}_{\text{rotation}}" class="ltx_Math" display="inline" id="F11.m5"><semantics><msub><mover accent="true"><mi>𝒁</mi><mo>¯</mo></mover><mtext>rotation</mtext></msub><annotation encoding="application/x-tex">\bar{\bm{Z}}_{\text{rotation}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT</annotation></semantics></math> for rotation invariance. (d) visualizes the training/val MCR<sup class="ltx_sup">2</sup> losses across layers.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample2.p3">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample2.p3.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.2 </span>White-Box Transformers from Unrolled Optimization</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">As we have seen in the previous section, we use the problem of classification to provide a rigorous interpretation for main architectural characteristics of popular deep networks such as the ResNet and the CNN: each layer of such networks can be viewed as to imitate a gradient step which increases the rate reduction (or information gain) objective. This perspective also leads to a somewhat surprising fact: the the parameters and operators of the layers of such a deep network, the ReduNet, can be computed in a purely forward fashion.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">Despite the theoretical and conceptual importance of the ReduNet, several factors limit it from being very practical. First, as we have discussed in the above, the computational cost of computing the matrix operators in each layer in a forward fashion can be very high. Second, the so-computed operators may not be so effective in optimizing the objective and it might take thousands of iterations (hence layers). As we have seen in <a class="ltx_ref" href="Ch2.html#S3.SS3" title="2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3.3</span></a> for LISTA, these two issues can be addressed by allowing to optimize those operators and make them learnable via back-propagation.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Or, perhaps, by a mixture of both forward and backward optimization.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">The supervised classification setting in which the ReduNet was derived is also somewhat limiting. In practice, an image might not belong to a single class as it may contain multiple objects. Hence it would be more general to assume that different regions of the image belong to different low-dimensional models (say a Gaussian or a subspace). As we will see, such a generalization would lead to a both simple and general architecture which unifies the rate reduction and the denoising operations that we have seen in the previous chapter. Moreover, the so-obtained architecture resembles the popular Transformer architecture.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2.1 </span>Unrolled Optimization for Sparse Rate Reduction</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">We consider a general learning setup associated with real-world signals. Let <math alttext="\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> denote random variables representing our data source. In vision tasks, each <math alttext="\bm{x}_{i}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.p1.m2"><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is interpreted as a <span class="ltx_text ltx_font_italic">token</span>, typically corresponding to an image patch. In language tasks, each <math alttext="\bm{x}_{i}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.p1.m3"><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is interpreted as an <span class="ltx_text ltx_font_italic">token embedding</span>, i.e., a continuous vector representation of a discrete token such as a word or subword.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>With a slight abuse of terminology, we refer to both the discrete tokens and their associated embeddings simply as tokens throughout this chapter for convenience.</span></span></span> The <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.m4"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>’s may have arbitrary correlation structures. We use <math alttext="\bm{Z}=\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{N}\end{bmatrix}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.SS1.p1.m5"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}=\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{N}\end{bmatrix}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> to denote the random variables that defines our representations, where <math alttext="\bm{z}_{i}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.p1.m6"><semantics><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{i}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the representation of the corresponding token <math alttext="\bm{x}_{i}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.p1.m7"><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark3.p1">
<p class="ltx_p">In transformers, each input sample is typically converted into a sequence of <span class="ltx_text ltx_font_italic">tokens</span>. A token is a basic unit of information derived from the raw input: in natural language processing, tokens are typically words or subwords; in computer vision, they correspond to image patches; and in other modalities, they may represent time steps, spatial locations, or other domain-specific units. A <span class="ltx_text ltx_font_italic">token embedding</span> is a continuous vector representation of a token that serves as the input to a transformer. It maps each token to a point in a high-dimensional space, enabling the model to process symbolic inputs using numerical computation.
A <span class="ltx_text ltx_font_italic">token representation</span> is a vector that encodes the semantic or structural information of a token, typically produced by the intermediate or final layers of a transformer. These representations are designed to capture meaningful features of the input that are useful for downstream tasks such as classification, generation, or regression. Please refer to <a class="ltx_ref" href="Ch7.html#S2" title="7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a> for more details about these concepts in implementations.</p>
</div>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Objective for Learning a Structured and Compact Representation.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">Following the framework of rate reduction <a class="ltx_ref" href="#S1" title="4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, we contend
that the goal of representation learning is to find a feature mapping <math alttext="f\colon\bm{X}\in\mathbb{R}^{D\times N}\to\bm{Z}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝑿</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup><mo stretchy="false">→</mo><mi>𝒁</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">f\colon\bm{X}\in\mathbb{R}^{D\times N}\to\bm{Z}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT → bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> which transforms input tokens <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}\subset\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m2"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}\subset\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> with a potentially nonlinear and multi-modal distribution to a (piecewise) <span class="ltx_text ltx_font_italic">linearized and compact</span> token representations <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}\subset\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m3"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊂</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}\subset\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. While the joint distribution of tokens representations <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m4"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> may be sophisticated (and task-specific), we further contend that it is reasonable and practical to
require that the target marginal distribution of individual token representations should be highly compressed and structured, amenable for compact coding. Particularly, we require the distribution to be <span class="ltx_text ltx_font_italic">a mixture of low-dimensional (say <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m5"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math>) Gaussian
distributions</span>, such that the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m6"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th Gaussian has mean <math alttext="\mathbf{0}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m7"><semantics><mrow><mn>𝟎</mn><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{0}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_0 ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, covariance <math alttext="\bm{\Sigma}_{k}\succeq\mathbf{0}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m8"><semantics><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>⪰</mo><mn>𝟎</mn><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Sigma}_{k}\succeq\mathbf{0}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⪰ bold_0 ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and support spanned by the orthonormal basis <math alttext="\bm{U}_{k}\in\mathbb{R}^{d\times p}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m9"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathbb{R}^{d\times p}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>.
We denote <math alttext="\bm{U}_{[K]}=\{\bm{U}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m10"><semantics><mrow><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\bm{U}_{[K]}=\{\bm{U}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT = { bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> to be the set of bases of all Gaussians. Hence, to maximize the <span class="ltx_text ltx_font_italic">information gain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx174" title="">MTS22</a>]</cite> for the final token representations, we wish to maximize their rate reduction (see <a class="ltx_ref" href="Ch3.html#S4.SS2" title="3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4.2</span></a>), i.e.,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx44">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathrm{max}_{\bm{Z}\in\mathbb{R}^{d\times N}}\ \Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})\doteq R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})." class="ltx_Math" display="inline" id="S2.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>max</mi><mrow><mi>𝒁</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow></msub><mo lspace="0.500em" rspace="0em">​</mo><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{max}_{\bm{Z}\in\mathbb{R}^{d\times N}}\ \Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})\doteq R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, the first term <math alttext="R_{\epsilon}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m11"><semantics><msub><mi>R</mi><mi>ϵ</mi></msub><annotation encoding="application/x-tex">R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> is an estimate of the lossy coding rate for the whole set of token representations. More specifically, if we view the token representations <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m12"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> as i.i.d. samples from a single zero-mean Gaussian, their lossy coding rate subject to a quantization precision <math alttext="\epsilon&gt;0" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m13"><semantics><mrow><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation><annotation encoding="application/x-llamapun">italic_ϵ &gt; 0</annotation></semantics></math> is given as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}(\bm{Z})\doteq\frac{1}{2}\textrm{logdet}\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}^{\top}\bm{Z}\right)=\frac{1}{2}\textrm{logdet}\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}\bm{Z}^{\top}\right)." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mtext>logdet</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>d</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mtext>logdet</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>d</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{Z})\doteq\frac{1}{2}\textrm{logdet}\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}^{\top}\bm{Z}\right)=\frac{1}{2}\textrm{logdet}\left(\bm{I}+\frac{d}{N\epsilon^{2}}\bm{Z}\bm{Z}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG logdet ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG logdet ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The second term <math alttext="R_{\epsilon}^{c}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m14"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R_{\epsilon}^{c}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> is an estimate of the lossy coding rate under the codebook <math alttext="\bm{U}_{[K]}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m15"><semantics><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\bm{U}_{[K]}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT</annotation></semantics></math>, which is given as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})" class="ltx_Math" display="inline" id="S2.E3X.m2"><semantics><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\sum_{k=1}^{K}R_{\epsilon}(\bm{U}_{k}^{\top}\bm{Z})=\frac{1}{2}\sum_{k=1}^{K}\log\det\left(\bm{I}+\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right)." class="ltx_Math" display="inline" id="S2.E3X.m3"><semantics><mrow><mrow><mi></mi><mo>≐</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\sum_{k=1}^{K}R_{\epsilon}(\bm{U}_{k}^{\top}\bm{Z})=\frac{1}{2}\sum_{k=1}^{K}\log\det\left(\bm{I}+\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right).</annotation><annotation encoding="application/x-llamapun">≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log roman_det ( bold_italic_I + divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4.2.3)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark4.p1">
<p class="ltx_p">The expression (<a class="ltx_ref" href="#S2.E3" title="Equation 4.2.3 ‣ Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>) for the coding rate can be viewed as a generalization of the coding rate <math alttext="R_{\epsilon}^{c}" class="ltx_Math" display="inline" id="Thmremark4.p1.m1"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R_{\epsilon}^{c}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> used in the original rate reduction objective (<a class="ltx_ref" href="Ch3.html#S4.E13" title="Equation 3.4.13 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.13</span></a>). In particular, the original objective is defined with respect to a set of known membership labels <math alttext="\{\bm{\Pi}_{k}\}" class="ltx_Math" display="inline" id="Thmremark4.p1.m2"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{\Pi}_{k}\}</annotation><annotation encoding="application/x-llamapun">{ bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> specific to the particular data realization <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmremark4.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. In contrast, the current objective is defined with respect to subspaces <math alttext="\bm{U}_{[K]}" class="ltx_Math" display="inline" id="Thmremark4.p1.m4"><semantics><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\bm{U}_{[K]}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT</annotation></semantics></math>, which are independent of any particular realization but are assumed to support the distribution of token representations. Suppose that a token representation <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="Thmremark4.p1.m5"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> belongs to a subspace <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmremark4.p1.m6"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and these subspaces are approximately orthogonal to each other, i.e., <math alttext="\bm{U}_{k}^{\top}\bm{U}_{l}\approx\bm{0}" class="ltx_Math" display="inline" id="Thmremark4.p1.m7"><semantics><mrow><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>l</mi></msub></mrow><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{\top}\bm{U}_{l}\approx\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ≈ bold_0</annotation></semantics></math> for all <math alttext="k\neq l" class="ltx_Math" display="inline" id="Thmremark4.p1.m8"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">k\neq l</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_l</annotation></semantics></math>. Then, one can verify that the projections <math alttext="\bm{U}_{k}\bm{U}_{k}^{\top}\bm{z}_{i}=\bm{z}_{i}" class="ltx_Math" display="inline" id="Thmremark4.p1.m9"><semantics><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>=</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\bm{U}_{k}^{\top}\bm{z}_{i}=\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{U}_{l}\bm{U}_{l}^{\top}\bm{z}_{i}\approx\bm{0}" class="ltx_Math" display="inline" id="Thmremark4.p1.m10"><semantics><mrow><mrow><msub><mi>𝑼</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>l</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{U}_{l}\bm{U}_{l}^{\top}\bm{z}_{i}\approx\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≈ bold_0</annotation></semantics></math> for all <math alttext="l\neq k" class="ltx_Math" display="inline" id="Thmremark4.p1.m11"><semantics><mrow><mi>l</mi><mo>≠</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">l\neq k</annotation><annotation encoding="application/x-llamapun">italic_l ≠ italic_k</annotation></semantics></math>. These orthogonal projections effectively serve as implicit membership labels, identifying the subspace to which each token representation belongs.</p>
</div>
</div>
<figure class="ltx_figure" id="F12"><img alt="Figure 4.12 : Comparison of three sets of representations via rate reduction and sparsity. Each S i S_{i} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents one linear subspace, and the number of blue balls represents the difference between the coding rates Δ ​ R ϵ ​ ( 𝒁 ∣ 𝑼 [ K ] ) = R ϵ ​ ( 𝒁 ) − R ϵ c ​ ( 𝒁 ∣ 𝑼 [ K ] ) \Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})=R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}) roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ." class="ltx_graphics ltx_img_landscape" height="135" id="F12.g1" src="chapters/chapter4/figs/coding-transform.png" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.12</span>: </span><span class="ltx_text" style="font-size:90%;"> <span class="ltx_text ltx_font_bold">Comparison of three sets of representations via rate reduction and sparsity.</span> Each <math alttext="S_{i}" class="ltx_Math" display="inline" id="F12.m3"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding="application/x-tex">S_{i}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents one linear subspace, and the number of blue balls represents the difference between the coding rates <math alttext="\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})=R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})" class="ltx_Math" display="inline" id="F12.m4"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})=R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math>.
</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Sparse Rate Reduction.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p">Note that the rate reduction objective (<a class="ltx_ref" href="#S2.E1" title="Equation 4.2.1 ‣ Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>) is invariant to arbitrary joint rotations of the representations and subspaces. In particular, optimizing the rate reduction objective may not naturally lead to axis-aligned (i.e., <span class="ltx_text ltx_font_italic">sparse</span>) representations. For instance, consider the three sets of learned representations in <a class="ltx_ref" href="#F12" title="In Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.12</span></a>. The coding rate reduction increases from (a) to (b), but because it is invariant under rotations, remains the same from (b) to (c). Therefore, we would like to transform the representations (and their supporting subspaces) so that the representations <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> eventually become sparse<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Concretely, having few nonzero entries.</span></span></span> with respect to the standard coordinates of the resulting representation space as in <a class="ltx_ref" href="#F12" title="In Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.12</span></a>(c). Therefore, to ensure the final representations are amenable to more compact coding, we would like to transform the representations (and their supporting subspaces) so that they become <span class="ltx_text ltx_font_italic">sparse</span> with respect to the standard coordinates of the resulting representation space.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>That is, having the fewest nonzero entries.</span></span></span> Computationally, we may combine the above two goals into a unified objective for optimization:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{f\in\mathcal{F}}\ [\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}]\qquad\text{s.t.}\ \bm{Z}=f(\bm{X})," class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>f</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℱ</mi></mrow></munder><mo lspace="0.500em">⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mspace width="2em"></mspace><mrow><mtext>s.t.</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{f\in\mathcal{F}}\ [\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}]\qquad\text{s.t.}\ \bm{Z}=f(\bm{X}),</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_f ∈ caligraphic_F end_POSTSUBSCRIPT [ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] s.t. bold_italic_Z = italic_f ( bold_italic_X ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">ℱ</mi><annotation encoding="application/x-tex">\mathcal{F}</annotation><annotation encoding="application/x-llamapun">caligraphic_F</annotation></semantics></math> denotes a general function class and the <math alttext="\ell_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m3"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\ell_{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> norm <math alttext="\|\bm{Z}\|_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m4"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><annotation encoding="application/x-tex">\|\bm{Z}\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> promotes the sparsity of the final token representations <math alttext="\bm{Z}=f(\bm{X})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m5"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p2">
<p class="ltx_p">In practice, the <math alttext="\ell_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m1"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\ell_{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> norm is often relaxed to the <math alttext="\ell_{1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m2"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> norm to improve computational traceability and enable convex optimization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>. Motivated by this, we relax Problem (<a class="ltx_ref" href="#S2.E4" title="Equation 4.2.4 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>) accordingly, leading to a formulation that remains faithful to the original sparsity objective while being more amenable to efficient algorithms as follow:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E5X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{f\in\mathcal{F}}\ [\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{1}]\qquad\text{s.t.}\ \bm{Z}=f(\bm{X})," class="ltx_Math" display="inline" id="S2.E5X.m2"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>f</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℱ</mi></mrow></munder><mo lspace="0.500em">⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mspace width="2em"></mspace><mrow><mtext>s.t.</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{f\in\mathcal{F}}\ [\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{1}]\qquad\text{s.t.}\ \bm{Z}=f(\bm{X}),</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_f ∈ caligraphic_F end_POSTSUBSCRIPT [ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] s.t. bold_italic_Z = italic_f ( bold_italic_X ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4.2.5)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">With a slight abuse of terminology, we often refer to this objective function also as the <span class="ltx_text ltx_font_italic">sparse rate reduction</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">White-Box Network Architecture via Unrolled Optimization.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p">Although easy to state, each term in the above objective is computationally challenging to optimize <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>. Hence it is natural to adopt an approximation approach that realizes the global transformation <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> to optimize
(<a class="ltx_ref" href="#S2.E4" title="Equation 4.2.4 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>) through a concatenation of multiple, say <math alttext="L" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m2"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation><annotation encoding="application/x-llamapun">italic_L</annotation></semantics></math>, simple <span class="ltx_text ltx_font_italic">incremental and local</span> operations <math alttext="f^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m3"><semantics><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">f^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> that push the representation distribution towards the desired parsimonious model distribution:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f\colon\bm{X}=\bm{Z}^{0}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{Z}^{1}\rightarrow\cdots\rightarrow\bm{Z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{Z}^{\ell+1}\rightarrow\cdots\xrightarrow{\hskip 2.84526ptf^{L-1}}\bm{Z}^{L}=\bm{Z}," class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝑿</mi><mo>=</mo><msup><mi>𝒁</mi><mn>0</mn></msup><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mn>0</mn></msup></mover><msup><mi>𝒁</mi><mn>1</mn></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">→</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup></mover><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup></mover><msup><mi>𝒁</mi><mi>L</mi></msup><mo>=</mo><mi>𝒁</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">f\colon\bm{X}=\bm{Z}^{0}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{Z}^{1}\rightarrow\cdots\rightarrow\bm{Z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{Z}^{\ell+1}\rightarrow\cdots\xrightarrow{\hskip 2.84526ptf^{L-1}}\bm{Z}^{L}=\bm{Z},</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_X = bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT → ⋯ → bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT → ⋯ start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = bold_italic_Z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="f^{0}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m4"><semantics><mrow><msup><mi>f</mi><mn>0</mn></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f^{0}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the pre-processing mapping that transforms each input token <math alttext="\bm{x}_{i}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m5"><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> to the initial token representations <math alttext="\bm{z}_{i}^{1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m6"><semantics><mrow><msubsup><mi>𝒛</mi><mi>i</mi><mn>1</mn></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{i}^{1}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
Each incremental <span class="ltx_text ltx_font_italic">forward mapping</span> <math alttext="\bm{Z}^{\ell+1}=f^{\ell}(\bm{Z}^{\ell})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m7"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}=f^{\ell}(\bm{Z}^{\ell})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math>, or a “layer”, transforms the token distribution to <span class="ltx_text ltx_font_italic">optimize</span> the above sparse rate reduction objective (<a class="ltx_ref" href="#S2.E4" title="Equation 4.2.4 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>), conditioned on the distribution of its input <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m8"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark5.p1">
<p class="ltx_p">In contrast to other unrolled optimization approaches such as the ReduNet (see <a class="ltx_ref" href="#S1" title="4.1 White-Box Deep Networks via Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>), we <span class="ltx_text ltx_font_italic">explicitly model</span> the distribution of <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="Thmremark5.p1.m1"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> at each layer, say as a mixture of linear subspaces or sparsely generated from a dictionary. The model parameters are learned from data (say via <span class="ltx_text ltx_font_italic">backward propagation</span> with end-to-end training). This separation between forward “optimization” and backward
“learning” clarifies the mathematical role of each layer as an operator
that transforms the distribution of its input, whereas the input distribution is in turn modeled (and subsequently learned) by the parameters of the layer.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p2">
<p class="ltx_p">Now, we show how to derive these incremental and local operations through an unrolled optimization perspective to solve Problem (<a class="ltx_ref" href="#S2.E5" title="Equation 4.2.5 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.5</span></a>). Once we decide on using an incremental approach to optimizing Problem
(<a class="ltx_ref" href="#S2.E5" title="Equation 4.2.5 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.5</span></a>), there are a variety of possible choices to achieve the optimization. Given a model for <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m1"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, say a mixture of subspaces <math alttext="\bm{U}_{[K]}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m2"><semantics><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\bm{U}_{[K]}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT</annotation></semantics></math>, we opt for a two-step <span class="ltx_text ltx_font_italic">alternating minimization</span> method with a strong conceptual basis. First, we <span class="ltx_text ltx_font_italic">compress</span> the tokens <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m3"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> via a gradient descent to minimize the coding rate term <math alttext="R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}^{\ell})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m4"><semantics><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}^{\ell})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math>. Specifically, we take a gradient step on <math alttext="R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m5"><semantics><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><annotation encoding="application/x-tex">R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> with a learning rate <math alttext="\kappa" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m6"><semantics><mi>κ</mi><annotation encoding="application/x-tex">\kappa</annotation><annotation encoding="application/x-llamapun">italic_κ</annotation></semantics></math> as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx45">
<tbody id="S2.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1/2}=\bm{Z}^{\ell}-\kappa\nabla_{\bm{Z}}R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}^{\ell})." class="ltx_Math" display="inline" id="S2.E7.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>=</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><mi>κ</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1/2}=\bm{Z}^{\ell}-\kappa\nabla_{\bm{Z}}R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]}^{\ell}).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - italic_κ ∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Next, we <span class="ltx_text ltx_font_italic">sparsify</span> the compressed tokens, generating <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m7"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> via a suitably-relaxed proximal gradient step to minimize the remaining term <math alttext="\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m8"><semantics><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>−</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math>. As we will argue in detail later, we can find such a <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m9"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> by solving a sparse presentation problem with respect to a dictionary <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m10"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1}=\operatorname*{arg\ min}_{{\bm{Z}}}\bigg{\{}\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}{\bm{Z}}\|_{F}^{2}\bigg{\}}." class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mi>𝒁</mi></munder><mo>⁡</mo><mrow><mo maxsize="210%" minsize="210%">{</mo><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>−</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo maxsize="210%" minsize="210%">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}=\operatorname*{arg\ min}_{{\bm{Z}}}\bigg{\{}\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}{\bm{Z}}\|_{F}^{2}\bigg{\}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT { italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT - bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the following, we provide technical details for each of the two steps above and derive efficient updates for their implementation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Self-Attention as Gradient Descent on Coding Rate of Token Representations.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p">For the first step (<a class="ltx_ref" href="#S2.E7" title="Equation 4.2.7 ‣ White-Box Network Architecture via Unrolled Optimization. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.7</span></a>), the gradient of the coding rate <math alttext="\nabla_{\bm{Z}}R^{c}_{\epsilon}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m1"><semantics><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">\nabla_{\bm{Z}}R^{c}_{\epsilon}</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> is costly to compute, as it involves <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m2"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> separate matrix inverses, one for each of the <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m3"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> subspaces with basis <math alttext="\bm{U}_{k}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m4"><semantics><msubsup><mi>𝑼</mi><mi>k</mi><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{U}_{k}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})=\frac{p}{N\epsilon^{2}}\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{Z}\Big{(}\bm{I}+\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\Big{)}^{-1}." class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})=\frac{p}{N\epsilon^{2}}\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{Z}\Big{(}\bm{I}+\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\Big{)}^{-1}.</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ( bold_italic_I + divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, we demonstrate that this gradient can be naturally approximated using a so-called multi-head subspace self-attention (MSSA) operator, which has a similar functional form to the multi-head self-attention operator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite> with <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m5"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> heads (i.e., one for each subspace, coming from each matrix inverse). Here, we approximate the gradient (<a class="ltx_ref" href="#S2.E9" title="Equation 4.2.9 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.9</span></a>) using the first-order Neumann series (see <a class="ltx_ref" href="#Thmexercise2" title="Exercise 4.2 (Neumann series for matrix inverse). ‣ 4.5 Exercises and Extensions ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx46">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})" class="ltx_Math" display="inline" id="S2.Ex1.m1"><semantics><mrow><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\approx\frac{p}{N\epsilon^{2}}\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{Z}\left(\bm{I}-\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right)" class="ltx_Math" display="inline" id="S2.Ex1.m2"><semantics><mrow><mi></mi><mo>≈</mo><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\approx\frac{p}{N\epsilon^{2}}\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{Z}\left(\bm{I}-\frac{p}{N\epsilon^{2}}(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right)</annotation><annotation encoding="application/x-llamapun">≈ divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ( bold_italic_I - divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle{=\frac{p}{N\epsilon^{2}}\left(\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{Z}-\left(\frac{p}{N\epsilon^{2}}\right)^{2}\sum_{k=1}^{K}\bm{U}_{k}(\bm{U}_{k}^{\top}\bm{Z})(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})}." class="ltx_Math" display="inline" id="S2.E10.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>−</mo><mrow><msup><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle{=\frac{p}{N\epsilon^{2}}\left(\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}\right)\bm{Z}-\left(\frac{p}{N\epsilon^{2}}\right)^{2}\sum_{k=1}^{K}\bm{U}_{k}(\bm{U}_{k}^{\top}\bm{Z})(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_Z - ( divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this approximation, we compute the similarity between projected token representations <math alttext="\{\bm{U}_{k}^{\top}\bm{z}_{i}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m6"><semantics><mrow><mo stretchy="false">{</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{U}_{k}^{\top}\bm{z}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> through an auto-correlation among the projected features as <math alttext="(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m7"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z )</annotation></semantics></math> and convert it to a distribution of membership with a softmax, namely <math alttext="\operatorname{\mathrm{softmax}}{(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})}" class="ltx_math_unparsed" display="inline" id="S2.SS1.SSS0.Px4.p1.m8"><semantics><mrow><mi>softmax</mi><msup><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathrm{softmax}}{(\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})}</annotation><annotation encoding="application/x-llamapun">roman_softmax ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z )</annotation></semantics></math>.
Suppose that a union of subspaces <math alttext="\bm{U}_{[K]}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m9"><semantics><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\bm{U}_{[K]}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT</annotation></semantics></math> spans the whole space. Then, we have <math alttext="\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}=\bm{I}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m10"><semantics><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>=</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{\top}=\bm{I}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = bold_italic_I</annotation></semantics></math>. Hence, (<a class="ltx_ref" href="#S2.Ex1" title="Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>) becomes</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx47">
<tbody id="S2.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})\approx\frac{p}{N\epsilon^{2}}\bm{Z}-\left(\frac{p}{N\epsilon^{2}}\right)^{2}\operatorname{MSSA}\left(\bm{Z}^{\ell}\mid\bm{U}_{[K]}^{\ell}\right)," class="ltx_Math" display="inline" id="S2.E11.m1"><semantics><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>−</mo><mrow><msup><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo>)</mo></mrow><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>MSSA</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∣</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\bm{Z}}R_{\epsilon}^{c}(\bm{Z}\mid\bm{U}_{[K]})\approx\frac{p}{N\epsilon^{2}}\bm{Z}-\left(\frac{p}{N\epsilon^{2}}\right)^{2}\operatorname{MSSA}\left(\bm{Z}^{\ell}\mid\bm{U}_{[K]}^{\ell}\right),</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≈ divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z - ( divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_MSSA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where MSSA is defined through an SSA operator as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx48">
<tbody id="S2.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathrm{SSA}\left(\bm{Z}\mid\bm{U}_{k}\right)\doteq(\bm{U}_{k}^{\top}\bm{Z})\mathrm{softmax}\left((\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right),\ \forall k\in[K]," class="ltx_Math" display="inline" id="S2.E12.m1"><semantics><mrow><mrow><mrow><mrow><mi>SSA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>softmax</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{SSA}\left(\bm{Z}\mid\bm{U}_{k}\right)\doteq(\bm{U}_{k}^{\top}\bm{Z})\mathrm{softmax}\left((\bm{U}_{k}^{\top}\bm{Z})^{\top}(\bm{U}_{k}^{\top}\bm{Z})\right),\ \forall k\in[K],</annotation><annotation encoding="application/x-llamapun">roman_SSA ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ≐ ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) roman_softmax ( ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) ) , ∀ italic_k ∈ [ italic_K ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.12)</span></td>
</tr></tbody>
<tbody id="S2.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathrm{MSSA}\left(\bm{Z}\mid\bm{U}_{[K]}\right)\doteq\frac{p}{N\epsilon^{2}}\begin{bmatrix}\bm{U}_{1},\dots,\bm{U}_{K}\end{bmatrix}\begin{bmatrix}\mathrm{SSA}({\bm{Z}\mid\bm{U}_{1}})\\
\vdots\\
\mathrm{SSA}({\bm{Z}\mid\bm{U}_{K}})\end{bmatrix}." class="ltx_Math" display="inline" id="S2.E13.m1"><semantics><mrow><mrow><mrow><mi>MSSA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo>)</mo></mrow></mrow><mo>≐</mo><mrow><mstyle displaystyle="true"><mfrac><mi>p</mi><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝑼</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑼</mi><mi>K</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>SSA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>SSA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mi>K</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{MSSA}\left(\bm{Z}\mid\bm{U}_{[K]}\right)\doteq\frac{p}{N\epsilon^{2}}\begin{bmatrix}\bm{U}_{1},\dots,\bm{U}_{K}\end{bmatrix}\begin{bmatrix}\mathrm{SSA}({\bm{Z}\mid\bm{U}_{1}})\\
\vdots\\
\mathrm{SSA}({\bm{Z}\mid\bm{U}_{K}})\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">roman_MSSA ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ divide start_ARG italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG [ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL roman_SSA ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_SSA ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Substituting (<a class="ltx_ref" href="#S2.E11" title="Equation 4.2.11 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.11</span></a>) into (<a class="ltx_ref" href="#S2.E7" title="Equation 4.2.7 ‣ White-Box Network Architecture via Unrolled Optimization. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.7</span></a>) yields that it can naturally approximated by</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1/2}=\left(1-\frac{\kappa p}{N\epsilon^{2}}\right)\bm{Z}^{\ell}+\frac{\kappa p}{N\epsilon^{2}}\mathrm{MSSA}\left(\bm{Z}^{\ell}\ \middle|\ \bm{U}_{[K]}^{\ell}\right)." class="ltx_math_unparsed" display="block" id="S2.E14.m1"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>=</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>κ</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mo>)</mo></mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mfrac><mrow><mi>κ</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mfrac><mi>MSSA</mi><mrow><mo>(</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0.500em" stretchy="true">|</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup><mo>)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}=\left(1-\frac{\kappa p}{N\epsilon^{2}}\right)\bm{Z}^{\ell}+\frac{\kappa p}{N\epsilon^{2}}\mathrm{MSSA}\left(\bm{Z}^{\ell}\ \middle|\ \bm{U}_{[K]}^{\ell}\right).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT = ( 1 - divide start_ARG italic_κ italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + divide start_ARG italic_κ italic_p end_ARG start_ARG italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_MSSA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT | bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.14)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.6</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark6.p1">
<p class="ltx_p">The SSA operator in (<a class="ltx_ref" href="#S2.E12" title="Equation 4.2.12 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.12</span></a>) resembles the <span class="ltx_text ltx_font_italic">attention operator</span> in a typical transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite>, except that here the linear operators of value, key, and query are all set to be <span class="ltx_text ltx_font_italic">the same</span> as the subspace basis, i.e., <math alttext="\bm{V}_{k}=\bm{K}_{k}=\bm{Q}_{k}=\bm{U}_{k}^{*}" class="ltx_Math" display="inline" id="Thmremark6.p1.m1"><semantics><mrow><msub><mi>𝑽</mi><mi>k</mi></msub><mo>=</mo><msub><mi>𝑲</mi><mi>k</mi></msub><mo>=</mo><msub><mi>𝑸</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">\bm{V}_{k}=\bm{K}_{k}=\bm{Q}_{k}=\bm{U}_{k}^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. Hence, we name <math alttext="\mathrm{SSA}({\,\cdot\,\mid\bm{U}_{k}}):\mathbb{R}^{d\times n}\rightarrow\mathbb{R}^{p\times n}" class="ltx_math_unparsed" display="inline" id="Thmremark6.p1.m2"><semantics><mrow><mi>SSA</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo rspace="0.278em" stretchy="false">)</mo></mrow><mo rspace="0.278em">:</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mrow><mi>p</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathrm{SSA}({\,\cdot\,\mid\bm{U}_{k}}):\mathbb{R}^{d\times n}\rightarrow\mathbb{R}^{p\times n}</annotation><annotation encoding="application/x-llamapun">roman_SSA ( ⋅ ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) : blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_p × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> the <span class="ltx_text ltx_font_bold">S</span>ubspace <span class="ltx_text ltx_font_bold">S</span>elf-<span class="ltx_text ltx_font_bold">A</span>ttention (SSA) operator. Then, the whole MSSA operator in (<a class="ltx_ref" href="#S2.E13" title="Equation 4.2.13 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.13</span></a>), formally defined as <math alttext="\mathrm{MSSA}({\,\cdot\,\mid\bm{U}_{[K]}})\colon\mathbb{R}^{d\times n}\to\mathbb{R}^{d\times n}" class="ltx_math_unparsed" display="inline" id="Thmremark6.p1.m3"><semantics><mrow><mi>MSSA</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><mo rspace="0.278em" stretchy="false">)</mo></mrow><mo rspace="0.278em">:</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathrm{MSSA}({\,\cdot\,\mid\bm{U}_{[K]}})\colon\mathbb{R}^{d\times n}\to\mathbb{R}^{d\times n}</annotation><annotation encoding="application/x-llamapun">roman_MSSA ( ⋅ ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) : blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> and called the <span class="ltx_text ltx_font_bold">M</span>ulti-Head <span class="ltx_text ltx_font_bold">S</span>ubspace <span class="ltx_text ltx_font_bold">S</span>elf-<span class="ltx_text ltx_font_bold">A</span>ttention (MSSA) operator, aggregates the attention head outputs by averaging using model-dependent weights, similar in concept to the popular multi-head self-attention operator in existing transformer networks. The overall gradient step (<a class="ltx_ref" href="#S2.E14" title="Equation 4.2.14 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.14</span></a>) resembles the multi-head self-attention implemented with a skip connection in transformers.</p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">MLP as Proximal Gradient Descent for Sparse Coding of Token Representations.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p1">
<p class="ltx_p">For the second step of alternating minimization, we need to minimize <math alttext="\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m1"><semantics><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>−</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math>. Note that the gradient <math alttext="\nabla R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m2"><semantics><mrow><mrow><mo rspace="0.167em">∇</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">∇ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math> involves a matrix inverse, and thus naive proximal gradient (see <a class="ltx_ref" href="A1.html#S1.SS3" title="A.1.3 Proximal Gradient Descent for Non-Smooth Problems ‣ A.1 Steepest Descent ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1.3</span></a>) to optimize this problem becomes intractable on large-scale problems. We therefore take a different, simplifying approach to trading off between representational diversity and sparsification: we posit a (complete) incoherent or orthogonal dictionary <math alttext="\bm{D}^{\ell}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m3"><semantics><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{D}^{\ell}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and ask to sparsify the intermediate iterates <math alttext="\bm{Z}^{\ell+1/2}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m4"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> with respect to <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m5"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>. That is, <math alttext="\bm{Z}^{\ell+1/2}\approx\bm{D}^{\ell}\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m6"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>≈</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}\approx\bm{D}^{\ell}\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ≈ bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m7"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is more sparse; that is, it is a <span class="ltx_text ltx_font_italic">sparse encoding</span> of <math alttext="\bm{Z}^{\ell+1/2}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m8"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. The dictionary <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m9"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is used to sparsify all tokens simultaneously.
By the incoherence assumption, we have <math alttext="(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell})\approx\bm{I}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m10"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">(\bm{D}^{\ell})^{\top}(\bm{D}^{\ell})\approx\bm{I}</annotation><annotation encoding="application/x-llamapun">( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ≈ bold_italic_I</annotation></semantics></math>. Thus from (<a class="ltx_ref" href="#S2.E2" title="Equation 4.2.2 ‣ Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>) we have</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{\epsilon}(\bm{Z}^{\ell+1/2})\approx R_{\epsilon}(\bm{D}^{\ell}\bm{Z}^{\ell+1})\approx R_{\epsilon}(\bm{Z}^{\ell+1})." class="ltx_Math" display="block" id="S2.E15.m1"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{Z}^{\ell+1/2})\approx R_{\epsilon}(\bm{D}^{\ell}\bm{Z}^{\ell+1})\approx R_{\epsilon}(\bm{Z}^{\ell+1}).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ) ≈ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ) ≈ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">To solve <math alttext="\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m11"><semantics><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>−</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lambda\|\bm{Z}\|_{1}-R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math>, we optimize the following problem</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx49">
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}}\|\bm{Z}\|_{1}\quad\mbox{subject to}\quad\bm{Z}^{\ell+1/2}=\bm{D}^{\ell}\bm{Z}." class="ltx_Math" display="inline" id="S2.Ex2.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>≈</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mi>𝒁</mi></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mspace width="1em"></mspace><mtext>subject to</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>=</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}}\|\bm{Z}\|_{1}\quad\mbox{subject to}\quad\bm{Z}^{\ell+1/2}=\bm{D}^{\ell}\bm{Z}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ≈ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT subject to bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT = bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The above sparse representation program is usually solved by relaxing it to an unconstrained convex program, known as LASSO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}}\left[\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{Z}\|_{F}^{2}\right]." class="ltx_Math" display="block" id="S2.E16.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>≈</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mi>𝒁</mi></munder><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>−</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}}\left[\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{Z}\|_{F}^{2}\right].</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ≈ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT [ italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT - bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In our implementation, we also add a non-negative constraint to <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m12"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, and solve the corresponding non-negative LASSO:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}\geq\bm{0}}\left[\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{Z}\|_{F}^{2}\right]." class="ltx_Math" display="block" id="S2.E17.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>≈</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mi>𝒁</mi><mo>≥</mo><mn>𝟎</mn></mrow></munder><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>−</mo><mrow><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}\approx\operatorname*{arg\ min}_{\bm{Z}\geq\bm{0}}\left[\lambda\|\bm{Z}\|_{1}+\frac{1}{2}\|\bm{Z}^{\ell+1/2}-\bm{D}^{\ell}\bm{Z}\|_{F}^{2}\right].</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ≈ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z ≥ bold_0 end_POSTSUBSCRIPT [ italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT - bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then, we incrementally optimize <a class="ltx_ref" href="#S2.E17" title="In MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">4.2.17</span></a> by performing an unrolled <span class="ltx_text ltx_font_italic">proximal gradient descent</span> step, known as an ISTA step <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx18" title="">BT09</a>]</cite>, to give the update:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx50">
<tbody id="S2.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="S2.E18.m1"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathrm{ISTA}({\bm{Z}^{\ell+1/2}\mid\bm{D}^{\ell}})," class="ltx_Math" display="inline" id="S2.E18.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>ISTA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>∣</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\mathrm{ISTA}({\bm{Z}^{\ell+1/2}\mid\bm{D}^{\ell}}),</annotation><annotation encoding="application/x-llamapun">= roman_ISTA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ∣ bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.18)</span></td>
</tr></tbody>
<tbody id="S2.E19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\text{where}\quad\mathrm{ISTA}({\bm{Z}\mid\bm{D}})" class="ltx_Math" display="inline" id="S2.E19.m1"><semantics><mrow><mtext>where</mtext><mspace width="1em"></mspace><mrow><mi>ISTA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝑫</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\text{where}\quad\mathrm{ISTA}({\bm{Z}\mid\bm{D}})</annotation><annotation encoding="application/x-llamapun">where roman_ISTA ( bold_italic_Z ∣ bold_italic_D )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\operatorname{ReLU}(\bm{Z}-\eta\bm{D}^{\top}(\bm{D}\bm{Z}-\bm{Z})-\eta\lambda\bm{1})." class="ltx_Math" display="inline" id="S2.E19.m2"><semantics><mrow><mrow><mi></mi><mo>≐</mo><mrow><mi>ReLU</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>−</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑫</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>−</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\operatorname{ReLU}(\bm{Z}-\eta\bm{D}^{\top}(\bm{D}\bm{Z}-\bm{Z})-\eta\lambda\bm{1}).</annotation><annotation encoding="application/x-llamapun">≐ roman_ReLU ( bold_italic_Z - italic_η bold_italic_D start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_D bold_italic_Z - bold_italic_Z ) - italic_η italic_λ bold_1 ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.19)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="F13"><img alt="Figure 4.13 : One layer of the CRATE encoder architecture. The full architecture is simply a concatenation of such layers, with some initial tokenizer, pre-processing head, and final task-specific head (i.e., a classification head)." class="ltx_graphics" id="F13.g1" src="chapters/chapter4/figs/crate_encoder_architecture.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.13</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer of the CRATE encoder architecture.<span class="ltx_text ltx_font_medium"> The full architecture is simply a concatenation of such layers, with some initial tokenizer, pre-processing head, and final task-specific head (i.e., a classification head).</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2.2 </span>Overall White-Box Transformer Architecture: CRATE</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">We now design a white-box transformer architecture, named the Coding RATE Transformer (<span class="ltx_text ltx_font_smallcaps">crate</span>), by unrolling the above updates. By combining the above two steps (<a class="ltx_ref" href="#S2.E14" title="Equation 4.2.14 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.14</span></a>) and (<a class="ltx_ref" href="#S2.E18" title="Equation 4.2.18 ‣ MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.18</span></a>):</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p">Local compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block – <span class="ltx_text ltx_font_typewriter">MSSA</span>;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">Global sparsification of token sets across all samples through sparse coding, leading to the sparsification block – <span class="ltx_text ltx_font_typewriter">ISTA</span>;</p>
</div>
</li>
</ol>
<p class="ltx_p">we can get the following rate-reduction-based transformer layer, illustrated in <a class="ltx_ref" href="#F13" title="In MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.13</span></a>,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{\ell+1/2}\doteq\bm{Z}^{\ell}+\texttt{MSSA}(\bm{Z}^{\ell}\mid\bm{U}_{[K]}^{\ell}),\qquad\bm{Z}^{\ell+1}\doteq\texttt{ISTA}(\bm{Z}^{\ell+1/2}\mid\bm{D}^{\ell})." class="ltx_Math" display="block" id="S2.E20.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>≐</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>+</mo><mrow><mtext class="ltx_mathvariant_monospace">MSSA</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>∣</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>≐</mo><mrow><mtext class="ltx_mathvariant_monospace">ISTA</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>∣</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}\doteq\bm{Z}^{\ell}+\texttt{MSSA}(\bm{Z}^{\ell}\mid\bm{U}_{[K]}^{\ell}),\qquad\bm{Z}^{\ell+1}\doteq\texttt{ISTA}(\bm{Z}^{\ell+1/2}\mid\bm{D}^{\ell}).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ≐ bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT + MSSA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) , bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ≐ ISTA ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT ∣ bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.2.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Composing multiple such layers following the incremental construction of our representation in (<a class="ltx_ref" href="#S2.E6" title="Equation 4.2.6 ‣ White-Box Network Architecture via Unrolled Optimization. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.6</span></a>), we obtain a white-box transformer architecture that transforms the data tokens towards a compact and sparse union of incoherent subspaces, where <math alttext="f^{\mathrm{pre}}:\mathbb{R}^{D\times N}\rightarrow\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.SS2.p1.m1"><semantics><mrow><msup><mi>f</mi><mi>pre</mi></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">f^{\mathrm{pre}}:\mathbb{R}^{D\times N}\rightarrow\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_pre end_POSTSUPERSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is the pre-processing mapping that transforms the input tokens <math alttext="\bm{X}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S2.SS2.p1.m2"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> to first-layer representations <math alttext="\bm{Z}^{1}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.SS2.p1.m3"><semantics><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}^{1}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. An overall flow of this architecture was shown in <a class="ltx_ref" href="#F14" title="In 4.2.2 Overall White-Box Transformer Architecture: CRATE ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.14</span></a>.</p>
</div>
<figure class="ltx_figure" id="F14"><img alt="Figure 4.14 : The ‘main loop’ of the crate white-box deep network design. After encoding input data as a sequence of tokens 𝒁 0 \bm{Z}^{0} bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , crate constructs a deep network that transforms the data to a canonical configuration of low-dimensional subspaces by successive compression against a local model for the distribution, generating 𝒁 ℓ + 1 / 2 \bm{Z}^{\ell+1/2} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT , and sparsification against a global dictionary, generating 𝒁 ℓ + 1 \bm{Z}^{\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT . Repeatedly stacking these blocks and training the model parameters via backpropagation yields a powerful and interpretable representation of the data." class="ltx_graphics" id="F14.g1" src="chapters/chapter4/figs/CRATE_fig1_patches.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.14</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">The ‘main loop’ of the <span class="ltx_text ltx_font_smallcaps">crate</span> white-box deep network design.</span>
After encoding input data as a sequence of tokens <math alttext="\bm{Z}^{0}" class="ltx_Math" display="inline" id="F14.m4"><semantics><msup><mi>𝒁</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_smallcaps">crate</span> constructs a deep network that transforms the data to a canonical configuration of low-dimensional subspaces by successive <span class="ltx_text ltx_font_bold ltx_font_italic">compression</span>
against a local model for the distribution, generating <math alttext="\bm{Z}^{\ell+1/2}" class="ltx_Math" display="inline" id="F14.m5"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <span class="ltx_text ltx_font_bold ltx_font_italic">sparsification</span>
against a global dictionary, generating <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="F14.m6"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Repeatedly stacking these blocks and training the model parameters via backpropagation yields a powerful and interpretable representation of the data.
</span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.7</span></span><span class="ltx_text ltx_font_italic"> </span>(<span class="ltx_text ltx_font_bold">The roles of the forward pass and backward propagation</span>)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark7.p1">
<p class="ltx_p">In contrast to other unrolled optimization approaches such as the ReduNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx41" title="">CYY+22</a>]</cite>, we <span class="ltx_text ltx_font_italic">explicitly model</span> the distribution of each <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="Thmremark7.p1.m1"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{Z}^{\ell+1/2}" class="ltx_Math" display="inline" id="Thmremark7.p1.m2"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> at each layer, either by a mixture of linear subspaces or sparsely generated from a dictionary. We introduced the interpretation that at each layer <math alttext="\ell" class="ltx_Math" display="inline" id="Thmremark7.p1.m3"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>, the learned bases for the subspaces <math alttext="\bm{U}_{[K]}^{\ell}" class="ltx_Math" display="inline" id="Thmremark7.p1.m4"><semantics><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{U}_{[K]}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> and the learned dictionaries <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="Thmremark7.p1.m5"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> together serve as a <span class="ltx_text ltx_font_italic">codebook</span> or <span class="ltx_text ltx_font_italic">analysis filter</span> that encodes and transforms the intermediate representations at each layer <math alttext="\ell" class="ltx_Math" display="inline" id="Thmremark7.p1.m6"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>. Since the input distribution to layer <math alttext="\ell" class="ltx_Math" display="inline" id="Thmremark7.p1.m7"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> is first modeled by <math alttext="\bm{U}_{[K]}^{\ell}" class="ltx_Math" display="inline" id="Thmremark7.p1.m8"><semantics><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup><annotation encoding="application/x-tex">\bm{U}_{[K]}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> then transformed by <math alttext="\bm{D}^{\ell}" class="ltx_Math" display="inline" id="Thmremark7.p1.m9"><semantics><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{D}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, the input distribution to each layer is different, and so we require a separate code book at each layer to obtain the most parsimonious encoding. Parameters of these codebooks (i.e., the subspace bases and the dictionaries), heretofore assumed as being perfectly known, are actually learned from data (say via <span class="ltx_text ltx_font_italic">backward propagation</span> within end-to-end training).</p>
</div>
<div class="ltx_para" id="Thmremark7.p2">
<p class="ltx_p">Hence, our methodology features a clear conceptual separation between forward “optimization” and backward “learning” for the so-derived white-box deep neural network. Namely, in its forward pass, we interpret each layer as an operator which, conditioned on a learned model (i.e., a codebook) for the distribution of its input, transforms this distribution towards a more parsimonious representation. In its backward propagation, the codebook of this model, for the distribution of the input to each layer, is updated to better fit a certain (supervised) input-output relationship, as illustrated in Figure <a class="ltx_ref" href="#F15" title="Figure 4.15 ‣ 4.2.2 Overall White-Box Transformer Architecture: CRATE ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.15</span></a>. This conceptual interpretation implies a certain agnosticism of the model representations towards the particular task and loss; in particular, many types of tasks and losses will ensure that the models at each layer are fit, which ensures that the model produces parsimonious representations.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">We now present the empirical performance of the proposed networks <span class="ltx_text ltx_font_smallcaps">crate</span> by measuring their top-1 classification accuracy on ImageNet-1K as well as transfer learning performance on several widely used downstream datasets.
We summarize the results in Table <a class="ltx_ref" href="#T1" title="Table 4.1 ‣ 4.2.2 Overall White-Box Transformer Architecture: CRATE ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>. The transfer learning methodology is to fine-tune using cross-entropy loss initializing from the pre-trained networks.
As the designed white-box transformer architecture leverages parameter sharing in both the attention block (<span class="ltx_text ltx_font_typewriter">MSSA</span>) and the nonlinearity block (<span class="ltx_text ltx_font_typewriter">ISTA</span>), the <span class="ltx_text ltx_font_smallcaps">crate</span>-Base model (22.80 million)
has a similar number of parameters to the ViT-Small (22.05 million) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx75" title="">DBK+21</a>]</cite>, and less than 30% of the parameters of an identically configured ViT-Base (86.54 million).
From Table <a class="ltx_ref" href="#T1" title="Table 4.1 ‣ 4.2.2 Overall White-Box Transformer Architecture: CRATE ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we find that with a similar number of model parameters, our proposed network achieves similar ImageNet-1K and transfer learning performance as ViT, while having a simple and principled design. Moreover, with the same set of training hyperparameters, we observe promising scaling behavior in <span class="ltx_text ltx_font_smallcaps">crate</span>—we consistently improve the performance by scaling up the model size. To summarize, <span class="ltx_text ltx_font_smallcaps">crate</span> achieve promising performance on real-world large-scale datasets by directly implementing our principled architecture. We will provide more details of the implementation and analysis of the experimental results on image classification in the final application Chapter <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F15.sf1"><img alt="(a) Forward pass" class="ltx_graphics ltx_img_landscape" height="253" id="F15.sf1.g1" src="chapters/chapter4/figs/forward.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Forward pass</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F15.sf2"><img alt="(a) Forward pass" class="ltx_graphics ltx_img_landscape" height="266" id="F15.sf2.g1" src="chapters/chapter4/figs/backward.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Backward propagation</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">The roles of forward pass and backward propagation in deep networks<span class="ltx_text ltx_font_medium">. (a) Given fixed subspaces and dictionaries <math alttext="\{(\bm{U}_{[K]}^{\ell},\bm{D}^{\ell})\}_{\ell=1}^{L}" class="ltx_Math" display="inline" id="F15.m3"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding="application/x-tex">\{(\bm{U}_{[K]}^{\ell},\bm{D}^{\ell})\}_{\ell=1}^{L}</annotation><annotation encoding="application/x-llamapun">{ ( bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>, each layer performs compression and sparsification on representations in the forward pass; (b) Backpropagation learn subspaces and dictionaries <math alttext="\{(\bm{U}_{[K]}^{\ell},\bm{D}^{\ell})\}_{\ell=1}^{L}" class="ltx_Math" display="inline" id="F15.m4"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow><mi mathvariant="normal">ℓ</mi></msubsup><mo>,</mo><msup><mi>𝑫</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding="application/x-tex">\{(\bm{U}_{[K]}^{\ell},\bm{D}^{\ell})\}_{\ell=1}^{L}</annotation><annotation encoding="application/x-llamapun">{ ( bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_D start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> from training data. </span></span></figcaption>
</figure>
<figure class="ltx_table" id="T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 4.1</span>: </span><span class="ltx_text" style="font-size:90%;">Top-1 classification accuracy of <span class="ltx_text ltx_font_smallcaps">crate</span> on various datasets with different model scales
when pre-trained on ImageNet-1K. For ImageNet-1K/ImageNet-1K ReaL, we directly evaluate the top-1 accuracy. For other datasets, we use models that are pre-trained on ImageNet as initialization and the evaluate the transfer learning performance via fine-tuning.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" style="width:424.9pt;height:140.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.1pt,1.7pt) scale(0.976436458539575,0.976436458539575) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:90%;">crate</span><span class="ltx_text" style="font-size:90%;">-T</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:90%;">crate</span><span class="ltx_text" style="font-size:90%;">-S</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:90%;">crate</span><span class="ltx_text" style="font-size:90%;">-B</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text ltx_font_smallcaps" style="font-size:90%;">crate</span><span class="ltx_text" style="font-size:90%;">-L</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> ViT-T</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;">ViT-S</span>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;"># parameters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">6.09M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">13.12M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">22.80M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">77.64M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 5.72M</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 22.05M</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">ImageNet-1K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">66.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">69.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">70.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">71.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 71.5</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 72.4</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">ImageNet-1K ReaL</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">74.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">76.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">76.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">77.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 78.3</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 78.4</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">CIFAR10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">95.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">96.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">96.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">97.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 96.6</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 97.2</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">CIFAR100</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">78.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">81.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">82.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">83.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 81.8</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 83.2</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">Oxford Flowers-102</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">84.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">87.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">88.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">88.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 85.1</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 88.5</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">Oxford-IIIT-Pets</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">81.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">84.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">85.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:13.6pt;padding-right:13.6pt;"><span class="ltx_text" style="font-size:90%;">87.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 88.5</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:13.6pt;padding-right:13.6pt;">
<span class="ltx_text" style="font-size:90%;"></span><span class="ltx_text" style="font-size:90%;color:#808080;"> 88.6</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.3 </span>Variants of Deep Architectures by Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">So far, we wish that we have provided compelling evidence that the role of (popular) deep networks is to realize certain optimization algorithms for minimizing the coding rate (or maximizing the information gain) of the learned representations. However, readers who are familiar with optimization methods might have noticed that the above architectures (the ReduNet or the CRATE) correspond to rather basic optimization techniques. They may have plenty of room for improvement in efficiency or effectiveness. Moreover, if we believe the proposed theoretical framework for interpreting deep networks is correct, it should not only help explain existing architectures, it should guide us develop more efficient and effective architectures. In this section, we show this could be the case: the resulting new architectures are not only fully interpretable but also with guaranteed correctness and improved efficiency.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3.1 </span>Attention-Only Transformer Architecture</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">In this subsection, we propose a minimalistic transformer architecture consisting of interpretable layers based on the MSSA operator. To derive a fully interpretable transformer architecture with only necessary components,
we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. Here, we assume that the initial token representations <math alttext="\bm{Z}^{(1)}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><msup><mi>𝒁</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{(1)}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> are sampled from a mixture of low-rank Gaussians perturbed by noise as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 4.1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition1.p1">
<p class="ltx_p">Let <math alttext="C_{1},\dots,C_{K}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m1"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>C</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">C_{1},\dots,C_{K}</annotation><annotation encoding="application/x-llamapun">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_C start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math> be a partition of the index set <math alttext="[N]" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m2"><semantics><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[N]</annotation><annotation encoding="application/x-llamapun">[ italic_N ]</annotation></semantics></math> and <math alttext="\bm{U}_{k}\in\mathcal{O}^{d\times p_{k}}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m3"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>p</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathcal{O}^{d\times p_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_O start_POSTSUPERSCRIPT italic_d × italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> denote the orthonormal basis of the <math alttext="k" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m4"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th subspace for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m5"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>. We say that the token representations <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m6"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are sampled from a mixture of noisy low-rank Gaussian distributions if for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m7"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx51">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{z}_{i}=\underbrace{\bm{U}_{k}\bm{a}_{i}}_{\bf signal}+\underbrace{\sum_{j\neq k}^{K}\bm{U}_{j}\bm{e}_{i,j}}_{\bf noise},\ \forall i\in C_{k}," class="ltx_Math" display="inline" id="S3.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒛</mi><mi>i</mi></msub><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒂</mi><mi>i</mi></msub></mrow><mo>⏟</mo></munder><mi>𝐬𝐢𝐠𝐧𝐚𝐥</mi></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>k</mi></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒆</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>⏟</mo></munder><mi>𝐧𝐨𝐢𝐬𝐞</mi></munder></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow><mo>∈</mo><msub><mi>C</mi><mi>k</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{z}_{i}=\underbrace{\bm{U}_{k}\bm{a}_{i}}_{\bf signal}+\underbrace{\sum_{j\neq k}^{K}\bm{U}_{j}\bm{e}_{i,j}}_{\bf noise},\ \forall i\in C_{k},</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = under⏟ start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT bold_signal end_POSTSUBSCRIPT + under⏟ start_ARG ∑ start_POSTSUBSCRIPT italic_j ≠ italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT bold_noise end_POSTSUBSCRIPT , ∀ italic_i ∈ italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{a}_{i}\overset{i.i.d.}{\sim}\mathcal{N}(\bm{0},\bm{I}_{p_{k}})" class="ltx_math_unparsed" display="inline" id="Thmdefinition1.p1.m8"><semantics><mrow><msub><mi>𝒂</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mo>∼</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi>i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi>d</mi><mo lspace="0em">.</mo></mrow></mover><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><msub><mi>𝑰</mi><msub><mi>p</mi><mi>k</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{a}_{i}\overset{i.i.d.}{\sim}\mathcal{N}(\bm{0},\bm{I}_{p_{k}})</annotation><annotation encoding="application/x-llamapun">bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG ∼ end_ARG caligraphic_N ( bold_0 , bold_italic_I start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math> and <math alttext="\bm{e}_{i,j}\overset{i.i.d.}{\sim}\mathcal{N}(\bm{0},\delta^{2}\bm{I}_{p_{j}})" class="ltx_math_unparsed" display="inline" id="Thmdefinition1.p1.m9"><semantics><mrow><msub><mi>𝒆</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mo>∼</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi>i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi>d</mi><mo lspace="0em">.</mo></mrow></mover><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>δ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑰</mi><msub><mi>p</mi><mi>j</mi></msub></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{e}_{i,j}\overset{i.i.d.}{\sim}\mathcal{N}(\bm{0},\delta^{2}\bm{I}_{p_{j}})</annotation><annotation encoding="application/x-llamapun">bold_italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG ∼ end_ARG caligraphic_N ( bold_0 , italic_δ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math> for all <math alttext="i\in C_{k}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m10"><semantics><mrow><mi>i</mi><mo>∈</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">i\in C_{k}</annotation><annotation encoding="application/x-llamapun">italic_i ∈ italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m11"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>, <math alttext="\{\bm{a}_{i}\}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m12"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒂</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{a}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> and <math alttext="\{\bm{e}_{i,j}\}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m13"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒆</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{e}_{i,j}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT }</annotation></semantics></math> are respectively mutually independent, and <math alttext="\{\bm{a}_{i}\}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m14"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒂</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{a}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> is independent of <math alttext="\{\bm{e}_{i,j}\}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m15"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒆</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{e}_{i,j}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT }</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">This model serves as an idealized framework for approximating token representations in real-world pretrained LLMs. It assumes that the token representations are sampled from a mixture of multiple low-rank Gaussian distributions with noise. Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. In addition, this model aligns well with two well-established hypotheses about the structure of token representations in pretrained large language models: the “linear representation hypothesis” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx123" title="">JRR+24</a>, <a class="ltx_ref" href="bib.html#bibx210" title="">PCV24</a>]</cite> and the “superposition hypothesis” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx79" title="">EHO+22</a>, <a class="ltx_ref" href="bib.html#bibx314" title="">YCO+21</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.8</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark8.p1">
<p class="ltx_p">The linear representation hypothesis posits that token representations in LLMs lie in low-dimensional linear subspaces that encode semantic features. Similarly, the superposition hypothesis suggests that these representations can be approximately expressed as a sparse linear combination of these feature vectors. In <a class="ltx_ref" href="#Thmdefinition1" title="Definition 4.1. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Definition</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, each basis <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmremark8.p1.m1"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> of the subspaces can be interpreted as a set of semantic features, where each feature corresponds to a specific aspect of the token’s meaning. Token representations are then approximately expressed as sparse linear combinations of these subspace bases, capturing the essential semantic components of the token while ignoring irrelevant dimensions.</p>
</div>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Denoising Operator for Token Representations.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p">Now, we show that the MSSA operator (see (<a class="ltx_ref" href="#S2.E13" title="Equation 4.2.13 ‣ Self-Attention as Gradient Descent on Coding Rate of Token Representations. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.13</span></a>)) can incrementally denoise token representations generated from the above model. Spefically, we consider for each <math alttext="\ell=1,\dots,L" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi></mrow></mrow><annotation encoding="application/x-tex">\ell=1,\dots,L</annotation><annotation encoding="application/x-llamapun">roman_ℓ = 1 , … , italic_L</annotation></semantics></math>,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx52">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{(\ell+1)}=\bm{Z}^{(\ell)}+\eta\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}^{(\ell)}\varphi\left(\bm{Z}^{(\ell)^{T}}\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}^{(\ell)}\right)," class="ltx_Math" display="inline" id="S3.E2.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>𝒁</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><msup><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{(\ell+1)}=\bm{Z}^{(\ell)}+\eta\sum_{k=1}^{K}\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}^{(\ell)}\varphi\left(\bm{Z}^{(\ell)^{T}}\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}^{(\ell)}\right),</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT + italic_η ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT italic_φ ( bold_italic_Z start_POSTSUPERSCRIPT ( roman_ℓ ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\{\bm{U}_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m2"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\bm{U}_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> is defined in <a class="ltx_ref" href="#Thmdefinition1" title="Definition 4.1. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Definition</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, <math alttext="\eta&gt;0" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m3"><semantics><mrow><mi>η</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\eta&gt;0</annotation><annotation encoding="application/x-llamapun">italic_η &gt; 0</annotation></semantics></math> is the step size, and <math alttext="\varphi" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m4"><semantics><mi>φ</mi><annotation encoding="application/x-tex">\varphi</annotation><annotation encoding="application/x-llamapun">italic_φ</annotation></semantics></math> is an element-wise operator, such as soft-max, ReLU, or other functions. To simplify our development, we assume that the subspaces in <a class="ltx_ref" href="#Thmdefinition1" title="Definition 4.1. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Definition</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> are orthogonal to each other, i.e., <math alttext="\bm{U}_{k}^{T}\bm{U}_{j}=\bm{0}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m5"><semantics><mrow><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>j</mi></msub></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{T}\bm{U}_{j}=\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = bold_0</annotation></semantics></math> for all <math alttext="k\neq j" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m6"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">k\neq j</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_j</annotation></semantics></math>. Note that this assumption is not restrictive, as in high-dimensional spaces, random low-dimensional subspaces are incoherent to each other with high probability, i.e., <math alttext="\bm{U}_{k}^{T}\bm{U}_{j}\approx\bm{0}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m7"><semantics><mrow><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>j</mi></msub></mrow><mo>≈</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{T}\bm{U}_{j}\approx\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ≈ bold_0</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx294" title="">WM21</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>One may straightforwardly generalize our results to non-orthogonal subspaces, with slightly more sophisticated analysis.</span></span></span></p>
</div>
<figure class="ltx_figure" id="F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F16.sf1"><img alt="(a) Noise level δ = 0.2 \delta=0.2 italic_δ = 0.2" class="ltx_graphics ltx_img_landscape" height="396" id="F16.sf1.g1" src="chapters/chapter4/figs/SNR1.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Noise level <math alttext="\delta=0.2" class="ltx_Math" display="inline" id="F16.sf1.m2"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\delta=0.2</annotation><annotation encoding="application/x-llamapun">italic_δ = 0.2</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F16.sf2"><img alt="(a) Noise level δ = 0.2 \delta=0.2 italic_δ = 0.2" class="ltx_graphics ltx_img_landscape" height="396" id="F16.sf2.g1" src="chapters/chapter4/figs/SNR2.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Noise level <math alttext="\delta=0.5" class="ltx_Math" display="inline" id="F16.sf2.m2"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\delta=0.5</annotation><annotation encoding="application/x-llamapun">italic_δ = 0.5</annotation></semantics></math></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.16</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Denosing performance of the attention-only transformer.<span class="ltx_text ltx_font_medium"> Here, we sample initial token representations from a mixture of low-rank Gassuains in <a class="ltx_ref" href="#Thmdefinition1" title="Definition 4.1. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Definition</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>. Then, we apply (<a class="ltx_ref" href="#S3.E2" title="Equation 4.3.2 ‣ Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>) to update token representations and report the SNR at each layer.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p">Now, let the columns of <math alttext="\bm{Z}_{k}^{(\ell)}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m1"><semantics><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\bm{Z}_{k}^{(\ell)}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the token representations from the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m2"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th subspace at the <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m3"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th layer. To quantify the denoising capability, we define the signal-to-noise ratio (SNR) for each block of the token representations at the <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m4"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math>-th layer as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx53">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathrm{SNR}(\bm{Z}_{k}^{(\ell)})\doteq\frac{\|\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}_{k}^{(\ell)}\|_{F}}{\|(\bm{I}-\bm{U}_{k}\bm{U}_{k}^{T})\bm{Z}_{k}^{(\ell)}\|_{F}},\quad\forall k\in[K]." class="ltx_Math" display="inline" id="S3.E3.m1"><semantics><mrow><mrow><mrow><mrow><mi>SNR</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mstyle displaystyle="true"><mfrac><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi></msub><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mi>T</mi></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi></msub></mfrac></mstyle></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{SNR}(\bm{Z}_{k}^{(\ell)})\doteq\frac{\|\bm{U}_{k}\bm{U}_{k}^{T}\bm{Z}_{k}^{(\ell)}\|_{F}}{\|(\bm{I}-\bm{U}_{k}\bm{U}_{k}^{T})\bm{Z}_{k}^{(\ell)}\|_{F}},\quad\forall k\in[K].</annotation><annotation encoding="application/x-llamapun">roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) ≐ divide start_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_ARG start_ARG ∥ ( bold_italic_I - bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_ARG , ∀ italic_k ∈ [ italic_K ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">To simplify our analysis, we assume that <math alttext="p=p_{1}=\dots=p_{K}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m5"><semantics><mrow><mi>p</mi><mo>=</mo><msub><mi>p</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="normal">⋯</mi><mo>=</mo><msub><mi>p</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">p=p_{1}=\dots=p_{K}</annotation><annotation encoding="application/x-llamapun">italic_p = italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ⋯ = italic_p start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="N_{1}=\dots=N_{K}=N/K" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.m6"><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="normal">⋯</mi><mo>=</mo><msub><mi>N</mi><mi>K</mi></msub><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">N_{1}=\dots=N_{K}=N/K</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ⋯ = italic_N start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT = italic_N / italic_K</annotation></semantics></math>, and</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx54">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\begin{bmatrix}\bm{U}_{1}&amp;\dots&amp;\bm{U}_{K}\end{bmatrix}\in\mathcal{O}^{d\times Kp}." class="ltx_Math" display="inline" id="S3.E4.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt"><mtr><mtd><msub><mi>𝑼</mi><mn>1</mn></msub></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><msub><mi>𝑼</mi><mi>K</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><mrow><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\begin{bmatrix}\bm{U}_{1}&amp;\dots&amp;\bm{U}_{K}\end{bmatrix}\in\mathcal{O}^{d\times Kp}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ caligraphic_O start_POSTSUPERSCRIPT italic_d × italic_K italic_p end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p3">
<p class="ltx_p">With the above setup, we now characterize the denoising performance of the MSSA operator.</p>
</div>
<figure class="ltx_figure" id="F17"><img alt="Figure 4.17 : Details of the attention-only transformer architecture. Each layer consists of the MSSA operator and a skip connection. In addition, LayeNnorm is included only for language tasks. In practice, backpropagation is applied to train the model parameters using training samples." class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="F17.g1" src="chapters/chapter4/figs/MSSA.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.17</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Details of the attention-only transformer architecture.<span class="ltx_text ltx_font_medium"> Each layer consists of the MSSA operator and a skip connection. In addition, LayeNnorm is included only for language tasks. In practice, backpropagation is applied to train the model parameters using training samples. </span></span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4.1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\bm{Z}^{(1)}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m1"><semantics><msup><mi>𝐙</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{(1)}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> be defined in <a class="ltx_ref" href="#Thmdefinition1" title="Definition 4.1. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Definition</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> and <math alttext="\varphi(\cdot)" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m2"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_φ ( ⋅ )</annotation></semantics></math> in Eq. (<a class="ltx_ref" href="#S3.E2" title="Equation 4.3.2 ‣ Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>) be
<math alttext="\varphi(\bm{x})=h\left(\sigma(\bm{x})\right)" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m3"><semantics><mrow><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\varphi(\bm{x})=h\left(\sigma(\bm{x})\right)</annotation><annotation encoding="application/x-llamapun">italic_φ ( bold_italic_x ) = italic_h ( italic_σ ( bold_italic_x ) )</annotation></semantics></math>,
where <math alttext="\sigma:\mathbb{R}^{N}\to\mathbb{R}^{N}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m4"><semantics><mrow><mi>σ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>N</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\sigma:\mathbb{R}^{N}\to\mathbb{R}^{N}</annotation><annotation encoding="application/x-llamapun">italic_σ : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is the soft-max function and <math alttext="h:\mathbb{R}^{N}\to\mathbb{R}^{N}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m5"><semantics><mrow><mi>h</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>N</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow></mrow><annotation encoding="application/x-tex">h:\mathbb{R}^{N}\to\mathbb{R}^{N}</annotation><annotation encoding="application/x-llamapun">italic_h : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is an element-wise thresholding function with <math alttext="h(x)=\tau\mathbb{I}\left\{x&gt;\tau\right\}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m6"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝕀</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>{</mo><mrow><mi>x</mi><mo>&gt;</mo><mi>τ</mi></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(x)=\tau\mathbb{I}\left\{x&gt;\tau\right\}</annotation><annotation encoding="application/x-llamapun">italic_h ( italic_x ) = italic_τ blackboard_I { italic_x &gt; italic_τ }</annotation></semantics></math> for each <math alttext="i\in[N]" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m7"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">i\in[N]</annotation><annotation encoding="application/x-llamapun">italic_i ∈ [ italic_N ]</annotation></semantics></math>. Suppose that <math alttext="p\gtrsim\log N" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m8"><semantics><mrow><mi>p</mi><mo>≳</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">p\gtrsim\log N</annotation><annotation encoding="application/x-llamapun">italic_p ≳ roman_log italic_N</annotation></semantics></math>, <math alttext="\delta\lesssim\sqrt{\log N}/\sqrt{p}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m9"><semantics><mrow><mi>δ</mi><mo>≲</mo><mrow><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow></msqrt><mo>/</mo><msqrt><mi>p</mi></msqrt></mrow></mrow><annotation encoding="application/x-tex">\delta\lesssim\sqrt{\log N}/\sqrt{p}</annotation><annotation encoding="application/x-llamapun">italic_δ ≲ square-root start_ARG roman_log italic_N end_ARG / square-root start_ARG italic_p end_ARG</annotation></semantics></math>, and</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx55">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\tau\in\left(\frac{1}{2},\frac{1}{1+N\exp(-9p/32)}\right]." class="ltx_Math" display="inline" id="S3.Ex1.m1"><semantics><mrow><mrow><mi>τ</mi><mo>∈</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>,</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>N</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mrow><mrow><mn>9</mn><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><mo>/</mo><mn>32</mn></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mfrac></mstyle><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\tau\in\left(\frac{1}{2},\frac{1}{1+N\exp(-9p/32)}\right].</annotation><annotation encoding="application/x-llamapun">italic_τ ∈ ( divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG 1 end_ARG start_ARG 1 + italic_N roman_exp ( - 9 italic_p / 32 ) end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For sufficiently large <math alttext="N" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m10"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math>, it holds with probability at least <math alttext="1-KLN^{-\Omega(1)}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m11"><semantics><mrow><mn>1</mn><mo>−</mo><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>N</mi><mrow><mo>−</mo><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">1-KLN^{-\Omega(1)}</annotation><annotation encoding="application/x-llamapun">1 - italic_K italic_L italic_N start_POSTSUPERSCRIPT - roman_Ω ( 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> that for each <math alttext="\ell\in[L]" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m12"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ell\in[L]</annotation><annotation encoding="application/x-llamapun">roman_ℓ ∈ [ italic_L ]</annotation></semantics></math>,</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx56">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathrm{SNR}(\bm{Z}_{k}^{(\ell+1)})=(1+\eta\tau)\mathrm{SNR}(\bm{Z}_{k}^{(\ell)}),\ \forall k\in[K]." class="ltx_Math" display="inline" id="S3.E5.m1"><semantics><mrow><mrow><mrow><mrow><mi>SNR</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>τ</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>SNR</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝒁</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathrm{SNR}(\bm{Z}_{k}^{(\ell+1)})=(1+\eta\tau)\mathrm{SNR}(\bm{Z}_{k}^{(\ell)}),\ \forall k\in[K].</annotation><annotation encoding="application/x-llamapun">roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) = ( 1 + italic_η italic_τ ) roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) , ∀ italic_k ∈ [ italic_K ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.5)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p4">
<p class="ltx_p">This theorem demonstrates that when the initial token representations are sampled from a mixture of low-rank Gaussian distributions with a noise level <math alttext="O(\sqrt{\log N}/\sqrt{p})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p4.m1"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>N</mi></mrow></msqrt><mo>/</mo><msqrt><mi>p</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\sqrt{\log N}/\sqrt{p})</annotation><annotation encoding="application/x-llamapun">italic_O ( square-root start_ARG roman_log italic_N end_ARG / square-root start_ARG italic_p end_ARG )</annotation></semantics></math>, we show that each layer of the proposed transformer denoises token representations at a linear rate. This indicates the MSSA operator’s efficiency in reducing noise across layers. Notably, our theoretical results are well-supported by experimental observations in <a class="ltx_ref" href="#F16" title="In Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.16</span></a>. This theorem provides a theoretical foundation for the practical denoising capability of the transformer architecture derived by unrolling (<a class="ltx_ref" href="#S3.E2" title="Equation 4.3.2 ‣ Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>).</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 4.9</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark9.p1">
<p class="ltx_p">Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. However, we should point out that in real-world applications, where token representations exhibit more complicated structures, the goal of representation learning is to find a compact and structured representation by compressing token sets.</p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Attention-Only Transformer.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p">Now, we formally propose an attention-only transformer architecture. Specifically, by unrolling the iterative optimization steps (<a class="ltx_ref" href="#S3.E2" title="Equation 4.3.2 ‣ Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>) as layers of a deep network, we construct a transformer architecture in <a class="ltx_ref" href="#F17" title="In Denoising Operator for Token Representations. ‣ 4.3.1 Attention-Only Transformer Architecture ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.17</span></a>. Each layer of the proposed architecture only consists of the MSSA operator and a skip connection. For language tasks, we additionally incorporate LayerNorm before the MSSA operator to improve performance. The complete architecture is built by stacking such layers, along with essential task-specific pre-processing and post-processing steps, such as positional encoding, token embedding, and final task-specific head to adapt to different applications.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p2">
<p class="ltx_p">Generally speaking, the standard decoder-only transformer architecture is composed of the following key components <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite>: (1) positional encoding, (2) multi-head QKV self-attention mechanisms, (3) feed-forward MLP networks, (4) layer normalization, and (5) residual connections. In contrast, our proposed transformer architecture adopts a streamlined design by incorporating several key simplications. Specifically, it employs shared-QKV subspace self-attention mechanisms, excludes MLP layers, and reduces the frequency of LayerNorm.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3.2 </span>Linear-Time Attention: Token Statistics Transformer</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">In this subsection, we propose a new transformer attention operator whose computational complexity scales linearly with the number of tokens based on the coding rate reduction objective. Specifically, we derive a novel variational form of the MCR<sup class="ltx_sup">2</sup> objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (<span class="ltx_text ltx_font_typewriter">TSSA</span>). <span class="ltx_text ltx_font_typewriter">TSSA</span> has <span class="ltx_text ltx_font_italic">linear computational and memory complexity</span> and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Recall from (<a class="ltx_ref" href="Ch3.html#S4.Ex2" title="Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) that <math alttext="\bm{\Pi}=[\bm{\pi}_{1},\ldots,\bm{\pi}_{K}]\in\mathbb{R}^{N\times K}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><mrow><mi>𝚷</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝝅</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝝅</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Pi}=[\bm{\pi}_{1},\ldots,\bm{\pi}_{K}]\in\mathbb{R}^{N\times K}</annotation><annotation encoding="application/x-llamapun">bold_Π = [ bold_italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_π start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> denotes a stochastic “group assignment” matrix (i.e., <math alttext="\bm{\Pi}\bm{1}=\bm{1}" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><mrow><mrow><mi>𝚷</mi><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn></mrow><mo>=</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">\bm{\Pi}\bm{1}=\bm{1}</annotation><annotation encoding="application/x-llamapun">bold_Π bold_1 = bold_1</annotation></semantics></math> and <math alttext="\Pi_{ik}\geq 0,\ \forall(i,k)\in[N]\times[K]" class="ltx_Math" display="inline" id="S3.SS2.p1.m4"><semantics><mrow><mrow><msub><mi mathvariant="normal">Π</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>≥</mo><mn>0</mn></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo>∀</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mrow><mo stretchy="false">[</mo><mi>N</mi><mo rspace="0.055em" stretchy="false">]</mo></mrow><mo rspace="0.222em">×</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pi_{ik}\geq 0,\ \forall(i,k)\in[N]\times[K]</annotation><annotation encoding="application/x-llamapun">roman_Π start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT ≥ 0 , ∀ ( italic_i , italic_k ) ∈ [ italic_N ] × [ italic_K ]</annotation></semantics></math> ), where <math alttext="\Pi_{ik}" class="ltx_Math" display="inline" id="S3.SS2.p1.m5"><semantics><msub><mi mathvariant="normal">Π</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><annotation encoding="application/x-tex">\Pi_{ik}</annotation><annotation encoding="application/x-llamapun">roman_Π start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT</annotation></semantics></math> denotes the probability of assigning the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.m6"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>-th token to the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.m7"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th group.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">A New Variational Form for Coding Rates.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p">To begin, we consider a general form of MCR<sup class="ltx_sup">2</sup>-like objectives based on concave functions of the spectrum of a matrix. Namely, for a given PSD matrix <math alttext="\bm{M}\in\mathsf{PSD}(d)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m2"><semantics><mrow><mi>𝑴</mi><mo>∈</mo><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{M}\in\mathsf{PSD}(d)</annotation><annotation encoding="application/x-llamapun">bold_italic_M ∈ sansserif_PSD ( italic_d )</annotation></semantics></math> and any scalar <math alttext="c\geq 0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m3"><semantics><mrow><mi>c</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c\geq 0</annotation><annotation encoding="application/x-llamapun">italic_c ≥ 0</annotation></semantics></math> we have that <math alttext="\log\det(\bm{I}+c\bm{M})=\sum_{i=1}^{d}\log(1+c\lambda_{i}(\bm{M}))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m4"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">det</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑴</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\log\det(\bm{I}+c\bm{M})=\sum_{i=1}^{d}\log(1+c\lambda_{i}(\bm{M}))</annotation><annotation encoding="application/x-llamapun">roman_log roman_det ( bold_italic_I + italic_c bold_italic_M ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log ( 1 + italic_c italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M ) )</annotation></semantics></math>, where <math alttext="\lambda_{i}(\bm{M})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m5"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda_{i}(\bm{M})</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M )</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m6"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>-th largest eigenvalue of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m7"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>. Further, note that <math alttext="\log(1+c\sigma)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m8"><semantics><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>σ</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(1+c\sigma)</annotation><annotation encoding="application/x-llamapun">roman_log ( 1 + italic_c italic_σ )</annotation></semantics></math> is a concave non-decreasing function of <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m9"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation><annotation encoding="application/x-llamapun">italic_σ</annotation></semantics></math>. Thus, we describe our results in terms of a more general form of MCR<sup class="ltx_sup">2</sup> based on general spectral functions of PSD matrices of the form <math alttext="F(\bm{M})=\sum_{i=1}^{d}f(\lambda_{i}(\bm{M}))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m11"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">F(\bm{M})=\sum_{i=1}^{d}f(\lambda_{i}(\bm{M}))</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_M ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M ) )</annotation></semantics></math>, where <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m12"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is concave and non-decreasing. In particular, recall from our above discussion that the attention mechanism arises from unrolling the compression component of MCR<sup class="ltx_sup">2</sup>, so we consider a more general MCR<sup class="ltx_sup">2</sup>-style compression function:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{c,f}(\bm{Z},\bm{\Pi})\doteq\frac{1}{2}\sum_{k=1}^{K}\frac{N_{k}}{N}F\left(\frac{1}{N_{k}}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}\right)." class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mi>𝚷</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><msub><mi>N</mi><mi>k</mi></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><msub><mi>N</mi><mi>k</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R_{c,f}(\bm{Z},\bm{\Pi})\doteq\frac{1}{2}\sum_{k=1}^{K}\frac{N_{k}}{N}F\left(\frac{1}{N_{k}}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}\right).</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_F ( divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p">For the above objective, we now note the following result:</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4.2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="f\colon[0,\infty)\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f\colon[0,\infty)\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_f : [ 0 , ∞ ) → blackboard_R</annotation></semantics></math> be non-decreasing, concave, and obey <math alttext="f(0)=0" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m2"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0)=0</annotation><annotation encoding="application/x-llamapun">italic_f ( 0 ) = 0</annotation></semantics></math>, and let <math alttext="F\colon\mathsf{PSD}(d)\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m3"><semantics><mrow><mi>F</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">F\colon\mathsf{PSD}(d)\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_F : sansserif_PSD ( italic_d ) → blackboard_R</annotation></semantics></math> have the form <math alttext="F(\bm{M})=\sum_{i=1}^{d}f(\lambda_{i}(\bm{M}))" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m4"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐌</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐌</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">F(\bm{M})=\sum_{i=1}^{d}f(\lambda_{i}(\bm{M}))</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_M ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M ) )</annotation></semantics></math>. Then for each <math alttext="\bm{M}\in\mathsf{PSD}(d)" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m5"><semantics><mrow><mi>𝐌</mi><mo>∈</mo><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{M}\in\mathsf{PSD}(d)</annotation><annotation encoding="application/x-llamapun">bold_italic_M ∈ sansserif_PSD ( italic_d )</annotation></semantics></math> and <math alttext="\bm{Q}\in\mathsf{O}(d)" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m6"><semantics><mrow><mi>𝐐</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Q}\in\mathsf{O}(d)</annotation><annotation encoding="application/x-llamapun">bold_italic_Q ∈ sansserif_O ( italic_d )</annotation></semantics></math>, we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F(\bm{M})\leq\sum_{i=1}^{d}f\left((\bm{Q}^{\top}\bm{M}\bm{Q})_{ii}\right)." class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑴</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≤</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">F(\bm{M})\leq\sum_{i=1}^{d}f\left((\bm{Q}^{\top}\bm{M}\bm{Q})_{ii}\right).</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_M ) ≤ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( ( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_Q ) start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Further, the inequality in (<a class="ltx_ref" href="#S3.E7" title="Equation 4.3.7 ‣ Theorem 4.2. ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.7</span></a>) is achieved with equality for any <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m7"><semantics><mi>𝐐</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> which diagonalizes <math alttext="\bm{M}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m8"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>, and if <math alttext="f" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m9"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is strictly concave then the inequality in (<a class="ltx_ref" href="#S3.E7" title="Equation 4.3.7 ‣ Theorem 4.2. ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.7</span></a>) is achieved with equality if and only if <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m10"><semantics><mi>𝐐</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> diagonalizes <math alttext="\bm{M}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m11"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p3">
<p class="ltx_p">Using the above result, we can replace (<a class="ltx_ref" href="#S3.E6" title="Equation 4.3.6 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.6</span></a>) with an equivalent variational objective with form</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})\doteq\frac{1}{2}\sum_{k=1}^{K}\frac{N_{k}}{N}\sum_{i=1}^{d}f\left(\frac{1}{N_{k}}(\bm{U}_{k}^{\top}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}\bm{U}_{k})_{ii}\right)," class="ltx_Math" display="block" id="S3.E8.m1"><semantics><mrow><mrow><mrow><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><mi>𝚷</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><msub><mi>N</mi><mi>k</mi></msub><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><msub><mi>N</mi><mi>k</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})\doteq\frac{1}{2}\sum_{k=1}^{K}\frac{N_{k}}{N}\sum_{i=1}^{d}f\left(\frac{1}{N_{k}}(\bm{U}_{k}^{\top}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}\bm{U}_{k})_{ii}\right),</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the equivalence is in the sense that for an optimal choice of <math alttext="\{\bm{U}_{k}\in\mathsf{O}(d)\}_{k=1}^{K}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\bm{U}_{k}\in\mathsf{O}(d)\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ sansserif_O ( italic_d ) } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> matrices as described in <a class="ltx_ref" href="#Thmtheorem2" title="Theorem 4.2. ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> (i.e., orthogonal matrices which diagonalize each <math alttext="\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m2"><semantics><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>) we will achieve a tight bound with <math alttext="R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=R_{c,f}(\bm{Z},\bm{\Pi})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m3"><semantics><mrow><mrow><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><mi>𝚷</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mi>𝚷</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=R_{c,f}(\bm{Z},\bm{\Pi})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π )</annotation></semantics></math>. Note that in general, achieving this bound would require selecting, for each sampled instance of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, a new optimal set of <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m5"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> parameter matrices which diagonalize each <math alttext="\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m6"><semantics><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, which is clearly impractical for network architecture.
Instead, as an alternative viewpoint, rather than considering the data (<math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>) as fixed and trying to optimize the <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m8"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> parameters to achieve the tight variational bound, we can instead take the algorithmic unrolling design principle described above and design an operator to perturb <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m9"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> to incrementally minimize <math alttext="R_{c,f}^{\rm var}(\cdot\mid\bm{U}_{[K]})" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS0.Px1.p3.m10"><semantics><mrow><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{c,f}^{\rm var}(\cdot\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT ( ⋅ ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math>. To make this point explicit, each variational bound becomes tight when the eigenspaces of <math alttext="\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m11"><semantics><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> align with the columns of <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m12"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, so by rotating the appropriate columns of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m13"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> (namely, those which correspond to large entries in <math alttext="\bm{\pi}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m14"><semantics><msub><mi>𝝅</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{\pi}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) to align with <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m15"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> we can approach a tight variational bound. That is, instead of rotating <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m16"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> to align with the data for each instance of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m17"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, we can instead rotate the token features in each <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m18"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> to align with <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p3.m19"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p4">
<p class="ltx_p">Following this approach, we compute a gradient descent step on <math alttext="R_{c,f}^{\rm var}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m1"><semantics><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><annotation encoding="application/x-tex">R_{c,f}^{\rm var}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT</annotation></semantics></math> w.r.t. <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>.
To begin this computation, first let <math alttext="\bm{\pi}\in\mathbb{R}^{N}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m3"><semantics><mrow><mi>𝝅</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\pi}\in\mathbb{R}^{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_π ∈ blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> be any element-wise non-negative vector. Then we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{Z}}\ \frac{1}{2}\sum_{i=1}^{d}f((\bm{Z}\mathrm{Diag}(\bm{\pi})\bm{Z}^{\top})_{ii})=\;\mathrm{Diag}(\nabla f[\bm{Z}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}])\bm{Z}\mathrm{Diag}(\bm{\pi})," class="ltx_Math" display="block" id="S3.E9.m1"><semantics><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝅</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="0.558em">=</mo><mrow><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mi>𝒁</mi><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝝅</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝅</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{Z}}\ \frac{1}{2}\sum_{i=1}^{d}f((\bm{Z}\mathrm{Diag}(\bm{\pi})\bm{Z}^{\top})_{ii})=\;\mathrm{Diag}(\nabla f[\bm{Z}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}])\bm{Z}\mathrm{Diag}(\bm{\pi}),</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( ( bold_italic_Z roman_Diag ( bold_italic_π ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT ) = roman_Diag ( ∇ italic_f [ bold_italic_Z start_POSTSUPERSCRIPT ⊙ 2 end_POSTSUPERSCRIPT bold_italic_π ] ) bold_italic_Z roman_Diag ( bold_italic_π ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\nabla f" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m4"><semantics><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation><annotation encoding="application/x-llamapun">∇ italic_f</annotation></semantics></math> is the gradient of <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m5"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, and (recall) <math alttext="\nabla f[\cdot]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m6"><semantics><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f[\cdot]</annotation><annotation encoding="application/x-llamapun">∇ italic_f [ ⋅ ]</annotation></semantics></math> applies <math alttext="\nabla f" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m7"><semantics><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation><annotation encoding="application/x-llamapun">∇ italic_f</annotation></semantics></math> to each element of the vector in the bracket. In particular, for <math alttext="f(x)=\log(1+(d/\epsilon^{2})x)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m8"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>/</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f(x)=\log(1+(d/\epsilon^{2})x)</annotation><annotation encoding="application/x-llamapun">italic_f ( italic_x ) = roman_log ( 1 + ( italic_d / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) italic_x )</annotation></semantics></math>, <math alttext="\nabla f(x)=(d/\epsilon^{2})(1+(d/\epsilon^{2})x)^{-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m9"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>/</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>/</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\nabla f(x)=(d/\epsilon^{2})(1+(d/\epsilon^{2})x)^{-1}</annotation><annotation encoding="application/x-llamapun">∇ italic_f ( italic_x ) = ( italic_d / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ( 1 + ( italic_d / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) italic_x ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is simply a non-linear activation. Also, (recall) <math alttext="N_{k}=\langle\bm{\pi}_{k},\bm{1}\rangle" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m10"><semantics><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><annotation encoding="application/x-tex">N_{k}=\langle\bm{\pi}_{k},\bm{1}\rangle</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩</annotation></semantics></math>. Thus, the gradient of <math alttext="R^{\rm var}_{c,f}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m11"><semantics><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><annotation encoding="application/x-tex">R^{\rm var}_{c,f}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT</annotation></semantics></math> w.r.t. <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m12"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx57">
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\bm{Z}}R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=\frac{1}{n}\sum_{k=1}^{K}\bm{U}_{k}\underbrace{\mathrm{Diag}\left(\nabla f\left[(\bm{U}_{k}^{\top}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\frac{\bm{\pi}_{k}}{\langle\bm{\pi}_{k},\bm{1}\rangle}\right]\right)}_{\doteq\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})}\bm{U}_{k}^{\top}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})." class="ltx_Math" display="inline" id="S3.E10.m1"><semantics><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒁</mi></msub><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><mi>𝚷</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><msub><mi>𝝅</mi><mi>k</mi></msub><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>⏟</mo></munder><mrow><mi></mi><mo>≐</mo><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\bm{Z}}R^{\rm var}_{c,f}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=\frac{1}{n}\sum_{k=1}^{K}\bm{U}_{k}\underbrace{\mathrm{Diag}\left(\nabla f\left[(\bm{U}_{k}^{\top}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\frac{\bm{\pi}_{k}}{\langle\bm{\pi}_{k},\bm{1}\rangle}\right]\right)}_{\doteq\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})}\bm{U}_{k}^{\top}\bm{Z}\mathrm{Diag}(\bm{\pi}_{k}).</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT under⏟ start_ARG roman_Diag ( ∇ italic_f [ ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊙ 2 end_POSTSUPERSCRIPT divide start_ARG bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩ end_ARG ] ) end_ARG start_POSTSUBSCRIPT ≐ bold_italic_D ( bold_italic_Z , bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">(Note that the <math alttext="1/N" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m13"><semantics><mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">1/N</annotation><annotation encoding="application/x-llamapun">1 / italic_N</annotation></semantics></math> constant arises from a <math alttext="(N_{k}/N)\cdot(1/N_{k})=1/N" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m14"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>N</mi><mi>k</mi></msub><mo>/</mo><mi>N</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>N</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">(N_{k}/N)\cdot(1/N_{k})=1/N</annotation><annotation encoding="application/x-llamapun">( italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / italic_N ) ⋅ ( 1 / italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = 1 / italic_N</annotation></semantics></math> constant in each term of the sum.) If we now consider a gradient step w.r.t. the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m15"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math>-th token <math alttext="\bm{z}_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m16"><semantics><msub><mi>𝒛</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>
, we arrive at our proposed incremental compression operator, i.e., our surrogate for a <span class="ltx_text ltx_font_italic">self attention</span> + residual operator:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}_{j}^{+}=\bm{z}_{j}-\tau\nabla_{\bm{z}_{j}}R_{c,f}^{\rm var}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=\bm{z}_{j}-\frac{\tau}{N}\sum_{k=1}^{K}\Pi_{jk}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}\bm{z}_{j}" class="ltx_Math" display="block" id="S3.E11.m1"><semantics><mrow><msubsup><mi>𝒛</mi><mi>j</mi><mo>+</mo></msubsup><mo>=</mo><mrow><msub><mi>𝒛</mi><mi>j</mi></msub><mo>−</mo><mrow><mi>τ</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>𝒛</mi><mi>j</mi></msub></msub><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><mi>𝚷</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>𝒛</mi><mi>j</mi></msub><mo>−</mo><mrow><mfrac><mi>τ</mi><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi mathvariant="normal">Π</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{j}^{+}=\bm{z}_{j}-\tau\nabla_{\bm{z}_{j}}R_{c,f}^{\rm var}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})=\bm{z}_{j}-\frac{\tau}{N}\sum_{k=1}^{K}\Pi_{jk}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}\bm{z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_τ ∇ start_POSTSUBSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT ( bold_italic_Z , bold_Π ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) = bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - divide start_ARG italic_τ end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Π start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_D ( bold_italic_Z , bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for each <math alttext="j\in[n]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m17"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">j\in[n]</annotation><annotation encoding="application/x-llamapun">italic_j ∈ [ italic_n ]</annotation></semantics></math>, where <math alttext="\tau&gt;0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p4.m18"><semantics><mrow><mi>τ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\tau&gt;0</annotation><annotation encoding="application/x-llamapun">italic_τ &gt; 0</annotation></semantics></math> is a step size parameter for the incremental optimization. Then, we can construct a layer of TOST in <a class="ltx_ref" href="#F18" title="In A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4.18</span></a>.</p>
</div>
<figure class="ltx_figure" id="F18"><img alt="Figure 4.18 : One layer ℓ \ell roman_ℓ of the proposed Token Statistics Transformer (ToST). Notably, the self-attention of ToST transforms tokens 𝒁 ℓ \bm{Z}^{\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT efficiently to 𝒁 ℓ + 1 \bm{Z}^{\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT , via multiplying each row of the projected token by only a scalar . This leads to reduced complexity of the attention: it has O ​ ( p ) O(p) italic_O ( italic_p ) space and O ​ ( p ​ n ) O(pn) italic_O ( italic_p italic_n ) time complexity, where p p italic_p is the dimension of the projected tokens of each head, and n n italic_n is the number of tokens." class="ltx_graphics" id="F18.g1" src="chapters/chapter4/figs/V-CRATE.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4.18</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">One layer <math alttext="\ell" class="ltx_Math" display="inline" id="F18.m8"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation><annotation encoding="application/x-llamapun">roman_ℓ</annotation></semantics></math> of the proposed Token Statistics Transformer (ToST).<span class="ltx_text ltx_font_medium"> Notably, the self-attention of ToST transforms tokens <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="F18.m9"><semantics><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> efficiently to <math alttext="\bm{Z}^{\ell+1}" class="ltx_Math" display="inline" id="F18.m10"><semantics><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, via multiplying each row of the projected token by <span class="ltx_text ltx_font_italic">only a scalar</span>. This leads to reduced complexity of the attention: it has <math alttext="O(p)" class="ltx_Math" display="inline" id="F18.m11"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(p)</annotation><annotation encoding="application/x-llamapun">italic_O ( italic_p )</annotation></semantics></math> space and <math alttext="O(pn)" class="ltx_Math" display="inline" id="F18.m12"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(pn)</annotation><annotation encoding="application/x-llamapun">italic_O ( italic_p italic_n )</annotation></semantics></math> time complexity, where <math alttext="p" class="ltx_Math" display="inline" id="F18.m13"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> is the dimension of the projected tokens of each head, and <math alttext="n" class="ltx_Math" display="inline" id="F18.m14"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math> is the number of tokens.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model interpretation.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p">Given the proposed attention operator in (<a class="ltx_ref" href="#S3.E11" title="Equation 4.3.11 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.11</span></a>), first recall that the rows of <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m1"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> are non-negative and sum to 1
, so our operator takes a weighted average of <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m2"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> “attention head”-esque operators and then adds a residual connection. Using that <math alttext="\sum_{k=1}^{K}\Pi_{jk}=1" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m3"><semantics><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi mathvariant="normal">Π</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}\Pi_{jk}=1</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Π start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT = 1</annotation></semantics></math>, we can rewrite (<a class="ltx_ref" href="#S3.E11" title="Equation 4.3.11 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.11</span></a>) as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}_{j}^{+}=\sum_{k=1}^{K}\Pi_{jk}\Big{[}\bm{z}_{j}\underbrace{-\frac{\tau}{n}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}}_{\text{action of one attention head}}\bm{z}_{j}\Big{]}." class="ltx_Math" display="block" id="S3.E12.m1"><semantics><mrow><mrow><msubsup><mi>𝒛</mi><mi>j</mi><mo>+</mo></msubsup><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi mathvariant="normal">Π</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">[</mo><mrow><msub><mi>𝒛</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><mo>−</mo><mrow><mfrac><mi>τ</mi><mi>n</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>⏟</mo></munder><mtext>action of one attention head</mtext></munder><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>j</mi></msub></mrow><mo maxsize="160%" minsize="160%">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}_{j}^{+}=\sum_{k=1}^{K}\Pi_{jk}\Big{[}\bm{z}_{j}\underbrace{-\frac{\tau}{n}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}}_{\text{action of one attention head}}\bm{z}_{j}\Big{]}.</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Π start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT [ bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT under⏟ start_ARG - divide start_ARG italic_τ end_ARG start_ARG italic_n end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_D ( bold_italic_Z , bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT action of one attention head end_POSTSUBSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, we can view each attention head as first projecting the token features
onto the basis <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m4"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> via multiplying by <math alttext="\bm{U}_{k}^{\top}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m5"><semantics><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><annotation encoding="application/x-tex">\bm{U}_{k}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, multiplying by the
diagonal matrix <math alttext="\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m6"><semantics><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})</annotation><annotation encoding="application/x-llamapun">bold_italic_D ( bold_italic_Z , bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> (abbreviated as <math alttext="\bm{D}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m7"><semantics><msub><mi>𝑫</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{D}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>), projecting back into the standard basis via multiplying by <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m8"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, and subtracting this from the original token features via the residual
connection. The core aspect of our attention layer is the computation of <math alttext="\bm{D}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m9"><semantics><msub><mi>𝑫</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{D}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. Namely, <math alttext="\Pi_{jk}\geq 0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m10"><semantics><mrow><msub><mi mathvariant="normal">Π</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\Pi_{jk}\geq 0</annotation><annotation encoding="application/x-llamapun">roman_Π start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT ≥ 0</annotation></semantics></math>, so <math alttext="\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle\in\mathbb{R}^{N}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m11"><semantics><mrow><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>/</mo><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle\in\mathbb{R}^{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩ ∈ blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> forms a probability distribution over which tokens belong to
the <math alttext="k^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m12"><semantics><msup><mi>k</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">k^{\text{th}}</annotation><annotation encoding="application/x-llamapun">italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> group. As a result, <math alttext="(\bm{U}^{\top}_{k}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m13"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝅</mi><mi>k</mi></msub></mrow><mo>/</mo><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><annotation encoding="application/x-tex">(\bm{U}^{\top}_{k}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊙ 2 end_POSTSUPERSCRIPT bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩</annotation></semantics></math> estimates the second moment of <math alttext="\bm{U}_{k}^{\top}\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m14"><semantics><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{\top}\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z</annotation></semantics></math> under the distribution given by <math alttext="\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m15"><semantics><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>/</mo><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle</annotation><annotation encoding="application/x-llamapun">bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩</annotation></semantics></math>. Further, since <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m16"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is a concave non-decreasing function, <math alttext="\nabla f(x)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m17"><semantics><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f(x)</annotation><annotation encoding="application/x-llamapun">∇ italic_f ( italic_x )</annotation></semantics></math> monotonically decreases towards <math alttext="0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m18"><mn>0</mn></math> as <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m19"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math> increases, so the entries of <math alttext="\bm{D}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m20"><semantics><msub><mi>𝑫</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{D}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> (which have form <math alttext="\nabla f(x)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m21"><semantics><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f(x)</annotation><annotation encoding="application/x-llamapun">∇ italic_f ( italic_x )</annotation></semantics></math>) achieve their maximum at <math alttext="x=0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m22"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation><annotation encoding="application/x-llamapun">italic_x = 0</annotation></semantics></math> and decay monotonically to <math alttext="0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m23"><mn>0</mn></math> as <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m24"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math> increases.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p">From this, we arrive at the core interpretation of our attention head + residual
operators <math alttext="[\bm{I}-(\tau/n)\bm{U}_{k}\bm{D}_{k}\bm{U}_{k}^{\top}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m1"><semantics><mrow><mo stretchy="false">[</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>τ</mi><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑫</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\bm{I}-(\tau/n)\bm{U}_{k}\bm{D}_{k}\bm{U}_{k}^{\top}]</annotation><annotation encoding="application/x-llamapun">[ bold_italic_I - ( italic_τ / italic_n ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math>. Namely, this
operator does an approximate low-rank data-dependent projection, where
directions which have a large amount of “power” after the projection <math alttext="\bm{U}_{k}^{\top}\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m2"><semantics><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><annotation encoding="application/x-tex">\bm{U}_{k}^{\top}\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z</annotation></semantics></math> (i.e., directions which have a large second moment <math alttext="(\bm{U}_{k}^{\top}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m3"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝅</mi><mi>k</mi></msub></mrow><mo>/</mo><mrow><mo stretchy="false">⟨</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><annotation encoding="application/x-tex">(\bm{U}_{k}^{\top}\bm{Z})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}2}\bm{\pi}_{k}/\langle\bm{\pi}_{k},\bm{1}\rangle</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z ) start_POSTSUPERSCRIPT ⊙ 2 end_POSTSUPERSCRIPT bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / ⟨ bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_1 ⟩</annotation></semantics></math>) are preserved, while directions which do not are suppressed. To see this, recall that the entries of <math alttext="\bm{D}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m4"><semantics><msub><mi>𝑫</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{D}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> decrease monotonically to 0 as the second moment increases, so for directions with large second moments the attention + residual operator acts largely as the identity operator. Conversely, for directions with a small second moment, our operator subtracts a projection of the tokens along those directions, resulting in those directions being suppressed. Compared to the standard self-attention operator, our method clearly does not compute any pairwise similarities between tokens. Rather, the interactions between the tokens in <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> impact the operator solely through their contribution to the second moment statistic used to construct the <math alttext="\bm{D}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m6"><semantics><msub><mi>𝑫</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{D}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>’s. Nevertheless, similar to the standard self-attention operator, our method still has a clear interpretation as performing a form of compression towards a data-dependent low-rank structure, in the sense that it performs an approximate low-rank projection, where the specific directions that are suppressed are those which are not strongly aligned with other tokens in the group.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Practical Implementation Details.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p">Having introduced our proposed attention operator, we now discuss further practical considerations. First, until this point in the presentation, we have avoided discussion of how tokens are “grouped” into various attention heads via the <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m1"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> matrix, but clearly a means of constructing <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m2"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> is needed to implement our method. Additionally, our variational form in <a class="ltx_ref" href="#Thmtheorem2" title="Theorem 4.2. ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> requires the <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> matrices to be square and orthogonal, but one would ideally like to use smaller matrices (i.e., reduce the number of columns in <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m4"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>) for efficiency as well as drop the orthogonality constraints.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p2">
<p class="ltx_p">In practice, we do not enforce the orthogonality constraints. To reduce the number of columns in the <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> matrices, we note that similar to CRATE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx312" title="">YBP+23</a>]</cite>, if we assume the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> within group <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> are (approximately) clustered around a low-dimensional subspace — say of dimension <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m4"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> — then the within-group-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m5"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> covariance <math alttext="\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m6"><semantics><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mi>Diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\mathrm{Diag}(\bm{\pi}_{k})\bm{Z}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z roman_Diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> is low-rank, where recall that <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx313" title="">YCY+20</a>]</cite> shows that the optimal geometry of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> will be for each group to be a low-rank subspace, orthogonal to the other groups. We can thus explicitly find a low-dimensional orthonormal basis for the image of this covariance, i.e., the linear span of the data in group <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m8"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>. If the dimension is <math alttext="p\leq d" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m9"><semantics><mrow><mi>p</mi><mo>≤</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">p\leq d</annotation><annotation encoding="application/x-llamapun">italic_p ≤ italic_d</annotation></semantics></math>, the basis can be represented by a <math alttext="d\times p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m10"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">d\times p</annotation><annotation encoding="application/x-llamapun">italic_d × italic_p</annotation></semantics></math> orthogonal matrix <math alttext="\bm{U}_{k}\in\mathsf{O}(d,p)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m11"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathsf{O}(d,p)</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ sansserif_O ( italic_d , italic_p )</annotation></semantics></math>. In this case, we can more efficiently upper-bound <math alttext="R_{c,f}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.m12"><semantics><msub><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">R_{c,f}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT</annotation></semantics></math> using these low-rank orthogonal basis matrices. To show this, we use a more general version of <a class="ltx_ref" href="#Thmtheorem2" title="Theorem 4.2. ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> to yield the following corollary.</p>
</div>
<div class="ltx_theorem ltx_theorem_corollary" id="Thmcorollary1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 4.1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmcorollary1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="f\colon[0,\infty)\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f\colon[0,\infty)\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_f : [ 0 , ∞ ) → blackboard_R</annotation></semantics></math> be non-decreasing, concave, and obey <math alttext="f(0)=0" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m2"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0)=0</annotation><annotation encoding="application/x-llamapun">italic_f ( 0 ) = 0</annotation></semantics></math>, and let <math alttext="F\colon\mathsf{PSD}(p)\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m3"><semantics><mrow><mi>F</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi>𝖯𝖲𝖣</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">F\colon\mathsf{PSD}(p)\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_F : sansserif_PSD ( italic_p ) → blackboard_R</annotation></semantics></math> have the form <math alttext="F(\bm{M})=\sum_{i=1}^{p}f(\lambda_{i}(\bm{M}))" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m4"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐌</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐌</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">F(\bm{M})=\sum_{i=1}^{p}f(\lambda_{i}(\bm{M}))</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_M ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT italic_f ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M ) )</annotation></semantics></math>. Let <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m5"><semantics><mi>𝐙</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m6"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> be fixed. Then, for all <math alttext="\bm{U}_{1},\dots,\bm{U}_{K}\in\mathsf{O}(d,p)" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m7"><semantics><mrow><mrow><msub><mi>𝐔</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝐔</mi><mi>K</mi></msub></mrow><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}_{1},\dots,\bm{U}_{K}\in\mathsf{O}(d,p)</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ∈ sansserif_O ( italic_d , italic_p )</annotation></semantics></math> such that <math alttext="\mathrm{image}(\bm{Z}\operatorname{diag}(\bm{\pi}_{k})\bm{Z}^{\top})\subset\mathrm{image}(\bm{U}_{k})" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m8"><semantics><mrow><mrow><mi>image</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝐙</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝛑</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>⊂</mo><mrow><mi>image</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐔</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{image}(\bm{Z}\operatorname{diag}(\bm{\pi}_{k})\bm{Z}^{\top})\subset\mathrm{image}(\bm{U}_{k})</annotation><annotation encoding="application/x-llamapun">roman_image ( bold_italic_Z roman_diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ⊂ roman_image ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> for all <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m9"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>, we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{c,f}(\bm{Z},\bm{\Pi})\leq R_{c,f}^{\rm var}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]})," class="ltx_Math" display="block" id="S3.E13.m1"><semantics><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mi>𝚷</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><mi>𝚷</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R_{c,f}(\bm{Z},\bm{\Pi})\leq R_{c,f}^{\rm var}(\bm{Z},\bm{\Pi}\mid\bm{U}_{[K]}),</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Π ) ≤ italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT ( bold_italic_Z , bold_Π ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="R_{c,f}^{\rm var}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m10"><semantics><msubsup><mi>R</mi><mrow><mi>c</mi><mo>,</mo><mi>f</mi></mrow><mi>var</mi></msubsup><annotation encoding="application/x-tex">R_{c,f}^{\rm var}</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT</annotation></semantics></math> is formally defined in (<a class="ltx_ref" href="#S3.E8" title="Equation 4.3.8 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.8</span></a>). Equality holds if <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m11"><semantics><msub><mi>𝐔</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> diagonalizes <math alttext="\bm{Z}\operatorname{diag}(\bm{\pi}_{k})\bm{Z}^{\top}" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m12"><semantics><mrow><mi>𝐙</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝛑</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐙</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{Z}\operatorname{diag}(\bm{\pi}_{k})\bm{Z}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z roman_diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m13"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>, and if <math alttext="f" class="ltx_Math" display="inline" id="Thmcorollary1.p1.m14"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is strongly concave then this equality condition becomes an “if and only if.”</span></p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p3">
<p class="ltx_p">The final step to define our attention operator is to estimate the group membership <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m1"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math>. For this we posit a simple model of how each feature <math alttext="\bm{z}_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m2"><semantics><msub><mi>𝒛</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> deviates from its supporting subspace and then find the optimal subspace assignment. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx312" title="">YBP+23</a>]</cite> show that if we independently model each <math alttext="\bm{z}_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m3"><semantics><msub><mi>𝒛</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> as belonging to a low-dimensional Gaussian mixture model, where each Gaussian has a covariance matrix with identical spectrum and is supported on a subspace with orthonormal basis <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m4"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, plus independent Gaussian noise with covariance <math alttext="\eta\bm{I}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m5"><semantics><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">\eta\bm{I}</annotation><annotation encoding="application/x-llamapun">italic_η bold_italic_I</annotation></semantics></math>, then the posterior probability that each token <math alttext="\bm{z}_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m6"><semantics><msub><mi>𝒛</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> belongs to each subspace is given by the assignment matrix <math alttext="\bm{\Pi}=\bm{\Pi}(\bm{Z}\mid\bm{U}_{[K]})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m7"><semantics><mrow><mi>𝚷</mi><mo>=</mo><mrow><mi>𝚷</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\Pi}=\bm{\Pi}(\bm{Z}\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">bold_Π = bold_Π ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math> as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx58">
<tbody id="S3.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{\Pi}=\begin{bmatrix}\bm{\nu}(\bm{z}_{1}\mid\bm{U}_{[K]})^{\top}\\
\vdots\\
\bm{\nu}(\bm{z}_{n}\mid\bm{U}_{[K]})^{\top}\end{bmatrix},\quad\text{where}\quad\bm{\nu}(\bm{z}_{j}\mid\bm{U}_{[K]})\doteq\operatorname{softmax}\left(\frac{1}{2\eta}\begin{bmatrix}\|\bm{U}_{1}^{\top}\bm{z}_{j}\|_{2}^{2}\\
\vdots\\
\|\bm{U}_{K}^{\top}\bm{z}_{j}\|_{2}^{2}\end{bmatrix}\right),\quad\forall j\in[n]," class="ltx_Math" display="inline" id="S3.E14.m1"><semantics><mrow><mrow><mrow><mi>𝚷</mi><mo>=</mo><mrow><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><mrow><mi>𝝂</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mn>1</mn></msub><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>𝝂</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>n</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo rspace="1.167em">,</mo><mtext>where</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><mrow><mi>𝝂</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒛</mi><mi>j</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>j</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>K</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>j</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{\Pi}=\begin{bmatrix}\bm{\nu}(\bm{z}_{1}\mid\bm{U}_{[K]})^{\top}\\
\vdots\\
\bm{\nu}(\bm{z}_{n}\mid\bm{U}_{[K]})^{\top}\end{bmatrix},\quad\text{where}\quad\bm{\nu}(\bm{z}_{j}\mid\bm{U}_{[K]})\doteq\operatorname{softmax}\left(\frac{1}{2\eta}\begin{bmatrix}\|\bm{U}_{1}^{\top}\bm{z}_{j}\|_{2}^{2}\\
\vdots\\
\|\bm{U}_{K}^{\top}\bm{z}_{j}\|_{2}^{2}\end{bmatrix}\right),\quad\forall j\in[n],</annotation><annotation encoding="application/x-llamapun">bold_Π = [ start_ARG start_ROW start_CELL bold_italic_ν ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL bold_italic_ν ( bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] , where bold_italic_ν ( bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ roman_softmax ( divide start_ARG 1 end_ARG start_ARG 2 italic_η end_ARG [ start_ARG start_ROW start_CELL ∥ bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL ∥ bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] ) , ∀ italic_j ∈ [ italic_n ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\eta" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m8"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation><annotation encoding="application/x-llamapun">italic_η</annotation></semantics></math> becomes a learnable temperature parameter. Thus, given an input feature <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m9"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, we estimate <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m10"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> using (<a class="ltx_ref" href="#S3.E14" title="Equation 4.3.14 ‣ Practical Implementation Details. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.14</span></a>) and then compute the attention operator. Combining the construction of <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m11"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E14" title="Equation 4.3.14 ‣ Practical Implementation Details. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.14</span></a>) with
(<a class="ltx_ref" href="#S3.E11" title="Equation 4.3.11 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.11</span></a>), we obtain the <span class="ltx_text ltx_font_italic">Token Statistics Self-Attention</span> operator:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\texttt{TSSA}(\bm{Z}\mid\bm{U}_{[K]})\doteq-\frac{\tau}{n}\sum_{k=1}^{K}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}\bm{Z}\operatorname{diag}(\bm{\pi}_{k})," class="ltx_Math" display="block" id="S3.E15.m1"><semantics><mrow><mrow><mrow><mtext class="ltx_mathvariant_monospace">TSSA</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>−</mo><mrow><mfrac><mi>τ</mi><mi>n</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mrow><msub><mi>𝝅</mi><mi>k</mi></msub><mo>∣</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝅</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\texttt{TSSA}(\bm{Z}\mid\bm{U}_{[K]})\doteq-\frac{\tau}{n}\sum_{k=1}^{K}\bm{U}_{k}\bm{D}(\bm{Z},\bm{\pi}_{k}\mid\bm{U}_{k})\bm{U}_{k}^{\top}\bm{Z}\operatorname{diag}(\bm{\pi}_{k}),</annotation><annotation encoding="application/x-llamapun">TSSA ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ≐ - divide start_ARG italic_τ end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_D ( bold_italic_Z , bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∣ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z roman_diag ( bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.3.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{\pi}_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m12"><semantics><msub><mi>𝝅</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{\pi}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are the columns of <math alttext="\bm{\Pi}=\bm{\Pi}(\bm{Z}\mid\bm{U}_{[K]})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m13"><semantics><mrow><mi>𝚷</mi><mo>=</mo><mrow><mi>𝚷</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\Pi}=\bm{\Pi}(\bm{Z}\mid\bm{U}_{[K]})</annotation><annotation encoding="application/x-llamapun">bold_Π = bold_Π ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT )</annotation></semantics></math> defined in (<a class="ltx_ref" href="#S3.E14" title="Equation 4.3.14 ‣ Practical Implementation Details. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.14</span></a>) and <math alttext="\bm{D}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p3.m14"><semantics><mi>𝑫</mi><annotation encoding="application/x-tex">\bm{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_D</annotation></semantics></math> is defined in (<a class="ltx_ref" href="#S3.E10" title="Equation 4.3.10 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.10</span></a>).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.4 </span>Summary and Notes</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">The materials presented in this chapter are based on a series of recent works on this topic, including <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx41" title="">CYY+22</a>, <a class="ltx_ref" href="bib.html#bibx283" title="">WLP+24</a>, <a class="ltx_ref" href="bib.html#bibx284" title="">WLY+25</a>, <a class="ltx_ref" href="bib.html#bibx302" title="">WDL+25</a>, <a class="ltx_ref" href="bib.html#bibx312" title="">YBP+23</a>]</cite>. These contributions encompass both theoretical advances and practical methodologies for constructing interpretable deep networks through unrolled optimization. Many of the key results and proofs discussed in this chapter are derived directly from, or inspired by, these foundational works.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">The idea of unrolling an optimization algorithm to construct a neural network traces back to the seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx95" title="">GL10</a>]</cite>. In this work, the authors demonstrated that sparse coding algorithms—such as the Iterative Shrinkage-Thresholding Algorithm (ISTA)—can be unrolled to form multilayer perceptrons (MLPs), effectively bridging iterative optimization and neural network design. Notably, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx191" title="">MLE19</a>]</cite> demonstrated that such unrolled networks are more interpretable, parameter-efficient, and effective compared to generic networks. In this chapter, we build on this perspective to develop principled, white-box deep network architectures by unrolling optimization algorithms that are designed to minimize well-motivated objectives—such as the (sparse) rate reduction objective introduced earlier. This approach not only clarifies the role of each layer in the network but also offers theoretical grounding for architectural choices, moving beyond empirical trial-and-error toward interpretable and goal-driven design. In the following, we compare conventional DNNs, which are typically constructed through empirical design and heuristic tuning, with our mathematically grounded ReduNet architectures:</p>
</div>
<div class="ltx_logical-block">
<div class="ltx_para" id="S4.p3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Conventional DNNs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">ReduNets</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" style="padding-bottom:2.15277pt;">Objectives</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:2.15277pt;">input/output fitting</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:2.15277pt;">information gain</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Deep architectures</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">trial &amp; error</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">iterative optimization</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Layer operators</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">empirical</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">projected gradient</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Shift invariance</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">CNNs+augmentation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">invariant ReduNets</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Initializations</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">random/pre-design</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">forward unrolled</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Training/fine-tuning</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">back prop</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">forward/back prop</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Interpretability</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">black box</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">white box</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Representations</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">hidden/latent</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">incoherent subspaces</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.5 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 4.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<p class="ltx_p">Let <math alttext="\bm{Z}=[\bm{Z}_{1},\dots,\bm{Z}_{K}]\in\mathbb{R}^{d\times m}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m1"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒁</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒁</mi><mi>K</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{Z}_{1},\dots,\bm{Z}_{K}]\in\mathbb{R}^{d\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="\bm{Z}_{k}\in\mathbb{R}^{d\times m_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m2"><semantics><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>m</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}_{k}\in\mathbb{R}^{d\times m_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmexercise1.p1.m3"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>. For some <math alttext="\alpha&gt;0" class="ltx_Math" display="inline" id="Thmexercise1.p1.m4"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;0</annotation><annotation encoding="application/x-llamapun">italic_α &gt; 0</annotation></semantics></math>, let</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx59">
<tbody id="S5.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R(\bm{Z})=\log\det\left(\bm{I}+\alpha\bm{Z}\bm{Z}^{T}\right)." class="ltx_Math" display="inline" id="S5.Ex1.m1"><semantics><mrow><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi>T</mi></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle R(\bm{Z})=\log\det\left(\bm{I}+\alpha\bm{Z}\bm{Z}^{T}\right).</annotation><annotation encoding="application/x-llamapun">italic_R ( bold_italic_Z ) = roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">1. Given any direction <math alttext="\bm{D}\in\mathbb{R}^{d\times m}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m5"><semantics><mrow><mi>𝑫</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{D}\in\mathbb{R}^{d\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_D ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>, please show that <math alttext="\nabla R(\bm{Z})=\alpha\bm{X}^{-1}\bm{Z}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m6"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><annotation encoding="application/x-tex">\nabla R(\bm{Z})=\alpha\bm{X}^{-1}\bm{Z}</annotation><annotation encoding="application/x-llamapun">∇ italic_R ( bold_italic_Z ) = italic_α bold_italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Z</annotation></semantics></math> and</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx60">
<tbody id="S5.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla^{2}R(\bm{Z})[\bm{D},\bm{D}]=\alpha\mathrm{Tr}\left(\bm{X}^{-1}\bm{D}\bm{D}^{T}\right)-\frac{\alpha^{2}}{2}\mathrm{Tr}\left(\bm{X}^{-1}\left(\bm{Z}\bm{D}^{T}+\bm{D}\bm{Z}^{T}\right)\bm{X}^{-1}\left(\bm{Z}\bm{D}^{T}+\bm{D}\bm{Z}^{T}\right)\right)," class="ltx_Math" display="inline" id="S5.Ex2.m1"><semantics><mrow><mrow><mrow><mrow><msup><mo>∇</mo><mn>2</mn></msup><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝑫</mi><mo>,</mo><mi>𝑫</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>Tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝑿</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑫</mi><mi>T</mi></msup></mrow><mo>)</mo></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><msup><mi>α</mi><mn>2</mn></msup><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>Tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝑿</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑫</mi><mi>T</mi></msup></mrow><mo>+</mo><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi>T</mi></msup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑫</mi><mi>T</mi></msup></mrow><mo>+</mo><mrow><mi>𝑫</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi>T</mi></msup></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\nabla^{2}R(\bm{Z})[\bm{D},\bm{D}]=\alpha\mathrm{Tr}\left(\bm{X}^{-1}\bm{D}\bm{D}^{T}\right)-\frac{\alpha^{2}}{2}\mathrm{Tr}\left(\bm{X}^{-1}\left(\bm{Z}\bm{D}^{T}+\bm{D}\bm{Z}^{T}\right)\bm{X}^{-1}\left(\bm{Z}\bm{D}^{T}+\bm{D}\bm{Z}^{T}\right)\right),</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_R ( bold_italic_Z ) [ bold_italic_D , bold_italic_D ] = italic_α roman_Tr ( bold_italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_D bold_italic_D start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) - divide start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG roman_Tr ( bold_italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Z bold_italic_D start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT + bold_italic_D bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) bold_italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Z bold_italic_D start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT + bold_italic_D bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{X}\doteq\bm{I}+\alpha\bm{Z}\bm{Z}^{T}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m7"><semantics><mrow><mi>𝑿</mi><mo>≐</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi>T</mi></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\doteq\bm{I}+\alpha\bm{Z}\bm{Z}^{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ≐ bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. <span class="ltx_text ltx_font_italic">Hint:</span> Note that</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx61">
<tbody id="S5.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla^{2}R(\bm{Z})[\bm{D},\bm{D}]\doteq\left\langle\lim_{t\to 0}\frac{\nabla R(\bm{Z}+t\bm{D})-\nabla R(\bm{Z})}{t},\bm{D}\right\rangle." class="ltx_Math" display="inline" id="S5.Ex3.m1"><semantics><mrow><mrow><mrow><mrow><msup><mo>∇</mo><mn>2</mn></msup><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝑫</mi><mo>,</mo><mi>𝑫</mi><mo stretchy="false">]</mo></mrow></mrow><mo>≐</mo><mrow><mo>⟨</mo><mrow><munder><mo lspace="0em" movablelimits="false" rspace="0.167em">lim</mo><mrow><mi>t</mi><mo stretchy="false">→</mo><mn>0</mn></mrow></munder><mstyle displaystyle="true"><mfrac><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑫</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mi>t</mi></mfrac></mstyle></mrow><mo>,</mo><mi>𝑫</mi><mo>⟩</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\nabla^{2}R(\bm{Z})[\bm{D},\bm{D}]\doteq\left\langle\lim_{t\to 0}\frac{\nabla R(\bm{Z}+t\bm{D})-\nabla R(\bm{Z})}{t},\bm{D}\right\rangle.</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_R ( bold_italic_Z ) [ bold_italic_D , bold_italic_D ] ≐ ⟨ roman_lim start_POSTSUBSCRIPT italic_t → 0 end_POSTSUBSCRIPT divide start_ARG ∇ italic_R ( bold_italic_Z + italic_t bold_italic_D ) - ∇ italic_R ( bold_italic_Z ) end_ARG start_ARG italic_t end_ARG , bold_italic_D ⟩ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="Thmexercise1.p2">
<p class="ltx_p">2. Please show that</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx62">
<tbody id="S5.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R(\bm{Z})\leq\sum_{k=1}^{K}\log\det\left(\bm{I}+\alpha\bm{Z}_{k}\bm{Z}_{k}^{T}\right)," class="ltx_Math" display="inline" id="S5.Ex4.m1"><semantics><mrow><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mi>T</mi></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle R(\bm{Z})\leq\sum_{k=1}^{K}\log\det\left(\bm{I}+\alpha\bm{Z}_{k}\bm{Z}_{k}^{T}\right),</annotation><annotation encoding="application/x-llamapun">italic_R ( bold_italic_Z ) ≤ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the equality holds if and only if <math alttext="\bm{Z}_{k}^{T}\bm{Z}_{l}=\bm{0}" class="ltx_Math" display="inline" id="Thmexercise1.p2.m1"><semantics><mrow><mrow><msubsup><mi>𝒁</mi><mi>k</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>l</mi></msub></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{Z}_{k}^{T}\bm{Z}_{l}=\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = bold_0</annotation></semantics></math> for all <math alttext="k\neq l\in[K]" class="ltx_Math" display="inline" id="Thmexercise1.p2.m2"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>l</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\neq l\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_l ∈ [ italic_K ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="Thmexercise1.p3">
<p class="ltx_p">3. Given some <math alttext="\alpha&gt;0" class="ltx_Math" display="inline" id="Thmexercise1.p3.m1"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;0</annotation><annotation encoding="application/x-llamapun">italic_α &gt; 0</annotation></semantics></math>, let <math alttext="\alpha_{k}=m\alpha/m_{k}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m2"><semantics><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>=</mo><mrow><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>α</mi></mrow><mo>/</mo><msub><mi>m</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha_{k}=m\alpha/m_{k}</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_m italic_α / italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="Thmexercise1.p3.m3"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>. Please derive the closed-form for the first-order critical point of the following function:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx63">
<tbody id="S5.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f(\bm{Z}_{k})=\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{Z}_{k}\bm{Z}_{k}^{T}\right)-\frac{m_{k}}{2m}\log\det\left(\bm{I}+\alpha_{k}\bm{Z}_{k}\bm{Z}_{k}^{T}\right)-\frac{\lambda}{2}\|\bm{Z}_{k}\|_{F}^{2}." class="ltx_Math" display="inline" id="S5.Ex5.m1"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mi>T</mi></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><msub><mi>m</mi><mi>k</mi></msub><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>k</mi><mi>T</mi></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mi>λ</mi><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle f(\bm{Z}_{k})=\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{Z}_{k}\bm{Z}_{k}^{T}\right)-\frac{m_{k}}{2m}\log\det\left(\bm{I}+\alpha_{k}\bm{Z}_{k}\bm{Z}_{k}^{T}\right)-\frac{\lambda}{2}\|\bm{Z}_{k}\|_{F}^{2}.</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) - divide start_ARG italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG 2 italic_m end_ARG roman_log roman_det ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) - divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ∥ bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Hint:</span> Let <math alttext="r_{k}=\mathrm{rank}(\bm{Z}_{k})" class="ltx_Math" display="inline" id="Thmexercise1.p3.m4"><semantics><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>rank</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{k}=\mathrm{rank}(\bm{Z}_{k})</annotation><annotation encoding="application/x-llamapun">italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_rank ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math>. Consider the following singular value decomposition of <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m5"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx64">
<tbody id="S5.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{k}=\bm{P}_{k}\bm{\Sigma}_{k}\bm{Q}_{k}^{T}=\begin{bmatrix}\bm{P}_{k,1}&amp;\bm{P}_{k,2}\end{bmatrix}\begin{bmatrix}\tilde{\bm{\Sigma}}_{k}&amp;\bm{0}\\
\bm{0}&amp;\bm{0}\end{bmatrix}\begin{bmatrix}\bm{Q}_{k,1}^{T}\\
\bm{Q}_{k,2}^{T}\end{bmatrix}," class="ltx_Math" display="inline" id="S5.Ex6.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>=</mo><mrow><msub><mi>𝑷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝚺</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑸</mi><mi>k</mi><mi>T</mi></msubsup></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable columnspacing="5pt"><mtr><mtd><msub><mi>𝑷</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><msub><mi>𝑷</mi><mrow><mi>k</mi><mo>,</mo><mn>2</mn></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd><msub><mover accent="true"><mi>𝚺</mi><mo>~</mo></mover><mi>k</mi></msub></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mn>𝟎</mn></mtd><mtd><mn>𝟎</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable rowspacing="0pt"><mtr><mtd><msubsup><mi>𝑸</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow><mi>T</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝑸</mi><mrow><mi>k</mi><mo>,</mo><mn>2</mn></mrow><mi>T</mi></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{k}=\bm{P}_{k}\bm{\Sigma}_{k}\bm{Q}_{k}^{T}=\begin{bmatrix}\bm{P}_{k,1}&amp;\bm{P}_{k,2}\end{bmatrix}\begin{bmatrix}\tilde{\bm{\Sigma}}_{k}&amp;\bm{0}\\
\bm{0}&amp;\bm{0}\end{bmatrix}\begin{bmatrix}\bm{Q}_{k,1}^{T}\\
\bm{Q}_{k,2}^{T}\end{bmatrix},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_italic_P start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT end_CELL start_CELL bold_italic_P start_POSTSUBSCRIPT italic_k , 2 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL over~ start_ARG bold_Σ end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_0 end_CELL start_CELL bold_0 end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_Q start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_Q start_POSTSUBSCRIPT italic_k , 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{P}_{k}\in\mathcal{O}^{d}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m6"><semantics><mrow><msub><mi>𝑷</mi><mi>k</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{P}_{k}\in\mathcal{O}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_O start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="\bm{P}_{k,1}\in\mathbb{R}^{d\times r_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m7"><semantics><mrow><msub><mi>𝑷</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>r</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{P}_{k,1}\in\mathbb{R}^{d\times r_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_P start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{P}_{k,2}\in\mathbb{R}^{d\times(d-r_{k})}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m8"><semantics><mrow><msub><mi>𝑷</mi><mrow><mi>k</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>−</mo><msub><mi>r</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{P}_{k,2}\in\mathbb{R}^{d\times(d-r_{k})}</annotation><annotation encoding="application/x-llamapun">bold_italic_P start_POSTSUBSCRIPT italic_k , 2 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × ( italic_d - italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{\Sigma}_{k}\in\mathbb{R}^{d\times m_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m9"><semantics><mrow><msub><mi>𝚺</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>m</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Sigma}_{k}\in\mathbb{R}^{d\times m_{k}}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="\tilde{\bm{\Sigma}}_{k}\in\mathbb{R}^{r_{k}\times r_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m10"><semantics><mrow><msub><mover accent="true"><mi>𝚺</mi><mo>~</mo></mover><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>r</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>r</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\tilde{\bm{\Sigma}}_{k}\in\mathbb{R}^{r_{k}\times r_{k}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_Σ end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> being a diagonal matrix, and <math alttext="\bm{Q}_{k}\in\mathcal{O}^{m_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m11"><semantics><mrow><msub><mi>𝑸</mi><mi>k</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒪</mi><msub><mi>m</mi><mi>k</mi></msub></msup></mrow><annotation encoding="application/x-tex">\bm{Q}_{k}\in\mathcal{O}^{m_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_O start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="\bm{Q}_{k,1}\in\mathbb{R}^{m_{k}\times r_{k}}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m12"><semantics><mrow><msub><mi>𝑸</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>r</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Q}_{k,1}\in\mathbb{R}^{m_{k}\times r_{k}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{P}_{k,2}\in\mathbb{R}^{m_{k}\times(m_{k}-r_{k})}" class="ltx_Math" display="inline" id="Thmexercise1.p3.m13"><semantics><mrow><msub><mi>𝑷</mi><mrow><mi>k</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>m</mi><mi>k</mi></msub><mo>−</mo><msub><mi>r</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{P}_{k,2}\in\mathbb{R}^{m_{k}\times(m_{k}-r_{k})}</annotation><annotation encoding="application/x-llamapun">bold_italic_P start_POSTSUBSCRIPT italic_k , 2 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × ( italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 4.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Neumann series for matrix inverse)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<p class="ltx_p">Let <math alttext="\bm{A}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m1"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>. If <math alttext="\|\bm{A}\|&lt;1" class="ltx_Math" display="inline" id="Thmexercise2.p1.m2"><semantics><mrow><mrow><mo stretchy="false">‖</mo><mi>𝑨</mi><mo stretchy="false">‖</mo></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\bm{A}\|&lt;1</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_A ∥ &lt; 1</annotation></semantics></math>, please show</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx65">
<tbody id="S5.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\left(\bm{I}-\bm{A}\right)^{-1}=\sum_{k=1}^{\infty}\bm{A}^{k}." class="ltx_Math" display="inline" id="S5.E1.m1"><semantics><mrow><mrow><msup><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mi>𝑨</mi></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></munderover></mstyle><msup><mi>𝑨</mi><mi>k</mi></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\left(\bm{I}-\bm{A}\right)^{-1}=\sum_{k=1}^{\infty}\bm{A}^{k}.</annotation><annotation encoding="application/x-llamapun">( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Hint:</span> The proof consists of two steps. 
<br class="ltx_break"/>(i) <span class="ltx_text ltx_font_bold">Step 1</span>: Please show that the infite series <math alttext="\sum_{k=1}^{\infty}\bm{A}^{k}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m3"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>𝑨</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{\infty}\bm{A}^{k}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> converges when <math alttext="\bm{A}&lt;1" class="ltx_Math" display="inline" id="Thmexercise2.p1.m4"><semantics><mrow><mi>𝑨</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\bm{A}&lt;1</annotation><annotation encoding="application/x-llamapun">bold_italic_A &lt; 1</annotation></semantics></math> using <math alttext="\|\bm{A}^{k}\|\leq\|\bm{A}\|^{k}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m5"><semantics><mrow><mrow><mo stretchy="false">‖</mo><msup><mi>𝑨</mi><mi>k</mi></msup><mo stretchy="false">‖</mo></mrow><mo>≤</mo><msup><mrow><mo stretchy="false">‖</mo><mi>𝑨</mi><mo stretchy="false">‖</mo></mrow><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\|\bm{A}^{k}\|\leq\|\bm{A}\|^{k}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ∥ ≤ ∥ bold_italic_A ∥ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>. 
<br class="ltx_break"/>(ii) <span class="ltx_text ltx_font_bold">Step 2</span>: Compute the matrix product <math alttext="(\bm{I}-\bm{A})\sum_{k=1}^{\infty}\bm{A}^{k}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m6"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>−</mo><mi>𝑨</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>𝑨</mi><mi>k</mi></msup></mrow></mrow><annotation encoding="application/x-tex">(\bm{I}-\bm{A})\sum_{k=1}^{\infty}\bm{A}^{k}</annotation><annotation encoding="application/x-llamapun">( bold_italic_I - bold_italic_A ) ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 4.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise3.p1">
<p class="ltx_p">Please compute the gradients in (<a class="ltx_ref" href="#S3.E9" title="Equation 4.3.9 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.9</span></a>) and (<a class="ltx_ref" href="#S3.E10" title="Equation 4.3.10 ‣ A New Variational Form for Coding Rates. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.3.10</span></a>).</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 4.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise4.p1">
<p class="ltx_p">Please show <a class="ltx_ref" href="#Thmcorollary1" title="Corollary 4.1. ‣ Practical Implementation Details. ‣ 4.3.2 Linear-Time Attention: Token Statistics Transformer ‣ 4.3 Variants of Deep Architectures by Design ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Corollary</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> when <math alttext="Kp\leq d" class="ltx_Math" display="inline" id="Thmexercise4.p1.m1"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><mo>≤</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">Kp\leq d</annotation><annotation encoding="application/x-llamapun">italic_K italic_p ≤ italic_d</annotation></semantics></math>.</p>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 09:16:00 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
