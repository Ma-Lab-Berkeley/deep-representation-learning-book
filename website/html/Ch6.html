<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 09:48:41 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Bayesian Inference and Constrained Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px1" title="In 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Leveraging Low-dimensionality for Stable and Robust Inference.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px2" title="In 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Statistical interpretation via Bayes’ rule.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px3" title="In 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Geometric interpretation as constrained optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px4" title="In 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Several representative practical settings for inference.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Conditional Inference with a Known Data Distribution</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS0.SSS0.Px1" title="In 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Low-rank matrix completion.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS0.SSS0.Px2" title="In 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Further extensions.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Conditional Inference with a Learned Data Representation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="In 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Image Completion with Masked Auto-Encoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="In 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Conditional Sampling with Measurement Matching</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px1" title="In 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">General linear measurements.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px2" title="In 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">General nonlinear measurements.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="In 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.3 </span>Body Pose Generation Conditioned on Head and Hands</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Conditional Inference with Paired Data and Measurements</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="In 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.1 </span>Class Conditioned Image Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="In 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4.2 </span>Caption Conditioned Image Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Conditional Inference with Measurement Self-Consistency</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="In 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>Linear Measurement Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS2" title="In 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>3D Visual Model from Calibrated Images</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S5.SS2.SSS0.Px1" title="In 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Visual World Model from Uncalibrated Image Sequences</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Summary and Notes</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS0.SSS0.Px1" title="In 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Measurement matching without clean samples.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS0.SSS0.Px2" title="In 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Corrections to the Diffusion Posterior Sampling (DPS) approximation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S6.SS0.SSS0.Px3" title="In 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">More about measurement matching and diffusion models for inverse
problems.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="In Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 6 </span>Inference with Low-Dimensional Distributions</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">Bayesian Inference and Constrained Optimization</a></li><li><a href="#S2">Conditional Inference with a Known Data Distribution</a></li><li><a href="#S3">Conditional Inference with a Learned Data Representation</a><div class="mini-toc-sub"><a href="#S3.SS1">Image Completion with Masked Auto-Encoding</a><a href="#S3.SS2">Conditional Sampling with Measurement Matching</a><a href="#S3.SS3">Body Pose Generation Conditioned on Head and Hands</a></div></li><li><a href="#S4">Conditional Inference with Paired Data and Measurements</a><div class="mini-toc-sub"><a href="#S4.SS1">Class Conditioned Image Generation</a><a href="#S4.SS2">Caption Conditioned Image Generation</a></div></li><li><a href="#S5">Conditional Inference with Measurement Self-Consistency</a><div class="mini-toc-sub"><a href="#S5.SS1">Linear Measurement Models</a><a href="#S5.SS2">3D Visual Model from Calibrated Images</a></div></li><li><a href="#S6">Summary and Notes</a></li><li><a href="#S7">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">Mathematics is the art of giving the same name to different things</span>.”</p>
<p class="ltx_p">   — Henri Poincaré</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">In the previous chapters of this book, we have studied how to effectively and efficiently learn a representation for a variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in the world with a distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> that has a low-dimensional support in a high-dimensional space. So far, we have mainly developed the methodology for learning representation and autoencoding in a general, distribution or task-agnostic fashion. With such a learned representation, one can already used it to perform some generic and basic tasks such as classification (if the encoding is supervised with the class) and generation of random samples that have the same distribution as the given data (say natural images or natural languages).</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">More generally, however, the universality and scalability of the theoretical and computational framework presented in this book have enabled us to learn the distribution of a variety of important real-world high-dimensional data such as natural languages, human poses, natural images, videos, and even 3D scenes. Once the intrinsic rich and low-dimensional structures of these real data can be learned and represented correctly, they start to enable a broad family of powerful, often seemingly miraculous, tasks. Hence, here onwards, we will start to show how to connect and tailor the general methods presented in previous chapters to learn useful representations for specific structured data distributions and for many popular tasks in modern practice of machine intelligence.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.1 </span>Bayesian Inference and Constrained Optimization</h2>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Leveraging Low-dimensionality for Stable and Robust Inference.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p">Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and its representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> for various subsequent classification, estimation, and generation tasks under different conditions. As we have alluded to earlier in Chapter <a class="ltx_ref" href="Ch1.html" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1</span></a> Section <a class="ltx_ref" href="Ch1.html#S2.SS2" title="1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.2</span></a>, the importance of the <span class="ltx_text ltx_font_italic">low-dimensionality</span> of the distribution is the key for us to conduct stable and robust inference related to the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, as illustrated by the few simple examples in Figure <a class="ltx_ref" href="Ch1.html#F9" title="Figure 1.9 ‣ Properties of Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.9</span></a>, from incomplete, noisy, and even corrupted observations. As it turns out, the very same concept carries over to real-world high-dimensional data whose distributions have a low-dimensional support, such as natural images and languages.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p2">
<p class="ltx_p">Despite a dazzling variety of applications in the practice of machine learning with data such as languages, images, videos and many other modalities, almost all practical applications can be viewed as a special case of the following inference problem: given an observation <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> that depends on <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, say</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=h(\bm{x})+\bm{w}," class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w},</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m3"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> represents measurements of a part of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> or its certain observed attributes and <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m5"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> represents some measurement noise and even (sparse) corruptions, solve the “inverse problem” of obtaining a most likely estimate <math alttext="\hat{\bm{x}}(\bm{y})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m6"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}(\bm{y})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ( bold_italic_y )</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> or generating a sample <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m8"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> that is at least consistent with the observation <math alttext="\bm{y}\approx h(\hat{\bm{x}})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m9"><semantics><mrow><mi>𝒚</mi><mo>≈</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}\approx h(\hat{\bm{x}})</annotation><annotation encoding="application/x-llamapun">bold_italic_y ≈ italic_h ( over^ start_ARG bold_italic_x end_ARG )</annotation></semantics></math>. Figure <a class="ltx_ref" href="#F1" title="Figure 6.1 ‣ Leveraging Low-dimensionality for Stable and Robust Inference. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.1</span></a> illustrates the general relationship between <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p2.m11"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F1"><img alt="Figure 6.1 : Inference with low-dimensional distributions. This is the generic picture for this chapter: we have a low-dimensional distribution for 𝒙 ∈ ℝ D \bm{x}\in\mathbb{R}^{D} bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (here depicted as a union of two 2 2 2 -dimensional manifolds in ℝ 3 \mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) and a measurement model 𝒚 = h ​ ( 𝒙 ) + 𝒘 ∈ ℝ d \bm{y}=h(\bm{x})+\bm{w}\in\mathbb{R}^{d} bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . We want to infer various things about this model, including the conditional distribution of 𝒙 \bm{x} bold_italic_x given 𝒚 \bm{y} bold_italic_y , or the conditional expectation 𝔼 ​ [ 𝒙 ∣ 𝒚 ] \mathbb{E}[\bm{x}\mid\bm{y}] blackboard_E [ bold_italic_x ∣ bold_italic_y ] , given various information about the model and (potentially finite) samples of either 𝒙 \bm{x} bold_italic_x or 𝒚 \bm{y} bold_italic_y ." class="ltx_graphics" id="F1.g1" src="chapters/chapter6/figs/inference_roadmap.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Inference with low-dimensional distributions.<span class="ltx_text ltx_font_medium"> This is the generic picture for this chapter: we have a low-dimensional distribution for <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="F1.m10"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> (here depicted as a union of two <math alttext="2" class="ltx_Math" display="inline" id="F1.m11"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation><annotation encoding="application/x-llamapun">2</annotation></semantics></math>-dimensional manifolds in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="F1.m12"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>) and a measurement model <math alttext="\bm{y}=h(\bm{x})+\bm{w}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="F1.m13"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. We want to infer various things about this model, including the conditional distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F1.m14"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{y}" class="ltx_Math" display="inline" id="F1.m15"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, or the conditional expectation <math alttext="\mathbb{E}[\bm{x}\mid\bm{y}]" class="ltx_Math" display="inline" id="F1.m16"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{y}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_y ]</annotation></semantics></math>, given various information about the model and (potentially finite) samples of either <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F1.m17"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> or <math alttext="\bm{y}" class="ltx_Math" display="inline" id="F1.m18"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 6.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Image Completion and Text Prediction)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">The popular natural image completion and natural language prediction are two typical tasks that require us to recover a full data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from its partial observations <math alttext="\bm{y}" class="ltx_Math" display="inline" id="Thmexample1.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, with parts of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> masked out and to be completed based on the rest. Figure <a class="ltx_ref" href="#F2" title="Figure 6.2 ‣ Example 6.1 (Image Completion and Text Prediction). ‣ Leveraging Low-dimensionality for Stable and Robust Inference. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.2</span></a> shows some examples of such tasks. In fact, it is precisely these tasks which have inspired how to train modern large models for text generation (such as GPT) and image completion (such as the masked autoencoder) that we will study in greater details later.</p>
</div>
<figure class="ltx_figure" id="F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 6.2 : Left: image completion. Right: text prediction. In particular, text prediction is the inspiration for the popular Generative Pre-trained Transformer (GPT)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="240" id="F2.g1" src="chapters/chapter6/figs/image-completion.jpg" width="290"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 6.2 : Left: image completion. Right: text prediction. In particular, text prediction is the inspiration for the popular Generative Pre-trained Transformer (GPT)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="269" id="F2.g2" src="chapters/chapter6/figs/text-prediction.jpg" width="221"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.2</span>: </span><span class="ltx_text" style="font-size:90%;">Left: image completion. Right: text prediction. In particular, text prediction is the inspiration for the popular Generative Pre-trained Transformer (GPT).</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample1.p2">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p2.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Statistical interpretation via Bayes’ rule.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p">Generally speaking, to accomplish such tasks well, we need to get ahold of the conditional distribution <math alttext="p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p1.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math>. If we had this, then we would be able to find the maximal likelihood estimate (prediction):</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}=\operatorname*{arg\ max}_{\bm{x}}p(\bm{x}\mid\bm{y});" class="ltx_Math" display="block" id="S1.E2.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mi>𝒙</mi></munder><mo>⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>;</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\operatorname*{arg\ max}_{\bm{x}}p(\bm{x}\mid\bm{y});</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_p ( bold_italic_x ∣ bold_italic_y ) ;</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">compute the conditional expectation estimate:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}=\mathbb{E}[\bm{x}\mid\bm{y}]=\int\bm{x}p(\bm{x}\mid\bm{y})\mathrm{d}\bm{x};" class="ltx_Math" display="block" id="S1.E3.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mo>∫</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒙</mi></mrow></mrow></mrow></mrow><mo>;</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\mathbb{E}[\bm{x}\mid\bm{y}]=\int\bm{x}p(\bm{x}\mid\bm{y})\mathrm{d}\bm{x};</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = blackboard_E [ bold_italic_x ∣ bold_italic_y ] = ∫ bold_italic_x italic_p ( bold_italic_x ∣ bold_italic_y ) roman_d bold_italic_x ;</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and sample from the conditional distribution:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}\sim p(\bm{x}\mid\bm{y})." class="ltx_Math" display="block" id="S1.E4.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}\sim p(\bm{x}\mid\bm{y}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ∼ italic_p ( bold_italic_x ∣ bold_italic_y ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p">Notice that from Bayes’ rule, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\bm{x}\mid\bm{y})=\frac{p(\bm{y}\mid\bm{x})p(\bm{x})}{p(\bm{y})}." class="ltx_Math" display="block" id="S1.E5.m1"><semantics><mrow><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})=\frac{p(\bm{y}\mid\bm{x})p(\bm{x})}{p(\bm{y})}.</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y ) = divide start_ARG italic_p ( bold_italic_y ∣ bold_italic_x ) italic_p ( bold_italic_x ) end_ARG start_ARG italic_p ( bold_italic_y ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For instance, the maximal likelihood estimate can be computed by solving the following (maximal log likelihood) program:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}=\operatorname*{arg\ max}_{\bm{x}}[\log p(\bm{y}\mid\bm{x})+\log p(\bm{x})]," class="ltx_Math" display="block" id="S1.E6.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mi>𝒙</mi></munder><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\operatorname*{arg\ max}_{\bm{x}}[\log p(\bm{y}\mid\bm{x})+\log p(\bm{x})],</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y ∣ bold_italic_x ) + roman_log italic_p ( bold_italic_x ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">say via gradient ascent:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{k+1}=\bm{x}_{k}+\alpha\cdot\big{(}\nabla_{\bm{x}}\log p(\bm{y}\mid\bm{x})+\nabla_{\bm{x}}\log p(\bm{x})\big{)}." class="ltx_Math" display="block" id="S1.E7.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>k</mi></msub><mo>+</mo><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒙</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒙</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{k+1}=\bm{x}_{k}+\alpha\cdot\big{(}\nabla_{\bm{x}}\log p(\bm{y}\mid\bm{x})+\nabla_{\bm{x}}\log p(\bm{x})\big{)}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_α ⋅ ( ∇ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_y ∣ bold_italic_x ) + ∇ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Efficiently computing the conditional distribution <math alttext="p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math> naturally depends on how we learn and exploit the low-dimensional distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the observation model <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p2.m4"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math> that determines the conditional distribution <math alttext="p(\bm{y}\mid\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px2.p2.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{y}\mid\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_y ∣ bold_italic_x )</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 6.1</span></span><span class="ltx_text ltx_font_italic"> </span>(End-to-End versus Bayesian)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark1.p1">
<p class="ltx_p">In the modern practices of data-driven machine learning, for certain popular tasks people often directly learn the conditional distribution <math alttext="p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="Thmremark1.p1.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math> or a (probabilistic) mapping or a regressor. Such a mapping is often modeled by some deep networks and trained end-to-end with sufficient paired samples <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="Thmremark1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>. Such an approach is very different from the above Bayesian approach in which both the distribution of <math alttext="\bm{x}\sim p(\bm{x})" class="ltx_Math" display="inline" id="Thmremark1.p1.m3"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim p(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ italic_p ( bold_italic_x )</annotation></semantics></math> and the (observation) mapping are needed. The benefits of the Bayesian approach is that the learned distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="Thmremark1.p1.m4"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> can facilitate many different tasks with very observation models and conditions.</p>
</div>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Geometric interpretation as constrained optimization.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p1">
<p class="ltx_p">As the support <math alttext="\mathcal{S}_{\bm{x}}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> of the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is low-dimensional, we may assume that there exists a function <math alttext="F" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m3"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation><annotation encoding="application/x-llamapun">italic_F</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F(\bm{x})=\bm{0}\qquad\iff\qquad\bm{x}\in\mathcal{S}_{\bm{x}}" class="ltx_Math" display="block" id="S1.E8.m1"><semantics><mrow><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>𝟎</mn><mspace width="2em"></mspace><mo stretchy="false">⇔</mo></mrow></mrow><mspace width="2em"></mspace><mrow><mi>𝒙</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub></mrow></mrow><annotation encoding="application/x-tex">F(\bm{x})=\bm{0}\qquad\iff\qquad\bm{x}\in\mathcal{S}_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x ) = bold_0 ⇔ bold_italic_x ∈ caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">such that <math alttext="\mathcal{S}_{\bm{x}}=F^{-1}(\{\bm{0}\})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m4"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><mo>=</mo><mrow><msup><mi>F</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">{</mo><mn>𝟎</mn><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}=F^{-1}(\{\bm{0}\})</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { bold_0 } )</annotation></semantics></math> is the low-dimensional support of the distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>. Geometrically, one natural choice of <math alttext="F(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m6"><semantics><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x )</annotation></semantics></math> is the “distance function” to the support <math alttext="\mathcal{S}_{\bm{x}}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m7"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>:<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Notice that, in reality, we only have discrete samples on the support of the distribution. In the same spirit of continuation, through diffusion or lossy coding studied in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, we may approximate the distance function as <math alttext="F(\bm{x})\approx\min_{\bm{x}_{p}\in\mathcal{C}_{\bm{x}}^{\epsilon}}\|\bm{x}-\bm{x}_{p}\|_{2}" class="ltx_Math" display="inline" id="footnote1.m1"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>min</mi><mrow><msub><mi>𝒙</mi><mi>p</mi></msub><mo>∈</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>𝒙</mi><mi>ϵ</mi></msubsup></mrow></msub><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝒙</mi><mi>p</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">F(\bm{x})\approx\min_{\bm{x}_{p}\in\mathcal{C}_{\bm{x}}^{\epsilon}}\|\bm{x}-\bm{x}_{p}\|_{2}</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x ) ≈ roman_min start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_C start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x - bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext="\mathcal{S}_{\bm{x}}" class="ltx_Math" display="inline" id="footnote1.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> is replaced by a covering <math alttext="\mathcal{C}_{\bm{x}}^{\epsilon}" class="ltx_Math" display="inline" id="footnote1.m3"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>𝒙</mi><mi>ϵ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{C}_{\bm{x}}^{\epsilon}</annotation><annotation encoding="application/x-llamapun">caligraphic_C start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT</annotation></semantics></math> of the samples with <math alttext="\epsilon" class="ltx_Math" display="inline" id="footnote1.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls. </span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F(\bm{x})=\min_{\bm{x}_{p}\in\mathcal{S}_{\bm{x}}}\|\bm{x}-\bm{x}_{p}\|_{2}." class="ltx_Math" display="block" id="S1.E9.m1"><semantics><mrow><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><msub><mi>𝒙</mi><mi>p</mi></msub><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝒙</mi><mi>p</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">F(\bm{x})=\min_{\bm{x}_{p}\in\mathcal{S}_{\bm{x}}}\|\bm{x}-\bm{x}_{p}\|_{2}.</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x - bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now given <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m8"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math>, to solve for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we can solve the following constrained optimization problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{x}}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\quad\mbox{s.t.}\quad F(\bm{x})=\bm{0}." class="ltx_Math" display="block" id="S1.E10.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>𝒙</mi></munder><mo lspace="0em">−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mspace width="1em"></mspace><mtext>s.t.</mtext><mspace width="1em"></mspace><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{x}}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\quad\mbox{s.t.}\quad F(\bm{x})=\bm{0}.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_italic_x ) - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT s.t. italic_F ( bold_italic_x ) = bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Using the method of augmented Lagrange multipliers, we can solve the following unconstrained program:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{x}}\left[-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}+\bm{\lambda}^{\top}F(\bm{x})-\frac{\mu}{2}\|F(\bm{x})\|_{2}^{2}\right]" class="ltx_Math" display="block" id="S1.E11.m1"><semantics><mrow><munder><mi>max</mi><mi>𝒙</mi></munder><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><msup><mi>𝝀</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mfrac><mi>μ</mi><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\bm{x}}\left[-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}+\bm{\lambda}^{\top}F(\bm{x})-\frac{\mu}{2}\|F(\bm{x})\|_{2}^{2}\right]</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_italic_x ) - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + bold_italic_λ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_F ( bold_italic_x ) - divide start_ARG italic_μ end_ARG start_ARG 2 end_ARG ∥ italic_F ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some constant Lagrange multipliers <math alttext="\bm{\lambda}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m10"><semantics><mi>𝝀</mi><annotation encoding="application/x-tex">\bm{\lambda}</annotation><annotation encoding="application/x-llamapun">bold_italic_λ</annotation></semantics></math>.
This is equivalent to the following program:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{x}}\left[\log\exp\Big{(}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\Big{)}+\log\exp\Big{(}-\frac{\mu}{2}\big{\|}F(\bm{x})-\bm{\lambda}/\mu\big{\|}_{2}^{2}\Big{)}\right]," class="ltx_Math" display="block" id="S1.E12.m1"><semantics><mrow><mrow><munder><mi>max</mi><mi>𝒙</mi></munder><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo>−</mo><mrow><mfrac><mi>μ</mi><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo maxsize="120%" minsize="120%">‖</mo><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>𝝀</mi><mo>/</mo><mi>μ</mi></mrow></mrow><mo maxsize="120%" minsize="120%">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{x}}\left[\log\exp\Big{(}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\Big{)}+\log\exp\Big{(}-\frac{\mu}{2}\big{\|}F(\bm{x})-\bm{\lambda}/\mu\big{\|}_{2}^{2}\Big{)}\right],</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_italic_x ) - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + roman_log roman_exp ( - divide start_ARG italic_μ end_ARG start_ARG 2 end_ARG ∥ italic_F ( bold_italic_x ) - bold_italic_λ / italic_μ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{c}\doteq{\bm{\lambda}}/{\mu}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m11"><semantics><mrow><mi>𝒄</mi><mo>≐</mo><mrow><mi>𝝀</mi><mo>/</mo><mi>μ</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{c}\doteq{\bm{\lambda}}/{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_c ≐ bold_italic_λ / italic_μ</annotation></semantics></math> can be viewed as a “mean” for the constraint function. As <math alttext="\mu" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m12"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation><annotation encoding="application/x-llamapun">italic_μ</annotation></semantics></math> becomes large when enforcing the constraint via continuation<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In the same spirit of continuation in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a> where we obtained better approximations of our distribution by sending <math alttext="\varepsilon\to 0" class="ltx_Math" display="inline" id="footnote2.m1"><semantics><mrow><mi>ε</mi><mo stretchy="false">→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\varepsilon\to 0</annotation><annotation encoding="application/x-llamapun">italic_ε → 0</annotation></semantics></math>, here we send <math alttext="\mu\to\infty" class="ltx_Math" display="inline" id="footnote2.m2"><semantics><mrow><mi>μ</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\mu\to\infty</annotation><annotation encoding="application/x-llamapun">italic_μ → ∞</annotation></semantics></math>. Larger values of <math alttext="\mu" class="ltx_Math" display="inline" id="footnote2.m3"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation><annotation encoding="application/x-llamapun">italic_μ</annotation></semantics></math> will constrain <math alttext="F" class="ltx_Math" display="inline" id="footnote2.m4"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation><annotation encoding="application/x-llamapun">italic_F</annotation></semantics></math> to take smaller and smaller values at optimum, meaning that the optimum lies within a smaller and smaller neighborhood of the support <math alttext="\mathcal{S}_{\bm{x}}" class="ltx_Math" display="inline" id="footnote2.m5"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>. Interestingly, the theory of Lagrange multipliers hints that, under certain benign conditions on <math alttext="F" class="ltx_Math" display="inline" id="footnote2.m6"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation><annotation encoding="application/x-llamapun">italic_F</annotation></semantics></math> and other terms in the objective, we only need to make <math alttext="\mu" class="ltx_Math" display="inline" id="footnote2.m7"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation><annotation encoding="application/x-llamapun">italic_μ</annotation></semantics></math> large enough in order to ensure <math alttext="F(\bm{x})=\bm{0}" class="ltx_Math" display="inline" id="footnote2.m8"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">F(\bm{x})=\bm{0}</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x ) = bold_0</annotation></semantics></math> at optimum, meaning that at <span class="ltx_text ltx_font_italic">finite</span> penalty we get <span class="ltx_text ltx_font_italic">perfect</span> approximation of the support. In general, we should have the intuition that <math alttext="\mu" class="ltx_Math" display="inline" id="footnote2.m9"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation><annotation encoding="application/x-llamapun">italic_μ</annotation></semantics></math> plays the same role as <math alttext="\epsilon^{-1}" class="ltx_Math" display="inline" id="footnote2.m10"><semantics><msup><mi>ϵ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\epsilon^{-1}</annotation><annotation encoding="application/x-llamapun">italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></span>, <math alttext="\|\bm{c}\|_{2}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p1.m13"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒄</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|\bm{c}\|_{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_c ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> becomes increasingly small.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p2">
<p class="ltx_p">The above program may be interpreted in two different ways. Firstly, one may view the first term as the conditional probability of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, and the second term as a probability density for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\bm{y}\mid\bm{x})\propto\exp\Big{(}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\Big{)},\quad p(\bm{x})\propto\exp\Big{(}-\frac{\mu}{2}\|F(\bm{x})-\bm{c}\|_{2}^{2}\Big{)}." class="ltx_Math" display="block" id="S1.E13.m1"><semantics><mrow><mrow><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mo>−</mo><mrow><mfrac><mi>μ</mi><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒄</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p(\bm{y}\mid\bm{x})\propto\exp\Big{(}-\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}\Big{)},\quad p(\bm{x})\propto\exp\Big{(}-\frac{\mu}{2}\|F(\bm{x})-\bm{c}\|_{2}^{2}\Big{)}.</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_y ∣ bold_italic_x ) ∝ roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_italic_x ) - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , italic_p ( bold_italic_x ) ∝ roman_exp ( - divide start_ARG italic_μ end_ARG start_ARG 2 end_ARG ∥ italic_F ( bold_italic_x ) - bold_italic_c ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, solving the constrained optimization for the inverse problem is equivalent to conducting Bayes inference with the above probability densities. Hence solving the above program (<a class="ltx_ref" href="#S1.E12" title="Equation 6.1.12 ‣ Geometric interpretation as constrained optimization. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.1.12</span></a>) via gradient ascent is equivalent to the above maximal likelihood estimate (<a class="ltx_ref" href="#S1.E7" title="Equation 6.1.7 ‣ Statistical interpretation via Bayes’ rule. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.1.7</span></a>),
in which the gradient takes the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{x}}\log p(\bm{y}\mid\bm{x})+\nabla_{\bm{x}}\log p(\bm{x})=\frac{\partial h}{\partial\bm{x}}(\bm{x})\big{(}\bm{y}-h(\bm{x})\big{)}+\mu\frac{\partial F}{\partial\bm{x}}(\bm{x})\big{(}\bm{c}-F(\bm{x})\big{)}," class="ltx_Math" display="block" id="S1.E14.m1"><semantics><mrow><mrow><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒙</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒙</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mi>h</mi></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝒚</mi><mo>−</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>+</mo><mrow><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mo rspace="0em">∂</mo><mi>F</mi></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝒄</mi><mo>−</mo><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{x}}\log p(\bm{y}\mid\bm{x})+\nabla_{\bm{x}}\log p(\bm{x})=\frac{\partial h}{\partial\bm{x}}(\bm{x})\big{(}\bm{y}-h(\bm{x})\big{)}+\mu\frac{\partial F}{\partial\bm{x}}(\bm{x})\big{(}\bm{c}-F(\bm{x})\big{)},</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_y ∣ bold_italic_x ) + ∇ start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x ) = divide start_ARG ∂ italic_h end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x ) ( bold_italic_y - italic_h ( bold_italic_x ) ) + italic_μ divide start_ARG ∂ italic_F end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x ) ( bold_italic_c - italic_F ( bold_italic_x ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\frac{\partial h}{\partial\bm{x}}(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m4"><semantics><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mi>h</mi></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial h}{\partial\bm{x}}(\bm{x})</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ italic_h end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x )</annotation></semantics></math> and <math alttext="\frac{\partial F}{\partial\bm{x}}(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m5"><semantics><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mi>F</mi></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial F}{\partial\bm{x}}(\bm{x})</annotation><annotation encoding="application/x-llamapun">divide start_ARG ∂ italic_F end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x )</annotation></semantics></math> are the Jacobians of <math alttext="h(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m6"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x )</annotation></semantics></math> and <math alttext="F(\bm{x})" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px3.p2.m7"><semantics><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x )</annotation></semantics></math>, respectively.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p3">
<p class="ltx_p">Secondly, notice that the above program (<a class="ltx_ref" href="#S1.E12" title="Equation 6.1.12 ‣ Geometric interpretation as constrained optimization. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.1.12</span></a>) is equivalent to:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{x}}\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}+\frac{\mu}{2}\big{\|}F(\bm{x})-\bm{\lambda}/\mu\big{\|}_{2}^{2}," class="ltx_Math" display="block" id="S1.E15.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mi>𝒙</mi></munder><mo lspace="0.167em">⁡</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒚</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mfrac><mi>μ</mi><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo maxsize="120%" minsize="120%">‖</mo><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>𝝀</mi><mo>/</mo><mi>μ</mi></mrow></mrow><mo maxsize="120%" minsize="120%">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{x}}\frac{1}{2}\|h(\bm{x})-\bm{y}\|_{2}^{2}+\frac{\mu}{2}\big{\|}F(\bm{x})-\bm{\lambda}/\mu\big{\|}_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ italic_h ( bold_italic_x ) - bold_italic_y ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_μ end_ARG start_ARG 2 end_ARG ∥ italic_F ( bold_italic_x ) - bold_italic_λ / italic_μ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.1.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Due to the conspicuous quadratic form of the two terms, they can also be interpreted as certain “energy” functions. Such a formulation is often referred to as “Energy Minimization” in the machine learning literature.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Several representative practical settings for inference.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px4.p1">
<p class="ltx_p">In practice, however, initial information about the distributions of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px4.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the relationship between <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px4.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px4.p1.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> can be given in many different ways or forms. In general, they can mostly be categorized into four cases, which are, conceptually, increasingly more challenging:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Case 1:</span> Both a model for the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the observation model <math alttext="\bm{y}=h(\bm{x})" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m2"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x )</annotation></semantics></math> <math alttext="(+\bm{w})" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m3"><semantics><mrow><mo stretchy="false">(</mo><mrow><mo>+</mo><mi>𝒘</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(+\bm{w})</annotation><annotation encoding="application/x-llamapun">( + bold_italic_w )</annotation></semantics></math> is known, even with an analytical form. This is typically the case for many classic signal processing problems, such as signal denoising, the sparse vector recovery problem we saw in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a> and the low-rank matrix recovery problem to be introduced below.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Case 2:</span> We do not have a model for the distribution but only samples <math alttext="\bm{X}=\{\bm{x}_{1},\ldots,\bm{x}_{N}\}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\{\bm{x}_{1},\ldots,\bm{x}_{N}\}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, and the observation model <math alttext="\bm{y}=h(\bm{x})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m3"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x )</annotation></semantics></math> <math alttext="(+\bm{w})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m4"><semantics><mrow><mo stretchy="false">(</mo><mrow><mo>+</mo><mi>𝒘</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(+\bm{w})</annotation><annotation encoding="application/x-llamapun">( + bold_italic_w )</annotation></semantics></math> is known.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>In the literature, this setting is sometimes referred to as the <span class="ltx_text ltx_font_italic">empirical Bayesian inference</span>.</span></span></span> A model for the distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> needs to be learned, and subsequently the conditional distribution <math alttext="p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m7"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math>. Natural image completion or natural language completion (e.g., BERT and GPT) are typical examples for this class of problems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Case 3:</span> We only have the paired samples: <math alttext="(\bm{X},\bm{Y})=\{(\bm{x}_{1},\bm{y}_{1}),\ldots,(\bm{x}_{N},\bm{y}_{N})\}" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m1"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒀</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒚</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{Y})=\{(\bm{x}_{1},\bm{y}_{1}),\ldots,(\bm{x}_{N},\bm{y}_{N})\}</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_Y ) = { ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , … , ( bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) }</annotation></semantics></math> of the two variables <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>. The distributions of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m4"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and their relationship <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S1.I1.i3.p1.m5"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> need to be learned from these paired sample data. For example, given many images and their captions, learning to conduct text-conditioned image generation is one such problem.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Case 4:</span> We only have the samples <math alttext="\bm{Y}=\{\bm{y}_{1},\ldots,\bm{y}_{N}\}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m1"><semantics><mrow><mi>𝒀</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>𝒚</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒚</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Y}=\{\bm{y}_{1},\ldots,\bm{y}_{N}\}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math> of the observations <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, and the observation model <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m3"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> needs to be known, at least in some parametric family <math alttext="h(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m4"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ , bold_italic_θ )</annotation></semantics></math>. The distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> and <math alttext="p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m6"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math> need to be learned from <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m7"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, estimated from <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.m8"><semantics><mi>𝒀</mi><annotation encoding="application/x-tex">\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y</annotation></semantics></math>. For example, learning to render a new view from a sequence of calibrated or uncalibrated views is one such problem.</p>
</div>
</li>
</ul>
<p class="ltx_p">In this chapter, we will discuss general approaches to learn the desired distributions and solve the associated conditional estimation or generation for these cases, typically with a representative problem. Throughout the chapter, you should keep <a class="ltx_ref" href="#F1" title="In Leveraging Low-dimensionality for Stable and Robust Inference. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.1</span></a> in mind.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.2 </span>Conditional Inference with a Known Data Distribution</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">Notice that in the setting we have discussed in previous Chapters, the
autoencoding network is trained to reconstruct a set of samples of the random vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. This would allow us to regenerate samples from the learned (low-dimensional) distribution. In practice, the low-dimensionality of the distribution, once given or learned, can be exploited for stable and robust recovery, completion, or prediction tasks. That is, under rather mild conditions, one can recover <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, from highly compressive, partial, noisy or even corrupted measures of <math alttext="x" class="ltx_Math" display="inline" id="S2.p1.m3"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation><annotation encoding="application/x-llamapun">italic_x</annotation></semantics></math> of the kind:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=h(\bm{x})+\bm{w}," class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w},</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S2.p1.m4"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> is typically an observation of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> that is of much lower dimension that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S2.p1.m7"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> can be random noise or even sparse gross corruptions. This is a class of problems that have been extensively studied in the classical signal processing literature, for low-dimensional structures such as sparse vectors, low-rank matrices, and beyond. Interested readers may see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> for a complete exposition of this topic.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">Here to put the classic work in a more general modern setting, we illustrate the basic idea and facts through the arguably simplest task of data (and particularlly image) completion. That is, we consider the problem of recovering a sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> when parts of it are missing (or even corrupted). We want to recover or predict the rest of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>
from observing only a fraction of it:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:\mathcal{P}_{\Omega}(\bm{x})\mapsto\hat{\bm{x}}," class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↦</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">f:\mathcal{P}_{\Omega}(\bm{x})\mapsto\hat{\bm{x}},</annotation><annotation encoding="application/x-llamapun">italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_x ) ↦ over^ start_ARG bold_italic_x end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{P}_{\Omega}(\,\cdot\,)" class="ltx_Math" display="inline" id="S2.p2.m3"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{\Omega}(\,\cdot\,)</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> represents a masking operation
(see <a class="ltx_ref" href="#F3" title="In Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.3</span></a> for an example).</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">In this section and the next, we will study the completion task under two different scenarios: One is when the distribution of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> of interest is already given <span class="ltx_text ltx_font_italic">apriori</span>, even in a certain analytical form. This is the case that prevails in classic signal processing where the structures of the signals are assumed to be known, for example, band-limited, sparse or low-rank. The other is when only raw samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p3.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> are available and we need to learn the low-dimensional distribution from the samples in order to solve the completion task well. This is the case for the tasks of natural image completion or video frame prediction. As a precursor to the rest of the chapter, we start with the simplest case of image completion: when the image to be completed can be well modeled as a low-rank matrix. We will move on to increasingly more general cases and more challenging settings later.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Low-rank matrix completion.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p">The low-rank <span class="ltx_text ltx_font_italic">matrix
completion</span> problem is a classical problem for data completion when its distribution is low-dimensional and known. Consider a random sample of a matrix <math alttext="\bm{X}_{o}=[\bm{x}_{1},\ldots,\bm{x}_{n}]\in\mathbb{R}^{m\times n}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.m1"><semantics><mrow><msub><mi>𝑿</mi><mi>o</mi></msub><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}_{o}=[\bm{x}_{1},\ldots,\bm{x}_{n}]\in\mathbb{R}^{m\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> from the space of all matrices of rank <math alttext="r" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.m2"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation><annotation encoding="application/x-llamapun">italic_r</annotation></semantics></math>. In general, we assume the rank
of the matrix is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{rank}(\bm{X}_{o})=r&lt;\min\{m,n\}." class="ltx_Math" display="block" id="S2.E3.m1"><semantics><mrow><mrow><mrow><mtext>rank</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>o</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>r</mi><mo>&lt;</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{rank}(\bm{X}_{o})=r&lt;\min\{m,n\}.</annotation><annotation encoding="application/x-llamapun">rank ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = italic_r &lt; roman_min { italic_m , italic_n } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">So it is clear that locally the intrinsic dimension of the space of all matrix of rank <math alttext="r" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.m3"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation><annotation encoding="application/x-llamapun">italic_r</annotation></semantics></math> is much lower than the ambient space <math alttext="mn" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.m4"><semantics><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">mn</annotation><annotation encoding="application/x-llamapun">italic_m italic_n</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p">Now, let <math alttext="\Omega" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.m1"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation><annotation encoding="application/x-llamapun">roman_Ω</annotation></semantics></math> indicate a set indices of observed entries of the matrix <math alttext="\bm{X}_{o}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.m2"><semantics><msub><mi>𝑿</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{X}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>. Let the observed entries to be:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Y}=\mathcal{P}_{\Omega}(\bm{X}_{o})." class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mi>𝒀</mi><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>o</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Y}=\mathcal{P}_{\Omega}(\bm{X}_{o}).</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The remaining entries supported on <math alttext="\Omega^{c}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.m3"><semantics><msup><mi mathvariant="normal">Ω</mi><mi>c</mi></msup><annotation encoding="application/x-tex">\Omega^{c}</annotation><annotation encoding="application/x-llamapun">roman_Ω start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> are unobserved or
missing. The problem is whether we can recover from <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.m4"><semantics><mi>𝒀</mi><annotation encoding="application/x-tex">\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y</annotation></semantics></math> the missing
entries of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> correctly and efficiently. Figure <a class="ltx_ref" href="#F3" title="Figure 6.3 ‣ Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3</span></a> shows one
example of completing such a matrix.</p>
</div>
<figure class="ltx_figure" id="F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 6.3 : Illustration of completing an image as low-rank matrix with some entries masked or corrupted. Left: the masked/corrupted image 𝒀 \bm{Y} bold_italic_Y ; middle: the mask Ω \Omega roman_Ω ; right: the completed image 𝑿 ^ \hat{\bm{X}} over^ start_ARG bold_italic_X end_ARG ." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F3.g1" src="chapters/chapter6/figs/masked-checkerboard.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 6.3 : Illustration of completing an image as low-rank matrix with some entries masked or corrupted. Left: the masked/corrupted image 𝒀 \bm{Y} bold_italic_Y ; middle: the mask Ω \Omega roman_Ω ; right: the completed image 𝑿 ^ \hat{\bm{X}} over^ start_ARG bold_italic_X end_ARG ." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F3.g2" src="chapters/chapter6/figs/mask.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 6.3 : Illustration of completing an image as low-rank matrix with some entries masked or corrupted. Left: the masked/corrupted image 𝒀 \bm{Y} bold_italic_Y ; middle: the mask Ω \Omega roman_Ω ; right: the completed image 𝑿 ^ \hat{\bm{X}} over^ start_ARG bold_italic_X end_ARG ." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F3.g3" src="chapters/chapter6/figs/checkerboard.png" width="180"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.3</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of completing an image as low-rank matrix
with some entries masked or corrupted. Left: the masked/corrupted
image <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="F3.m4"><semantics><mi>𝒀</mi><annotation encoding="application/x-tex">\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y</annotation></semantics></math>; middle: the mask <math alttext="\Omega" class="ltx_Math" display="inline" id="F3.m5"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation><annotation encoding="application/x-llamapun">roman_Ω</annotation></semantics></math>; right: the completed image <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F3.m6"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p">Notice that the fundamental reason why such a matrix can be completed is because columns and rows of the matrix are highly correlated and
they all lie on a low-dimensional subspace. For the example shown in
Figure <a class="ltx_ref" href="#F3" title="Figure 6.3 ‣ Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3</span></a>, the dimension or the rank of the matrix completed is only two. Hence the fundamental idea to recover such a matrix is to seek a matrix that has the lowest rank among all
matrices that have entries agreeing with the observed ones:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{X}}\mbox{rank}(\bm{X})\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X})." class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>𝑿</mi></munder><mo lspace="0.167em">⁡</mo><mtext>rank</mtext></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><mi>𝒀</mi></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{X}}\mbox{rank}(\bm{X})\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X}).</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT rank ( bold_italic_X ) subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is known as the <span class="ltx_text ltx_font_italic">low-rank matrix completion</span> problem. See <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> for a full characterization of the space of all low-rank matrices. As the rank function is discontinuous and rank minimization
is in general an NP-hard problem, we like to relax it with
something easier to optimize.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p4">
<p class="ltx_p">Based on our knowledge about compression from Chapter
<a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, we could promote the low-rankness of the
recovered matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> by enforcing the lossy coding rate (or the
volume spanned by <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p4.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>) of the data in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p4.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to be small:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min R_{\epsilon}(\bm{X})=\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{X}\bm{X}^{\top}\right)\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X})." class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><mrow><mrow><mrow><mi>min</mi><mo lspace="0.167em">⁡</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mspace width="1em"></mspace><mtext>subject to</mtext></mrow></mrow><mspace width="1em"></mspace><mrow><mi>𝒀</mi><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min R_{\epsilon}(\bm{X})=\frac{1}{2}\log\det\left(\bm{I}+\alpha\bm{X}\bm{X}^{\top}\right)\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X}).</annotation><annotation encoding="application/x-llamapun">roman_min italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The problem can viewed as a continuous relaxation of the above
low-rank matrix completion problem (<a class="ltx_ref" href="#S2.E5" title="Equation 6.2.5 ‣ Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.2.5</span></a>) and it can be
solved via gradient descent. One can show that the gradient descent
operator for the <math alttext="\log\det" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p4.m4"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mo>det</mo></mrow><annotation encoding="application/x-tex">\log\det</annotation><annotation encoding="application/x-llamapun">roman_log roman_det</annotation></semantics></math> objective is precisely minimizing a close
surrogate of the rank of the matrix <math alttext="\bm{X}\bm{X}^{\top}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p4.m5"><semantics><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p5">
<p class="ltx_p">The rate distortion function is a nonconvex function, and its gradient
descent does not always guarantee finding the globally optimal solution. Nevertheless, since the underlying structure sought for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is piecewise linear, the rank function admits a rather effective convex relaxation: the
nuclear norm—the sum of all singular values of the matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. As
shown in the compressive sensing literature, under fairly
broad conditions,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Typically, such conditions specify the
necessary and sufficient amount of entries needed for the completion
to be computationally feasible. These conditions have been
systematically characterized in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</span></span></span> the matrix
completion problem (<a class="ltx_ref" href="#S2.E5" title="Equation 6.2.5 ‣ Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.2.5</span></a>)
can be effectively solved by the following convex program:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\|\bm{X}\|_{*}\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X})," class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><mrow><mrow><mi>min</mi><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝑿</mi><mo stretchy="false">‖</mo></mrow><mo>∗</mo></msub></mrow><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><mi>𝒀</mi></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min\|\bm{X}\|_{*}\quad\mbox{subject to}\quad\bm{Y}=\mathcal{P}_{\Omega}(\bm{X}),</annotation><annotation encoding="application/x-llamapun">roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the nuclear norm <math alttext="\|\bm{X}\|_{*}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m3"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝑿</mi><mo stretchy="false">‖</mo></mrow><mo>∗</mo></msub><annotation encoding="application/x-tex">\|\bm{X}\|_{*}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT</annotation></semantics></math> is the sum of singular values of
<math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. In practice, we often convert the above constrained convex optimization
program to an unconstrained one:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\|\bm{X}\|_{*}+\lambda\|\bm{Y}-\mathcal{P}_{\Omega}(\bm{X})\|_{F}^{2}," class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><mrow><mi>min</mi><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝑿</mi><mo stretchy="false">‖</mo></mrow><mo>∗</mo></msub></mrow><mo>+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒀</mi><mo>−</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min\|\bm{X}\|_{*}+\lambda\|\bm{Y}-\mathcal{P}_{\Omega}(\bm{X})\|_{F}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min ∥ bold_italic_X ∥ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT + italic_λ ∥ bold_italic_Y - caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X ) ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some properly chosen <math alttext="\lambda&gt;0" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m5"><semantics><mrow><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda&gt;0</annotation><annotation encoding="application/x-llamapun">italic_λ &gt; 0</annotation></semantics></math>. Interested readers may refer to
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> for how to develop algorithms that can solve
the above programs efficiently and effectively.
Figure <a class="ltx_ref" href="#F3" title="Figure 6.3 ‣ Low-rank matrix completion. ‣ 6.2 Conditional Inference with a Known Data Distribution ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3</span></a> shows a real example in which the
matrix <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p5.m6"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> is actually recovered by solving the above program.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Further extensions.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p">It has been shown that images (or more accurately textures) and 3D scenes
with low-rank structures can be very effectively completed via
solving optimization programs of the above kind, even if there is
additional corruption and distortion
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx320" title="">ZLG+10</a>, <a class="ltx_ref" href="bib.html#bibx162" title="">LRZ+12</a>, <a class="ltx_ref" href="bib.html#bibx310" title="">YZB+23</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Y}\circ\tau=\bm{X}_{o}+\bm{E}," class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mrow><mi>𝒀</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>τ</mi></mrow><mo>=</mo><mrow><msub><mi>𝑿</mi><mi>o</mi></msub><mo>+</mo><mi>𝑬</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Y}\circ\tau=\bm{X}_{o}+\bm{E},</annotation><annotation encoding="application/x-llamapun">bold_italic_Y ∘ italic_τ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.m1"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math> is some unknown nonlinear distortion of the image and <math alttext="\bm{E}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.m2"><semantics><mi>𝑬</mi><annotation encoding="application/x-tex">\bm{E}</annotation><annotation encoding="application/x-llamapun">bold_italic_E</annotation></semantics></math> is an unknown matrix that models some (sparse) occlusion and corruption. Again, interested readers may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> for a more detailed account.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.3 </span>Conditional Inference with a Learned Data Representation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">In the previous subsection, the reason we can infer <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from the partial observation <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> is because (support of) the distribution of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is known or specified <span class="ltx_text ltx_font_italic">apriori</span>, say as the set of all low-rank matrices. For many practical dataset, we do not have its distribution in an analytical form as the low-rank matrices, say the set to of all natural images. Nevertheless, if we have sufficient samples of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we should be able to learn its low-dimensional distribution first and leverage it for future inference tasks based on an observation <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S3.p1.m5"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math>. In this section, we assume the observation model <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S3.p1.m6"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> is given and known. We will study the case when <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S3.p1.m7"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> is not explicitly given in the next section.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3.1 </span>Image Completion with Masked Auto-Encoding</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">For a general image <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> such as the one shown on the left of Figure
<a class="ltx_ref" href="#F4" title="Figure 6.4 ‣ 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4</span></a>, we can no longer view it as a low-rank
matrix. However, humans still demonstrate remarkable ability to
complete a scene and recognize familiar objects despite severe
occlusion. This suggests that our brain has learned the
low-dimensional distribution of natural images and can use it for
completion, hence recognition. However, the distribution of all
natural images is not as simple as a low-dimensional linear subspace.
Hence a natural question is whether we can learn the more
sophisticated distribution of natural images and use it to perform
image completion?</p>
</div>
<figure class="ltx_figure" id="F4"><img alt="Figure 6.4 : Diagram of the overall (masked) autoencoding process. The (image) token representations are transformed iteratively towards a parsimonious (e.g., compressed and sparse) representation by each encoder layer f ℓ f^{\ell} italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT . Furthermore, such representations are transformed back to the original image by the decoder layers g ℓ g^{\ell} italic_g start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT . Each encoder layer f ℓ f^{\ell} italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT is meant to be (partially) inverted by a corresponding decoder layer g L − ℓ g^{L-\ell} italic_g start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT ." class="ltx_graphics ltx_centering ltx_img_landscape" height="139" id="F4.g1" src="chapters/chapter6/figs/crate_mae_pipeline.png" width="592"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Diagram of the overall (masked)
autoencoding process.<span class="ltx_text ltx_font_medium"> The (image) token representations are
transformed iteratively towards a parsimonious (e.g., compressed
and sparse) representation by each encoder layer <math alttext="f^{\ell}" class="ltx_Math" display="inline" id="F4.m5"><semantics><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">f^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.
Furthermore, such representations are transformed back to the
original image by the decoder layers <math alttext="g^{\ell}" class="ltx_Math" display="inline" id="F4.m6"><semantics><msup><mi>g</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">g^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>. Each encoder
layer <math alttext="f^{\ell}" class="ltx_Math" display="inline" id="F4.m7"><semantics><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">f^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is meant to be (partially) inverted by a
corresponding decoder layer <math alttext="g^{L-\ell}" class="ltx_Math" display="inline" id="F4.m8"><semantics><msup><mi>g</mi><mrow><mi>L</mi><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><annotation encoding="application/x-tex">g^{L-\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">One empirical approach to the image completion task is to find an encoding and decoding scheme by
solving the following <span class="ltx_text ltx_font_italic">masked autoencoding</span> (MAE) program that
minimizes the reconstruction loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{f,g}L_{\mathrm{MAE}}(f,g)\doteq\mathbb{E}\big{[}\|(g\circ f)(\mathcal{P}_{\Omega}(\bm{X}))-\bm{X}\|_{2}^{2}]." class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow></munder><mo lspace="0.167em">⁡</mo><msub><mi>L</mi><mi>MAE</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>,</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{f,g}L_{\mathrm{MAE}}(f,g)\doteq\mathbb{E}\big{[}\|(g\circ f)(\mathcal{P}_{\Omega}(\bm{X}))-\bm{X}\|_{2}^{2}].</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_f , italic_g ) ≐ blackboard_E [ ∥ ( italic_g ∘ italic_f ) ( caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X ) ) - bold_italic_X ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Unlike the matrix completion problem which has a simple underlying
structure, we should no longer expect that the encoding and decoding
mappings admit simple closed forms or the program can be solved by
explicit algorithms.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">For a general natural image, we can no longer assume its columns or
rows are sampled from a low-dimensional subspace or a low-rank
Gaussian. However, it is reasonable to assume that the image consists
of multiple regions. Image patches in each region are similar and can
be modeled as one (low-rank) Gaussian or subspace. Hence, to exploit
the low-dimensionality of the distribution, the objective <span class="ltx_text ltx_font_italic">of the
encoder</span> <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p3.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is to transform <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to a representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS1.p3.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:\bm{X}\mapsto\bm{Z}" class="ltx_Math" display="block" id="S3.E2.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝑿</mi><mo stretchy="false">↦</mo><mi>𝒁</mi></mrow></mrow><annotation encoding="application/x-tex">f:\bm{X}\mapsto\bm{Z}</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_X ↦ bold_italic_Z</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">such that the distribution of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS1.p3.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> can be well modeled as a mixture of subspaces, say <math alttext="\{\bm{U}_{[K]}\}" class="ltx_Math" display="inline" id="S3.SS1.p3.m5"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{U}_{[K]}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT }</annotation></semantics></math>,
such that the rate reduction is maximized while the sparsity is minimized:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}_{\bm{Z}=f(\bm{X})}[\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}]=\mathbb{E}_{\bm{Z}=f(\bm{X})}[R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}]," class="ltx_Math" display="block" id="S3.E3.m1"><semantics><mrow><mrow><mrow><msub><mi>𝔼</mi><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{\bm{Z}=f(\bm{X})}[\Delta R_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}]=\mathbb{E}_{\bm{Z}=f(\bm{X})}[R_{\epsilon}(\bm{Z})-R^{c}_{\epsilon}(\bm{Z}\mid\bm{U}_{[K]})-\lambda\|\bm{Z}\|_{0}],</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_Z = italic_f ( bold_italic_X ) end_POSTSUBSCRIPT [ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] = blackboard_E start_POSTSUBSCRIPT bold_italic_Z = italic_f ( bold_italic_X ) end_POSTSUBSCRIPT [ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the functions <math alttext="R_{\epsilon}(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p3.m6"><semantics><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> and <math alttext="R^{c}_{\epsilon}(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p3.m7"><semantics><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R^{c}_{\epsilon}(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> are defined in (<a class="ltx_ref" href="Ch4.html#S2.E2" title="Equation 4.2.2 ‣ Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>) and (<a class="ltx_ref" href="Ch4.html#S2.E3" title="Equation 4.2.3 ‣ Objective for Learning a Structured and Compact Representation. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>), respectively.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p">As we have shown in the previous Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, the
encoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p4.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> that minimizes the above objective can be constructed by
a sequence of transformer-like operators. As shown in the work of
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx205" title="">PBW+24</a>]</cite>, the decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.p4.m2"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> can be viewed and hence
constructed explicitly as the inverse process of the encoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p4.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>.
Figure <a class="ltx_ref" href="#F5" title="Figure 6.5 ‣ 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.5</span></a> illustrates the overall
architectures of both the encoder and the corresponding decoder at
each layer. The parameters of the encoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p4.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.p4.m5"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> can be
learned by optimizing the reconstruction loss (<a class="ltx_ref" href="#S3.E1" title="Equation 6.3.1 ‣ 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>) via
gradient descent.</p>
</div>
<figure class="ltx_figure" id="F5"><img alt="Figure 6.5 : Diagram of each encoder layer ( top ) and decoder layer ( bottom ). Notice that the two layers are highly anti-parallel — each is constructed to do the operations of the other in reverse order. That is, in the decoder layer g ℓ g^{\ell} italic_g start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , the ISTA \operatorname{ISTA} roman_ISTA block of f L − ℓ f^{L-\ell} italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT is partially inverted first using a linear layer, then the MSSA \operatorname{MSSA} roman_MSSA block of f L − ℓ f^{L-\ell} italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT is reversed; this order unravels the transformation done in f L − ℓ f^{L-\ell} italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT ." class="ltx_graphics ltx_img_landscape" height="99" id="F5.g1" src="chapters/chapter6/figs/crate_mae_layers.png" width="592"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Diagram of each encoder layer
(<span class="ltx_text ltx_font_italic">top</span>) and decoder layer (<span class="ltx_text ltx_font_italic">bottom</span>).<span class="ltx_text ltx_font_medium"> Notice that
the two layers are highly anti-parallel — each is constructed to
do the operations of the other in reverse order. That is, in the
decoder layer <math alttext="g^{\ell}" class="ltx_Math" display="inline" id="F5.m7"><semantics><msup><mi>g</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">g^{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>, the <math alttext="\operatorname{ISTA}" class="ltx_Math" display="inline" id="F5.m8"><semantics><mi>ISTA</mi><annotation encoding="application/x-tex">\operatorname{ISTA}</annotation><annotation encoding="application/x-llamapun">roman_ISTA</annotation></semantics></math> block of <math alttext="f^{L-\ell}" class="ltx_Math" display="inline" id="F5.m9"><semantics><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><annotation encoding="application/x-tex">f^{L-\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>
is partially inverted first using a linear layer, then the
<math alttext="\operatorname{MSSA}" class="ltx_Math" display="inline" id="F5.m10"><semantics><mi>MSSA</mi><annotation encoding="application/x-tex">\operatorname{MSSA}</annotation><annotation encoding="application/x-llamapun">roman_MSSA</annotation></semantics></math> block of <math alttext="f^{L-\ell}" class="ltx_Math" display="inline" id="F5.m11"><semantics><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><annotation encoding="application/x-tex">f^{L-\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> is reversed; this order
unravels the transformation done in <math alttext="f^{L-\ell}" class="ltx_Math" display="inline" id="F5.m12"><semantics><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><annotation encoding="application/x-tex">f^{L-\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUPERSCRIPT italic_L - roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F6" title="Figure 6.6 ‣ 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.6</span></a> shows some representative
results of the so-designed masked auto-encoder. More implementation details and
results of the masked autoencoder for natural image completion can be
found in Chapter <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="F6"><img alt="Figure 6.6 : Autoencoding visualizations of CRATE-Base and ViT-MAE-Base [ HCX+22 ] with 75% patches masked. We observe that the reconstructions from CRATE-Base are on par with the reconstructions from ViT-MAE-Base, despite using &lt; 1 / 3 &lt;1/3 &lt; 1 / 3 of the parameters." class="ltx_graphics" id="F6.g1" src="chapters/chapter6/figs/crate_mae_autoencoding_small.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Autoencoding visualizations of CRATE-Base
and ViT-MAE-Base <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx101" title="">HCX+22</a>]</cite> with 75% patches masked.<span class="ltx_text ltx_font_medium">
We observe that the reconstructions from CRATE-Base are on par
with the reconstructions from ViT-MAE-Base, despite using <math alttext="&lt;1/3" class="ltx_Math" display="inline" id="F6.m2"><semantics><mrow><mi></mi><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></mrow><annotation encoding="application/x-tex">&lt;1/3</annotation><annotation encoding="application/x-llamapun">&lt; 1 / 3</annotation></semantics></math> of the parameters.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3.2 </span>Conditional Sampling with Measurement Matching</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">The above (masked) autoencoding problem aims to generate a sample image that is consistent with certain observations or conditions. But let us examine the approach more closely: Given the
visual part of an image <math alttext="\bm{X}_{v}=\mathcal{P}_{\Omega}(\bm{X})" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><mrow><msub><mi>𝑿</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{v}=\mathcal{P}_{\Omega}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>, we try to
estimate the masked part <math alttext="\bm{X}_{m}=\mathcal{P}_{\Omega^{c}}(\bm{X})" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><mrow><msub><mi>𝑿</mi><mi>m</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><msup><mi mathvariant="normal">Ω</mi><mi>c</mi></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{m}=\mathcal{P}_{\Omega^{c}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_Ω start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_X )</annotation></semantics></math>. For realizations
<math alttext="(\bm{\Xi}_{v},\bm{\Xi}_{m})" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝚵</mi><mi>v</mi></msub><mo>,</mo><msub><mi>𝚵</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{\Xi}_{v},\bm{\Xi}_{m})</annotation><annotation encoding="application/x-llamapun">( bold_Ξ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , bold_Ξ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> of the random variable <math alttext="\bm{X}=(\bm{X}_{v},\bm{X}_{m})" class="ltx_Math" display="inline" id="S3.SS2.p1.m4"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>v</mi></msub><mo>,</mo><msub><mi>𝑿</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=(\bm{X}_{v},\bm{X}_{m})</annotation><annotation encoding="application/x-llamapun">bold_italic_X = ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math>, let</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{X}_{m}\mid\bm{X}_{v}}(\bm{\Xi}_{m}\mid\bm{\Xi}_{v})" class="ltx_Math" display="block" id="S3.Ex1.m1"><semantics><mrow><msub><mi>p</mi><mrow><msub><mi>𝑿</mi><mi>m</mi></msub><mo>∣</mo><msub><mi>𝑿</mi><mi>v</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝚵</mi><mi>m</mi></msub><mo>∣</mo><msub><mi>𝚵</mi><mi>v</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\bm{X}_{m}\mid\bm{X}_{v}}(\bm{\Xi}_{m}\mid\bm{\Xi}_{v})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_Ξ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_Ξ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">be the conditional distribution of <math alttext="\bm{X}_{m}" class="ltx_Math" display="inline" id="S3.SS2.p1.m5"><semantics><msub><mi>𝑿</mi><mi>m</mi></msub><annotation encoding="application/x-tex">\bm{X}_{m}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> given
<math alttext="\bm{X}_{v}" class="ltx_Math" display="inline" id="S3.SS2.p1.m6"><semantics><msub><mi>𝑿</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\bm{X}_{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>. It is easy to show that the optimal solution to the MAE
formulation (<a class="ltx_ref" href="#S3.E1" title="Equation 6.3.1 ‣ 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>) is given by the conditional expectation:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname*{arg\ min}_{h=g\circ f}\,L_{\mathrm{MAE}}(h)=\bm{\Xi}_{v}\mapsto\bm{\Xi}_{v}+\mathbb{E}[\bm{X}_{m}\mid\bm{X}_{v}=\bm{\Xi}_{v}]." class="ltx_Math" display="block" id="S3.E4.m1"><semantics><mrow><mrow><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mi>h</mi><mo>=</mo><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow></mrow></munder><mo lspace="0.337em">⁡</mo><msub><mi>L</mi><mi>MAE</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>𝚵</mi><mi>v</mi></msub><mo stretchy="false">↦</mo><mrow><msub><mi>𝚵</mi><mi>v</mi></msub><mo>+</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mi>𝑿</mi><mi>m</mi></msub><mo>∣</mo><msub><mi>𝑿</mi><mi>v</mi></msub></mrow><mo>=</mo><msub><mi>𝚵</mi><mi>v</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname*{arg\ min}_{h=g\circ f}\,L_{\mathrm{MAE}}(h)=\bm{\Xi}_{v}\mapsto\bm{\Xi}_{v}+\mathbb{E}[\bm{X}_{m}\mid\bm{X}_{v}=\bm{\Xi}_{v}].</annotation><annotation encoding="application/x-llamapun">start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_h = italic_g ∘ italic_f end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_h ) = bold_Ξ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ↦ bold_Ξ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + blackboard_E [ bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = bold_Ξ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In general, however, this expectation may not even be on the
low-dimensional distribution of natural images! This partially
explains why some of the recovered patches in <a class="ltx_ref" href="#F6" title="In 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.6</span></a>
are a little blurry.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">For many practical purposes, we would like to learn (a representation
of) the conditional distribution <math alttext="p_{\bm{X}_{m}\mid\bm{X}_{v}}" class="ltx_Math" display="inline" id="S3.SS2.p2.m1"><semantics><msub><mi>p</mi><mrow><msub><mi>𝑿</mi><mi>m</mi></msub><mo>∣</mo><msub><mi>𝑿</mi><mi>v</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{X}_{m}\mid\bm{X}_{v}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, or equivalently
<math alttext="p_{\bm{X}\mid\bm{X}_{v}}" class="ltx_Math" display="inline" id="S3.SS2.p2.m2"><semantics><msub><mi>p</mi><mrow><mi>𝑿</mi><mo>∣</mo><msub><mi>𝑿</mi><mi>v</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{X}\mid\bm{X}_{v}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_X ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>,
and then get a clear (most likely) sample from this distribution directly. Notice that, when the distribution of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is low-dimensional, it is possible that if a
sufficient part of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, <math alttext="\bm{X}_{v}" class="ltx_Math" display="inline" id="S3.SS2.p2.m5"><semantics><msub><mi>𝑿</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\bm{X}_{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, is observed, it fully determines
<math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> hence the missing part <math alttext="\bm{X}_{m}" class="ltx_Math" display="inline" id="S3.SS2.p2.m7"><semantics><msub><mi>𝑿</mi><mi>m</mi></msub><annotation encoding="application/x-tex">\bm{X}_{m}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math>. In other words, the distribution
<math alttext="p_{\bm{X}\mid\bm{X}_{v}}" class="ltx_Math" display="inline" id="S3.SS2.p2.m8"><semantics><msub><mi>p</mi><mrow><mi>𝑿</mi><mo>∣</mo><msub><mi>𝑿</mi><mi>v</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{X}\mid\bm{X}_{v}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_X ∣ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is a generalized function—if <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p2.m9"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is fully determined by <math alttext="\bm{X}_{v}" class="ltx_Math" display="inline" id="S3.SS2.p2.m10"><semantics><msub><mi>𝑿</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\bm{X}_{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> it is the delta function, and more generally one of its exotic cousins.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">Hence, instead of solving the completion task as a conditional estimation problem, we should address it as a conditional sampling problem. To that end, we should first learn the (low-dimensional) distribution of all natural images <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p3.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. If we have sufficient samples of natural images, we can learn the distribution via a denoising process <math alttext="\bm{X}_{t}" class="ltx_Math" display="inline" id="S3.SS2.p3.m2"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{X}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> described in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>. Then the problem of recovering <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p3.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> from its partial observation <math alttext="\bm{Y}=\mathcal{P}_{\Omega}(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S3.SS2.p3.m4"><semantics><mrow><mi>𝒀</mi><mo>=</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{Y}=\mathcal{P}_{\Omega}(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( bold_italic_x ) + bold_italic_w</annotation></semantics></math> becomes a conditional generation problem – to sample the distribution conditioned on the observation.</p>
</div>
<figure class="ltx_figure" id="F7"><img alt="Figure 6.7 : Sampling visualizations from models trained via ambient diffusion [ DSD+23a ] with 80% of the pixels masked. Using a similar ratio of masked pixels as in Figure 6.6 , the ambient diffusion sampling algorithm recovers a much sharper image than the blurry image recovered by the MAE-based method. The former method samples from the distribution of natural images, while the latter approximates the conditional expectation (i.e., average) of this distribution given the observation; this averaging causes the blurriness." class="ltx_graphics ltx_img_landscape" height="148" id="F7.g1" src="chapters/chapter6/figs/ambient_diffusion.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Sampling visualizations from models trained via ambient diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx66" title="">DSD+23a</a>]</cite> with 80% of the pixels masked.<span class="ltx_text ltx_font_medium"> Using a similar ratio of masked pixels as in <a class="ltx_ref" href="#F6" title="In 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.6</span></a>, the ambient diffusion sampling algorithm recovers a much sharper image than the blurry image recovered by the MAE-based method. The former method samples from the distribution of natural images, while the latter approximates the conditional expectation (i.e., average) of this distribution given the observation; this averaging causes the blurriness.</span></span></figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">General linear measurements.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p">In fact, we may even consider recovering <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> from a more general linear observation model:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Y}=\bm{A}\bm{X}_{0},\quad\bm{X}_{t}=\bm{X}_{0}+\sigma_{t}\bm{G}," class="ltx_Math" display="block" id="S3.E5.m1"><semantics><mrow><mrow><mrow><mi>𝒀</mi><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mn>0</mn></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑮</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Y}=\bm{A}\bm{X}_{0},\quad\bm{X}_{t}=\bm{X}_{0}+\sigma_{t}\bm{G},</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_G ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is a linear operator on matrix space<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>i.e., if we imagine unrolling <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote5.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into a long vector then <math alttext="\bm{A}" class="ltx_Math" display="inline" id="footnote5.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> takes the role of a matrix on <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote5.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>-space.</span></span></span> and <math alttext="\bm{G}\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m3"><semantics><mrow><mi>𝑮</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{G}\sim\mathcal{N}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_G ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>. The masking operator <math alttext="\mathcal{P}_{\Omega}(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m4"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi mathvariant="normal">Ω</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{\Omega}(\cdot)</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> in the image completion task is one example of such a linear model. Then it has been shown by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx65" title="">DSD+23</a>]</cite> that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{X}}_{*}=\operatorname*{arg\ min}_{\hat{\bm{X}}}\mathbb{E}[\|\bm{A}(\hat{\bm{X}}(\bm{A}\bm{X}_{t},\bm{A})-\bm{X}_{0})\|^{2}]" class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>∗</mo></msub><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></munder><mo>⁡</mo><mi>𝔼</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo>,</mo><mi>𝑨</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msub><mi>𝑿</mi><mn>0</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msup><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{*}=\operatorname*{arg\ min}_{\hat{\bm{X}}}\mathbb{E}[\|\bm{A}(\hat{\bm{X}}(\bm{A}\bm{X}_{t},\bm{A})-\bm{X}_{0})\|^{2}]</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over^ start_ARG bold_italic_X end_ARG end_POSTSUBSCRIPT blackboard_E [ ∥ bold_italic_A ( over^ start_ARG bold_italic_X end_ARG ( bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ) - bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">satisfies the condition that:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}\hat{\bm{X}}_{*}(\bm{A}(\bm{X}_{t}),\bm{A})=\bm{A}\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}]." class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>∗</mo></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝑨</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo>,</mo><mi>𝑨</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{A}\hat{\bm{X}}_{*}(\bm{A}(\bm{X}_{t}),\bm{A})=\bm{A}\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}].</annotation><annotation encoding="application/x-llamapun">bold_italic_A over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ( bold_italic_A ( bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , bold_italic_A ) = bold_italic_A blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that in the special case when <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is of full column rank, we have <math alttext="\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}]=\mathbb{E}[\bm{X}_{0}\mid\bm{X}_{t}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m6"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo>,</mo><mi>𝑨</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}]=\mathbb{E}[\bm{X}_{0}\mid\bm{X}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ] = blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math>. Hence, in the more general case, it has been suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx65" title="">DSD+23</a>]</cite> that one could still use the so obtained <math alttext="\mathbb{E}[\bm{X}_{0}\mid\bm{A}(\bm{X}_{t}),\bm{A}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m7"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝑨</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{X}_{0}\mid\bm{A}(\bm{X}_{t}),\bm{A}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_A ( bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , bold_italic_A ]</annotation></semantics></math> to replace the <math alttext="\mathbb{E}[\bm{X}_{0}\mid\bm{X}_{t}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m8"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{X}_{0}\mid\bm{X}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math> in the normal denoising process for <math alttext="\bm{X}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m9"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{X}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}_{t-s}=\gamma_{t}\bm{X}_{t}+(1-\gamma_{t})\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}]." class="ltx_Math" display="block" id="S3.E8.m1"><semantics><mrow><mrow><msub><mi>𝑿</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>γ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>γ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝑿</mi><mn>0</mn></msub><mo>∣</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow><mo>,</mo><mi>𝑨</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}_{t-s}=\gamma_{t}\bm{X}_{t}+(1-\gamma_{t})\mathbb{E}[\bm{X}_{0}\mid\bm{A}\bm{X}_{t},\bm{A}].</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = italic_γ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - italic_γ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∣ bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This usually works very well in practice, say for many image restoration tasks, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx65" title="">DSD+23</a>]</cite>. Compared to the blurry images recovered from MAE, the images recovered by the above method are much sharper as it leverages a learned distribution of natural image and sampls a (sharp) image from the distribution that is consistent with the measurement, as shown in <a class="ltx_ref" href="#F7" title="In 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.7</span></a> (cf <a class="ltx_ref" href="#F6" title="In 6.3.1 Image Completion with Masked Auto-Encoding ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.6</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">General nonlinear measurements.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p">To generalize the above (image) completion problems and make things more rigorous, we may consider that a random vector <math alttext="\bm{x}\sim p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\bm{x}\sim p</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ italic_p</annotation></semantics></math> is partially observed through a more
general observation function:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=h(\bm{x})+\bm{w}," class="ltx_Math" display="block" id="S3.E9.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w},</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m2"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> usually stands for some random measurement noise, say of
a Gaussian distribution <math alttext="\bm{w}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m3"><semantics><mrow><mi>𝒘</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{w}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_w ∼ caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>. It is easy to see that, for so related <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m5"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, their joint distribution <math alttext="p(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m6"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x , bold_italic_y )</annotation></semantics></math> is naturally nearly degenerate if the noise <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m7"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> is small. To a large extent, we may view <math alttext="p(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m8"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x , bold_italic_y )</annotation></semantics></math> as a noisy version of a hypersurface defined by the function <math alttext="\bm{y}=h(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m9"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x )</annotation></semantics></math> in the joint space <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m10"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>. Practically speaking, we will consider a setting more akin to masked autoencoding than to pure matrix completion, where we always have access to a corresponding clean sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> for every observation <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m12"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> we receive.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>In some more specialized applications, in particular in scientific imaging, it is of interest to be able to learn to generate samples
from the posterior <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="footnote6.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> without access to any clean/ground-truth samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="footnote6.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. We give a brief overview of methods for this setting in the
end-of-chapter notes.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p">Like image/matrix completion, we
are often faced with a setting where <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> denotes a degraded or otherwise
“lossy”
observation of the input <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. This can manifest in quite different
forms. For example, in various scientific or
medical imaging problems, the measured data <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> may be a compressed and
corrupted observation of the underlying data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>; whereas in 3D vision tasks,
<math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m5"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> may represent an image captured by a camera of a physical object with an
unknown (low-dimensional) pose <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
Generally, by virtue of mathematical modeling (and, in some cases, co-design
of the measurement system), we know <math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m7"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> and can evaluate it on any input, and
we can exploit this knowledge to help reconstruct and sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F8.sf1"><svg class="ltx_picture" height="20.58" id="F8.sf1.pic1" overflow="visible" version="1.1" width="262.72"><g fill="#000000" stroke="#000000" transform="translate(0,20.58) matrix(1 0 0 -1 0 0) translate(131.36,0) translate(0,10.36)"><g stroke-width="0.4pt" transform="matrix(1.0 0.0 0.0 1.0 -131.36 -10.22)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 10.22)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 12.69)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 0 0) translate(9.91,0) matrix(1.0 0.0 0.0 1.0 -3.95 0)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.91"><math alttext="{\bm{x}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">{\bm{x}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 19.82 0) translate(68.65,0) matrix(1.0 0.0 0.0 1.0 -3.64 0)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.28"><math alttext="{\bm{y}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">{\bm{y}}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 98.07 0) translate(70.69,0) matrix(1.0 0.0 0.0 1.0 -5.68 0)"><foreignobject height="7.63" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="11.35"><math alttext="{\bm{x}^{\mathrm{c}}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m3"><semantics><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup><annotation encoding="application/x-tex">{\bm{x}^{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT</annotation></semantics></math></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 180.4 0) translate(70.69,0) matrix(1.0 0.0 0.0 1.0 -5.68 0)"><foreignobject height="10.01" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="11.35"><math alttext="{\bm{x}^{\mathrm{c}}_{t}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m4"><semantics><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup><annotation encoding="application/x-tex">{\bm{x}^{\mathrm{c}}_{t}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g></g></g></g><g stroke-width="0.39998pt"><path d="M -111.26 -6.76 L -53.31 -6.76" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 -53.03 -6.76)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -98.7 -1.09)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="33.38"><math alttext="\scriptstyle{h(\bm{x})+\bm{w}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m5"><semantics><mrow><mrow><mi mathsize="70%">h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="70%" minsize="70%">(</mo><mi mathsize="70%">𝒙</mi><mo maxsize="70%" minsize="70%">)</mo></mrow></mrow><mo mathsize="70%">+</mo><mi mathsize="70%">𝒘</mi></mrow><annotation encoding="application/x-tex">\scriptstyle{h(\bm{x})+\bm{w}}</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math></foreignobject></g></g><g stroke-width="0.39998pt"><path d="M -33.01 -6.76 L 24.94 -6.76" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 25.22 -6.76)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -9.85 -0.82)"><foreignobject height="6.86" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="12.18"><math alttext="\scriptstyle{p_{\bm{x}\mid\bm{y}}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m6"><semantics><msub><mi mathsize="70%">p</mi><mrow><mi mathsize="71%">𝒙</mi><mo mathsize="71%">∣</mo><mi mathsize="71%">𝒚</mi></mrow></msub><annotation encoding="application/x-tex">\scriptstyle{p_{\bm{x}\mid\bm{y}}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g></g><g stroke-width="0.39998pt"><path d="M 49.31 -6.76 L 107.26 -6.76" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 107.54 -6.76)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 66.57 -1.62)"><foreignobject height="7.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="23.99"><math alttext="\scriptstyle{\bm{x}^{\mathrm{c}}+t\bm{g}}" class="ltx_Math" display="inline" id="F8.sf1.pic1.m7"><semantics><mrow><msup><mi mathsize="70%">𝒙</mi><mi mathsize="71%" mathvariant="normal">c</mi></msup><mo mathsize="70%">+</mo><mrow><mi mathsize="70%">t</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="70%">𝒈</mi></mrow></mrow><annotation encoding="application/x-tex">\scriptstyle{\bm{x}^{\mathrm{c}}+t\bm{g}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT + italic_t bold_italic_g</annotation></semantics></math></foreignobject></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F8.sf2"><svg class="ltx_picture" height="92.7" id="F8.sf2.pic1" overflow="visible" version="1.1" width="101.5"><g fill="#000000" stroke="#000000" transform="translate(0,92.7) matrix(1 0 0 -1 0 0) translate(50.75,0) translate(0,46.35)"><g stroke-width="0.4pt" transform="matrix(1.0 0.0 0.0 1.0 -50.75 -46.35)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 83.465)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 11.02)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 21.54 0) translate(68.65,0) matrix(1.0 0.0 0.0 1.0 -3.64 0)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.28"><math alttext="{\bm{y}}" class="ltx_Math" display="inline" id="F8.sf2.pic1.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">{\bm{y}}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math></foreignobject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 49.48)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 0 0) translate(9.91,0) matrix(1.0 0.0 0.0 1.0 -3.95 0)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.91"><math alttext="{\bm{x}}" class="ltx_Math" display="inline" id="F8.sf2.pic1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">{\bm{x}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math></foreignobject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 85.25)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 19.82 0) translate(70.37,0) matrix(1.0 0.0 0.0 1.0 -5.35 0)"><foreignobject height="8.34" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.71"><math alttext="{\bm{x}_{t}}" class="ltx_Math" display="inline" id="F8.sf2.pic1.m3"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">{\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g></g></g></g><g stroke-width="0.39998pt"><path d="M -30.65 -2.24 L 29.07 26.37" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(0.90182 0.4321 -0.4321 0.90182 29.32 26.49)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -37.18 17.86)"><foreignobject height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="33.38"><math alttext="\scriptstyle{h(\bm{x})+\bm{w}}" class="ltx_Math" display="inline" id="F8.sf2.pic1.m4"><semantics><mrow><mrow><mi mathsize="70%">h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="70%" minsize="70%">(</mo><mi mathsize="70%">𝒙</mi><mo maxsize="70%" minsize="70%">)</mo></mrow></mrow><mo mathsize="70%">+</mo><mi mathsize="70%">𝒘</mi></mrow><annotation encoding="application/x-tex">\scriptstyle{h(\bm{x})+\bm{w}}</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math></foreignobject></g></g><g stroke-width="0.39998pt"><path d="M -30.65 -11.66 L 27.35 -37.51" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(0.9134 -0.40704 0.40704 0.9134 27.6 -37.62)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 1.86 -19.56)"><foreignobject height="7.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="21.53"><math alttext="\scriptstyle{\bm{x}+t\bm{g}}" class="ltx_Math" display="inline" id="F8.sf2.pic1.m5"><semantics><mrow><mi mathsize="70%">𝒙</mi><mo mathsize="70%">+</mo><mrow><mi mathsize="70%">t</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="70%">𝒈</mi></mrow></mrow><annotation encoding="application/x-tex">\scriptstyle{\bm{x}+t\bm{g}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x + italic_t bold_italic_g</annotation></semantics></math></foreignobject></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.8</span>: </span><span class="ltx_text" style="font-size:90%;">Statistical dependency diagrams for the conditional sampling process.
<span class="ltx_text ltx_font_bold">Left:</span> In a direct (conceptual) application of the diffusion-denoising scheme
we have developed in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a> to conditional sampling, we use
samples from the posterior <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="F8.m9"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> to train denoisers directly on
the posterior at different noise levels, then use them to generate new
samples. In practice, however, we do not normally have direct samples from the
posterior, but rather paired samples <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="F8.m10"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math> from the joint.
<span class="ltx_text ltx_font_bold">Right:</span> It turns out that it suffices to have only noisy observations
of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F8.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to realize the denoisers corresponding to <math alttext="p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{x}^{\mathrm{c}}}" class="ltx_Math" display="inline" id="F8.m12"><semantics><msub><mi>p</mi><mrow><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup><mo>∣</mo><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{x}^{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>: this follows from conditional independence of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="F8.m13"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and
<math alttext="\bm{y}" class="ltx_Math" display="inline" id="F8.m14"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F8.m15"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. It implies that <math alttext="p_{\bm{x}^{\mathrm{c}}_{t}}=p_{\bm{x}_{t}\mid\bm{y}}" class="ltx_Math" display="inline" id="F8.m16"><semantics><mrow><msub><mi>p</mi><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup></msub><mo>=</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_{\bm{x}^{\mathrm{c}}_{t}}=p_{\bm{x}_{t}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, which gives a score function for denoising that consists of the
unconditional score function, plus a correction term that enforces measurement
consistency.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p3">
<p class="ltx_p">At a technical level, we want the learned representation of the data
to facilitate us to sample the conditional distribution <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p3.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, also known as the posterior, effectively and efficiently. More precisely,
write <math alttext="\bm{\nu}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p3.m2"><semantics><mi>𝝂</mi><annotation encoding="application/x-tex">\bm{\nu}</annotation><annotation encoding="application/x-llamapun">bold_italic_ν</annotation></semantics></math> to denote a realization of the random variable <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p3.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>.
We want to generate samples <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p3.m4"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> such that:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{y}=\bm{\nu})." class="ltx_math_unparsed" display="block" id="S3.E10.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∼</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{y}=\bm{\nu}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_y = bold_italic_ν ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p4">
<p class="ltx_p">Recall that in <a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we have developed a natural and
effective way to produce <span class="ltx_text ltx_font_italic">unconditional</span> samples of the data distribution
<math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m1"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>. The ingredients are the denoisers <math alttext="\bar{\bm{x}}^{\ast}(t,\bm{\xi})=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m2"><semantics><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\ast}(t,\bm{\xi})=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_t , bold_italic_ξ ) = blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math>, or their learned approximations <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m3"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ )</annotation></semantics></math>,
for different levels of noisy observations <math alttext="\bm{x}_{t}=\bm{x}+t\bm{g}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m4"><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\bm{x}+t\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g</annotation></semantics></math> (and <math alttext="\bm{\xi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m5"><semantics><mi>𝝃</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation><annotation encoding="application/x-llamapun">bold_italic_ξ</annotation></semantics></math> for
their realizations) under
Gaussian noise <math alttext="\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m6"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>, and <math alttext="t\in[0,T]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m7"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation><annotation encoding="application/x-llamapun">italic_t ∈ [ 0 , italic_T ]</annotation></semantics></math> with a choice of times <math alttext="0=t_{1}&lt;\ldots&lt;t_{L}=T" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m8"><semantics><mrow><mn>0</mn><mo>=</mo><msub><mi>t</mi><mn>1</mn></msub><mo>&lt;</mo><mi mathvariant="normal">…</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0=t_{1}&lt;\ldots&lt;t_{L}=T</annotation><annotation encoding="application/x-llamapun">0 = italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &lt; … &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = italic_T</annotation></semantics></math> at which to
perform the iterative denoising, starting from <math alttext="\hat{\bm{x}}_{t_{L}}\sim\mathcal{N}(\mathbf{0},T^{2}\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m9"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi>L</mi></msub></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>T</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{L}}\sim\mathcal{N}(\mathbf{0},T^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> (recall <a class="ltx_ref" href="Ch3.html#S2.E66" title="In 2nd item ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.66</span></a>).<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Recall from our discussion in <a class="ltx_ref" href="Ch3.html#S2.SS2" title="3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>
that a few small improvements to this basic iterative denoising scheme are
sufficient to bring competitive practical performance. For clarity as we develop
conditional sampling, we will focus here on the simplest instantiation.</span></span></span>
We could directly apply this scheme to generate samples from the posterior
<math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m10"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_italic">if</span> we had access to a dataset of samples
<math alttext="\bm{x}^{\mathrm{c}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS0.Px2.p4.m11"><semantics><mrow><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup><mo>∼</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}^{\mathrm{c}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν )</annotation></semantics></math> for each realization
<math alttext="\bm{\nu}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m12"><semantics><mi>𝝂</mi><annotation encoding="application/x-tex">\bm{\nu}</annotation><annotation encoding="application/x-llamapun">bold_italic_ν</annotation></semantics></math> of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m13"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, by generating noisy observations
<math alttext="\bm{x}^{\mathrm{c}}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m14"><semantics><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup><annotation encoding="application/x-tex">\bm{x}^{\mathrm{c}}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and training denoisers to approximate <math alttext="\mathbb{E}[\bm{x}^{\mathrm{c}}\mid\bm{x}^{\mathrm{c}}_{t}=\,\cdot\,,\bm{y}=\bm{\nu}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m15"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup><mo>∣</mo><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup></mrow><mo rspace="0em">=</mo><mo>⋅</mo></mrow><mo>,</mo><mrow><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}^{\mathrm{c}}\mid\bm{x}^{\mathrm{c}}_{t}=\,\cdot\,,\bm{y}=\bm{\nu}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT ∣ bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ⋅ , bold_italic_y = bold_italic_ν ]</annotation></semantics></math>, the mean of the posterior under
the noisy observation (see <a class="ltx_ref" href="#F8" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.8</span></a>(a)).
However, performing this resampling given only paired samples <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m16"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math> from the
joint distribution (say by binning the samples over values of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p4.m17"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>) requires
prohibitively many samples for high-dimensional data, and alternate approaches
explicitly or implicitly rely on density estimation, which similarly suffers from the
curse of dimensionality.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p5">
<p class="ltx_p">Fortunately, it turns out that this is not necessary.
Consider the alternate statistical dependency diagram in
<a class="ltx_ref" href="#F8" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.8</span></a>(b), which corresponds to the random variables
in the usual denoising-diffusion process, together with the measurement <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>.
Because our assumed observation model (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>)
implies that <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> are independent conditioned on <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we have for
any realization <math alttext="\bm{\nu}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m5"><semantics><mi>𝝂</mi><annotation encoding="application/x-tex">\bm{\nu}</annotation><annotation encoding="application/x-llamapun">bold_italic_ν</annotation></semantics></math> of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m6"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})&amp;=\int\underbrace{p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{x}^{\mathrm{c}}}(\,\cdot\,\mid\bm{\xi})}_{=\mathcal{N}(\bm{\xi},t^{2}\bm{I})}\,\cdot\,\underbrace{p_{\bm{x}^{\mathrm{c}}\mid\bm{y}}}_{=p_{\bm{x}\mid\bm{y}}}(\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=\int p_{\bm{x}_{t}\mid\bm{x},\bm{y}}(\,\cdot\,\mid\bm{\xi},\bm{\nu})\,\cdot\,p_{\bm{x}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=\int p_{\bm{x}_{t},\bm{x}\mid\bm{y}}(\,\cdot\,,\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=p_{\bm{x}_{t}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu}).\end{split}" class="ltx_math_unparsed" display="block" id="S3.E11.m1"><semantics><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><msub><mi>p</mi><mrow><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo rspace="0.111em">=</mo><mrow><mo>∫</mo><mrow><mrow><munder><munder accentunder="true"><mrow><msub><mi>p</mi><mrow><msubsup><mi>𝒙</mi><mi>t</mi><mi mathvariant="normal">c</mi></msubsup><mo>∣</mo><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝃</mi><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mrow><mi></mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo>,</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo rspace="0.392em">⋅</mo><munder><munder accentunder="true"><msub><mi>p</mi><mrow><msup><mi>𝒙</mi><mi mathvariant="normal">c</mi></msup><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo>⏟</mo></munder><mrow><mi></mi><mo>=</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub></mrow></munder></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝃</mi><mo>∣</mo><mi>𝝂</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mo rspace="0.111em">=</mo><mo>∫</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝃</mi><mo>,</mo><mi>𝝂</mi><mo rspace="0.225em" stretchy="false">)</mo></mrow><mo rspace="0.392em">⋅</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><mi mathvariant="normal">d</mi><mi>𝝃</mi></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo rspace="0.111em">=</mo><mrow><mo>∫</mo><mrow><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo>,</mo><mrow><mi>𝝃</mi><mo>∣</mo><mi>𝝂</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mo>=</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})&amp;=\int\underbrace{p_{\bm{x}^{\mathrm{c}}_{t}\mid\bm{x}^{\mathrm{c}}}(\,\cdot\,\mid\bm{\xi})}_{=\mathcal{N}(\bm{\xi},t^{2}\bm{I})}\,\cdot\,\underbrace{p_{\bm{x}^{\mathrm{c}}\mid\bm{y}}}_{=p_{\bm{x}\mid\bm{y}}}(\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=\int p_{\bm{x}_{t}\mid\bm{x},\bm{y}}(\,\cdot\,\mid\bm{\xi},\bm{\nu})\,\cdot\,p_{\bm{x}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=\int p_{\bm{x}_{t},\bm{x}\mid\bm{y}}(\,\cdot\,,\bm{\xi}\mid\bm{\nu})\mathrm{d}\bm{\xi}\\
&amp;=p_{\bm{x}_{t}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu}).\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν ) end_CELL start_CELL = ∫ under⏟ start_ARG italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ξ ) end_ARG start_POSTSUBSCRIPT = caligraphic_N ( bold_italic_ξ , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_POSTSUBSCRIPT ⋅ under⏟ start_ARG italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT roman_c end_POSTSUPERSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT = italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ∣ bold_italic_ν ) roman_d bold_italic_ξ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ∫ italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_x , bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ξ , bold_italic_ν ) ⋅ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ξ ∣ bold_italic_ν ) roman_d bold_italic_ξ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ∫ italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ , bold_italic_ξ ∣ bold_italic_ν ) roman_d bold_italic_ξ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν ) . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Above, the first line recognizes an equivalence between the distributions
arising in <a class="ltx_ref" href="#F8" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.8</span></a> (a,b); the second line applies this
together with conditional independence of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m8"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>; the
third line uses the definition of conditional probability; and the final line
marginalizes over <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
Thus, the denoisers from the conceptual posterior sampling process are equal to
<math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\,\cdot\,,\bm{y}=\bm{\nu}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m11"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo rspace="0em">=</mo><mo>⋅</mo></mrow><mo>,</mo><mrow><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\,\cdot\,,\bm{y}=\bm{\nu}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ⋅ , bold_italic_y = bold_italic_ν ]</annotation></semantics></math>, which we can learn solely from paired samples <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m12"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>,
and by Tweedie’s formula (<a class="ltx_ref" href="Ch3.html#Thmtheorem3" title="Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>), we can express these denoisers in
terms of the score function of <math alttext="p_{\bm{x}_{t}\mid\bm{y}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m13"><semantics><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}_{t}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, which, by Bayes’ rule,
satisfies</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{x}_{t}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})=\frac{p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})p_{\bm{x}_{t}}(\bm{\xi})}{p_{\bm{y}}(\bm{\nu})}." class="ltx_Math" display="block" id="S3.E12.m1"><semantics><mrow><mrow><mrow><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝃</mi><mo>∣</mo><mi>𝝂</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>p</mi><mi>𝒚</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{x}_{t}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})=\frac{p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})p_{\bm{x}_{t}}(\bm{\xi})}{p_{\bm{y}}(\bm{\nu})}.</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ξ ∣ bold_italic_ν ) = divide start_ARG italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ν ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Recall that the density of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m14"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is
given by <math alttext="p_{t}=\varphi_{t}\ast p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m15"><semantics><mrow><msub><mi>p</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>φ</mi><mi>t</mi></msub><mo lspace="0.222em" rspace="0.222em">∗</mo><mi>p</mi></mrow></mrow><annotation encoding="application/x-tex">p_{t}=\varphi_{t}\ast p</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∗ italic_p</annotation></semantics></math>, where <math alttext="\varphi_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m16"><semantics><msub><mi>φ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\varphi_{t}</annotation><annotation encoding="application/x-llamapun">italic_φ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the standard
Gaussian density with zero mean and covariance <math alttext="t^{2}\bm{I}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m17"><semantics><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">t^{2}\bm{I}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I</annotation></semantics></math> and <math alttext="\ast" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m18"><semantics><mo>∗</mo><annotation encoding="application/x-tex">\ast</annotation><annotation encoding="application/x-llamapun">∗</annotation></semantics></math> denotes
convolution. This is nothing but the <span class="ltx_text ltx_font_italic">unconditional score function</span>
obtained from the standard diffusion training that we developed in
<a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>!
The conditional score function then satisfies, for any realization <math alttext="(\bm{\xi},\bm{\nu})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m19"><semantics><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo>,</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{\xi},\bm{\nu})</annotation><annotation encoding="application/x-llamapun">( bold_italic_ξ , bold_italic_ν )</annotation></semantics></math> of <math alttext="(\bm{x}_{t},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m20"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_y )</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})=\underbrace{\nabla_{\bm{\xi}}\log p_{t}(\bm{\xi})}_{\text{score matching}}+\underbrace{\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})}_{\text{measurement matching}}," class="ltx_Math" display="block" id="S3.E13.m1"><semantics><mrow><mrow><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝃</mi><mo>∣</mo><mi>𝝂</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>score matching</mtext></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>measurement matching</mtext></munder></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}\mid\bm{y}}(\bm{\xi}\mid\bm{\nu})=\underbrace{\nabla_{\bm{\xi}}\log p_{t}(\bm{\xi})}_{\text{score matching}}+\underbrace{\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})}_{\text{measurement matching}},</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ξ ∣ bold_italic_ν ) = under⏟ start_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) end_ARG start_POSTSUBSCRIPT score matching end_POSTSUBSCRIPT + under⏟ start_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) end_ARG start_POSTSUBSCRIPT measurement matching end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">giving (by Tweedie’s formula) our proposed denoisers as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx80">
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]" class="ltx_Math" display="inline" id="S3.Ex2.m1"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo>,</mo><mrow><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , bold_italic_y = bold_italic_ν ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{\xi}+t^{2}\nabla_{\bm{\xi}}\log p_{t}(\bm{\xi})+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})" class="ltx_Math" display="inline" id="S3.Ex2.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>𝝃</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{\xi}+t^{2}\nabla_{\bm{\xi}}\log p_{t}(\bm{\xi})+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})</annotation><annotation encoding="application/x-llamapun">= bold_italic_ξ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ξ ) + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})." class="ltx_Math" display="inline" id="S3.Ex3.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi}).</annotation><annotation encoding="application/x-llamapun">= blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The resulting operators are interpretable as a <span class="ltx_text ltx_font_italic">corrected</span> version of the
unconditional denoiser for the noisy observation, where the correction term (the
so-called “measurement matching” term) enforces consistency with the
observations <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p5.m21"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>. The reader should take care to note to which argument the
gradient operators are applying in the above score functions in order to fully
grasp the meaning of this operator.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p6">
<p class="ltx_p">The key remaining issue in making this procedure computational is to prescribe
how to compute the measurement matching correction, since in general we do not
have a closed-form
expression for the likelihood <math alttext="p_{\bm{y}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p6.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> except for when <math alttext="t=0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p6.m2"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation><annotation encoding="application/x-llamapun">italic_t = 0</annotation></semantics></math>. Before taking up this problem, we discuss an illustrative concrete example
of the entire process, continuing from those we have developed in
<a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 6.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample2.p1">
<p class="ltx_p">Consider the case where the data distribution is Gaussian with mean <math alttext="\bm{\mu}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexample2.p1.m1"><semantics><mrow><mi>𝝁</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\mu}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and
covariance <math alttext="\bm{\Sigma}\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="Thmexample2.p1.m2"><semantics><mrow><mi>𝚺</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Sigma}\in\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun">bold_Σ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="\bm{x}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample2.p1.m3"><semantics><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝁</mi><mo>,</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ caligraphic_N ( bold_italic_μ , bold_Σ )</annotation></semantics></math>.
Assume that <math alttext="\bm{\Sigma}\succeq\mathbf{0}" class="ltx_Math" display="inline" id="Thmexample2.p1.m4"><semantics><mrow><mi>𝚺</mi><mo>⪰</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{\Sigma}\succeq\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_Σ ⪰ bold_0</annotation></semantics></math> is nonzero.
Moreover, in the measurement model (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>),
suppose we obtain linear measurements of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with independent Gaussian
noise, where <math alttext="\bm{A}\in\mathbb{R}^{d\times D}" class="ltx_Math" display="inline" id="Thmexample2.p1.m6"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{d\times D}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}=\bm{A}\bm{x}+\sigma\bm{w}" class="ltx_Math" display="inline" id="Thmexample2.p1.m7"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>+</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒘</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=\bm{A}\bm{x}+\sigma\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = bold_italic_A bold_italic_x + italic_σ bold_italic_w</annotation></semantics></math>
with <math alttext="\bm{w}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="Thmexample2.p1.m8"><semantics><mrow><mi>𝒘</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{w}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_w ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> independent of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
Then <math alttext="\bm{x}=_{d}\bm{\Sigma}^{1/2}\bm{g}+\bm{\mu}" class="ltx_Math" display="inline" id="Thmexample2.p1.m10"><semantics><mrow><mi>𝒙</mi><msub><mo>=</mo><mi>d</mi></msub><mrow><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow><mo>+</mo><mi>𝝁</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=_{d}\bm{\Sigma}^{1/2}\bm{g}+\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT bold_italic_g + bold_italic_μ</annotation></semantics></math>, where <math alttext="\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="Thmexample2.p1.m11"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> is
independent of <math alttext="\bm{w}" class="ltx_Math" display="inline" id="Thmexample2.p1.m12"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> and <math alttext="\bm{\Sigma}^{1/2}" class="ltx_Math" display="inline" id="Thmexample2.p1.m13"><semantics><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\bm{\Sigma}^{1/2}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math> is the unique positive square root of
the covariance matrix <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p1.m14"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math>, and
after some algebra, we can then write</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{x}\\
\bm{y}\end{bmatrix}=_{d}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{g}\\
\bm{w}\end{bmatrix}+\begin{bmatrix}\bm{\mu}\\
\bm{A}\bm{\mu}\end{bmatrix}." class="ltx_Math" display="block" id="S3.Ex4.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒙</mi></mtd></mtr><mtr><mtd><mi>𝒚</mi></mtd></mtr></mtable><mo>]</mo></mrow><msub><mo>=</mo><mi>d</mi></msub><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒈</mi></mtd></mtr><mtr><mtd><mi>𝒘</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝝁</mi></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝁</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{x}\\
\bm{y}\end{bmatrix}=_{d}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{g}\\
\bm{w}\end{bmatrix}+\begin{bmatrix}\bm{\mu}\\
\bm{A}\bm{\mu}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_y end_CELL end_ROW end_ARG ] = start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_g end_CELL end_ROW start_ROW start_CELL bold_italic_w end_CELL end_ROW end_ARG ] + [ start_ARG start_ROW start_CELL bold_italic_μ end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_italic_μ end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">By independence, we have that <math alttext="(\bm{g},\bm{w})" class="ltx_Math" display="inline" id="Thmexample2.p1.m15"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo>,</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{g},\bm{w})</annotation><annotation encoding="application/x-llamapun">( bold_italic_g , bold_italic_w )</annotation></semantics></math> is jointly Gaussian, which means that
<math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="Thmexample2.p1.m16"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math> is also jointly Gaussian, as the affine image of a jointly Gaussian
vector.
Its covariance matrix is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}^{\top}=\begin{bmatrix}\bm{\Sigma}&amp;\bm{\Sigma}\bm{A}^{\top}\\
\bm{A}\bm{\Sigma}&amp;\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\end{bmatrix}." class="ltx_Math" display="block" id="S3.Ex5.m1"><semantics><mrow><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>⊤</mo></msup></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝚺</mi></mtd><mtd><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi></mrow></mtd><mtd><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\sigma\bm{I}\end{bmatrix}^{\top}=\begin{bmatrix}\bm{\Sigma}&amp;\bm{\Sigma}\bm{A}^{\top}\\
\bm{A}\bm{\Sigma}&amp;\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_Σ end_CELL start_CELL bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ end_CELL start_CELL bold_italic_A bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, we apply the fact that conditioning a random vector with joint Gaussian
distribution on a subset of coordinates is again a Gaussian distribution
(<a class="ltx_ref" href="Ch3.html#Thmexercise2" title="Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).
By this, we obtain that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})=\mathcal{N}\left(\underbrace{\bm{\mu}+\bm{\Sigma}\bm{A}^{\top}\left(\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\right)^{-1}(\bm{\nu}-\bm{A}\bm{\mu})}_{\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})},\underbrace{\bm{\Sigma}-\bm{\Sigma}\bm{A}^{\top}\left(\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\right)^{-1}\bm{A}\bm{\Sigma}}_{\bm{\Sigma}_{\bm{x}\mid\bm{y}}}\right)." class="ltx_math_unparsed" display="block" id="S3.E15.m1"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mrow><mo>(</mo><munder><munder accentunder="true"><mrow><mi>𝝁</mi><mo>+</mo><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝁</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msub><mi>𝝁</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow></munder><mo>,</mo><munder><munder accentunder="true"><mrow><mi>𝚺</mi><mo>−</mo><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi></mrow></mrow><mo>⏟</mo></munder><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub></munder><mo>)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})=\mathcal{N}\left(\underbrace{\bm{\mu}+\bm{\Sigma}\bm{A}^{\top}\left(\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\right)^{-1}(\bm{\nu}-\bm{A}\bm{\mu})}_{\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})},\underbrace{\bm{\Sigma}-\bm{\Sigma}\bm{A}^{\top}\left(\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\right)^{-1}\bm{A}\bm{\Sigma}}_{\bm{\Sigma}_{\bm{x}\mid\bm{y}}}\right).</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν ) = caligraphic_N ( under⏟ start_ARG bold_italic_μ + bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ν - bold_italic_A bold_italic_μ ) end_ARG start_POSTSUBSCRIPT bold_italic_μ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ν ) end_POSTSUBSCRIPT , under⏟ start_ARG bold_Σ - bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_A bold_Σ end_ARG start_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By the equivalence we have derived above, we get by another application of
<a class="ltx_ref" href="Ch3.html#Thmexercise2" title="Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.2</span></a></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]=\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})+\bm{\Sigma}_{\bm{x}\mid\bm{y}}\left(\bm{\Sigma}_{\bm{x}\mid\bm{y}}+t^{2}\bm{I}\right)^{-1}\left(\bm{\xi}-\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})\right)." class="ltx_Math" display="block" id="S3.E16.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo>,</mo><mrow><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>𝝁</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><msub><mi>𝚺</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝝃</mi><mo>−</mo><mrow><msub><mi>𝝁</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]=\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})+\bm{\Sigma}_{\bm{x}\mid\bm{y}}\left(\bm{\Sigma}_{\bm{x}\mid\bm{y}}+t^{2}\bm{I}\right)^{-1}\left(\bm{\xi}-\bm{\mu}_{\bm{x}\mid\bm{y}}(\bm{\nu})\right).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , bold_italic_y = bold_italic_ν ] = bold_italic_μ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ν ) + bold_Σ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_Σ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ξ - bold_italic_μ start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ν ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The functional form of this denoiser is quite simple, but it carries an
unwieldy dependence on the problem data <math alttext="\bm{\mu}" class="ltx_Math" display="inline" id="Thmexample2.p1.m17"><semantics><mi>𝝁</mi><annotation encoding="application/x-tex">\bm{\mu}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ</annotation></semantics></math>, <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p1.m18"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math>, <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexample2.p1.m19"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, and
<math alttext="\sigma^{2}" class="ltx_Math" display="inline" id="Thmexample2.p1.m20"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^{2}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
We can gain further insight into its behavior by comparing it with
<a class="ltx_ref" href="#S3.Ex3" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.14</span></a>.
We have as usual</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]=\bm{\mu}+\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}(\bm{\xi}-\bm{\mu})," class="ltx_Math" display="block" id="S3.E17.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝝁</mi><mo>+</mo><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝃</mi><mo>−</mo><mi>𝝁</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]=\bm{\mu}+\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}(\bm{\xi}-\bm{\mu}),</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] = bold_italic_μ + bold_Σ ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ξ - bold_italic_μ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is rather simple—suggesting that the measurement matching term is
rather complicated. To confirm this, we can calculate the likelihood <math alttext="p_{\bm{y}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m21"><semantics><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> directly using the following expression for the joint
distribution of <math alttext="(\bm{x}_{t},\bm{y})" class="ltx_Math" display="inline" id="Thmexample2.p1.m22"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_y )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\\
\bm{y}\end{bmatrix}=_{d}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\mathbf{0}\\
\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{g}\\
\bm{g}^{\prime}\\
\bm{w}\end{bmatrix}+\begin{bmatrix}\bm{\mu}\\
\bm{\mu}\\
\bm{A}\bm{\mu}\end{bmatrix}," class="ltx_Math" display="block" id="S3.E18.m1"><semantics><mrow><mrow><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒙</mi></mtd></mtr><mtr><mtd><msub><mi>𝒙</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd><mi>𝒚</mi></mtd></mtr></mtable><mo>]</mo></mrow><msub><mo>=</mo><mi>d</mi></msub><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mn>𝟎</mn></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mn>𝟎</mn></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝒈</mi></mtd></mtr><mtr><mtd><msup><mi>𝒈</mi><mo>′</mo></msup></mtd></mtr><mtr><mtd><mi>𝒘</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mi>𝝁</mi></mtd></mtr><mtr><mtd><mi>𝝁</mi></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝁</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{x}\\
\bm{x}_{t}\\
\bm{y}\end{bmatrix}=_{d}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\mathbf{0}\\
\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{g}\\
\bm{g}^{\prime}\\
\bm{w}\end{bmatrix}+\begin{bmatrix}\bm{\mu}\\
\bm{\mu}\\
\bm{A}\bm{\mu}\end{bmatrix},</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_y end_CELL end_ROW end_ARG ] = start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_t bold_italic_I end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_italic_g end_CELL end_ROW start_ROW start_CELL bold_italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_w end_CELL end_ROW end_ARG ] + [ start_ARG start_ROW start_CELL bold_italic_μ end_CELL end_ROW start_ROW start_CELL bold_italic_μ end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_italic_μ end_CELL end_ROW end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{g}^{\prime}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="Thmexample2.p1.m23"><semantics><mrow><msup><mi>𝒈</mi><mo>′</mo></msup><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}^{\prime}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> independent of the other Gaussians.
This is again a jointly Gaussian distribution; restricting to only the final
two rows, we have the covariance</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}^{\top}=\begin{bmatrix}\bm{\Sigma}+t^{2}\bm{I}&amp;\bm{\Sigma}\bm{A}^{\top}\\
\bm{A}\bm{\Sigma}&amp;\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\end{bmatrix}." class="ltx_Math" display="block" id="S3.Ex6.m1"><semantics><mrow><mrow><mrow><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mn>𝟎</mn></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mtd><mtd><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd><mtd><mn>𝟎</mn></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mn>𝟎</mn></mtd><mtd><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>⊤</mo></msup></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow></mtd><mtd><mrow><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow></mtd></mtr><mtr><mtd><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi></mrow></mtd><mtd><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}\begin{bmatrix}\bm{\Sigma}^{1/2}&amp;t\bm{I}&amp;\mathbf{0}\\
\bm{A}\bm{\Sigma}^{1/2}&amp;\mathbf{0}&amp;\sigma\bm{I}\end{bmatrix}^{\top}=\begin{bmatrix}\bm{\Sigma}+t^{2}\bm{I}&amp;\bm{\Sigma}\bm{A}^{\top}\\
\bm{A}\bm{\Sigma}&amp;\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">[ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_t bold_italic_I end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL italic_t bold_italic_I end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL start_CELL italic_σ bold_italic_I end_CELL end_ROW end_ARG ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I end_CELL start_CELL bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_A bold_Σ end_CELL start_CELL bold_italic_A bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Another application of <a class="ltx_ref" href="Ch3.html#Thmexercise2" title="Exercise 3.2. ‣ 3.6 Exercises and Extensions ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> then gives us</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{y}\mid\bm{x}_{t}}(\,\cdot\,\mid\bm{\xi})=\mathcal{N}\left(\underbrace{\bm{A}\bm{\mu}+\bm{A}\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\left(\bm{\xi}-\bm{\mu}\right)}_{\bm{\mu}_{\bm{y}\mid\bm{x}_{t}}(\bm{\xi})},\underbrace{\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}-\bm{A}\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}\bm{A}^{\top}}_{\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}}\right)." class="ltx_math_unparsed" display="block" id="S3.E19.m1"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mrow><mo>(</mo><munder><munder accentunder="true"><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝁</mi></mrow><mo>+</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝝃</mi><mo>−</mo><mi>𝝁</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msub><mi>𝝁</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></munder><mo>,</mo><munder><munder accentunder="true"><mrow><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow></mrow><mo>⏟</mo></munder><msub><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></munder><mo>)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}_{t}}(\,\cdot\,\mid\bm{\xi})=\mathcal{N}\left(\underbrace{\bm{A}\bm{\mu}+\bm{A}\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\left(\bm{\xi}-\bm{\mu}\right)}_{\bm{\mu}_{\bm{y}\mid\bm{x}_{t}}(\bm{\xi})},\underbrace{\bm{A}\bm{\Sigma}\bm{A}^{\top}+\sigma^{2}\bm{I}-\bm{A}\bm{\Sigma}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}\bm{A}^{\top}}_{\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}}\right).</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ξ ) = caligraphic_N ( under⏟ start_ARG bold_italic_A bold_italic_μ + bold_italic_A bold_Σ ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ξ - bold_italic_μ ) end_ARG start_POSTSUBSCRIPT bold_italic_μ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) end_POSTSUBSCRIPT , under⏟ start_ARG bold_italic_A bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I - bold_italic_A bold_Σ ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now notice that <math alttext="\bm{\mu}_{\bm{y}\mid\bm{x}_{t}}(\bm{\xi})=\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="Thmexample2.p1.m24"><semantics><mrow><mrow><msub><mi>𝝁</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\mu}_{\bm{y}\mid\bm{x}_{t}}(\bm{\xi})=\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">bold_italic_μ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) = bold_italic_A blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math>.
So, by the chain rule,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx81">
<tbody id="S3.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})" class="ltx_Math" display="inline" id="S3.Ex7.m1"><semantics><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=t^{2}\nabla_{\bm{\xi}}\left[-\frac{1}{2}(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])^{\top}\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}^{-1}(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])\right]" class="ltx_Math" display="inline" id="S3.Ex7.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo>∇</mo><mi>𝝃</mi></msub><mrow><mo>[</mo><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=t^{2}\nabla_{\bm{\xi}}\left[-\frac{1}{2}(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])^{\top}\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}^{-1}(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])\right]</annotation><annotation encoding="application/x-llamapun">= italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT [ - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_italic_ν - bold_italic_A blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ν - bold_italic_A blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=t^{2}(\bm{\Sigma}+t^{2}\bm{I})^{-1}\bm{\Sigma}\bm{A}^{\top}\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}^{-1}\left(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]\right)." class="ltx_Math" display="inline" id="S3.Ex8.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝝂</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=t^{2}(\bm{\Sigma}+t^{2}\bm{I})^{-1}\bm{\Sigma}\bm{A}^{\top}\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}^{-1}\left(\bm{\nu}-\bm{A}\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]\right).</annotation><annotation encoding="application/x-llamapun">= italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_ν - bold_italic_A blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This gives us a more interpretable decomposition of the conditional posterior
denoiser (<a class="ltx_ref" href="#S3.E16" title="Equation 6.3.16 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.16</span></a>): following
<a class="ltx_ref" href="#S3.Ex3" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.14</span></a>, it is the sum of the
unconditional posterior denoiser (<a class="ltx_ref" href="#S3.E17" title="Equation 6.3.17 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.17</span></a>)
and the measurement matching term
(<a class="ltx_ref" href="#S3.Ex8" title="Equation 6.3.20 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.20</span></a>).
We can further analyze the measurement matching term. Notice that</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}=\sigma^{2}\bm{I}+\bm{A}\bm{\Sigma}^{1/2}\left(\bm{I}-\bm{\Sigma}^{1/2}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}^{1/2}\right)\bm{\Sigma}\bm{A}^{\top}." class="ltx_Math" display="block" id="S3.E21.m1"><semantics><mrow><mrow><msub><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><mrow><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo>+</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}=\sigma^{2}\bm{I}+\bm{A}\bm{\Sigma}^{1/2}\left(\bm{I}-\bm{\Sigma}^{1/2}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}^{1/2}\right)\bm{\Sigma}\bm{A}^{\top}.</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I + bold_italic_A bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_I - bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ) bold_Σ bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we let <math alttext="\bm{\Sigma}=\bm{V}\bm{\Lambda}\bm{V}^{\top}" class="ltx_Math" display="inline" id="Thmexample2.p1.m25"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{\Sigma}=\bm{V}\bm{\Lambda}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_Σ = bold_italic_V bold_Λ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> denote an eigenvalue decomposition
of <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p1.m26"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math>, where <math alttext="(\bm{v}_{i})" class="ltx_Math" display="inline" id="Thmexample2.p1.m27"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒗</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{v}_{i})</annotation><annotation encoding="application/x-llamapun">( bold_italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> are the columns of <math alttext="\bm{V}" class="ltx_Math" display="inline" id="Thmexample2.p1.m28"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math>, we can further write</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx82">
<tbody id="S3.E22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{\Sigma}^{1/2}\left(\bm{I}-\bm{\Sigma}^{1/2}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}^{1/2}\right)\bm{\Sigma}^{1/2}" class="ltx_Math" display="inline" id="S3.E22.m1"><semantics><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚺</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚺</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\displaystyle\bm{\Sigma}^{1/2}\left(\bm{I}-\bm{\Sigma}^{1/2}\left(\bm{\Sigma}+t^{2}\bm{I}\right)^{-1}\bm{\Sigma}^{1/2}\right)\bm{\Sigma}^{1/2}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_I - bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_Σ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ) bold_Σ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=t^{2}\bm{V}\bm{\Lambda}^{1/2}\left(\bm{\Lambda}+t^{2}\bm{I}\right)^{-1}\bm{\Lambda}^{1/2}\bm{V}^{\top}" class="ltx_Math" display="inline" id="S3.E22.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚲</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mi>𝚲</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚲</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=t^{2}\bm{V}\bm{\Lambda}^{1/2}\left(\bm{\Lambda}+t^{2}\bm{I}\right)^{-1}\bm{\Lambda}^{1/2}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">= italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_V bold_Λ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_Λ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Λ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.22)</span></td>
</tr></tbody>
<tbody id="S3.E23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=t^{2}\sum_{i=1}^{D}\frac{\lambda_{i}}{\lambda_{i}+t^{2}}\bm{v}_{i}\bm{v}_{i}^{\ast}." class="ltx_Math" display="inline" id="S3.E23.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><msub><mi>λ</mi><mi>i</mi></msub><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒗</mi><mi>i</mi><mo>∗</mo></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=t^{2}\sum_{i=1}^{D}\frac{\lambda_{i}}{\lambda_{i}+t^{2}}\bm{v}_{i}\bm{v}_{i}^{\ast}.</annotation><annotation encoding="application/x-llamapun">= italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT divide start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then for any eigenvalue of <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p1.m29"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math> equal to zero, the corresponding summand
is zero; and
writing <math alttext="\lambda_{\min}(\bm{\Sigma})" class="ltx_Math" display="inline" id="Thmexample2.p1.m30"><semantics><mrow><msub><mi>λ</mi><mi>min</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda_{\min}(\bm{\Sigma})</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_Σ )</annotation></semantics></math> for the smallest positive eigenvalue of
<math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample2.p1.m31"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math> (it has at least one positive eigenvalue, by assumption), we have
(in a sense that can be made quantitatively precise) that whenever <math alttext="t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}" class="ltx_Math" display="inline" id="Thmexample2.p1.m32"><semantics><mrow><mi>t</mi><mo>≪</mo><msqrt><mrow><msub><mi>λ</mi><mi>min</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></msqrt></mrow><annotation encoding="application/x-tex">t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}</annotation><annotation encoding="application/x-llamapun">italic_t ≪ square-root start_ARG italic_λ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_Σ ) end_ARG</annotation></semantics></math>, it holds</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\lambda_{i}t^{2}}{\lambda_{i}+t^{2}}\approx 0." class="ltx_Math" display="block" id="S3.E24.m1"><semantics><mrow><mrow><mfrac><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo>≈</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\frac{\lambda_{i}t^{2}}{\lambda_{i}+t^{2}}\approx 0.</annotation><annotation encoding="application/x-llamapun">divide start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≈ 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">So, when <math alttext="t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}" class="ltx_Math" display="inline" id="Thmexample2.p1.m33"><semantics><mrow><mi>t</mi><mo>≪</mo><msqrt><mrow><msub><mi>λ</mi><mi>min</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></msqrt></mrow><annotation encoding="application/x-tex">t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}</annotation><annotation encoding="application/x-llamapun">italic_t ≪ square-root start_ARG italic_λ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_Σ ) end_ARG</annotation></semantics></math>, we have the approximation</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}\approx\sigma^{2}\bm{I}." class="ltx_Math" display="block" id="S3.E25.m1"><semantics><mrow><mrow><msub><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo>≈</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{\Sigma}_{\bm{y}\mid\bm{x}_{t}}\approx\sigma^{2}\bm{I}.</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≈ italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The righthand side of this approximation is equal to <math alttext="\bm{\Sigma}_{\bm{y}\mid\bm{x}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m34"><semantics><msub><mi>𝚺</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><annotation encoding="application/x-tex">\bm{\Sigma}_{\bm{y}\mid\bm{x}}</annotation><annotation encoding="application/x-llamapun">bold_Σ start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>.
So we have in turn</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})\approx\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])." class="ltx_math_unparsed" display="block" id="S3.E26.m1"><semantics><mrow><msub><mo>∇</mo><mi>𝝃</mi></msub><mi>log</mi><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo>≈</mo><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝔼</mi><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mi>𝝃</mi><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})\approx\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]).</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) ≈ ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ν ∣ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.26)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig1"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g1" src="chapters/chapter6/figs/samples_step_000_t_100_largenoise.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig2"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g2" src="chapters/chapter6/figs/samples_step_050_t_50_largenoise.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig3"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g3" src="chapters/chapter6/figs/samples_step_100_t_0_largenoise.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig4"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g4" src="chapters/chapter6/figs/samples_step_000_t_100_smallnoise.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig5"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g5" src="chapters/chapter6/figs/samples_step_010_t_90_smallnoise.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.fig6"><img alt="Figure 6.9 : Numerical simulation of the conditional sampling setup ( 6.3.9 ), with Gaussian data, linear measurements, and Gaussian noise. We simulate D = 2 D=2 italic_D = 2 and d = 1 d=1 italic_d = 1 , with 𝚺 = 𝒆 1 ​ 𝒆 1 ⊤ + 1 4 ​ 𝒆 2 ​ 𝒆 2 ⊤ \bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top} bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , 𝝁 = 𝟎 \bm{\mu}=\mathbf{0} bold_italic_μ = bold_0 , and 𝑨 = 𝒆 1 ⊤ \bm{A}=\bm{e}_{1}^{\top} bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT . The underlying signal 𝒙 \bm{x} bold_italic_x is marked with a black star, and the measurement 𝒚 \bm{y} bold_italic_y is marked with a black circle. Each individual plot corresponds to a different value of sampler time t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , with different rows corresponding to different observation noise levels σ 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . In each plot, the covariance matrix of 𝒙 \bm{x} bold_italic_x is plotted in gray, the posterior covariance matrix and posterior mean of p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT are plotted in blue (with the posterior mean marked by a blue “x”), and contours for p 𝒙 t ℓ ∣ 𝒚 p_{\bm{x}_{t_{\ell}}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT are drawn in green. The sampler hyperparameters are T = 1 T=1 italic_T = 1 , L = 100 L=100 italic_L = 100 , and we draw 100 100 100 independent samples to initialize the samplers. Samplers are implemented with the closed-form denoisers derived in Example 6.2 , with those using the approximation ( 6.3.26 ) marked with red triangles, and those using the exact conditional posterior denoiser marked with blue circles. Top: For large observation noise σ = 0.5 \sigma=0.5 italic_σ = 0.5 , both the exact conditional posterior denoiser and the approximate one do a good job of converging to the posterior p 𝒙 ∣ 𝒚 p_{\bm{x}\mid\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT . Sampling time (corresponding to time in the “forward process”, so larger times mean larger noise) decreases from left to right. The convergence dynamics for the exact and approximate measurement matching term are similar. Bottom: For smaller observation noise σ = 0.1 \sigma=0.1 italic_σ = 0.1 , the approximate measurement matching term leads to extreme bias in the sampler (red triangles): samples rapidly converge to an affine subspace of points that are consistent, modulo some shrinkage from the posterior mean denoiser, with the measured ground truth, and later sampling iterations are unable to recover the lost posterior variance along this dimension. Note that different times t ℓ t_{\ell} italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT are plotted in the bottom row, compared to the top row, to show the rapid collapse of the approximation to the posterior along the measurement dimension." class="ltx_graphics ltx_img_landscape" height="479" id="F9.g6" src="chapters/chapter6/figs/samples_step_100_t_0_smallnoise.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.9</span>: </span><span class="ltx_text" style="font-size:90%;">Numerical simulation of the conditional sampling setup
(<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>), with Gaussian data, linear
measurements, and Gaussian noise. We simulate <math alttext="D=2" class="ltx_Math" display="inline" id="F9.m20"><semantics><mrow><mi>D</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">D=2</annotation><annotation encoding="application/x-llamapun">italic_D = 2</annotation></semantics></math> and <math alttext="d=1" class="ltx_Math" display="inline" id="F9.m21"><semantics><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d=1</annotation><annotation encoding="application/x-llamapun">italic_d = 1</annotation></semantics></math>, with <math alttext="\bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top}" class="ltx_Math" display="inline" id="F9.m22"><semantics><mrow><mi>𝚺</mi><mo>=</mo><mrow><mrow><msub><mi>𝒆</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒆</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒆</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒆</mi><mn>2</mn><mo>⊤</mo></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\Sigma}=\bm{e}_{1}\bm{e}_{1}^{\top}+\tfrac{1}{4}\bm{e}_{2}\bm{e}_{2}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_Σ = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 end_ARG bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\bm{\mu}=\mathbf{0}" class="ltx_Math" display="inline" id="F9.m23"><semantics><mrow><mi>𝝁</mi><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{\mu}=\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_μ = bold_0</annotation></semantics></math>, and
<math alttext="\bm{A}=\bm{e}_{1}^{\top}" class="ltx_Math" display="inline" id="F9.m24"><semantics><mrow><mi>𝑨</mi><mo>=</mo><msubsup><mi>𝒆</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow><annotation encoding="application/x-tex">\bm{A}=\bm{e}_{1}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_A = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>. The underlying signal <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F9.m25"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is marked with a black star,
and the measurement <math alttext="\bm{y}" class="ltx_Math" display="inline" id="F9.m26"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> is marked with a black circle.
Each individual plot corresponds to a different value of sampler time
<math alttext="t_{\ell}" class="ltx_Math" display="inline" id="F9.m27"><semantics><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">t_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math>, with different rows corresponding to different observation noise
levels <math alttext="\sigma^{2}" class="ltx_Math" display="inline" id="F9.m28"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^{2}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
In each plot, the covariance matrix of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F9.m29"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is plotted in gray, the
posterior covariance matrix and posterior mean of <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="F9.m30"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> are
plotted in blue (with the posterior mean marked by a blue “x”), and
contours for <math alttext="p_{\bm{x}_{t_{\ell}}\mid\bm{y}}" class="ltx_Math" display="inline" id="F9.m31"><semantics><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}_{t_{\ell}}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> are drawn in green. The sampler
hyperparameters are <math alttext="T=1" class="ltx_Math" display="inline" id="F9.m32"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=1</annotation><annotation encoding="application/x-llamapun">italic_T = 1</annotation></semantics></math>, <math alttext="L=100" class="ltx_Math" display="inline" id="F9.m33"><semantics><mrow><mi>L</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">L=100</annotation><annotation encoding="application/x-llamapun">italic_L = 100</annotation></semantics></math>, and we draw <math alttext="100" class="ltx_Math" display="inline" id="F9.m34"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation><annotation encoding="application/x-llamapun">100</annotation></semantics></math>
independent samples to initialize the samplers. Samplers are implemented
with the closed-form denoisers derived in
<a class="ltx_ref" href="#Thmexample2" title="Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>, with those using the
approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>)
marked with red triangles, and those using the exact conditional posterior
denoiser marked with blue circles.
<span class="ltx_text ltx_font_bold">Top:</span> For large observation noise <math alttext="\sigma=0.5" class="ltx_Math" display="inline" id="F9.m35"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\sigma=0.5</annotation><annotation encoding="application/x-llamapun">italic_σ = 0.5</annotation></semantics></math>, both the exact
conditional posterior denoiser and the approximate one do a good job of
converging to the posterior <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="F9.m36"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>. Sampling time (corresponding
to time in the “forward process”, so larger times mean larger noise)
decreases from left to right. The convergence dynamics for the exact and
approximate measurement matching term are similar. <span class="ltx_text ltx_font_bold">Bottom:</span> For
smaller observation noise <math alttext="\sigma=0.1" class="ltx_Math" display="inline" id="F9.m37"><semantics><mrow><mi>σ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\sigma=0.1</annotation><annotation encoding="application/x-llamapun">italic_σ = 0.1</annotation></semantics></math>, the
approximate measurement matching term leads to extreme bias in the sampler
(red triangles): samples rapidly converge to an affine subspace
of points that are consistent, modulo some shrinkage from the posterior mean
denoiser, with the measured ground truth, and later sampling iterations are
unable to recover the lost posterior variance along this dimension. Note
that different times <math alttext="t_{\ell}" class="ltx_Math" display="inline" id="F9.m38"><semantics><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">t_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math> are plotted in the bottom row, compared to
the top row, to show the rapid collapse of the approximation to the
posterior along the measurement dimension.</span></figcaption>
</figure>
<div class="ltx_para" id="Thmexample2.p2">
<p class="ltx_p"><a class="ltx_ref" href="#S3.E26" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.26</span></a>
is, of course, a direct consequence of our calculations above. However, notice
that if we directly interpret this approximation, it is <span class="ltx_text ltx_font_italic">ab initio</span>
tractable: the likelihood <math alttext="p_{\bm{y}\mid\bm{x}}=\mathcal{N}(\bm{A}\bm{x},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="Thmexample2.p2.m1"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}}=\mathcal{N}(\bm{A}\bm{x},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT = caligraphic_N ( bold_italic_A bold_italic_x , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> is
a simple Gaussian distribution centered at the observation, and the
approximation to the measurement matching term that we arrive at can be
interpreted as simply evaluating the log-likelihood at the conditional
expectation <math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="Thmexample2.p2.m2"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math>, then taking gradients with respect
to <math alttext="\bm{\xi}" class="ltx_Math" display="inline" id="Thmexample2.p2.m3"><semantics><mi>𝝃</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation><annotation encoding="application/x-llamapun">bold_italic_ξ</annotation></semantics></math> (and backpropagating through the conditional expectation, which is
given here by <a class="ltx_ref" href="#S3.E17" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.17</span></a>).
Nevertheless, note that the approximation in
<a class="ltx_ref" href="#S3.E26" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.26</span></a>
requires <math alttext="t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}" class="ltx_Math" display="inline" id="Thmexample2.p2.m4"><semantics><mrow><mi>t</mi><mo>≪</mo><msqrt><mrow><msub><mi>λ</mi><mi>min</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝚺</mi><mo stretchy="false">)</mo></mrow></mrow></msqrt></mrow><annotation encoding="application/x-tex">t\ll\sqrt{\lambda_{\min}(\bm{\Sigma})}</annotation><annotation encoding="application/x-llamapun">italic_t ≪ square-root start_ARG italic_λ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_Σ ) end_ARG</annotation></semantics></math>, and that it is never accurate
in general when this condition does not hold, even in this Gaussian setting.</p>
</div>
<div class="ltx_para" id="Thmexample2.p3">
<p class="ltx_p">To gain insight into the effect of the convenient approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>),
we implement and simulate a simple numerical experiment in the Gaussian
setting in <a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>.
The sampler we implement is a direct implementation of the simple scheme
(<a class="ltx_ref" href="Ch3.html#S2.E66" title="Equation 3.2.66 ‣ 2nd item ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.66</span></a>) we have developed in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>
and recalled above, using the true conditional posterior denoiser, i.e. <a class="ltx_ref" href="#S3.E16" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.16</span></a> (top row of
<a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>), and the
convenient approximation to this denoiser made with the decomposition
(<a class="ltx_ref" href="#S3.Ex3" title="Equation 6.3.14 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.14</span></a>), the posterior denoiser
(<a class="ltx_ref" href="#S3.E17" title="Equation 6.3.17 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.17</span></a>), and the measurement
matching approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>)
(bottom row of <a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>).
We see that even in the simple Gaussian setting, the approximation to the
measurement matching term we have made is not without its
drawbacks—specifically, at small noise levels <math alttext="\sigma^{2}\ll 1" class="ltx_Math" display="inline" id="Thmexample2.p3.m1"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>≪</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^{2}\ll 1</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≪ 1</annotation></semantics></math>, it leads to
rapid collapse of the variance of the sampling distribution along directions
that are parallel to the rows of the linear measurement operator <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexample2.p3.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, which
cannot be corrected by later iterations of sampling. We can intuit this from
the approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>)
and the definition of the denoising iteration
(<a class="ltx_ref" href="Ch3.html#S2.E66" title="Equation 3.2.66 ‣ 2nd item ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.2.66</span></a>), given
<a class="ltx_ref" href="#S3.Ex3" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.14</span></a>: for <math alttext="\sigma^{2}\ll 1" class="ltx_Math" display="inline" id="Thmexample2.p3.m3"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>≪</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^{2}\ll 1</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≪ 1</annotation></semantics></math>,
early steps of sampling effectively take gradient descent steps with a very
large step size on the likelihood, via
<a class="ltx_ref" href="#S3.E26" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.26</span></a>,
which leads the sampling distribution to get “stuck” in a collapsed state.</p>
</div>
<div class="ltx_para" id="Thmexample2.p4">
<p class="ltx_p"><math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample2.p4.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p7">
<p class="ltx_p"><a class="ltx_ref" href="#Thmexample2" title="Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.2</span></a> suggests a convenient
approximation for the measurement matching term
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>),
which can be made beyond the Gaussian setting of the example. To
motivate this approximation in greater generality, notice that
by conditional independence of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m1"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})=\int p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bm{\xi}^{\prime})p_{\bm{x}\mid\bm{x}_{t}}(\bm{\xi}^{\prime}\mid\bm{\xi})\mathrm{d}\bm{\xi}^{\prime}." class="ltx_Math" display="block" id="S3.E27.m1"><semantics><mrow><mrow><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mo>∫</mo><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><msup><mi>𝝃</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝝃</mi><mo>′</mo></msup><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><msup><mi>𝝃</mi><mo>′</mo></msup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})=\int p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bm{\xi}^{\prime})p_{\bm{x}\mid\bm{x}_{t}}(\bm{\xi}^{\prime}\mid\bm{\xi})\mathrm{d}\bm{\xi}^{\prime}.</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) = ∫ italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∣ bold_italic_ξ ) roman_d bold_italic_ξ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Formally, when the posterior <math alttext="p_{\bm{x}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m4"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is a delta function centered at
its mean <math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m5"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math>, the approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>) is
exact. More generally, when the posterior <math alttext="p_{\bm{x}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m6"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is highly
concentrated around its mean, the approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>) is
accurate. This holds, for example, for sufficiently small <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p7.m7"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, which we saw
explicitly in the Gaussian setting of
<a class="ltx_ref" href="#Thmexample2" title="Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>.
Although the numerical simulation in
<a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a> suggests that this
approximation is not without its caveats in certain regimes, it has proved to be
a reliable baseline in practice, after being proposed by Chung et al. as
“Diffusion Posterior Sampling” (DPS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx53" title="">CKM+23</a>]</cite>.
In addition, there are even principled and generalizable approaches to improve it by
incorporating better estimates of the posterior variance (which turn out to be
exact in the Gaussian setting of <a class="ltx_ref" href="#Thmexample2" title="Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>),
which we discuss further in the end-of-chapter summary.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p8">
<p class="ltx_p">Thus, with the DPS approximation, we arrive at the following approximation for
the conditional posterior denoisers <math alttext="\mathbb{E}[\bm{x}\mid\bm{y},\bm{x}_{t}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m1"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒚</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{y},\bm{x}_{t}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_y , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]</annotation></semantics></math>, via
<a class="ltx_ref" href="#S3.Ex3" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.14</span></a>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E28">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]\approx\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}])." class="ltx_math_unparsed" display="block" id="S3.E28.m1"><semantics><mrow><mi>𝔼</mi><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mi>𝝃</mi><mo>,</mo><mi>𝒚</mi><mo>=</mo><mi>𝝂</mi><mo stretchy="false">]</mo></mrow><mo>≈</mo><mi>𝔼</mi><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mi>𝝃</mi><mo stretchy="false">]</mo></mrow><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup><msub><mo lspace="0.167em" rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝔼</mi><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mi>𝝃</mi><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},\bm{y}=\bm{\nu}]\approx\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , bold_italic_y = bold_italic_ν ] ≈ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ν ∣ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.28)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">And, for a neural network or other model <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m2"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ )</annotation></semantics></math>
trained as in <a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> to approximate the denoisers
<math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m3"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math> for each <math alttext="t\in[0,T]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m4"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation><annotation encoding="application/x-llamapun">italic_t ∈ [ 0 , italic_T ]</annotation></semantics></math>, we arrive at the learned
conditional posterior denoisers</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi},\bm{\nu})=\bar{\bm{x}}_{\theta}(t,\bm{\xi})+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bar{\bm{x}}_{\theta}(t,\bm{\xi}))." class="ltx_Math" display="block" id="S3.E29.m1"><semantics><mrow><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo>,</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi},\bm{\nu})=\bar{\bm{x}}_{\theta}(t,\bm{\xi})+t^{2}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bar{\bm{x}}_{\theta}(t,\bm{\xi})).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ , bold_italic_ν ) = over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ ) + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ν ∣ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that the approximation
(<a class="ltx_ref" href="#S3.E28" title="Equation 6.3.28 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.28</span></a>) is valid for arbitrary
forward models <math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m5"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> in the
observation model (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>), including
nonlinear <math alttext="h" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m6"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math>, and even to
arbitrary noise models for which a clean expression for the likelihood <math alttext="p_{\bm{y}\mid\bm{x}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p8.m7"><semantics><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> is known. Indeed, in the case of Gaussian noise, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bm{\xi})\propto\exp\left(-\frac{1}{2\sigma^{2}}\left\|h(\bm{\xi})-\bm{\nu}\right\|_{2}^{2}\right)." class="ltx_Math" display="block" id="S3.E30.m1"><semantics><mrow><mrow><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><mi>𝒙</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo>‖</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝝂</mi></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}}(\bm{\nu}\mid\bm{\xi})\propto\exp\left(-\frac{1}{2\sigma^{2}}\left\|h(\bm{\xi})-\bm{\nu}\right\|_{2}^{2}\right).</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) ∝ roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ italic_h ( bold_italic_ξ ) - bold_italic_ν ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.3.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, evaluating the righthand side of
(<a class="ltx_ref" href="#S3.E29" title="Equation 6.3.29 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.29</span></a>)
requires only</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">A pretrained denoiser <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi})" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ )</annotation></semantics></math> for the data distribution <math alttext="p" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>
(of <math alttext="\bm{x})" class="ltx_math_unparsed" display="inline" id="S3.I1.i1.p1.m3"><semantics><mrow><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_x )</annotation></semantics></math>, learned as in <a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> via
<a class="ltx_ref" href="Ch3.html#alg2" title="In Step 3: optimizing training pipelines. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">Forward and backward pass access to the forward model <math alttext="h" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m1"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> for the
measurements (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>);</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p">A forward and backward pass through <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi})" class="ltx_Math" display="inline" id="S3.I1.i3.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ )</annotation></semantics></math>, which can be evaluated
efficiently using (say) backpropagation.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 6.1</span> </span> Conditional sampling under measurements
(<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>), with an unconditional denoiser and DPS.</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span>An ordered list of timesteps <math alttext="0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T" class="ltx_Math" display="inline" id="alg1.l1.m1"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T</annotation><annotation encoding="application/x-llamapun">0 ≤ italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ≤ italic_T</annotation></semantics></math> to use for sampling.
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>An unconditional denoiser <math alttext="\bar{\bm{x}}_{\theta}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\to\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg1.l2.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo rspace="0.055em" stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo rspace="0.222em">×</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\to\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> for <math alttext="p_{\bm{x}}" class="ltx_Math" display="inline" id="alg1.l2.m2"><semantics><msub><mi>p</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">p_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span>Measurement realization <math alttext="\bm{\nu}" class="ltx_Math" display="inline" id="alg1.l3.m1"><semantics><mi>𝝂</mi><annotation encoding="application/x-tex">\bm{\nu}</annotation><annotation encoding="application/x-llamapun">bold_italic_ν</annotation></semantics></math> of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="alg1.l3.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> (<a class="ltx_ref" href="#S3.E9" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.9</span></a>) to condition on.
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>Forward model <math alttext="h:\mathbb{R}^{D}\to\mathbb{R}^{d}" class="ltx_Math" display="inline" id="alg1.l4.m1"><semantics><mrow><mi>h</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">h:\mathbb{R}^{D}\to\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_h : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and measurement noise variance
<math alttext="\sigma^{2}&gt;0" class="ltx_Math" display="inline" id="alg1.l4.m2"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma^{2}&gt;0</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT &gt; 0</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>Scale and noise level functions <math alttext="\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="alg1.l5.m1"><semantics><mrow><mrow><mi>α</mi><mo>,</mo><mi>σ</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_α , italic_σ : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>A sample <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="alg1.l6.m1"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, approximately from <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="alg1.l6.m2"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span><span class="ltx_text ltx_font_bold">function</span> <span class="ltx_text ltx_font_smallcaps">DDIMSamplerConditionalDPS</span>(<math alttext="\bar{\bm{x}}_{\theta},\bm{\nu},h,\sigma^{2},(t_{\ell})_{\ell=0}^{L}" class="ltx_Math" display="inline" id="alg1.l7.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo>,</mo><mi>𝝂</mi><mo>,</mo><mi>h</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>,</mo><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta},\bm{\nu},h,\sigma^{2},(t_{\ell})_{\ell=0}^{L}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , bold_italic_ν , italic_h , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>     Initialize <math alttext="\hat{\bm{x}}_{t_{L}}\sim" class="ltx_Math" display="inline" id="alg1.l8.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi>L</mi></msub></msub><mo>∼</mo><mi></mi></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{L}}\sim</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∼</annotation></semantics></math> approximate distribution of <math alttext="\bm{x}_{t_{L}}" class="ltx_Math" display="inline" id="alg1.l8.m2"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>L</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{L}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l8.m3"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> VP <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="alg1.l8.m4"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>, VE <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="alg1.l8.m5"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.
</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="\ell=L,L-1,\dots,1" class="ltx_Math" display="inline" id="alg1.l9.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mrow><mi>L</mi><mo>,</mo><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\ell=L,L-1,\dots,1</annotation><annotation encoding="application/x-llamapun">roman_ℓ = italic_L , italic_L - 1 , … , 1</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span>         Compute
<table class="ltx_equation ltx_eqn_table" id="S3.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\left(\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})-\frac{\sigma_{t_{\ell}}^{2}}{2\alpha_{t_{\ell}}\sigma^{2}}\nabla_{\bm{\xi}}\left[\left\|h(\bar{\bm{x}}_{\theta}(t_{\ell},\bm{\xi}))-\bm{\nu}\right\|_{2}^{2}\right]\biggl{|}_{\bm{\xi}=\hat{\bm{x}}_{t_{\ell}}}\biggr{.}\right)" class="ltx_math_unparsed" display="block" id="S3.Ex9.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>≐</mo><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>+</mo><mrow><mo>(</mo><msub><mi>α</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>−</mo><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>)</mo></mrow><mrow><mo>(</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo stretchy="false">)</mo></mrow><mo>−</mo><mfrac><msubsup><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mn>2</mn></msubsup><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><msub><mo lspace="0.167em">∇</mo><mi>𝝃</mi></msub><mrow><mo>[</mo><mo lspace="0em" rspace="0.167em" stretchy="true">∥</mo><mi>h</mi><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>−</mo><mi>𝝂</mi><msubsup><mo lspace="0em" rspace="0.167em" stretchy="true">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo>]</mo></mrow><mrow><msub><mo maxsize="210%" minsize="210%">|</mo><mrow><mi>𝝃</mi><mo>=</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></msub><mo lspace="0em" mathsize="210%" rspace="0.167em">.</mo><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\left(\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}})-\frac{\sigma_{t_{\ell}}^{2}}{2\alpha_{t_{\ell}}\sigma^{2}}\nabla_{\bm{\xi}}\left[\left\|h(\bar{\bm{x}}_{\theta}(t_{\ell},\bm{\xi}))-\bm{\nu}\right\|_{2}^{2}\right]\biggl{|}_{\bm{\xi}=\hat{\bm{x}}_{t_{\ell}}}\biggr{.}\right)</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≐ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT - divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) - divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT [ ∥ italic_h ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , bold_italic_ξ ) ) - bold_italic_ν ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] | start_POSTSUBSCRIPT bold_italic_ξ = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT . )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\hat{\bm{x}}_{t_{0}}" class="ltx_Math" display="inline" id="alg1.l12.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mn>0</mn></msub></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{0}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p9">
<p class="ltx_p">Combining this scheme with the basic implementation of unconditional sampling we
developed in <a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we obtain a practical algorithm for
conditional sampling of the posterior <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p9.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> given measurements
following (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>).
<a class="ltx_ref" href="#alg1" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.1</span></a> records this scheme
for the case of
Gaussian observation noise with known standard deviation <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p9.m2"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation><annotation encoding="application/x-llamapun">italic_σ</annotation></semantics></math>, with minor
modifications to extend to a general noising process, as in
<a class="ltx_ref" href="Ch3.html#S2.E69" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.69</span></a> and the surrounding discussion in
<a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a> (our discussion above made the simplifying choices <math alttext="\alpha_{t}=1" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p9.m3"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_{t}=1</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1</annotation></semantics></math>, <math alttext="\sigma_{t}=t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p9.m4"><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\sigma_{t}=t</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t</annotation></semantics></math>, and
<math alttext="t_{\ell}=T\ell/L" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p9.m5"><semantics><mrow><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>=</mo><mrow><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">ℓ</mi></mrow><mo>/</mo><mi>L</mi></mrow></mrow><annotation encoding="application/x-tex">t_{\ell}=T\ell/L</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT = italic_T roman_ℓ / italic_L</annotation></semantics></math>, as for <a class="ltx_ref" href="Ch3.html#S2.E66" title="In 2nd item ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.66</span></a> in
<a class="ltx_ref" href="Ch3.html#S2" title="3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3.3 </span>Body Pose Generation Conditioned on Head and Hands</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">The type of
conditional estimation or generation problems arise rather naturally in many
practical applications.
A typical problem of this kind is how to estimate and generate body pose and hand gesture conditioned on a given head pose and egocentric images, as illustrated in Figure <a class="ltx_ref" href="#F10" title="Figure 6.10 ‣ 6.3.3 Body Pose Generation Conditioned on Head and Hands ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.10</span></a>. This is often the problem we need to solve when one is wearing a head-mounted device such as the Vision-pro from Apple or the Project Aria from Meta. The pose of the whole body and the gesture of the hands need to be inferred so that we can use the information to control virtual objects that the person interacts with.</p>
</div>
<figure class="ltx_figure" id="F10"><img alt="Figure 6.10 : A system that estimates human body height, pose, and hand parameters (middle), conditioned on ego-centric SLAM poses and images (left). Outputs capture the wearer’s actions in the allocentric reference frame of the scene, which we visualize here with 3D reconstructions (right)." class="ltx_graphics" id="F10.g1" src="chapters/chapter6/figs/pose-teaser.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.10</span>: </span><span class="ltx_text" style="font-size:90%;">
A system that estimates human body height, pose, and hand parameters (middle), conditioned on ego-centric SLAM poses and images (left). Outputs capture the wearer’s actions in the allocentric reference frame of the scene, which we visualize here with 3D reconstructions (right).
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">Notice that in this case, one only has the head pose provided by the device and a very limited field of view for part of one’s hands and upper limbs. The pose of the rest of the body needs to “inferred” or “completed” based on such partial information. The only way one can estimate the body pose over time is by learning the joint distribution of the head and body pose sequences in advance and then sample this prior distribution conditioned on the real-time partial inputs. Figure <a class="ltx_ref" href="#F11" title="Figure 6.11 ‣ 6.3.3 Body Pose Generation Conditioned on Head and Hands ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.11</span></a> outlines a system called EgoAllo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx309" title="">YYZ+24</a>]</cite> to solve this problem based on a learned conditional diffusion-denoising model.</p>
</div>
<figure class="ltx_figure" id="F11"><img alt="Figure 6.11 : Overview of technical components of EgoAllo [ YYZ+24 ] . A diffusion model is pretrained that can generate body pose sequence based on local body parameters (middle). An invariant parameterization g ​ ( ⋅ ) g(\cdot) italic_g ( ⋅ ) of SLAM poses (left) is used to condition the diffusion model. These can be placed into the global coordinate frame via global alignment to input poses. When available, egocentric video is used for hand detection (left) via HaMeR [ PSR+23 ] , which can be incorporated into samples via guidance by the generated gesture." class="ltx_graphics" id="F11.g1" src="chapters/chapter6/figs/pose-method.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.11</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">Overview of technical components of EgoAllo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx309" title="">YYZ+24</a>]</cite>.</span>
A diffusion model is pretrained that can generate body pose sequence based on local body parameters (middle).
An invariant parameterization <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="F11.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ )</annotation></semantics></math> of SLAM poses (left) is used to condition the diffusion model. These can be placed into the global coordinate frame via global alignment to input poses.
When available, egocentric video is used for hand detection (left) via HaMeR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx211" title="">PSR+23</a>]</cite>, which can be incorporated into samples via guidance by the generated gesture.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p">Figure <a class="ltx_ref" href="#F12" title="Figure 6.12 ‣ 6.3.3 Body Pose Generation Conditioned on Head and Hands ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.12</span></a> compares some ground truth motion sequences with sampled results generated by the EgoAllo. Although the figure shows one result for each input head pose sequence, different runs can generate different body pose sequences that are consistent with the given head pose, all drawing from the distribution of natural full-body motion sequences.</p>
</div>
<figure class="ltx_figure" id="F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F12.sf1"><img alt="(a) Ground-truth" class="ltx_graphics ltx_img_landscape" height="293" id="F12.sf1.g1" src="chapters/chapter6/figs/qual0_gt.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Ground-truth</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F12.sf2"><img alt="(a) Ground-truth" class="ltx_graphics ltx_img_landscape" height="293" id="F12.sf2.g1" src="chapters/chapter6/figs/qual0_ours.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">EgoAllo</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F12.sf3"><img alt="(a) Ground-truth" class="ltx_graphics ltx_img_landscape" height="170" id="F12.sf3.g1" src="chapters/chapter6/figs/qual1_gt.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">Ground-truth</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F12.sf4"><img alt="(a) Ground-truth" class="ltx_graphics ltx_img_landscape" height="170" id="F12.sf4.g1" src="chapters/chapter6/figs/qual1_ours.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">EgoAllo</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.12</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">Egocentric human motion estimation for a running (top) and squatting (bottom) sequence.</span>
The ground-truth motion is compared with one output from EgoAllo that is consistent with the given head pose sequence.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p">Strictly speaking, the solution proposed in EgoAllo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx309" title="">YYZ+24</a>]</cite> does
not enforce measurement matching using the techniques introduced above. Instead
it heuristically enforces the condition by utilizing the cross-attention
mechanism in a transformer architecture. As we will describe with more precision
in the paired data setting in <a class="ltx_ref" href="#S4.SS2" title="6.4.2 Caption Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.2</span></a>, there is reason to believe that the
cross-attention mechanism is in a way approximately realizing the conditional
sampling of the denoising a posteriori. We believe the more principled
techniques introduced here, if properly implemented, can lead to better methods
that further improve the body pose and hand gesture estimation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.4 </span>Conditional Inference with Paired Data and Measurements</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">In many practical applications, we do not know either the distribution of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> of interest nor the explicit relationship between the data and certain observed attributes <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> of the data. We only have a (large) set of paired samples <math alttext="(\bm{X},\bm{Y})=\{(\bm{x}_{1},\bm{y}_{1}),\ldots,(\bm{x}_{N},\bm{y}_{N})\}" class="ltx_Math" display="inline" id="S4.p1.m3"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒀</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒚</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{Y})=\{(\bm{x}_{1},\bm{y}_{1}),\ldots,(\bm{x}_{N},\bm{y}_{N})\}</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_Y ) = { ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , … , ( bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) }</annotation></semantics></math> from which we need to infer the data distribution and a mapping that models their relationship:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h:\bm{x}\mapsto\bm{y}." class="ltx_Math" display="block" id="S4.E1.m1"><semantics><mrow><mrow><mi>h</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒚</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h:\bm{x}\mapsto\bm{y}.</annotation><annotation encoding="application/x-llamapun">italic_h : bold_italic_x ↦ bold_italic_y .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">The problem of image classification can be viewed as one such an example. In a sense, the classification problem is to learn a (extremely lossy) compressive encoder for natural images. Say, given a random sample of an image <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we would like to predict its class label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p2.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> that best correlates the content in <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. We know the distribution of natural images of objects is low-dimensional compared to the dimension of the pixel space. From the previous chapters, we have learned that given sufficient samples, in principle, we can learn a structured low-dimensional representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.p2.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> through a learned compressive encoding:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:\bm{x}\mapsto\bm{z}." class="ltx_Math" display="block" id="S4.E2.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒛</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f:\bm{x}\mapsto\bm{z}.</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_x ↦ bold_italic_z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.p2.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> can also be viewed as a learned (lossy but structured) code for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. It is rather reasonable to assume that if the class assignment <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p2.m8"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> truly depends on the low-dimensional structures of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p2.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the learned code <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.p2.m10"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> truly reflect such structures, <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p2.m11"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.p2.m12"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> can be made highly correlated hence their joint distribution <math alttext="p(\bm{z},\bm{y})" class="ltx_Math" display="inline" id="S4.p2.m13"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z , bold_italic_y )</annotation></semantics></math> should be extremely low-dimensional. Therefore, we may combine the two desired codes <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p2.m14"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.p2.m15"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> together and try to learn a combined encoder:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:\bm{x}\mapsto(\bm{z},\bm{y})" class="ltx_Math" display="block" id="S4.E3.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f:\bm{x}\mapsto(\bm{z},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_x ↦ ( bold_italic_z , bold_italic_y )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the joint distribution of <math alttext="(\bm{z},\bm{y})" class="ltx_Math" display="inline" id="S4.p2.m16"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{z},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_z , bold_italic_y )</annotation></semantics></math> is highly low-dimensional.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">From our study in previous chapters, the mapping <math alttext="f" class="ltx_Math" display="inline" id="S4.p3.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is usually learned as a sequence of compression or denoising operators in the same space. Hence to leverage on such family of operations, we may introduce an auxiliary vector <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S4.p3.m2"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> that can be viewed as an initial random guess of the class label <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.p3.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>. In this way, we can learn a compression or denoising mapping:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:(\bm{x},\bm{w})\mapsto(\bm{z},\bm{y})" class="ltx_Math" display="block" id="S4.E4.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">↦</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f:(\bm{x},\bm{w})\mapsto(\bm{z},\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_f : ( bold_italic_x , bold_italic_w ) ↦ ( bold_italic_z , bold_italic_y )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">within a common space. In fact, the common practice of introducing an auxiliary “class token” in the training of a Transformer for classification tasks, such as in ViT, can be viewed as learning such a representation by compressing (the coding rate of) given (noisy) samples of <math alttext="(\bm{x},\bm{w})" class="ltx_Math" display="inline" id="S4.p3.m4"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{w})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_w )</annotation></semantics></math>. If the distribution of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.p3.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is already a mixture of (low-dimensional) Gaussians, the work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx296" title="">WTL+08</a>]</cite> has shown that classification can be done effectively by directly minimizing <span class="ltx_text ltx_font_italic">the (lossy) coding length</span> associated with the given samples.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4.1 </span>Class Conditioned Image Generation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">While a learned classifier allows us to classify a given image <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to its
corresponding class, we often like to generate an image of a given class, by
sampling the learned distribution of natural images. To some extent, this can be
viewed as the “inverse” problem to image classification. Let <math alttext="p_{\bm{x}}" class="ltx_Math" display="inline" id="S4.SS1.p1.m2"><semantics><msub><mi>p</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">p_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> denote
the distribution of natural images, say modeled by a diffusion-denoising
process. Given a class label random variable <math alttext="y\in[K]" class="ltx_Math" display="inline" id="S4.SS1.p1.m3"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y\in[K]</annotation><annotation encoding="application/x-llamapun">italic_y ∈ [ italic_K ]</annotation></semantics></math> with realization <math alttext="\nu" class="ltx_Math" display="inline" id="S4.SS1.p1.m4"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation><annotation encoding="application/x-llamapun">italic_ν</annotation></semantics></math>, say
an “Apple”, we would like to sample the conditional distribution <math alttext="p_{\bm{x}\mid y}(\,\cdot\,\mid\nu)" class="ltx_math_unparsed" display="inline" id="S4.SS1.p1.m5"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>y</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\bm{x}\mid y}(\,\cdot\,\mid\nu)</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ italic_y end_POSTSUBSCRIPT ( ⋅ ∣ italic_ν )</annotation></semantics></math> to generate an image of an apple:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu})." class="ltx_math_unparsed" display="block" id="S4.E5.m1"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>∼</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}\sim p_{\bm{x}\mid\bm{y}}(\,\cdot\,\mid\bm{\nu}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT ( ⋅ ∣ bold_italic_ν ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We call this <span class="ltx_text ltx_font_italic">class-conditioned image generation</span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">In <a class="ltx_ref" href="#S3.SS2" title="6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.3.2</span></a>, we have seen how to use the denoising-diffusion paradigm for
conditional sampling from the posterior <math alttext="p_{\bm{x}\mid\bm{y}}" class="ltx_Math" display="inline" id="S4.SS1.p2.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math> given
<span class="ltx_text ltx_font_italic">model-based</span> measurements <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S4.SS1.p2.m2"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math>
(<a class="ltx_ref" href="#S3.E9" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.9</span></a>), culminating in the
DPS algorithm (<a class="ltx_ref" href="#alg1" title="Algorithm 6.1 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.1</span></a>). This is a powerful
framework, but it does not apply to the class (or text) conditioned image
generation problem here, where an explicit generative model <math alttext="h" class="ltx_Math" display="inline" id="S4.SS1.p2.m3"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> for the
observations/attributes <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p2.m4"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> is not available due to the
intractability of analytical modeling. In this section, we will present techniques for extending
conditional sampling to this setting.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p">Thus, we now assume only that we have access to samples from the joint
distribution of <math alttext="(\bm{x},y)" class="ltx_Math" display="inline" id="S4.SS1.p3.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},y)</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , italic_y )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{x},y)\sim p_{\bm{x},y}." class="ltx_Math" display="block" id="S4.E6.m1"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>∼</mo><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>,</mo><mi>y</mi></mrow></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\bm{x},y)\sim p_{\bm{x},y}.</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , italic_y ) ∼ italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As in the previous section, we define <math alttext="\bm{x}_{t}=\alpha_{t}\bm{x}+\sigma_{t}\bm{g}" class="ltx_Math" display="inline" id="S4.SS1.p3.m2"><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>+</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\alpha_{t}\bm{x}+\sigma_{t}\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_g</annotation></semantics></math>
with <math alttext="\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="S4.SS1.p3.m3"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> independent of <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S4.SS1.p3.m4"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>, as in
<a class="ltx_ref" href="Ch3.html#S2.E69" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.69</span></a> in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>,
and we will repeatedly use the notation <math alttext="\bm{\xi}" class="ltx_Math" display="inline" id="S4.SS1.p3.m5"><semantics><mi>𝝃</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation><annotation encoding="application/x-llamapun">bold_italic_ξ</annotation></semantics></math> to denote realizations of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p3.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>
and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p3.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p">To proceed, we note that our development of conditional sampling under
measurements <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S4.SS1.p4.m1"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math> only explicitly used the
forward model <math alttext="h" class="ltx_Math" display="inline" id="S4.SS1.p4.m2"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> in making the DPS approximation
(<a class="ltx_ref" href="#S3.E26" title="Equation 6.3.26 ‣ Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.26</span></a>).
In particular, the conditional posterior denoiser decomposition
(<a class="ltx_ref" href="#S3.Ex3" title="Equation 6.3.14 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.14</span></a>) <span class="ltx_text ltx_font_italic">still holds in the
paired data setting</span>, by virtue of Bayes’ rule
and conditional independence of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S4.SS1.p4.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p4.m4"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p4.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (recall
<a class="ltx_ref" href="#F8" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.8</span></a>). Thus we can still write in the paired
data setting</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu]=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{y\mid\bm{x}_{t}}(\nu\mid\bm{\xi})." class="ltx_Math" display="block" id="S4.E7.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>=</mo><mi>ν</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>y</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ν</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu]=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{y\mid\bm{x}_{t}}(\nu\mid\bm{\xi}).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , italic_y = italic_ν ] = blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] + divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_ν ∣ bold_italic_ξ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">A natural ideal is then to directly implement the likelihood correction term in
(<a class="ltx_ref" href="#S4.E7" title="Equation 6.4.7 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.7</span></a>) using a deep network
<math alttext="f_{\theta_{\mathrm{c}}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m6"><semantics><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub><annotation encoding="application/x-tex">f_{\theta_{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> with parameters <math alttext="\theta_{\mathrm{c}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m7"><semantics><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{c}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT</annotation></semantics></math>, as in
<a class="ltx_ref" href="#S4.E4" title="In 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.4</span></a>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\theta_{\mathrm{c}}}:(t,\bm{x}_{t})\mapsto\operatorname{\mathrm{softmax}}(\bm{W}_{\mathrm{head}}\bm{z}(t,\bm{x}_{t}))." class="ltx_Math" display="block" id="S4.E8.m1"><semantics><mrow><mrow><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">↦</mo><mrow><mi>softmax</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑾</mi><mi>head</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f_{\theta_{\mathrm{c}}}:(t,\bm{x}_{t})\mapsto\operatorname{\mathrm{softmax}}(\bm{W}_{\mathrm{head}}\bm{z}(t,\bm{x}_{t})).</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT : ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ↦ roman_softmax ( bold_italic_W start_POSTSUBSCRIPT roman_head end_POSTSUBSCRIPT bold_italic_z ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This expression combines the final representations <math alttext="\bm{z}(t,\bm{x}_{t})" class="ltx_Math" display="inline" id="S4.SS1.p4.m8"><semantics><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}(t,\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> (which also depend on
<math alttext="\theta_{\mathrm{c}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m9"><semantics><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{c}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT</annotation></semantics></math>) of the noisy inputs
<math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p4.m10"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with a classification head <math alttext="\bm{W}_{\mathrm{head}}\in\mathbb{R}^{K\times d}" class="ltx_Math" display="inline" id="S4.SS1.p4.m11"><semantics><mrow><msub><mi>𝑾</mi><mi>head</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>K</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{W}_{\mathrm{head}}\in\mathbb{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_W start_POSTSUBSCRIPT roman_head end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, which maps the
representations to a probability distribution over the <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p4.m12"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> possible classes.
As is common in practice, it also takes the time <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p4.m13"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> in the noising process as
input.
Thus, with appropriate training, it provides an approximation to the log-likelihood
<math alttext="\log p_{y\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m14"><semantics><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>y</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">\log p_{y\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, and differentiating <math alttext="\log f_{\theta_{\mathrm{c}}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m15"><semantics><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub></mrow><annotation encoding="application/x-tex">\log f_{\theta_{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">roman_log italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> with
respect to its input <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p4.m16"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
allows an approximation to the second term in
<a class="ltx_ref" href="#S4.E7" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.7</span></a>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}_{\theta}^{\mathrm{naive}}(t,\bm{x}_{t},y)=\bar{\bm{x}}_{\theta_{\mathrm{d}}}(t,\bm{x}_{t})+\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{x}_{t}}\left\langle\log f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t}),\bm{e}_{y}\right\rangle" class="ltx_Math" display="block" id="S4.E9.m1"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mi>naive</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mrow><mo>⟨</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>𝒆</mi><mi>y</mi></msub><mo>⟩</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}^{\mathrm{naive}}(t,\bm{x}_{t},y)=\bar{\bm{x}}_{\theta_{\mathrm{d}}}(t,\bm{x}_{t})+\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{x}_{t}}\left\langle\log f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t}),\bm{e}_{y}\right\rangle</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_naive end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ⟨ roman_log italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , bold_italic_e start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ⟩</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where, as usual, we approximate the first term in
<a class="ltx_ref" href="#S4.E7" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.7</span></a> via a learned
unconditional denoiser for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p4.m17"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with parameters <math alttext="\theta_{\mathrm{d}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m18"><semantics><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{d}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT</annotation></semantics></math>, and
where we write <math alttext="\bm{e}_{k}" class="ltx_Math" display="inline" id="S4.SS1.p4.m19"><semantics><msub><mi>𝒆</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{e}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext="k\in[K]" class="ltx_Math" display="inline" id="S4.SS1.p4.m20"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math> to denote the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p4.m21"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th canonical basis
vector for <math alttext="\mathbb{R}^{K}" class="ltx_Math" display="inline" id="S4.SS1.p4.m22"><semantics><msup><mi>ℝ</mi><mi>K</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{K}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> (i.e., the vector with
a one in the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p4.m23"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th position, and zeros elsewhere).
The reader should note that the conditional denoiser <math alttext="\bar{\bm{x}}_{\theta}" class="ltx_Math" display="inline" id="S4.SS1.p4.m24"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>
requires two separate training runs, with separate losses: one for the
classifier parameters <math alttext="\theta_{\mathrm{c}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m25"><semantics><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{c}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT</annotation></semantics></math>, on a classification
loss,<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>In <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we review the process of training such
a classifier in full detail.</span></span></span> and one for the denoiser parameters
<math alttext="\theta_{\mathrm{d}}" class="ltx_Math" display="inline" id="S4.SS1.p4.m26"><semantics><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub><annotation encoding="application/x-tex">\theta_{\mathrm{d}}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT</annotation></semantics></math>, on a denoising loss. Such an approach to conditional
sampling was already recognized and exploited to perform conditional sampling in
pioneering early works on diffusion models, notably those by
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx247" title="">SWM+15</a>]</cite> and by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx250" title="">SSK+21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p">However, this straightforward methodology has two key drawbacks (which is why we
label it as “naive”). The first is
that, empirically, such a trained deep network classifier frequently does not
provide a strong enough guidance signal (in
<a class="ltx_ref" href="#S4.E7" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.7</span></a>) to ensure that
generated samples reflect the conditioning information <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p5.m1"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>. This was first
emphasized by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite>, who noted that in the setting of
class-conditional ImageNet generation, the learned deep network
classifier’s probability outputs for the class <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p5.m2"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> being conditioned on were
frequently around
<math alttext="0.5" class="ltx_Math" display="inline" id="S4.SS1.p5.m3"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation><annotation encoding="application/x-llamapun">0.5</annotation></semantics></math>—large enough to be the dominant class, but not large enough to provide
a strong guidance signal—and that upon inspection, generations were not
consistent with the conditioning class <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p5.m4"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> proposed to
address this heuristically by incorporating an “inverse temperature”
hyperparameter <math alttext="\gamma&gt;0" class="ltx_Math" display="inline" id="S4.SS1.p5.m5"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\gamma&gt;0</annotation><annotation encoding="application/x-llamapun">italic_γ &gt; 0</annotation></semantics></math>
into the definition of the naive conditional denoiser
(<a class="ltx_ref" href="#S4.E9" title="Equation 6.4.9 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.9</span></a>), referring to the
resulting conditional denoiser as having incorporated “classifier guidance”
(CG):</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}_{\theta}^{\mathrm{CG}}(t,\bm{x}_{t},y)=\bar{\bm{x}}_{\theta_{\mathrm{d}}}(t,\bm{x}_{t})+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{x}_{t}}\left\langle\log f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t}),\bm{e}_{y}\right\rangle" class="ltx_Math" display="block" id="S4.E10.m1"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mi>CG</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo>∇</mo><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mrow><mo>⟨</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>𝒆</mi><mi>y</mi></msub><mo>⟩</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}^{\mathrm{CG}}(t,\bm{x}_{t},y)=\bar{\bm{x}}_{\theta_{\mathrm{d}}}(t,\bm{x}_{t})+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{x}_{t}}\left\langle\log f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t}),\bm{e}_{y}\right\rangle</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_γ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ⟨ roman_log italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , bold_italic_e start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ⟩</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with the case <math alttext="\gamma=1" class="ltx_Math" display="inline" id="S4.SS1.p5.m6"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation><annotation encoding="application/x-llamapun">italic_γ = 1</annotation></semantics></math> coinciding with
(<a class="ltx_ref" href="#S4.E9" title="Equation 6.4.9 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.9</span></a>).
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> found that a setting <math alttext="\gamma&gt;1" class="ltx_Math" display="inline" id="S4.SS1.p5.m7"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&gt;1</annotation><annotation encoding="application/x-llamapun">italic_γ &gt; 1</annotation></semantics></math> performed best empirically.
One possible interpretation for this is as follows: note that, in the context of
the <span class="ltx_text ltx_font_italic">true</span> likelihood term
<a class="ltx_ref" href="#S4.E7" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.7</span></a>, scaling by <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS1.p5.m8"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation><annotation encoding="application/x-llamapun">italic_γ</annotation></semantics></math>
gives equivalently</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx83">
<tbody id="S4.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})" class="ltx_Math" display="inline" id="S4.E11.m1"><semantics><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})</annotation><annotation encoding="application/x-llamapun">italic_γ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log\left(p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})^{\gamma}\right)," class="ltx_Math" display="inline" id="S4.E11.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow><mi>γ</mi></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log\left(p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})^{\gamma}\right),</annotation><annotation encoding="application/x-llamapun">= divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which suggests the natural interpretation of the parameter <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS1.p5.m9"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation><annotation encoding="application/x-llamapun">italic_γ</annotation></semantics></math> performing
(inverse) <span class="ltx_text ltx_font_italic">temperature scaling</span> on the likelihood
<math alttext="p_{\bm{y}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S4.SS1.p5.m10"><semantics><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{y}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, which is precise if we consider the renormalized
distribution
<math alttext="{p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})^{\gamma}}/{\int p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}^{\prime}\mid\bm{\xi})^{\gamma}\mathrm{d}\bm{\nu}^{\prime}}" class="ltx_Math" display="inline" id="S4.SS1.p5.m11"><semantics><mrow><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝝂</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow><mi>γ</mi></msup></mrow><mo rspace="0.055em">/</mo><mrow><mo>∫</mo><mrow><msub><mi>p</mi><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝝂</mi><mo>′</mo></msup><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow><mi>γ</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><msup><mi>𝝂</mi><mo>′</mo></msup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">{p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}\mid\bm{\xi})^{\gamma}}/{\int p_{\bm{y}\mid\bm{x}_{t}}(\bm{\nu}^{\prime}\mid\bm{\xi})^{\gamma}\mathrm{d}\bm{\nu}^{\prime}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν ∣ bold_italic_ξ ) start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT / ∫ italic_p start_POSTSUBSCRIPT bold_italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ν start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∣ bold_italic_ξ ) start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT roman_d bold_italic_ν start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>.
However, note that this <span class="ltx_text ltx_font_italic">is not</span> a rigorous interpretation in the context
of <a class="ltx_ref" href="#S4.E7" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.7</span></a>, because the
gradients are taken with respect to <math alttext="\bm{\xi}" class="ltx_Math" display="inline" id="S4.SS1.p5.m12"><semantics><mi>𝝃</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation><annotation encoding="application/x-llamapun">bold_italic_ξ</annotation></semantics></math>, and the normalization constant in
the temperature-scaled distribution is in general a function of <math alttext="\bm{\xi}" class="ltx_Math" display="inline" id="S4.SS1.p5.m13"><semantics><mi>𝝃</mi><annotation encoding="application/x-tex">\bm{\xi}</annotation><annotation encoding="application/x-llamapun">bold_italic_ξ</annotation></semantics></math>.
Instead, the parameter <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS1.p5.m14"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation><annotation encoding="application/x-llamapun">italic_γ</annotation></semantics></math> should simply be understood as amplifying large
values of the deep network classifier’s output probabilities
<math alttext="f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t})" class="ltx_Math" display="inline" id="S4.SS1.p5.m15"><semantics><mrow><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\theta_{\mathrm{c}}}(t,\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> <span class="ltx_text ltx_font_italic">relative to</span> smaller ones,
which effectively amplifies the guidance signal provided in cases where the deep
network <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.p5.m16"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> assigns it the largest probability among the <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p5.m17"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> classes.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p">Nevertheless, classifier guidance does not address the second key drawback of
the naive methodology: it is both cumbersome and wasteful to have to
train an auxiliary classifier <math alttext="f_{\theta_{\mathrm{c}}}" class="ltx_Math" display="inline" id="S4.SS1.p6.m1"><semantics><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub><annotation encoding="application/x-tex">f_{\theta_{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> in addition to the unconditional
denoiser <math alttext="\bar{\bm{x}}_{\theta_{\mathrm{d}}}" class="ltx_Math" display="inline" id="S4.SS1.p6.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta_{\mathrm{d}}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, given
that it is not possible to directly adapt a pretrained classifier
due to the need for it to work well on noisy inputs <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p6.m3"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and incorporate
other empirically-motivated architecture modifications.
In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> found that it was necessary to explicitly
design the architecture of the deep network implementing the classifier to match
that of the denoiser.
Moreover, from a purely practical perspective—trying to obtain the best
possible performance from the resulting sampler—the best-performing
configuration of classifier guidance-based sampling departs even further from
the idealized and conceptually sound framework we have presented above.
To obtain the best performance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> found it
necessary to provide the class label <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p6.m4"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> as an additional input to the denoiser
<math alttext="\bar{\bm{x}}_{\theta_{\mathrm{d}}}" class="ltx_Math" display="inline" id="S4.SS1.p6.m5"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta_{\mathrm{d}}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>. As a result, the idealized classifier-guided
denoiser (<a class="ltx_ref" href="#S4.E10" title="Equation 6.4.10 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.10</span></a>),
derived by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> as we have done above from the conditional
posterior denoiser decomposition
(<a class="ltx_ref" href="#S4.E7" title="Equation 6.4.7 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.7</span></a>), is not exactly
reflective of the best-performing denoiser in practice—such a denoiser
actually combines a <span class="ltx_text ltx_font_italic">conditional</span> denoiser for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p6.m6"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p6.m7"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> with an
additional guidance signal from an auxiliary classifier!</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p">This state of affairs, empirically-motivated as it is, led <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx107" title="">HS22</a>]</cite> in
subsequent work to propose a more empirically pragmatic methodology, known as
classifier-free guidance (CFG). Instead of representing the conditional denoiser
(<a class="ltx_ref" href="#S4.E7" title="Equation 6.4.7 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.7</span></a>) as a weighted sum
of an unconditional denoiser for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p7.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with a log-likelihood correction term
(with possibly modified weights, as in classifier guidance), they accept the
apparent necessity of training a conditional denoiser for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p7.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p7.m3"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>, as
demonstrated by the experimental results of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite>, and replace
the log-likelihood gradient term with a correctly-weighted sum of this
conditional denoiser with an <span class="ltx_text ltx_font_italic">unconditional</span> denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p7.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given
<math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p7.m5"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>That said, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx107" title="">HS22</a>]</cite> actually proposed to use
a different weighting than what we present here, based on the fact that
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx71" title="">DN21</a>]</cite> heuristically replaced the unconditional denoiser in
(<a class="ltx_ref" href="#S4.E7" title="Equation 6.4.7 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.7</span></a>) with
a conditional denoiser. In fact, the weighting we derive and present here
reflects modern practice, and in particular is used in state-of-the-art
diffusion models such as Stable Diffusion 3.5
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx82" title="">EKB+24</a>]</cite>.</span></span></span> To see how this structure arises, we
begin with an ‘idealized’ version of the classifier guidance denoiser
<math alttext="\bar{\bm{x}}_{\theta}^{\mathrm{CG}}" class="ltx_Math" display="inline" id="S4.SS1.p7.m6"><semantics><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mi>CG</mi></msubsup><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}^{\mathrm{CG}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG end_POSTSUPERSCRIPT</annotation></semantics></math> defined in
(<a class="ltx_ref" href="#S4.E10" title="Equation 6.4.10 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.10</span></a>),
for which the denoiser <math alttext="\bar{\bm{x}}_{\theta_{\mathrm{d}}}" class="ltx_Math" display="inline" id="S4.SS1.p7.m7"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><msub><mi>θ</mi><mi mathvariant="normal">d</mi></msub></msub><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta_{\mathrm{d}}}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and the classifier
<math alttext="f_{\theta_{\mathrm{c}}}" class="ltx_Math" display="inline" id="S4.SS1.p7.m8"><semantics><msub><mi>f</mi><msub><mi>θ</mi><mi mathvariant="normal">c</mi></msub></msub><annotation encoding="application/x-tex">f_{\theta_{\mathrm{c}}}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> perfectly approximate their targets, via
(<a class="ltx_ref" href="#S4.E7" title="Equation 6.4.7 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.7</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}_{\theta}^{\mathrm{CG,\,ideal}}(t,\bm{\xi},\nu)=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{y\mid\bm{x}_{t}}(\nu\mid\bm{\xi})." class="ltx_Math" display="block" id="S4.E12.m1"><semantics><mrow><mrow><mrow><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mrow><mi>CG</mi><mo rspace="0.337em">,</mo><mi>ideal</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo>,</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>y</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ν</mi><mo>∣</mo><mi>𝝃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}^{\mathrm{CG,\,ideal}}(t,\bm{\xi},\nu)=\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{y\mid\bm{x}_{t}}(\nu\mid\bm{\xi}).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_ξ , italic_ν ) = blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] + italic_γ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_ν ∣ bold_italic_ξ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We then use Bayes’ rule, in the form</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\log p_{y\mid\bm{x}_{t}}=\log p_{\bm{x}_{t}\mid y}+\log p_{y}-\log p_{\bm{x}_{t}}," class="ltx_Math" display="block" id="S4.E13.m1"><semantics><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><mi>y</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub></mrow><mo>=</mo><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>y</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>y</mi></msub></mrow></mrow><mo>−</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\log p_{y\mid\bm{x}_{t}}=\log p_{\bm{x}_{t}\mid y}+\log p_{y}-\log p_{\bm{x}_{t}},</annotation><annotation encoding="application/x-llamapun">roman_log italic_p start_POSTSUBSCRIPT italic_y ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ italic_y end_POSTSUBSCRIPT + roman_log italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT - roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">together with Tweedie’s formula (<a class="ltx_ref" href="Ch3.html#Thmtheorem3" title="Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, modified as in
<a class="ltx_ref" href="Ch3.html#S2.E70" title="In Step 2: different noise models. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.70</span></a>) to convert between score functions and denoisers,
to obtain</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx84">
<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bar{\bm{x}}_{\theta}^{\mathrm{CG,\,ideal}}(t,\bm{\xi},\nu)" class="ltx_Math" display="inline" id="S4.Ex1.m1"><semantics><mrow><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mrow><mi>CG</mi><mo rspace="0.337em">,</mo><mi>ideal</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo>,</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bar{\bm{x}}_{\theta}^{\mathrm{CG,\,ideal}}(t,\bm{\xi},\nu)</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_ξ , italic_ν )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{\alpha_{t}}\bm{\xi}+(1-\gamma)\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}}(\bm{\xi})+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}\mid y}(\bm{\xi}\mid\nu)" class="ltx_Math" display="inline" id="S4.Ex1.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>α</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><msub><mi>α</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝃</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>∣</mo><mi>y</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝝃</mi><mo>∣</mo><mi>ν</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{\alpha_{t}}\bm{\xi}+(1-\gamma)\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}}(\bm{\xi})+\gamma\frac{\sigma_{t}^{2}}{\alpha_{t}}\nabla_{\bm{\xi}}\log p_{\bm{x}_{t}\mid y}(\bm{\xi}\mid\nu)</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_italic_ξ + ( 1 - italic_γ ) divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) + italic_γ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∇ start_POSTSUBSCRIPT bold_italic_ξ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∣ italic_y end_POSTSUBSCRIPT ( bold_italic_ξ ∣ italic_ν )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\gamma)\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\gamma\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu]," class="ltx_Math" display="inline" id="S4.Ex2.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>=</mo><mi>ν</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\gamma)\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]+\gamma\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu],</annotation><annotation encoding="application/x-llamapun">= ( 1 - italic_γ ) blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] + italic_γ blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , italic_y = italic_ν ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where in the last line, we apply
<a class="ltx_ref" href="#S3.E11" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.11</span></a>.
Now, <a class="ltx_ref" href="#S4.Ex2" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.14</span></a> suggests a natural approximation strategy: we
combine a learned unconditional denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p7.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p7.m10"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, as previously,
with a learned <span class="ltx_text ltx_font_italic">conditional</span> denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p7.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p7.m12"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p7.m13"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p">However, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx107" title="">HS22</a>]</cite> and the common practice of training deep network
denoisers, it is standard to use the <span class="ltx_text ltx_font_italic">same</span> deep network to represent
both the conditional and unconditional denoisers, by introducing an additional
label, which we will denote by <math alttext="\varnothing" class="ltx_Math" display="inline" id="S4.SS1.p8.m1"><semantics><mi mathvariant="normal">∅</mi><annotation encoding="application/x-tex">\varnothing</annotation><annotation encoding="application/x-llamapun">∅</annotation></semantics></math>, to denote the “unconditional”
case.
This leads to the form of the CFG denoiser:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}_{\theta}^{\mathrm{CFG}}(t,\bm{x}_{t},y)=(1-\gamma)\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},\varnothing)+\gamma\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},y)." class="ltx_Math" display="block" id="S4.E15.m1"><semantics><mrow><mrow><mrow><msubsup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi><mi>CFG</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi mathvariant="normal">∅</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}^{\mathrm{CFG}}(t,\bm{x}_{t},y)=(1-\gamma)\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},\varnothing)+\gamma\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},y).</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CFG end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = ( 1 - italic_γ ) over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , ∅ ) + italic_γ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">To train a denoiser <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},y^{+})" class="ltx_Math" display="inline" id="S4.SS1.p8.m2"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><msup><mi>y</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{x}_{t},y^{+})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT )</annotation></semantics></math> for use with
classifier-free guidance sampling, where <math alttext="y^{+}\in\{1,\dots,K,\varnothing\}" class="ltx_Math" display="inline" id="S4.SS1.p8.m3"><semantics><mrow><msup><mi>y</mi><mo>+</mo></msup><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi mathvariant="normal">∅</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">y^{+}\in\{1,\dots,K,\varnothing\}</annotation><annotation encoding="application/x-llamapun">italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ∈ { 1 , … , italic_K , ∅ }</annotation></semantics></math>, we proceed almost identically to the
unconditional training procedure in <a class="ltx_ref" href="Ch3.html#alg2" title="In Step 3: optimizing training pipelines. ‣ 3.2.2 Learning and Sampling a Distribution via Iterative Denoising ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, but with two
modifications:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p">When we sample from the dataset, we sample a pair <math alttext="(\bm{x},y)" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},y)</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , italic_y )</annotation></semantics></math> rather than
just a sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p">Every time we sample a pair from the dataset, we sample the augmented
label <math alttext="y^{+}" class="ltx_Math" display="inline" id="S4.I1.i2.p1.m1"><semantics><msup><mi>y</mi><mo>+</mo></msup><annotation encoding="application/x-tex">y^{+}</annotation><annotation encoding="application/x-llamapun">italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math> via</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y^{+}=\begin{cases}\varnothing&amp;\text{with probability }p_{\mathrm{uncond}};\\
y&amp;\text{else}.\end{cases}" class="ltx_Math" display="block" id="S4.E16.m1"><semantics><mrow><msup><mi>y</mi><mo>+</mo></msup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mi mathvariant="normal">∅</mi></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>with probability </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>uncond</mi></msub></mrow><mo>;</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mi>y</mi></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>else</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">y^{+}=\begin{cases}\varnothing&amp;\text{with probability }p_{\mathrm{uncond}};\\
y&amp;\text{else}.\end{cases}</annotation><annotation encoding="application/x-llamapun">italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { start_ROW start_CELL ∅ end_CELL start_CELL with probability italic_p start_POSTSUBSCRIPT roman_uncond end_POSTSUBSCRIPT ; end_CELL end_ROW start_ROW start_CELL italic_y end_CELL start_CELL else . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="p_{\mathrm{uncond}}\in[0,1]" class="ltx_Math" display="inline" id="S4.I1.i2.p1.m2"><semantics><mrow><msub><mi>p</mi><mi>uncond</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\mathrm{uncond}}\in[0,1]</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT roman_uncond end_POSTSUBSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> is a new hyperparameter.
This can be viewed as a form of dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx252" title="">SHK+14</a>]</cite>.</p>
</div>
</li>
</ol>
<p class="ltx_p">In this way, we train a conditional denoiser suitable for use in classifier-free
guidance sampling. We summarize the overall sampling process for
class-conditioned sampling with classifier-free guidance in
<a class="ltx_ref" href="#alg2" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 6.2</span> </span> Conditional sampling with classification data, using class-conditioned denoiser.</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">1:</span></span>An ordered list of timesteps <math alttext="0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T" class="ltx_Math" display="inline" id="alg2.l1.m1"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>t</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">⋯</mi><mo>&lt;</mo><msub><mi>t</mi><mi>L</mi></msub><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">0\leq t_{0}&lt;\cdots&lt;t_{L}\leq T</annotation><annotation encoding="application/x-llamapun">0 ≤ italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT &lt; ⋯ &lt; italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ≤ italic_T</annotation></semantics></math> to use for sampling.
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">2:</span></span>Class label <math alttext="\nu\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="alg2.l2.m1"><semantics><mrow><mi>ν</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\nu\in\{1,\dots,K\}</annotation><annotation encoding="application/x-llamapun">italic_ν ∈ { 1 , … , italic_K }</annotation></semantics></math> to condition on.
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">3:</span></span>A denoiser <math alttext="\bar{\bm{x}}_{\theta}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\times\{1,\dots,K,\varnothing\}\to\mathbb{R}^{D}" class="ltx_Math" display="inline" id="alg2.l3.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo rspace="0.055em" stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mo rspace="0.222em">×</mo><msup><mi>ℝ</mi><mi>D</mi></msup><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi mathvariant="normal">∅</mi><mo stretchy="false">}</mo></mrow></mrow><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}\colon\{t_{\ell}\}_{\ell=1}^{L}\times\mathbb{R}^{D}\times\{1,\dots,K,\varnothing\}\to\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT × { 1 , … , italic_K , ∅ } → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> for <math alttext="p_{\bm{x}\mid y}" class="ltx_Math" display="inline" id="alg2.l3.m2"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid y}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ italic_y end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p_{\bm{x}}" class="ltx_Math" display="inline" id="alg2.l3.m3"><semantics><msub><mi>p</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">p_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> (input <math alttext="\varnothing" class="ltx_Math" display="inline" id="alg2.l3.m4"><semantics><mi mathvariant="normal">∅</mi><annotation encoding="application/x-tex">\varnothing</annotation><annotation encoding="application/x-llamapun">∅</annotation></semantics></math> for
<math alttext="p_{\bm{x}}" class="ltx_Math" display="inline" id="alg2.l3.m5"><semantics><msub><mi>p</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">p_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>).
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">4:</span></span>Scale and noise level functions <math alttext="\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}" class="ltx_Math" display="inline" id="alg2.l4.m1"><semantics><mrow><mrow><mi>α</mi><mo>,</mo><mi>σ</mi></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup><mo stretchy="false">→</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\alpha,\sigma\colon\{t_{\ell}\}_{\ell=0}^{L}\to\mathbb{R}_{\geq 0}</annotation><annotation encoding="application/x-llamapun">italic_α , italic_σ : { italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT } start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">5:</span></span>Guidance strength <math alttext="\gamma\geq 0" class="ltx_Math" display="inline" id="alg2.l5.m1"><semantics><mrow><mi>γ</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\gamma\geq 0</annotation><annotation encoding="application/x-llamapun">italic_γ ≥ 0</annotation></semantics></math> (<math alttext="\gamma&gt;1" class="ltx_Math" display="inline" id="alg2.l5.m2"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&gt;1</annotation><annotation encoding="application/x-llamapun">italic_γ &gt; 1</annotation></semantics></math> preferred for
performance).
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">6:</span></span>A sample <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="alg2.l6.m1"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>, approximately from <math alttext="p_{\bm{x}\mid y}(\,\cdot\,\mid\nu)" class="ltx_math_unparsed" display="inline" id="alg2.l6.m2"><semantics><mrow><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><mi>y</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\bm{x}\mid y}(\,\cdot\,\mid\nu)</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ italic_y end_POSTSUBSCRIPT ( ⋅ ∣ italic_ν )</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">7:</span></span><span class="ltx_text ltx_font_bold">function</span> <span class="ltx_text ltx_font_smallcaps">DDIMSamplerConditionalCFG</span>(<math alttext="\bar{\bm{x}}_{\theta},\nu,\gamma,(t_{\ell})_{\ell=0}^{L}" class="ltx_Math" display="inline" id="alg2.l7.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo>,</mo><mi>ν</mi><mo>,</mo><mi>γ</mi><mo>,</mo><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>0</mn></mrow><mi>L</mi></msubsup></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta},\nu,\gamma,(t_{\ell})_{\ell=0}^{L}</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_ν , italic_γ , ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>)
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">8:</span></span>     Initialize <math alttext="\hat{\bm{x}}_{t_{L}}\sim" class="ltx_Math" display="inline" id="alg2.l8.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi>L</mi></msub></msub><mo>∼</mo><mi></mi></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{L}}\sim</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∼</annotation></semantics></math> approximate distribution of <math alttext="\bm{x}_{t_{L}}" class="ltx_Math" display="inline" id="alg2.l8.m2"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>L</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{L}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l8.m3"><semantics><mo>▷</mo><annotation encoding="application/x-tex">\triangleright</annotation><annotation encoding="application/x-llamapun">▷</annotation></semantics></math> VP <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="alg2.l8.m4"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>, VE <math alttext="\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})" class="ltx_Math" display="inline" id="alg2.l8.m5"><semantics><mrow><mi></mi><mo stretchy="false">⟹</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msubsup><mi>t</mi><mi>L</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\implies\operatorname{\mathcal{N}}(\bm{0},t_{L}^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">⟹ caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.
</span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">9:</span></span>     <span class="ltx_text ltx_font_bold">for</span> <math alttext="\ell=L,L-1,\dots,1" class="ltx_Math" display="inline" id="alg2.l9.m1"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mrow><mi>L</mi><mo>,</mo><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\ell=L,L-1,\dots,1</annotation><annotation encoding="application/x-llamapun">roman_ℓ = italic_L , italic_L - 1 , … , 1</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">10:</span></span>         Compute
<table class="ltx_equation ltx_eqn_table" id="S4.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bigl{(}(1-\gamma)\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}},\varnothing)+\gamma\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}},\nu)\bigr{)}" class="ltx_Math" display="block" id="S4.Ex3.m1"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>≐</mo><mrow><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>α</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>−</mo><mrow><mfrac><msub><mi>σ</mi><msub><mi>t</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><msub><mi>σ</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>,</mo><mi mathvariant="normal">∅</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mi mathvariant="normal">ℓ</mi></msub></msub><mo>,</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{\ell-1}}\doteq\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\hat{\bm{x}}_{t_{\ell}}+\left(\alpha_{t_{\ell-1}}-\frac{\sigma_{t_{\ell-1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}\right)\bigl{(}(1-\gamma)\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}},\varnothing)+\gamma\bar{\bm{x}}_{\theta}(t_{\ell},\hat{\bm{x}}_{t_{\ell}},\nu)\bigr{)}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≐ divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT - divide start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG italic_α start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ( ( 1 - italic_γ ) over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ∅ ) + italic_γ over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_ν ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">11:</span></span>     <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">12:</span></span>     <span class="ltx_text ltx_font_bold">return</span> <math alttext="\hat{\bm{x}}_{t_{0}}" class="ltx_Math" display="inline" id="alg2.l12.m1"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><msub><mi>t</mi><mn>0</mn></msub></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{t_{0}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" style="font-size:80%;">13:</span></span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">function</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx107" title="">HS22</a>]</cite> report strong empirical performance for class-conditional
image generation with classifier-free guidance, and it has become a mainstay of
the largest-scale practical diffusion models, such as Stable Diffusion
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite> and its derivatives.
At the same time, its derivation is rather opaque and empirically-motivated,
giving little insight into the mechanisms behind its strong performance.
A number of theoretical works have studied this, providing explanations for some
parts of the overall CFG methodology
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx28" title="">BN24a</a>, <a class="ltx_ref" href="bib.html#bibx160" title="">LWQ25</a>, <a class="ltx_ref" href="bib.html#bibx300" title="">WCL+24</a>]</cite>—itself encompassing denoiser
parameterization and training, as well as configuration of the guidance strength
and performance at sampling time.
Below, we will give an interpretation in the simplifying setting
of a Gaussian mixture model data distribution and denoiser, which will
demonstrate an insight into the <span class="ltx_text ltx_font_italic">parameterization</span> of the denoiser in the presence of
such low-dimensional structures.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 6.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample3.p1">
<p class="ltx_p">Let us recall the low-rank mixture of Gaussians data generating process we studied in
<a class="ltx_ref" href="Ch3.html#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> (and specifically, the form in
<a class="ltx_ref" href="Ch3.html#S2.E42" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.42</span></a>). Given <math alttext="K\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmexample3.p1.m1"><semantics><mrow><mi>K</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">K\in\mathbb{N}</annotation><annotation encoding="application/x-llamapun">italic_K ∈ blackboard_N</annotation></semantics></math> classes, we assume that</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\sim\frac{1}{K}\sum_{k=1}^{K}\operatorname{\mathcal{N}}(\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top})," class="ltx_Math" display="block" id="S4.E17.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mfrac><mn>1</mn><mi>K</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}\sim\frac{1}{K}\sum_{k=1}^{K}\operatorname{\mathcal{N}}(\bm{0},\bm{U}_{k}\bm{U}_{k}^{\top}),</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where each <math alttext="\bm{U}_{k}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}" class="ltx_Math" display="inline" id="Thmexample3.p1.m2"><semantics><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>P</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\in\mathsf{O}(D,P)\subseteq\mathbb{R}^{D\times P}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ sansserif_O ( italic_D , italic_P ) ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> is a matrix with
orthogonal columns, and <math alttext="P\ll D" class="ltx_Math" display="inline" id="Thmexample3.p1.m3"><semantics><mrow><mi>P</mi><mo>≪</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">P\ll D</annotation><annotation encoding="application/x-llamapun">italic_P ≪ italic_D</annotation></semantics></math>.
Moreover, we assume that the class label <math alttext="y\in[K]" class="ltx_Math" display="inline" id="Thmexample3.p1.m4"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y\in[K]</annotation><annotation encoding="application/x-llamapun">italic_y ∈ [ italic_K ]</annotation></semantics></math> is a deterministic
function of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample3.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> mapping an example to its corresponding mixture component.
Applying the analysis in <a class="ltx_ref" href="Ch3.html#Thmexample2" title="Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> (and the
subsequent analysis of the low-rank case, culminating in
<a class="ltx_ref" href="Ch3.html#S2.E56" title="In 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3.2.56</span></a>), we obtain for the class-conditional optimal
denoisers</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu]=\frac{1}{1+t^{2}}\bm{U}_{\nu}\bm{U}_{\nu}^{\top}\bm{\xi}" class="ltx_Math" display="block" id="S4.E18.m1"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>=</mo><mi>ν</mi></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>ν</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>ν</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi},y=\nu]=\frac{1}{1+t^{2}}\bm{U}_{\nu}\bm{U}_{\nu}^{\top}\bm{\xi}</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ , italic_y = italic_ν ] = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_ν end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_ν end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_ξ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for each <math alttext="\nu\in[K]" class="ltx_Math" display="inline" id="Thmexample3.p1.m6"><semantics><mrow><mi>ν</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\nu\in[K]</annotation><annotation encoding="application/x-llamapun">italic_ν ∈ [ italic_K ]</annotation></semantics></math>, and for the optimal unconditional denoiser, we obtain</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]=\frac{1}{1+t^{2}}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{\xi}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{\xi}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{\xi}." class="ltx_Math" display="block" id="S4.E19.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]=\frac{1}{1+t^{2}}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{\xi}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{\xi}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{\xi}.</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ] = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_ξ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_ξ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_ξ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As a result, we can express the CFG denoiser with guidance strength <math alttext="\gamma&gt;1" class="ltx_Math" display="inline" id="Thmexample3.p1.m7"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&gt;1</annotation><annotation encoding="application/x-llamapun">italic_γ &gt; 1</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{x}}^{\mathrm{CFG,\,ideal}}(t,\bm{x}_{t},y)=\frac{1}{1+t^{2}}\left((1-\gamma)\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}+\gamma\bm{U}_{y}\bm{U}_{y}^{\top}\right)\bm{x}_{t}." class="ltx_Math" display="block" id="S4.E20.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mrow><mi>CFG</mi><mo rspace="0.337em">,</mo><mi>ideal</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}^{\mathrm{CFG,\,ideal}}(t,\bm{x}_{t},y)=\frac{1}{1+t^{2}}\left((1-\gamma)\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}+\gamma\bm{U}_{y}\bm{U}_{y}^{\top}\right)\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT roman_CFG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ( 1 - italic_γ ) ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_γ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This denoiser has a simple, interpretable form. The first term, corresponding
to the unconditional denoiser, performs denoising of the signal <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p1.m8"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
against an average of the denoisers associated to each subspace, weighted by
how correlated <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p1.m9"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is to each subspace. The second term, corresponding to
the conditional denoiser, simply performs denoising with the conditioning
class’s denoiser. The CFG scheme further averages these two denoisers:
the effect can be gleaned from the refactoring</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\bar{\bm{x}}^{\mathrm{CFG,\,ideal}}(t,\bm{x}_{t},y)=\frac{1}{1+t^{2}}&amp;\Biggl{(}\left[\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\right]\bm{U}_{y}\bm{U}_{y}^{\top}\\
&amp;\quad+(1-\gamma)\sum_{k\neq y}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\Biggr{)}\bm{x}_{t}.\end{split}" class="ltx_Math" display="block" id="S4.E21.m1"><semantics><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mrow><msup><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mrow><mi>CFG</mi><mo rspace="0.337em">,</mo><mi>ideal</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mo maxsize="260%" minsize="260%">(</mo><mrow><mo>[</mo><mi>γ</mi><mo>+</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>γ</mi><mo stretchy="false">)</mo></mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo>]</mo></mrow><msub><mi>𝑼</mi><mi>y</mi></msub><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mo>+</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>γ</mi><mo stretchy="false">)</mo></mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>y</mi></mrow></munder><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><msub><mi>𝑼</mi><mi>k</mi></msub><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo maxsize="260%" minsize="260%">)</mo><mi>𝒙</mi><msub><mi></mi><mi>t</mi></msub><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}\bar{\bm{x}}^{\mathrm{CFG,\,ideal}}(t,\bm{x}_{t},y)=\frac{1}{1+t^{2}}&amp;\Biggl{(}\left[\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\right]\bm{U}_{y}\bm{U}_{y}^{\top}\\
&amp;\quad+(1-\gamma)\sum_{k\neq y}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\Biggr{)}\bm{x}_{t}.\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL over¯ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT roman_CFG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_CELL start_CELL ( [ italic_γ + ( 1 - italic_γ ) divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ] bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL + ( 1 - italic_γ ) ∑ start_POSTSUBSCRIPT italic_k ≠ italic_y end_POSTSUBSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We have</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}=1," class="ltx_Math" display="block" id="S4.E22.m1"><semantics><mrow><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{k}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}=1,</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG = 1 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and each summand is nonnegative, hence also bounded above by <math alttext="1" class="ltx_Math" display="inline" id="Thmexample3.p1.m10"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>.
So we can conclude two regimes for the terms in
<a class="ltx_ref" href="#S4.E21" title="In Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.21</span></a>:</p>
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Well-correlated regime:</span> If <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> correlates well with
<math alttext="\bm{U}_{y}" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m2"><semantics><msub><mi>𝑼</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\bm{U}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math>, then the normalized weight corresponding to the <math alttext="k=y" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m3"><semantics><mrow><mi>k</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">k=y</annotation><annotation encoding="application/x-llamapun">italic_k = italic_y</annotation></semantics></math> summand in
the unconditional denoiser is near to <math alttext="1" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m4"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>.
Then</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\approx 1," class="ltx_Math" display="block" id="S4.E23.m1"><semantics><mrow><mrow><mrow><mi>γ</mi><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mrow></mrow><mo>≈</mo><mn>1</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\approx 1,</annotation><annotation encoding="application/x-llamapun">italic_γ + ( 1 - italic_γ ) divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ≈ 1 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">all other weights are necessarily near to zero, and the CFG denoiser
is approximately equal to the denoiser associated to the conditioning
class <math alttext="y" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m5"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Poorly-correlated regime:</span> In contrast, if <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
<span class="ltx_text ltx_font_italic">does not</span> correlate well with <math alttext="\bm{U}_{y}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m2"><semantics><msub><mi>𝑼</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\bm{U}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> (say because <math alttext="t" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m3"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> is large),
then the normalized weight corresponding to the <math alttext="k=y" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m4"><semantics><mrow><mi>k</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">k=y</annotation><annotation encoding="application/x-llamapun">italic_k = italic_y</annotation></semantics></math> summand in the
unconditional denoiser is near to <math alttext="0" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m5"><mn>0</mn></math>. As a result,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\approx\gamma," class="ltx_Math" display="block" id="S4.E24.m1"><semantics><mrow><mrow><mrow><mi>γ</mi><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mrow></mrow><mo>≈</mo><mi>γ</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\gamma+(1-\gamma)\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{i}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}\approx\gamma,</annotation><annotation encoding="application/x-llamapun">italic_γ + ( 1 - italic_γ ) divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ≈ italic_γ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and thus the guidance strength <math alttext="\gamma\gg 1" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m6"><semantics><mrow><mi>γ</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma\gg 1</annotation><annotation encoding="application/x-llamapun">italic_γ ≫ 1</annotation></semantics></math> places a large positive
weight on the denoiser associated to <math alttext="y" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m7"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>.
Meanwhile, in the second term of <a class="ltx_ref" href="#S4.E21" title="In Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.4.21</span></a>,
any classes <math alttext="k\neq y" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m8"><semantics><mrow><mi>k</mi><mo>≠</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">k\neq y</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_y</annotation></semantics></math> that are well-correlated with <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m9"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
receive a large <span class="ltx_text ltx_font_italic">negative</span> weight from the <math alttext="1-\gamma" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m10"><semantics><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">1-\gamma</annotation><annotation encoding="application/x-llamapun">1 - italic_γ</annotation></semantics></math>
coefficient.
This simultaneously has the effect of making the denoised signal vastly
more correlated with the conditioning class <math alttext="y" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m11"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>, and making it negatively
correlated with the previous iterate (i.e., the iterate before denoising).
In other words, CFG steers the iterative denoising process towards the
conditioning class and away from the previous iterate, a different
dynamics from purely conditional sampling (i.e., the case <math alttext="\gamma=1" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m12"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma=1</annotation><annotation encoding="application/x-llamapun">italic_γ = 1</annotation></semantics></math>).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Thmexample3.p2">
<p class="ltx_p">We now perform a further analysis of the form of this guided denoiser in
order to make some inferences about the role of CFG. Many of these insights
will be relevant to <span class="ltx_text ltx_font_italic">general</span> data distributions with low-dimensional
geometric structure, as well.
First, notice that the CFG denoiser (<a class="ltx_ref" href="#S4.E20" title="Equation 6.4.20 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.20</span></a>)
takes a simple form in the setting where <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p2.m1"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> correlates significantly more
strongly with a single subspace <math alttext="\bm{U}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p2.m2"><semantics><msub><mi>𝑼</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\bm{U}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> than any other <math alttext="\bm{U}_{y^{\prime}}" class="ltx_Math" display="inline" id="Thmexample3.p2.m3"><semantics><msub><mi>𝑼</mi><msup><mi>y</mi><mo>′</mo></msup></msub><annotation encoding="application/x-tex">\bm{U}_{y^{\prime}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>. Indeed,
because the ratio of weights in the class-conditional denoiser is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y^{\prime}}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}=\exp\left(\frac{1}{2t^{2}(1+t^{2})}\left(\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}-\|\bm{U}_{y^{\prime}}^{\top}\bm{x}_{t}\|_{2}^{2}\right)\right)," class="ltx_Math" display="block" id="S4.E25.m1"><semantics><mrow><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><msup><mi>y</mi><mo>′</mo></msup><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>−</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>𝑼</mi><msup><mi>y</mi><mo>′</mo></msup><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\|\bm{U}_{y^{\prime}}^{\top}\bm{x}_{t}\|_{2}^{2}\right)}=\exp\left(\frac{1}{2t^{2}(1+t^{2})}\left(\|\bm{U}_{y}^{\top}\bm{x}_{t}\|_{2}^{2}-\|\bm{U}_{y^{\prime}}^{\top}\bm{x}_{t}\|_{2}^{2}\right)\right),</annotation><annotation encoding="application/x-llamapun">divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ∥ bold_italic_U start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG = roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ( ∥ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - ∥ bold_italic_U start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">a large separation between the correlation of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p2.m4"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="\bm{U}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p2.m5"><semantics><msub><mi>𝑼</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\bm{U}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> and other
subspaces <math alttext="\bm{U}_{y^{\prime}}" class="ltx_Math" display="inline" id="Thmexample3.p2.m6"><semantics><msub><mi>𝑼</mi><msup><mi>y</mi><mo>′</mo></msup></msub><annotation encoding="application/x-tex">\bm{U}_{y^{\prime}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> implies that the sum over <math alttext="k" class="ltx_Math" display="inline" id="Thmexample3.p2.m7"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> concentrates on the <math alttext="k=y" class="ltx_Math" display="inline" id="Thmexample3.p2.m8"><semantics><mrow><mi>k</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">k=y</annotation><annotation encoding="application/x-llamapun">italic_k = italic_y</annotation></semantics></math>
summand, giving that <span class="ltx_text ltx_font_italic">the CFG denoiser remains equal to the
class-conditioned denoiser</span>. Moreover, when <math alttext="t\approx 0" class="ltx_Math" display="inline" id="Thmexample3.p2.m9"><semantics><mrow><mi>t</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\approx 0</annotation><annotation encoding="application/x-llamapun">italic_t ≈ 0</annotation></semantics></math>, the magnitude of
any such gap is amplified in the exponential, making this concentration on the
<math alttext="k=y" class="ltx_Math" display="inline" id="Thmexample3.p2.m10"><semantics><mrow><mi>k</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">k=y</annotation><annotation encoding="application/x-llamapun">italic_k = italic_y</annotation></semantics></math> summand even stronger. In particular, for small times <math alttext="t" class="ltx_Math" display="inline" id="Thmexample3.p2.m11"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> (i.e., near to
the support of the data distribution), CFG denoising is no different from
standard class-conditional denoising—implying that it will converge stably
once it has reached such a configuration.
Thus, the empirical benefits of CFG should be due to its behavior in cases
where <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p2.m12"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is not unambiguously from a single class.</p>
</div>
<div class="ltx_para" id="Thmexample3.p3">
<p class="ltx_p">Next, we consider the problem of parameterizing a learnable denoiser
<math alttext="\bm{x}_{\theta}^{\mathrm{CFG}}" class="ltx_Math" display="inline" id="Thmexample3.p3.m1"><semantics><msubsup><mi>𝒙</mi><mi>θ</mi><mi>CFG</mi></msubsup><annotation encoding="application/x-tex">\bm{x}_{\theta}^{\mathrm{CFG}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CFG end_POSTSUPERSCRIPT</annotation></semantics></math> to represent the optimal denoiser
(<a class="ltx_ref" href="#S4.E20" title="Equation 6.4.20 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.20</span></a>).
Here, it may initially seem that the setting of classification of a mixture
distribution is too much of a special case relative to learning practical data
distributions, as the ideal denoiser (<a class="ltx_ref" href="#S4.E20" title="Equation 6.4.20 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.20</span></a>) has
in this setting the simple form of a <span class="ltx_text ltx_font_italic">hard assignment</span> of the
noisy signal <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to the (denoiser associated to) the subspace <math alttext="\bm{U}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p3.m3"><semantics><msub><mi>𝑼</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\bm{U}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math>
corresponding to the true class label <math alttext="y" class="ltx_Math" display="inline" id="Thmexample3.p3.m4"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample3.p3.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, averaged with the
<span class="ltx_text ltx_font_italic">soft assignment</span> denoiser associated to all subspaces <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmexample3.p3.m6"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, with
weights given by the correlations of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with these different subspaces.
However, we can extract a more general form for the class-conditional denoiser
in this example which is relevant for practical parameterization using the
geometric structure of the mixture of Gaussians distribution, which actually
parallels the kinds of geometric structure common in real-world data.
More precisely, we add an additional assumption associated to the subspaces
<math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmexample3.p3.m8"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> being ‘distinguishable’ from one another, which is natural in
practice: specifically, we assume that for any pair of indices <math alttext="k,k^{\prime}\in[K]" class="ltx_Math" display="inline" id="Thmexample3.p3.m9"><semantics><mrow><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>′</mo></msup></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k,k^{\prime}\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k , italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ [ italic_K ]</annotation></semantics></math> with <math alttext="k\neq k^{\prime}" class="ltx_Math" display="inline" id="Thmexample3.p3.m10"><semantics><mrow><mi>k</mi><mo>≠</mo><msup><mi>k</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">k\neq k^{\prime}</annotation><annotation encoding="application/x-llamapun">italic_k ≠ italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, we can find a set of <math alttext="K" class="ltx_Math" display="inline" id="Thmexample3.p3.m11"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> directions <math alttext="\bm{v}_{k}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexample3.p3.m12"><semantics><mrow><msub><mi>𝒗</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{v}_{k}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> such
that</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{k}=\bm{v}_{k},\quad\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{\top}\bm{v}_{k}=\mathbf{0},\enspace k^{\prime}\neq k." class="ltx_Math" display="block" id="S4.E26.m1"><semantics><mrow><mrow><mrow><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>k</mi></msub></mrow><mo>=</mo><msub><mi>𝒗</mi><mi>k</mi></msub></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><msub><mi>𝑼</mi><msup><mi>k</mi><mo>′</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><msup><mi>k</mi><mo>′</mo></msup><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>k</mi></msub></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo rspace="0.667em">,</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>≠</mo><mi>k</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{k}=\bm{v}_{k},\quad\bm{U}_{k^{\prime}}\bm{U}_{k^{\prime}}^{\top}\bm{v}_{k}=\mathbf{0},\enspace k^{\prime}\neq k.</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_U start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_0 , italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≠ italic_k .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is a slightly stronger assumption than simple distinguishability, but it
should be noted that it is not overly restrictive: for example, it still
allows the subspaces <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="Thmexample3.p3.m13"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> to have significant correlations with one
another.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span> More generally, this assumption is naturally formulated as an
<span class="ltx_text ltx_font_italic">incoherence condition</span> between the subspaces <math alttext="\bm{U}_{[K]}" class="ltx_Math" display="inline" id="footnote10.m1"><semantics><msub><mi>𝑼</mi><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></msub><annotation encoding="application/x-tex">\bm{U}_{[K]}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT</annotation></semantics></math>, a familiar
notion from the theory of compressive sensing.</span></span></span>
These vectors <math alttext="\bm{v}_{k}" class="ltx_Math" display="inline" id="Thmexample3.p3.m14"><semantics><msub><mi>𝒗</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{v}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> can then be thought of as <span class="ltx_text ltx_font_italic">embeddings</span> of the
class label <math alttext="y\in[K]" class="ltx_Math" display="inline" id="Thmexample3.p3.m15"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y\in[K]</annotation><annotation encoding="application/x-llamapun">italic_y ∈ [ italic_K ]</annotation></semantics></math>, and we can use them to define a more general operator
that can represent both the unconditional and class-conditional denoisers.
More precisely, consider the mapping</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{x}_{t},\bm{v})\mapsto\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}." class="ltx_Math" display="block" id="S4.E27.m1"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>𝒗</mi><mo stretchy="false">)</mo></mrow><mo rspace="0.111em" stretchy="false">↦</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{v})\mapsto\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_v ) ↦ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we substitute <math alttext="\bm{v}=\bm{v}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p3.m16"><semantics><mrow><mi>𝒗</mi><mo>=</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">\bm{v}=\bm{v}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_v = bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> for some <math alttext="y\in[K]" class="ltx_Math" display="inline" id="Thmexample3.p3.m17"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y\in[K]</annotation><annotation encoding="application/x-llamapun">italic_y ∈ [ italic_K ]</annotation></semantics></math>, we get</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx85">
<tbody id="S4.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{y}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}_{y}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}&amp;=\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)+K-1}\bm{U}_{y}\bm{U}_{y}^{\top}\bm{x}_{t}\\
&amp;\quad+\sum_{k\neq y}\frac{1}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)+K-1}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}.\end{split}" class="ltx_Math" display="inline" id="S4.E28.m1"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mi>K</mi></mrow><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>y</mi></mrow></munder></mstyle><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mi>K</mi></mrow><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{y}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}_{y}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}&amp;=\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)+K-1}\bm{U}_{y}\bm{U}_{y}^{\top}\bm{x}_{t}\\
&amp;\quad+\sum_{k\neq y}\frac{1}{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{v}_{y}\right)+K-1}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}.\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) + italic_K - 1 end_ARG bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL + ∑ start_POSTSUBSCRIPT italic_k ≠ italic_y end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) + italic_K - 1 end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.28)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, because <math alttext="\bm{x}_{t}=\alpha_{t}\bm{x}+\sigma_{t}\bm{g}" class="ltx_Math" display="inline" id="Thmexample3.p3.m18"><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>α</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>+</mo><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\alpha_{t}\bm{x}+\sigma_{t}\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_σ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_g</annotation></semantics></math>, if the subspace dimension
<math alttext="P" class="ltx_Math" display="inline" id="Thmexample3.p3.m19"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation><annotation encoding="application/x-llamapun">italic_P</annotation></semantics></math> is sufficiently large—for example, if we consider a large-scale,
asymptotic regime where <math alttext="P,D\to\infty" class="ltx_Math" display="inline" id="Thmexample3.p3.m20"><semantics><mrow><mrow><mi>P</mi><mo>,</mo><mi>D</mi></mrow><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">P,D\to\infty</annotation><annotation encoding="application/x-llamapun">italic_P , italic_D → ∞</annotation></semantics></math> with their ratio <math alttext="P/D" class="ltx_Math" display="inline" id="Thmexample3.p3.m21"><semantics><mrow><mi>P</mi><mo>/</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">P/D</annotation><annotation encoding="application/x-llamapun">italic_P / italic_D</annotation></semantics></math> converging to
a fixed constant—we have for <math alttext="t\approx 0" class="ltx_Math" display="inline" id="Thmexample3.p3.m22"><semantics><mrow><mi>t</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\approx 0</annotation><annotation encoding="application/x-llamapun">italic_t ≈ 0</annotation></semantics></math> that <math alttext="\|\bm{x}_{t}\|_{2}" class="ltx_Math" display="inline" id="Thmexample3.p3.m23"><semantics><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|\bm{x}_{t}\|_{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is close to
<math alttext="\sqrt{P}" class="ltx_Math" display="inline" id="Thmexample3.p3.m24"><semantics><msqrt><mi>P</mi></msqrt><annotation encoding="application/x-tex">\sqrt{P}</annotation><annotation encoding="application/x-llamapun">square-root start_ARG italic_P end_ARG</annotation></semantics></math>, by the concentration of measure phenomenon<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>One of
a handful of <span class="ltx_text ltx_font_italic">blessings of dimensionality</span>—see
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</span></span></span>. Then
by the argument in the previous paragraph, we have in this regime that for
<span class="ltx_text ltx_font_italic">almost all</span> realizations of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m25"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, the following approximation
holds:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{y}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}_{y}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}\approx\bm{U}_{y}\bm{U}_{y}^{\top}\bm{x}_{t}." class="ltx_Math" display="block" id="S4.E29.m1"><semantics><mrow><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo>≈</mo><mrow><msub><mi>𝑼</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>y</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}_{y}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}_{y}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}\approx\bm{U}_{y}\bm{U}_{y}^{\top}\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≈ bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This argument shows that the operator
(<a class="ltx_ref" href="#S4.E27" title="Equation 6.4.27 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.27</span></a>) is, with overwhelming
probability, <span class="ltx_text ltx_font_italic">equal to the optimal class-conditional denoiser
(<a class="ltx_ref" href="#S4.E18" title="Equation 6.4.18 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.18</span></a>) for <math alttext="y" class="ltx_Math" display="inline" id="Thmexample3.p3.m26"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>
when <math alttext="\bm{v}=\bm{v}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p3.m27"><semantics><mrow><mi>𝐯</mi><mo>=</mo><msub><mi>𝐯</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">\bm{v}=\bm{v}_{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_v = bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math></span>! In intuitive terms, at small noise levels <math alttext="t\approx 0" class="ltx_Math" display="inline" id="Thmexample3.p3.m28"><semantics><mrow><mi>t</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\approx 0</annotation><annotation encoding="application/x-llamapun">italic_t ≈ 0</annotation></semantics></math>—corresponding to the structure-enforcing portion of the denoising
process—plugging in the embedding for a given class <math alttext="y" class="ltx_Math" display="inline" id="Thmexample3.p3.m29"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> to the second
argument of the operator (<a class="ltx_ref" href="#S4.E27" title="Equation 6.4.27 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.27</span></a>)
leads the resulting function of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m30"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to well-approximate the optimal
class-conditional denoiser (<a class="ltx_ref" href="#S4.E18" title="Equation 6.4.18 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.18</span></a>) for
<math alttext="y" class="ltx_Math" display="inline" id="Thmexample3.p3.m31"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math>. Moreover, it is evident that plugging in <math alttext="\bm{v}=\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m32"><semantics><mrow><mi>𝒗</mi><mo>=</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\bm{v}=\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_v = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to the operator
(<a class="ltx_ref" href="#S4.E27" title="Equation 6.4.27 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.27</span></a>) yields (exactly) the
optimal unconditional denoiser (<a class="ltx_ref" href="#S4.E19" title="Equation 6.4.19 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.19</span></a>) for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmexample3.p3.m33"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
Thus, this operator provides a unified way to parameterize the constituent
operators in the optimal denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample3.p3.m34"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> <span class="ltx_text ltx_font_italic">within a single
‘network’</span>: it is enough to add the output of an instantiation of
(<a class="ltx_ref" href="#S4.E27" title="Equation 6.4.27 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.27</span></a>) with input <math alttext="(\bm{x}_{t},\bm{x}_{t})" class="ltx_Math" display="inline" id="Thmexample3.p3.m35"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> to an instantiation with input <math alttext="(\bm{x}_{t},\bm{v}_{y}" class="ltx_math_unparsed" display="inline" id="Thmexample3.p3.m36"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{v}_{y}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math>). The resulting
operator is a function of <math alttext="(\bm{x}_{t},y)" class="ltx_Math" display="inline" id="Thmexample3.p3.m37"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},y)</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y )</annotation></semantics></math>, and computationally, the subspaces
<math alttext="(\bm{U}_{k})_{k=1}^{K}" class="ltx_Math" display="inline" id="Thmexample3.p3.m38"><semantics><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">(\bm{U}_{k})_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> and embeddings <math alttext="y\mapsto\bm{v}_{y}" class="ltx_Math" display="inline" id="Thmexample3.p3.m39"><semantics><mrow><mi>y</mi><mo stretchy="false">↦</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">y\mapsto\bm{v}_{y}</annotation><annotation encoding="application/x-llamapun">italic_y ↦ bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> become its learnable
parameters.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample3.p3.m40"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p"><a class="ltx_ref" href="#Thmexample3" title="Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.3</span></a> shows that in the special case of
a low-rank mixture of Gaussians data distribution for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p10.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with incoherent
components, operators of the form</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{x}_{t},\bm{v})\mapsto\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}" class="ltx_Math" display="block" id="S4.E30.m1"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo>,</mo><mi>𝒗</mi><mo stretchy="false">)</mo></mrow><mo rspace="0.111em" stretchy="false">↦</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒙</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">(\bm{x}_{t},\bm{v})\mapsto\sum_{k=1}^{K}\frac{\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{v}\right)}{\sum_{i=1}^{K}\exp\left(\frac{1}{2t^{2}(1+t^{2})}\bm{x}_{t}^{\top}\bm{U}_{i}\bm{U}_{i}^{\top}\bm{v}\right)}\bm{U}_{k}\bm{U}_{k}^{\top}\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_v ) ↦ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v ) end_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">provide a sufficiently rich class of operators to parameterize the MMSE-optimal
denoiser for noisy observations <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p10.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p10.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, in the setting of
classifier-free guidance where one network is to be used to represent both the
unconditional and class-conditional denoisers for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p10.m4"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. For such operators,
the auxiliary input <math alttext="\bm{v}" class="ltx_Math" display="inline" id="S4.SS1.p10.m5"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> can be taken as either <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S4.SS1.p10.m6"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> or a suitable embedding
of the class label <math alttext="y\mapsto\bm{v}_{y}" class="ltx_Math" display="inline" id="S4.SS1.p10.m7"><semantics><mrow><mi>y</mi><mo stretchy="false">↦</mo><msub><mi>𝒗</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">y\mapsto\bm{v}_{y}</annotation><annotation encoding="application/x-llamapun">italic_y ↦ bold_italic_v start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT</annotation></semantics></math> in order to realize such a denoiser.
Based on the framework in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, which develops deep network
architectures suitable for transforming more general data distributions to
structured representations using the low-rank mixture of Gaussians model as
a primitive, it is natural to imagine that operators of the type
(<a class="ltx_ref" href="#S4.E30" title="Equation 6.4.30 ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.30</span></a>) may be leveraged in
denoisers for
general data distributions <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS1.p10.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with low-dimensional geometrically-structured
components that are sufficiently distinguishable (say, incoherent) from one
another.
The next section demonstrates that this is indeed the case.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4.2 </span>Caption Conditioned Image Generation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">In the previous subsection, we have formulated denoisers for class-conditional
denoising with classifier-free guidance, a ubiquitous practical methodology used
in the largest-scale diffusion models, and shown how to parameterize them (in
<a class="ltx_ref" href="#Thmexample3" title="Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.3</span></a>) in the special case of a low-rank
Gaussian mixture model data distribution.
One interesting byproduct of this example is that it highlights the crucial role
of <span class="ltx_text ltx_font_italic">embeddings</span> of the class label <math alttext="y" class="ltx_Math" display="inline" id="S4.SS2.p1.m1"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> into a common space with the image <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in
order to provide a concise and unified scheme for parameterizing the optimal
denoisers (conditional and unconditional).
Below, we will describe an early such instantiation, which
formed the basis for the original open-source Stable Diffusion implementation
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite>.
In this setting, the embedding and subsequent conditioning is performed not on
a class label, but a text prompt, which describes the desired image content
(<a class="ltx_ref" href="#F13" title="In 6.4.2 Caption Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.13</span></a>). We denote the raw tokenized text prompt as <math alttext="\bm{Y}\in\mathbb{R}^{D_{\mathrm{text}}\times N}" class="ltx_Math" display="inline" id="S4.SS2.p1.m3"><semantics><mrow><mi>𝒀</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>D</mi><mi>text</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Y}\in\mathbb{R}^{D_{\mathrm{text}}\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT roman_text end_POSTSUBSCRIPT × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> in this context, since it corresponds to a sequence of
vectors—in <a class="ltx_ref" href="Ch7.html#S4" title="7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>, we describe the process of encoding a text
sequence as a vector representation in detail.</p>
</div>
<figure class="ltx_figure" id="F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf1"><img alt="(a)" class="ltx_graphics" id="F13.sf1.g1" src="chapters/chapter6/figs/tti-train.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf2"><img alt="(a)" class="ltx_graphics" id="F13.sf2.g1" src="chapters/chapter6/figs/tti-inf.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.13</span>: </span><span class="ltx_text" style="font-size:90%;">A high-level schematic of training and applying a text-to-image
generative model, via conditional generation with a text prompt. <span class="ltx_text ltx_font_bold">Left:</span>
To train a text-to-image model, a large dataset of images paired with
corresponding text captions is used. An encoder is used to map the captions to
sequences of vectors, which are used as conditioning signals for a conditional
denoiser, trained as described in <a class="ltx_ref" href="#S4.SS1" title="6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.1</span></a>. The text encoder may be
pretrained and frozen, or jointly trained with the denoiser. <span class="ltx_text ltx_font_bold">Right:</span>
When applying a trained model, a desired text prompt is used as conditioning,
then sampling is performed with the trained model, as in
<a class="ltx_ref" href="#alg2" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.2</span></a> (<span class="ltx_text ltx_font_italic">mutatis mutandis</span> for
use with an encoded text prompt). For full details of the process of encoding
text to a sequence of vectors, see <a class="ltx_ref" href="Ch7.html#S4" title="7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">Stable Diffusion follows the conditional generation methodology we outline in
<a class="ltx_ref" href="#S4.SS1" title="6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.1</span></a>, with two key modifications: (i) The conditioning signal is a
tokenized text prompt <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S4.SS2.p2.m1"><semantics><mi>𝒀</mi><annotation encoding="application/x-tex">\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y</annotation></semantics></math>, rather than a class label; (ii) Image denoising is
performed in “latent” space rather than on raw pixels, using a specialized,
pretrained variational autoencoder pair <math alttext="f:\mathbb{R}^{D_{\mathrm{img}}}\to\mathbb{R}^{d_{\mathrm{img}}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m2"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><msub><mi>D</mi><mi>img</mi></msub></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><msub><mi>d</mi><mi>img</mi></msub></msup></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{D_{\mathrm{img}}}\to\mathbb{R}^{d_{\mathrm{img}}}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="g:\mathbb{R}^{d_{\mathrm{img}}}\to\mathbb{R}^{D_{\mathrm{img}}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m3"><semantics><mrow><mi>g</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><msub><mi>d</mi><mi>img</mi></msub></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mi>img</mi></msub></msup></mrow></mrow><annotation encoding="application/x-tex">g:\mathbb{R}^{d_{\mathrm{img}}}\to\mathbb{R}^{D_{\mathrm{img}}}</annotation><annotation encoding="application/x-llamapun">italic_g : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> (see <a class="ltx_ref" href="Ch5.html#S1.SS4" title="5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1.4</span></a>), where <math alttext="f" class="ltx_Math" display="inline" id="S4.SS2.p2.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>
is the encoder and <math alttext="g" class="ltx_Math" display="inline" id="S4.SS2.p2.m5"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> the decoder. Subsequent model development has shown that
point (ii) is an efficiency issue, rather than a core conceptual one, so we will
not focus on it, other than to mention that it simply leads to the following
straightforward modifications to the text-to-image pipeline sketched in
<a class="ltx_ref" href="#F13" title="In 6.4.2 Caption Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.13</span></a>:</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">At training time</span>,
the encoder <math alttext="f:\bm{x}\mapsto\bm{z}" class="ltx_Math" display="inline" id="S4.I3.i1.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">f:\bm{x}\mapsto\bm{z}</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_x ↦ bold_italic_z</annotation></semantics></math> is used to generate the denoising targets, and
all denoising is performed on the encoded representations <math alttext="\bm{z}_{t}\in\mathbb{R}^{d_{\mathrm{img}}}" class="ltx_Math" display="inline" id="S4.I3.i1.p1.m2"><semantics><mrow><msub><mi>𝒛</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>d</mi><mi>img</mi></msub></msup></mrow><annotation encoding="application/x-tex">\bm{z}_{t}\in\mathbb{R}^{d_{\mathrm{img}}}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>;</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">At generation time</span>, sampling is performed on the
representations <math alttext="\hat{\bm{z}}_{t}" class="ltx_Math" display="inline" id="S4.I3.i2.p1.m1"><semantics><msub><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\hat{\bm{z}}_{t}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and
the final image is generated by applying the decoder <math alttext="g(\hat{\bm{z}}_{0})" class="ltx_Math" display="inline" id="S4.I3.i2.p1.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\hat{\bm{z}}_{0})</annotation><annotation encoding="application/x-llamapun">italic_g ( over^ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
<p class="ltx_p">In contrast, issue (i) is essential, and the approach proposed to treat it
represents one of the lasting methodological innovations of
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite>.
In the context of the iterative conditional denoising framework we have
developed in <a class="ltx_ref" href="#S4.SS1" title="6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.1</span></a>, this concerns the parameterization of the denoisers
<math alttext="\bar{\bm{z}}_{\theta}(t,\bm{z}_{t},\bm{Y}^{+})" class="ltx_Math" display="inline" id="S4.SS2.p2.m6"><semantics><mrow><msub><mover accent="true"><mi>𝒛</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>𝒛</mi><mi>t</mi></msub><mo>,</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{z}}_{\theta}(t,\bm{z}_{t},\bm{Y}^{+})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT )</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>In the setting of text
conditioning, the ‘augmented’ label <math alttext="\bm{Y}^{+}" class="ltx_Math" display="inline" id="footnote12.m1"><semantics><msup><mi>𝒀</mi><mo>+</mo></msup><annotation encoding="application/x-tex">\bm{Y}^{+}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>, which is either the encoded text
prompt or <math alttext="\varnothing" class="ltx_Math" display="inline" id="footnote12.m2"><semantics><mi mathvariant="normal">∅</mi><annotation encoding="application/x-tex">\varnothing</annotation><annotation encoding="application/x-llamapun">∅</annotation></semantics></math>, denoting unconditional denoising, is often implemented
by mapping <math alttext="\varnothing" class="ltx_Math" display="inline" id="footnote12.m3"><semantics><mi mathvariant="normal">∅</mi><annotation encoding="application/x-tex">\varnothing</annotation><annotation encoding="application/x-llamapun">∅</annotation></semantics></math> to the empty string “”, then encoding this text
prompt with the tokenizer as usual. This gives a simple, unified way to treat
conditional and unconditional denoising with text conditioning.</span></span></span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite> implement text conditioning in the denoiser using
a layer known as cross attention, inspired by the original encoder-decoder
transformer architecture of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite>. Cross attention is
implemented as follows. We let <math alttext="\tau:\mathbb{R}^{D_{\mathrm{text}}\times N}\to\mathbb{R}^{d_{\mathrm{model}}\times N_{\mathrm{text}}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m7"><semantics><mrow><mi>τ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mrow><msub><mi>D</mi><mi>text</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mrow><msub><mi>d</mi><mi>model</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>text</mi></msub></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\tau:\mathbb{R}^{D_{\mathrm{text}}\times N}\to\mathbb{R}^{d_{\mathrm{model}}\times N_{\mathrm{text}}}</annotation><annotation encoding="application/x-llamapun">italic_τ : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT roman_text end_POSTSUBSCRIPT × italic_N end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT × italic_N start_POSTSUBSCRIPT roman_text end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>
denote an encoding network for the text embeddings (often a causal
transformer—see <a class="ltx_ref" href="Ch7.html#S4" title="7.4 Causal Language Modeling ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>), and let <math alttext="\psi:\mathbb{R}^{d_{\mathrm{img}}}\to\mathbb{R}^{d_{\mathrm{model}}\times N_{\mathrm{img}}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m8"><semantics><mrow><mi>ψ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><msub><mi>d</mi><mi>img</mi></msub></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mrow><msub><mi>d</mi><mi>model</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>img</mi></msub></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\psi:\mathbb{R}^{d_{\mathrm{img}}}\to\mathbb{R}^{d_{\mathrm{model}}\times N_{\mathrm{img}}}</annotation><annotation encoding="application/x-llamapun">italic_ψ : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT × italic_N start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> denote the mapping corresponding to one of
the intermediate representations in the denoiser.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>In practice,
text-conditioned denoisers add cross attention layers at regular intervals
within the forward pass of the denoiser, so <math alttext="\psi" class="ltx_Math" display="inline" id="footnote13.m1"><semantics><mi>ψ</mi><annotation encoding="application/x-tex">\psi</annotation><annotation encoding="application/x-llamapun">italic_ψ</annotation></semantics></math> should be seen as
layer-dependent, in contrast to <math alttext="\tau" class="ltx_Math" display="inline" id="footnote13.m2"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math>. See <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite> for
details.</span></span></span> Here, <math alttext="N_{\mathrm{text}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m9"><semantics><msub><mi>N</mi><mi>text</mi></msub><annotation encoding="application/x-tex">N_{\mathrm{text}}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_text end_POSTSUBSCRIPT</annotation></semantics></math> is the maximum tokenized text prompt length,
and <math alttext="N_{\mathrm{img}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m10"><semantics><msub><mi>N</mi><mi>img</mi></msub><annotation encoding="application/x-tex">N_{\mathrm{img}}</annotation><annotation encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT</annotation></semantics></math> roughly corresponds to the number of image channels
(layer-dependent) in the representation, which is fixed if the input image
resolution is fixed. Cross attention (with <math alttext="K" class="ltx_Math" display="inline" id="S4.SS2.p2.m11"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> heads, and no bias) is defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{MHCA}(\bm{z}_{t},\bm{Y}^{+})=\bm{U}_{\mathrm{out}}\begin{bmatrix}\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{1}]^{\top}\psi(\bm{z}_{t}),[\bm{U}_{\mathrm{key}}^{1}]^{\top}\tau(\bm{Y}^{+}),[\bm{U}_{\mathrm{val}}^{1}]^{\top}\tau(\bm{Y}^{+}))\\
\vdots\\
\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{K}]^{\top}\psi(\bm{z}_{t}),[\bm{U}_{\mathrm{key}}^{K}]^{\top}\tau(\bm{Y}^{+}),[\bm{U}_{\mathrm{val}}^{K}]^{\top}\tau(\bm{Y}^{+}))\end{bmatrix}," class="ltx_Math" display="block" id="S4.E31.m1"><semantics><mrow><mrow><mrow><mi>MHCA</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mi>t</mi></msub><mo>,</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝑼</mi><mi>out</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mn>1</mn></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>ψ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mn>1</mn></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>val</mi><mn>1</mn></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><mrow><mi>SA</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>qry</mi><mi>K</mi></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>ψ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>key</mi><mi>K</mi></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝑼</mi><mi>val</mi><mi>K</mi></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒀</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{MHCA}(\bm{z}_{t},\bm{Y}^{+})=\bm{U}_{\mathrm{out}}\begin{bmatrix}\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{1}]^{\top}\psi(\bm{z}_{t}),[\bm{U}_{\mathrm{key}}^{1}]^{\top}\tau(\bm{Y}^{+}),[\bm{U}_{\mathrm{val}}^{1}]^{\top}\tau(\bm{Y}^{+}))\\
\vdots\\
\operatorname{SA}([\bm{U}_{\mathrm{qry}}^{K}]^{\top}\psi(\bm{z}_{t}),[\bm{U}_{\mathrm{key}}^{K}]^{\top}\tau(\bm{Y}^{+}),[\bm{U}_{\mathrm{val}}^{K}]^{\top}\tau(\bm{Y}^{+}))\end{bmatrix},</annotation><annotation encoding="application/x-llamapun">roman_MHCA ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) = bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT [ start_ARG start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_ψ ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_τ ( bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) , [ bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_τ ( bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL roman_SA ( [ bold_italic_U start_POSTSUBSCRIPT roman_qry end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_ψ ( bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , [ bold_italic_U start_POSTSUBSCRIPT roman_key end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_τ ( bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) , [ bold_italic_U start_POSTSUBSCRIPT roman_val end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_τ ( bold_italic_Y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) ) end_CELL end_ROW end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.4.31)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathrm{SA}" class="ltx_Math" display="inline" id="S4.SS2.p2.m12"><semantics><mi>SA</mi><annotation encoding="application/x-tex">\mathrm{SA}</annotation><annotation encoding="application/x-llamapun">roman_SA</annotation></semantics></math> denotes the ubiquitous self attention operation
in the transformer (which we recall in detail in <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">7</span></a>: see
<a class="ltx_ref" href="Ch7.html#S2.E15" title="In 1st item ‣ Backbone. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equations</span> <span class="ltx_text ltx_ref_tag">7.2.15</span></a> and <a class="ltx_ref" href="Ch7.html#S2.E16" title="Equation 7.2.16 ‣ 1st item ‣ Backbone. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7.2.16</span></a>), and <math alttext="\bm{U}_{*}^{k}\in\mathbb{R}^{d_{\mathrm{model}}\times d_{\mathrm{attn}}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m13"><semantics><mrow><msubsup><mi>𝑼</mi><mo>∗</mo><mi>k</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>d</mi><mi>model</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>d</mi><mi>attn</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}_{*}^{k}\in\mathbb{R}^{d_{\mathrm{model}}\times d_{\mathrm{attn}}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT roman_attn end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> for <math alttext="*\in\{\mathrm{qry},\mathrm{key},\mathrm{val}\}" class="ltx_math_unparsed" display="inline" id="S4.SS2.p2.m14"><semantics><mrow><mo rspace="0em">∗</mo><mo lspace="0em">∈</mo><mrow><mo stretchy="false">{</mo><mi>qry</mi><mo>,</mo><mi>key</mi><mo>,</mo><mi>val</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">*\in\{\mathrm{qry},\mathrm{key},\mathrm{val}\}</annotation><annotation encoding="application/x-llamapun">∗ ∈ { roman_qry , roman_key , roman_val }</annotation></semantics></math>
(as well as the output projection <math alttext="\bm{U}_{\mathrm{out}}" class="ltx_Math" display="inline" id="S4.SS2.p2.m15"><semantics><msub><mi>𝑼</mi><mi>out</mi></msub><annotation encoding="application/x-tex">\bm{U}_{\mathrm{out}}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT</annotation></semantics></math>) are the learnable
parameters of the layer.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p">Notice that, by the definition of the self-attention operation, cross attention
<span class="ltx_text ltx_font_italic">outputs linear combinations of the value-projected text embeddings,
weighted by correlations between the image features and the text embeddings.</span>
In the denoiser architecture used by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite>,
self-attention residual blocks in the denoiser architecture, applied to the
image representation at the current layer and defined analogously
to those in <a class="ltx_ref" href="Ch7.html#S2.E13" title="In Backbone. ‣ 7.2.3 Architecture: Vision Transformer ‣ 7.2 Simplified Contrastive Learning ‣ Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">7.2.13</span></a> for the vision transformer, are followed by cross
attention residual blocks of the form (<a class="ltx_ref" href="#S4.E31" title="Equation 6.4.31 ‣ 6.4.2 Caption Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.31</span></a>).
Such a structure requires the text encoder <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS2.p3.m1"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math> to, in a certain sense, share some
structure in its output with the image feature embedding <math alttext="\psi" class="ltx_Math" display="inline" id="S4.SS2.p3.m2"><semantics><mi>ψ</mi><annotation encoding="application/x-tex">\psi</annotation><annotation encoding="application/x-llamapun">italic_ψ</annotation></semantics></math>: this can be
enforced either by appropriate joint text-image pretraining (such as with CLIP
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx220" title="">RKH+21</a>]</cite>), or by joint training with
the denoiser itself (which was proposed and demonstrated by
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx231" title="">RBL+22</a>]</cite>, but has fallen out of favor due to high data and
training costs for strong performance).
Conceptually, this joint text-image embedding space and the cross attention
layer itself bear a strong resemblance to the
conditional mixture of Gaussians denoiser that we derived in the previous
section (recall (<a class="ltx_ref" href="#S4.E27" title="Equation 6.4.27 ‣ Example 6.3. ‣ 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.4.27</span></a>)), in the
special case of a single token sequence. Deeper
connections can be drawn in the multi-token setting following the rate reduction
framework for deriving deep network architectures discussed in
<a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>, and manifested in the derivation of the CRATE
transformer-like architecture.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p">This same basic design has been further scaled to even larger model and dataset
sizes, in particular in modern instantiations of Stable Diffusion
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx82" title="">EKB+24</a>]</cite>, as well as in competing models such as
FLUX.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx147" title="">LBB+25</a>]</cite>, Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx237" title="">SCS+22</a>]</cite>, and DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx223" title="">RDN+22</a>]</cite>.
The conditioning mechanism of cross attention has also become ubiquitous in
other applications, as in EgoAllo (<a class="ltx_ref" href="#S3.SS3" title="6.3.3 Body Pose Generation Conditioned on Head and Hands ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.3.3</span></a>) for conditioned pose generation and in Michelangelo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx321" title="">ZLC+23</a>]</cite> for conditional 3D shape generation based on images or texts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.5 </span>Conditional Inference with Measurement Self-Consistency</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">In this last section, we consider the more extreme, but actually ubiquitous, case for distribution learning in which we only have a set of observed samples <math alttext="\bm{Y}=\{\bm{y}_{1},\ldots,\bm{y}_{N}\}" class="ltx_Math" display="inline" id="S5.p1.m1"><semantics><mrow><mi>𝒀</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>𝒚</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒚</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Y}=\{\bm{y}_{1},\ldots,\bm{y}_{N}\}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math> of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, but no samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> directly! In general, the observation <math alttext="\bm{y}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S5.p1.m4"><semantics><mrow><mi>𝒚</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{y}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is of lower dimension that <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S5.p1.m5"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. To make the problem well-defined, we do assume that the observation model between <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.p1.m6"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is known to belong to a certain family of analytical models, denoted as <math alttext="\bm{y}=h(\bm{x},\theta)+\bm{w}" class="ltx_Math" display="inline" id="S5.p1.m8"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x},\theta)+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x , italic_θ ) + bold_italic_w</annotation></semantics></math>, with <math alttext="\theta" class="ltx_Math" display="inline" id="S5.p1.m9"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> either known or not known.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p">Let us first try to understand the problem conceptually with the simple case when the measurement function <math alttext="h" class="ltx_Math" display="inline" id="S5.p2.m1"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> is known and the observed <math alttext="\bm{y}=h(\bm{x})+\bm{w}" class="ltx_Math" display="inline" id="S5.p2.m2"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒘</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})+\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w</annotation></semantics></math> is informative about <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. That is, we assume that <math alttext="h" class="ltx_Math" display="inline" id="S5.p2.m4"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> is surjective from the space of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to that of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.p2.m6"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> and the support of the distribution <math alttext="\bm{y}_{0}=h(\bm{x}_{0})" class="ltx_Math" display="inline" id="S5.p2.m7"><semantics><mrow><msub><mi>𝒚</mi><mn>0</mn></msub><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}_{0}=h(\bm{x}_{0})</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math> is low-dimensional. This typically requires the <span class="ltx_text ltx_font_italic">extrinsic</span> dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.p2.m8"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.p2.m9"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> is higher than the <span class="ltx_text ltx_font_italic">intrinsic</span> dimension of the support of the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.p2.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. Without loss of generality, we may assume that there exist functions:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F(\bm{x})=\bm{0},\quad G(\bm{y})=\bm{0}." class="ltx_Math" display="block" id="S5.E1.m1"><semantics><mrow><mrow><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">F(\bm{x})=\bm{0},\quad G(\bm{y})=\bm{0}.</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x ) = bold_0 , italic_G ( bold_italic_y ) = bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that here we may assume that we know <math alttext="G(\bm{y})" class="ltx_Math" display="inline" id="S5.p2.m11"><semantics><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_G ( bold_italic_y )</annotation></semantics></math> but not <math alttext="F(\bm{x})" class="ltx_Math" display="inline" id="S5.p2.m12"><semantics><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_x )</annotation></semantics></math>. Let <math alttext="\mathcal{S}_{\bm{y}}\doteq\{\bm{y}\mid G(\bm{y})=\bm{0}\}" class="ltx_Math" display="inline" id="S5.p2.m13"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒚</mi></msub><mo>≐</mo><mrow><mo stretchy="false">{</mo><mi>𝒚</mi><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}_{\bm{y}}\doteq\{\bm{y}\mid G(\bm{y})=\bm{0}\}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ≐ { bold_italic_y ∣ italic_G ( bold_italic_y ) = bold_0 }</annotation></semantics></math> be the support of <math alttext="p(\bm{y})" class="ltx_Math" display="inline" id="S5.p2.m14"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_y )</annotation></semantics></math>. In general, <math alttext="h^{-1}(\mathcal{S}_{\bm{y}})=\{\bm{x}\mid G(h(\bm{x}))=\bm{0}\}" class="ltx_Math" display="inline" id="S5.p2.m15"><semantics><mrow><mrow><msup><mi>h</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒚</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>𝒙</mi><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">h^{-1}(\mathcal{S}_{\bm{y}})=\{\bm{x}\mid G(h(\bm{x}))=\bm{0}\}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( caligraphic_S start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT ) = { bold_italic_x ∣ italic_G ( italic_h ( bold_italic_x ) ) = bold_0 }</annotation></semantics></math> is a super set of <math alttext="\mathcal{S}_{\bm{x}}\doteq\{\bm{x}\mid F(\bm{x})=\bm{0}\}" class="ltx_Math" display="inline" id="S5.p2.m16"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><mo>≐</mo><mrow><mo stretchy="false">{</mo><mi>𝒙</mi><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}_{\bm{x}}\doteq\{\bm{x}\mid F(\bm{x})=\bm{0}\}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ≐ { bold_italic_x ∣ italic_F ( bold_italic_x ) = bold_0 }</annotation></semantics></math>. That is, we have <math alttext="h(\mathcal{S}_{\bm{x}})\subseteq\mathcal{S}_{\bm{y}}" class="ltx_Math" display="inline" id="S5.p2.m17"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒙</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>𝒚</mi></msub></mrow><annotation encoding="application/x-tex">h(\mathcal{S}_{\bm{x}})\subseteq\mathcal{S}_{\bm{y}}</annotation><annotation encoding="application/x-llamapun">italic_h ( caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ) ⊆ caligraphic_S start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5.1 </span>Linear Measurement Models</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">First, for simplicity, let us consider the measurement is a linear function of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> of interest:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=\bm{A}\bm{x}." class="ltx_Math" display="block" id="S5.E2.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}=\bm{A}\bm{x}.</annotation><annotation encoding="application/x-llamapun">bold_italic_y = bold_italic_A bold_italic_x .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here the matrix <math alttext="\bm{A}\in\mathbb{R}^{m\times n}" class="ltx_Math" display="inline" id="S5.SS1.p1.m2"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{m\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is of full row rank and <math alttext="m" class="ltx_Math" display="inline" id="S5.SS1.p1.m3"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math> is typically smaller than <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.p1.m4"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>. We assume <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.SS1.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is known for now. We are interested in how to learn the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from such measurements. Since we no longer have direct samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we wonder whether we can still develop a denoiser for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with observations <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.SS1.p1.m9"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>. Let us consider the following diffusion process:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{t}=\bm{y}_{0}+t\bm{g},\quad\bm{y}_{0}=\bm{A}(\bm{x}_{0})," class="ltx_Math" display="block" id="S5.E3.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝒚</mi><mn>0</mn></msub><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>𝒚</mi><mn>0</mn></msub><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}_{t}=\bm{y}_{0}+t\bm{g},\quad\bm{y}_{0}=\bm{A}(\bm{x}_{0}),</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t bold_italic_g , bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{g}\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S5.SS1.p1.m10"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p">Without loss of generality, we assume <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.SS1.p2.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is of full row rank, i.e., under-determined. Let us define the corresponding process <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p2.m2"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> as one that satisfies:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{t}=\bm{A}\bm{x}_{t}." class="ltx_Math" display="block" id="S5.E4.m1"><semantics><mrow><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}_{t}=\bm{A}\bm{x}_{t}.</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p">From the denoising process of <math alttext="\bm{y}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p3.m1"><semantics><msub><mi>𝒚</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{y}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{t-s}\approx\bm{y}_{t}+st\nabla\log p_{t}(\bm{y}_{t})." class="ltx_Math" display="block" id="S5.E5.m1"><semantics><mrow><mrow><msub><mi>𝒚</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>≈</mo><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒚</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}_{t-s}\approx\bm{y}_{t}+st\nabla\log p_{t}(\bm{y}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then we have:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}\bm{x}_{t-s}\approx\bm{A}\bm{x}_{t}+st\nabla\log p_{t}(\bm{A}\bm{x}_{t})," class="ltx_Math" display="block" id="S5.E6.m1"><semantics><mrow><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub></mrow><mo>≈</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{A}\bm{x}_{t-s}\approx\bm{A}\bm{x}_{t}+st\nabla\log p_{t}(\bm{A}\bm{x}_{t}),</annotation><annotation encoding="application/x-llamapun">bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for a small <math alttext="s&gt;0" class="ltx_Math" display="inline" id="S5.SS1.p3.m2"><semantics><mrow><mi>s</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">s&gt;0</annotation><annotation encoding="application/x-llamapun">italic_s &gt; 0</annotation></semantics></math>. So <math alttext="\bm{x}_{t-s}" class="ltx_Math" display="inline" id="S5.SS1.p3.m3"><semantics><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">\bm{x}_{t-s}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p3.m4"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> need to satisfy:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}(\bm{x}_{t-s}-\bm{x}_{t})\approx st\nabla\log p_{t}(\bm{A}\bm{x}_{t})." class="ltx_Math" display="block" id="S5.E7.m1"><semantics><mrow><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>−</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{A}(\bm{x}_{t-s}-\bm{x}_{t})\approx st\nabla\log p_{t}(\bm{A}\bm{x}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ≈ italic_s italic_t ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Among all <math alttext="\bm{x}_{t_{s}}" class="ltx_Math" display="inline" id="S5.SS1.p3.m5"><semantics><msub><mi>𝒙</mi><msub><mi>t</mi><mi>s</mi></msub></msub><annotation encoding="application/x-tex">\bm{x}_{t_{s}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> that satisfy the above constraint, we arbitrarily choose the one that minimizes the distance <math alttext="\|\bm{x}_{t-s}-\bm{x}_{t}\|_{2}^{2}" class="ltx_Math" display="inline" id="S5.SS1.p3.m6"><semantics><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>−</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">\|\bm{x}_{t-s}-\bm{x}_{t}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. Therefore, we obtain a “denoising” process for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p3.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{t-s}\approx\bm{x}_{t}+st\bm{A}^{\dagger}\nabla\log p_{t}(\bm{A}\bm{x}_{t})." class="ltx_Math" display="block" id="S5.E8.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>≈</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>†</mo></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{t-s}\approx\bm{x}_{t}+st\bm{A}^{\dagger}\nabla\log p_{t}(\bm{A}\bm{x}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t bold_italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that this process does not sample from the distribution of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S5.SS1.p3.m8"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. In particular, there are components of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p3.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in the null space/kernel of <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.SS1.p3.m10"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> which can never be recovered from observations. Thus more information is needed to recover the full distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p3.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, strictly speaking. But this recovers the component of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS1.p3.m12"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> that is orthogonal to the null space of <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.SS1.p3.m13"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5.2 </span>3D Visual Model from Calibrated Images</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">In practice, the measurement model is often nonlinear or only partially known. A typical problem of this kind is actually behind how we can learn a working model of the external world from the images perceived, say through our eyes, telescopes or microscopes. In particular, humans and animals are able to build a model of the 3D world ((or 4D for a dynamical world) through a sequence of its 2D projections – a sequence of 2D images (or stereo image pairs). The mathematical or geometric model of the projection is generally known:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}^{i}=h(\bm{x},\theta^{i})+\bm{w}^{i}," class="ltx_Math" display="block" id="S5.E9.m1"><semantics><mrow><mrow><msup><mi>𝒚</mi><mi>i</mi></msup><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msup><mi>θ</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝒘</mi><mi>i</mi></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}^{i}=h(\bm{x},\theta^{i})+\bm{w}^{i},</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S5.SS2.p1.m1"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> represents a (perspective) projection of the 3D (or 4D) scene from a certain camera view at time <math alttext="t_{i}" class="ltx_Math" display="inline" id="S5.SS2.p1.m2"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding="application/x-tex">t_{i}</annotation><annotation encoding="application/x-llamapun">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to a 2D image (or a stereo pair) and <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S5.SS2.p1.m3"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> is some possibly additive small measurement noise. <a class="ltx_ref" href="#F14" title="In 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.14</span></a> illustrates this relationship concretely, while <a class="ltx_ref" href="#F15" title="In 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.15</span></a> illustrates the model problem in the abstract. A full exposition of geometry related to multiple 2D views of a 3D scene is beyond the scope of this book. Interested readers may refer to the book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx172" title="">MKS+04</a>]</cite>. For now, all we need to proceed is that such projections are well understood and multiple images of a scene contains sufficient information about the scene.</p>
</div>
<figure class="ltx_figure" id="F14"><img alt="Figure 6.14 : Relationship between a 3D object/scene and its 2D projections. Here we illustrate the projection of a point 𝒙 \bm{x} bold_italic_x and a line intersecting the point." class="ltx_graphics ltx_img_landscape" height="224" id="F14.g1" src="chapters/chapter6/figs/3D-2D-projection.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.14</span>: </span><span class="ltx_text" style="font-size:90%;">Relationship between a 3D object/scene and its 2D projections. Here we illustrate the projection of a point <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F14.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and a line intersecting the point.</span></figcaption>
</figure>
<figure class="ltx_figure" id="F15"><img alt="Figure 6.15 : Inference with distributed measurements. We have a low-dimensional distribution 𝒙 \bm{x} bold_italic_x (here, similarly to Figure 6.1 , depicted as a union of two 2 2 2 -dimensional manifolds in ℝ 3 \mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) and a measurement model 𝒚 i = h i ​ ( 𝒙 ) + 𝒘 i \bm{y}^{i}=h^{i}(\bm{x})+\bm{w}^{i} bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT . As before, we want to infer various properties of the conditional distribution of 𝒙 \bm{x} bold_italic_x given 𝒚 \bm{y} bold_italic_y , where 𝒚 \bm{y} bold_italic_y is the collection of all the measurements 𝒚 i \bm{y}^{i} bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ." class="ltx_graphics" id="F15.g1" src="chapters/chapter6/figs/inference_distributed.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6.15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Inference with distributed measurements.<span class="ltx_text ltx_font_medium"> We have a low-dimensional distribution <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F15.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (here, similarly to <a class="ltx_ref" href="#F1" title="In Leveraging Low-dimensionality for Stable and Robust Inference. ‣ 6.1 Bayesian Inference and Constrained Optimization ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.1</span></a>, depicted as a union of two <math alttext="2" class="ltx_Math" display="inline" id="F15.m10"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation><annotation encoding="application/x-llamapun">2</annotation></semantics></math>-dimensional manifolds in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="F15.m11"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>) and a measurement model <math alttext="\bm{y}^{i}=h^{i}(\bm{x})+\bm{w}^{i}" class="ltx_Math" display="inline" id="F15.m12"><semantics><mrow><msup><mi>𝒚</mi><mi>i</mi></msup><mo>=</mo><mrow><mrow><msup><mi>h</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝒘</mi><mi>i</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{y}^{i}=h^{i}(\bm{x})+\bm{w}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>. As before, we want to infer various properties of the conditional distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F15.m13"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> given <math alttext="\bm{y}" class="ltx_Math" display="inline" id="F15.m14"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>, where <math alttext="\bm{y}" class="ltx_Math" display="inline" id="F15.m15"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> is the collection of all the measurements <math alttext="\bm{y}^{i}" class="ltx_Math" display="inline" id="F15.m16"><semantics><msup><mi>𝒚</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{y}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p">In general, we would like to learn the distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S5.SS2.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> of the 3D (or 4D) world scene <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Here by abuse of notation, we use <math alttext="\bm{x}" class="ltx_Math" display="inline" id="footnote14.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to represent either a point in 3D or a sample of an entire 3D object or a scene which consists of many points.</span></span></span> from the perceived 2D images of the world so far. The primary function of such a (visual) world model is to allow us to recognize places where we had been before or predict what the current scene would look alike in a future time at a new viewpoint.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p">Let us first examine the special but important case of stereo vision. In this case, we have two calibrated views of the 3D scene <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}^{0}=h(\bm{x},\theta^{0})+\bm{w}^{0},\quad\bm{y}^{1}=h(\bm{x},\theta^{1})+\bm{w}^{1}," class="ltx_Math" display="block" id="S5.E10.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒚</mi><mn>0</mn></msup><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msup><mi>θ</mi><mn>0</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝒘</mi><mn>0</mn></msup></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msup><mi>𝒚</mi><mn>1</mn></msup><mo>=</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msup><mi>θ</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝒘</mi><mn>1</mn></msup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}^{0}=h(\bm{x},\theta^{0})+\bm{w}^{0},\quad\bm{y}^{1}=h(\bm{x},\theta^{1})+\bm{w}^{1},</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_θ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where parameters <math alttext="\theta_{0}" class="ltx_Math" display="inline" id="S5.SS2.p3.m2"><semantics><msub><mi>θ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\theta_{0}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\theta_{1}" class="ltx_Math" display="inline" id="S5.SS2.p3.m3"><semantics><msub><mi>θ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\theta_{1}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> for the view poses can be assumed to be known. <math alttext="\bm{y}^{0}" class="ltx_Math" display="inline" id="S5.SS2.p3.m4"><semantics><msup><mi>𝒚</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\bm{y}^{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{y}^{1}" class="ltx_Math" display="inline" id="S5.SS2.p3.m5"><semantics><msup><mi>𝒚</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{y}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> are two 2D-projections of the 3D scene <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p3.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. We may also assume that they have the same marginal distribution <math alttext="p(\bm{y})" class="ltx_Math" display="inline" id="S5.SS2.p3.m7"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_y )</annotation></semantics></math> and we have learned a diffusion and denoising model for it. That, we know the denoiser:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[\bm{y}\mid\bm{y}_{t}=\bm{\nu}]=\bm{\nu}+t^{2}\nabla_{\bm{\nu}}\log p_{t}(\bm{\nu})." class="ltx_Math" display="block" id="S5.E11.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒚</mi><mo>∣</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝂</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝝂</mi><mo>+</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝝂</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝂</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{y}\mid\bm{y}_{t}=\bm{\nu}]=\bm{\nu}+t^{2}\nabla_{\bm{\nu}}\log p_{t}(\bm{\nu}).</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_y ∣ bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ν ] = bold_italic_ν + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_ν end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ν ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Or, furthermore, we may assume that we have a sufficient number of samples of stereo pairs <math alttext="(\bm{y}^{0},\bm{y}^{1})" class="ltx_Math" display="inline" id="S5.SS2.p3.m8"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>𝒚</mi><mn>0</mn></msup><mo>,</mo><msup><mi>𝒚</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{y}^{0},\bm{y}^{1})</annotation><annotation encoding="application/x-llamapun">( bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT )</annotation></semantics></math> and have also learned the joint distribution of the pairs. By a little abuse of notation, we also use <math alttext="\bm{y}=h(\bm{x})" class="ltx_Math" display="inline" id="S5.SS2.p3.m9"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x )</annotation></semantics></math> to indicate the pair <math alttext="\bm{y}=(\bm{y}^{0},\bm{y}^{1})" class="ltx_Math" display="inline" id="S5.SS2.p3.m10"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒚</mi><mn>0</mn></msup><mo>,</mo><msup><mi>𝒚</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=(\bm{y}^{0},\bm{y}^{1})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = ( bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="p(\bm{y})" class="ltx_Math" display="inline" id="S5.SS2.p3.m11"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_y )</annotation></semantics></math> as the learned probability distribution of the pair (say via a denoiser as above).</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p">The main question now is: How to learn (a representation for) the distribution of the 3D scene <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p4.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from its two projections with known relationships?
People might question the rationale for doing this: why this is necessary if the function <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S5.SS2.p4.m2"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> is largely invertible? That is, the observation <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.SS2.p4.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math> can largely determine the unknown <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p4.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, which is kind of the case for stereo – in general, two (calibrated) images contain sufficient information about the scene depth, from the given vintage point. However, 2D images are far from the most compact representation of the 3D scene as the same scene can produce infinite many (highly correlated) 2D images or image pairs. In fact, a good representation of a 3D scene should be invariant to the view point. Hence, a correct representation of the distribution of 3D scenes should be much more compact and structured than the distribution of 2D images, stereo pairs, or image-depth pairs.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p">Consider the (inverse) denoising processing for the diffusion: <math alttext="\bm{y}_{t}=\bm{y}+t\bm{g}" class="ltx_Math" display="inline" id="S5.SS2.p5.m1"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝒚</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}_{t}=\bm{y}+t\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y + italic_t bold_italic_g</annotation></semantics></math> in (<a class="ltx_ref" href="#S5.E11" title="Equation 6.5.11 ‣ 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.5.11</span></a>), where <math alttext="\bm{g}" class="ltx_Math" display="inline" id="S5.SS2.p5.m2"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_g</annotation></semantics></math> is standard Gaussian. From the denoising process of (<a class="ltx_ref" href="#S5.E11" title="Equation 6.5.11 ‣ 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.5.11</span></a>), we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{t-s}=\bm{y}_{t}+st\nabla_{\bm{y}}\log p_{t}(\bm{y}_{t})." class="ltx_Math" display="block" id="S5.E12.m1"><semantics><mrow><mrow><msub><mi>𝒚</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒚</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒚</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}_{t-s}=\bm{y}_{t}+st\nabla_{\bm{y}}\log p_{t}(\bm{y}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ∇ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We try to find a corresponding “denoising” process of <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S5.SS2.p5.m3"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> such that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p5.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is related to <math alttext="y" class="ltx_Math" display="inline" id="S5.SS2.p5.m5"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation><annotation encoding="application/x-llamapun">italic_y</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=h(\bm{x})." class="ltx_Math" display="block" id="S5.E13.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x}).</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.13)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p">Then we have:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x}_{t-s})\approx h(\bm{x}_{t})+st\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t}))," class="ltx_Math" display="block" id="S5.E14.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒚</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t-s})\approx h(\bm{x}_{t})+st\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t})),</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ≈ italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_s italic_t ∇ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for a small <math alttext="s&gt;0" class="ltx_Math" display="inline" id="S5.SS2.p6.m1"><semantics><mrow><mi>s</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">s&gt;0</annotation><annotation encoding="application/x-llamapun">italic_s &gt; 0</annotation></semantics></math>.
Suppose <math alttext="\bm{x}_{t-s}=\bm{x}_{t}+s\bm{v}" class="ltx_Math" display="inline" id="S5.SS2.p6.m2"><semantics><mrow><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t-s}=\bm{x}_{t}+s\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s bold_italic_v</annotation></semantics></math> for some vector <math alttext="\bm{v}" class="ltx_Math" display="inline" id="S5.SS2.p6.m3"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> and small increment <math alttext="s" class="ltx_Math" display="inline" id="S5.SS2.p6.m4"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation><annotation encoding="application/x-llamapun">italic_s</annotation></semantics></math>. We have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x}_{t-s})\approx h(\bm{x}_{t})+\frac{\partial h}{\partial\bm{x}}(\bm{x}_{t})\cdot\bm{v}s\doteq h(\bm{x}_{t})+\bm{A}(\bm{x}_{t})\bm{v}s." class="ltx_Math" display="block" id="S5.E15.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mfrac><mrow><mo rspace="0em">∂</mo><mi>h</mi></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mi>𝒗</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow><mo>≐</mo><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{x}_{t-s})\approx h(\bm{x}_{t})+\frac{\partial h}{\partial\bm{x}}(\bm{x}_{t})\cdot\bm{v}s\doteq h(\bm{x}_{t})+\bm{A}(\bm{x}_{t})\bm{v}s.</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ≈ italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + divide start_ARG ∂ italic_h end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ⋅ bold_italic_v italic_s ≐ italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v italic_s .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}(\bm{x}_{t})\bm{v}=t\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t}))." class="ltx_Math" display="block" id="S5.E16.m1"><semantics><mrow><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>=</mo><mrow><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒚</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{A}(\bm{x}_{t})\bm{v}=t\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t})).</annotation><annotation encoding="application/x-llamapun">bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v = italic_t ∇ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Geometrically the vector <math alttext="\bm{v}" class="ltx_Math" display="inline" id="S5.SS2.p6.m5"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> in the domain of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.p6.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be viewed as the pullback of the vector field <math alttext="t\nabla\log p_{t}(\bm{y})" class="ltx_Math" display="inline" id="S5.SS2.p6.m7"><semantics><mrow><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">t\nabla\log p_{t}(\bm{y})</annotation><annotation encoding="application/x-llamapun">italic_t ∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y )</annotation></semantics></math> under the map <math alttext="\bm{y}=h(\bm{x})" class="ltx_Math" display="inline" id="S5.SS2.p6.m8"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x )</annotation></semantics></math>. In general, as before, we may (arbitrarily) choose <math alttext="\bm{v}" class="ltx_Math" display="inline" id="S5.SS2.p6.m9"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> to be the minimum 2-norm vector that satisfies the pullback relationship. Hence, we can express <math alttext="\hat{\bm{x}}_{t-s}" class="ltx_Math" display="inline" id="S5.SS2.p6.m10"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{t-s}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT</annotation></semantics></math> approximately as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{t-s}\approx\bm{x}_{t}+st\bm{A}(\bm{x}_{t})^{\dagger}\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t}))." class="ltx_Math" display="block" id="S5.E17.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>−</mo><mi>s</mi></mrow></msub><mo>≈</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>†</mo></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝒚</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{t-s}\approx\bm{x}_{t}+st\bm{A}(\bm{x}_{t})^{\dagger}\nabla_{\bm{y}}\log p_{t}(h(\bm{x}_{t})).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ≈ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.17)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 6.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Parallel Sensing and Distributed Denoising.)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark2.p1">
<p class="ltx_p">There is something very interesting about the above equation (<a class="ltx_ref" href="#S5.E17" title="Equation 6.5.17 ‣ 6.5.2 3D Visual Model from Calibrated Images ‣ 6.5 Conditional Inference with Measurement Self-Consistency ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.5.17</span></a>). It seems to suggest we could try to learn the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> through a process that coupled with (many of) its (partial) observations:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}^{i}=h^{i}(\bm{x})+\bm{w}^{i},i=1,\ldots,K." class="ltx_Math" display="block" id="S5.E18.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒚</mi><mi>i</mi></msup><mo>=</mo><mrow><mrow><msup><mi>h</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msup><mi>𝒘</mi><mi>i</mi></msup></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{y}^{i}=h^{i}(\bm{x})+\bm{w}^{i},i=1,\ldots,K.</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_i = 1 , … , italic_K .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this case, we obtain a set of equations that the vector field <math alttext="\bm{v}" class="ltx_Math" display="inline" id="Thmremark2.p1.m2"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> in the domain of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark2.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> should satisfy:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}^{i}(\bm{x}_{t})\bm{v}=t\nabla_{\bm{y}^{i}}\log p_{t}(h^{i}(\bm{x}_{t}))," class="ltx_Math" display="block" id="S5.E19.m1"><semantics><mrow><mrow><mrow><msup><mi>𝑨</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo>=</mo><mrow><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><msup><mi>𝒚</mi><mi>i</mi></msup></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>h</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{A}^{i}(\bm{x}_{t})\bm{v}=t\nabla_{\bm{y}^{i}}\log p_{t}(h^{i}(\bm{x}_{t})),</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v = italic_t ∇ start_POSTSUBSCRIPT bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{A}^{i}(\bm{x}_{t})=\frac{\partial h^{i}}{\partial\bm{x}}(\bm{x}_{t})" class="ltx_Math" display="inline" id="Thmremark2.p1.m4"><semantics><mrow><mrow><msup><mi>𝑨</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mrow><mo rspace="0em">∂</mo><msup><mi>h</mi><mi>i</mi></msup></mrow><mrow><mo rspace="0em">∂</mo><mi>𝒙</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{A}^{i}(\bm{x}_{t})=\frac{\partial h^{i}}{\partial\bm{x}}(\bm{x}_{t})</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG ∂ italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG start_ARG ∂ bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>. The final <math alttext="\bm{v}" class="ltx_Math" display="inline" id="Thmremark2.p1.m5"><semantics><mi>𝒗</mi><annotation encoding="application/x-tex">\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_v</annotation></semantics></math> can be chosen as a “centralized” solution that satisfies all the above equations, or it could be chosen as a certain (stochastically) “aggregated” version of all <math alttext="\bm{v}^{i}" class="ltx_Math" display="inline" id="Thmremark2.p1.m6"><semantics><msup><mi>𝒗</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{v}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}^{i}=t\bm{A}^{i}(\bm{x}_{t})^{\dagger}\big{[}\nabla_{\bm{y}^{i}}\log p_{t}(h^{i}(\bm{x}_{t}))\big{]},\quad i=1,\ldots,K," class="ltx_Math" display="block" id="S5.E20.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒗</mi><mi>i</mi></msup><mo>=</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>†</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mrow><mrow><msub><mo rspace="0.167em">∇</mo><msup><mi>𝒚</mi><mi>i</mi></msup></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>h</mi><mi>i</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{v}^{i}=t\bm{A}^{i}(\bm{x}_{t})^{\dagger}\big{[}\nabla_{\bm{y}^{i}}\log p_{t}(h^{i}(\bm{x}_{t}))\big{]},\quad i=1,\ldots,K,</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_t bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT [ ∇ start_POSTSUBSCRIPT bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) ] , italic_i = 1 , … , italic_K ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">that are computed in a parallel and distributed fashion? An open question here is, exactly what the so-defined “denoising” process for <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="Thmremark2.p1.m7"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> converges to, even in the linear measurement model case? When would it converges to a distribution that has the same low-dimensional support as the original <math alttext="\bm{x}_{0}" class="ltx_Math" display="inline" id="Thmremark2.p1.m8"><semantics><msub><mi>𝒙</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{x}_{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, as <math alttext="\bm{y}_{t}" class="ltx_Math" display="inline" id="Thmremark2.p1.m9"><semantics><msub><mi>𝒚</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{y}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> converges to <math alttext="\bm{y}=h(\bm{x}_{0})" class="ltx_Math" display="inline" id="Thmremark2.p1.m10"><semantics><mrow><mi>𝒚</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{y}=h(\bm{x}_{0})</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_h ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math>?</p>
</div>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Visual World Model from Uncalibrated Image Sequences</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p">In the above derivation, we have assumed that the measurement model <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m1"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> is fully known. In the case of stereo vision, this is rather reasonable as the relative pose (and calibration) of the two camera views (or two eyes<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>The relative pose of our two eyes is well known to our brain.</span></span></span>) are usually known in advance. Hence, through the stereo image pairs, in principle we should be able to learn the distribution of 3D scenes, at least the ego-centric distribution of 3D scenes. However, the low-dimensional structures of the so-called learned distribution contains variation caused by changing the viewpoints. That is, the appearance of the stereo images varies when we change our viewpoints with respect to the same 3D scene. For many practical vision tasks (such as localization and navigation), it is important if we can decouple this variation of viewpoints from an invariant representation of (the distribution of) 3D scenes.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 6.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark3.p1">
<p class="ltx_p">Note that the above goal aligns well with Klein’s Erlangen Program for modern geometry, which is to study invariants of a manifold under a group of transformations. Here, we may view the manifold of interest as the distribution of ego-centric representations of 3D scenes. We have learned that it admits a group of three-dimensional rigid-body motion acting on it. It is remarkable that our brain has learned to effectively decouple such transformations from the observed 3D world.</p>
</div>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p2">
<p class="ltx_p">Notice that we have studied learning representations that are invariant to translation and rotation in a limited setting in Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>. We know that the associated compression operators take the necessary form of (multi-channel) convolutions, hence leading to the (deep) convolution neural networks. Nevertheless, operators that are associated with compression or denoising that are invariant to more general transformation groups remain elusive to characterize <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx55" title="">CW16a</a>]</cite>.
For the 3D Vision problem in its most general setting, we know the change of our viewpoints can be well modeled as a rigid-body motion. However, the exact relative motion of our eyes between different viewpoints is usually not known. More generally, there could also be objects (e.g., cars, humans, hands) moving in the scene and we normally do not know their motion either. How can we generalize the problem of learning the distribution of 3D scenes with calibrated stereo pairs to such more general settings? More precisely, we want to learn a compact representation <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> of the 3D scenes that is invariant to the camera/eye motions. Once such a representation is learned, we could sample and generate a 3D scene and render images or stereo pairs from arbitrary poses.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p3">
<p class="ltx_p">To this end, not that we can model a sequence of stereo pairs as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}^{k}=h(\bm{x}^{k},\theta^{k}),\quad k=1,\ldots,K," class="ltx_Math" display="block" id="S5.E21.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒚</mi><mi>k</mi></msup><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>k</mi></msup><mo>,</mo><msup><mi>θ</mi><mi>k</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}^{k}=h(\bm{x}^{k},\theta^{k}),\quad k=1,\ldots,K,</annotation><annotation encoding="application/x-llamapun">bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) , italic_k = 1 , … , italic_K ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="h(\cdot)" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m1"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_h ( ⋅ )</annotation></semantics></math> represents the projection map from 3D to 2D. <math alttext="\theta^{k}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m2"><semantics><msup><mi>θ</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\theta^{k}</annotation><annotation encoding="application/x-llamapun">italic_θ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the rigid-body motion parameters of the <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>th view, with respect to some canonical frame in the world. <math alttext="\bm{x}^{k}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m4"><semantics><msup><mi>𝒙</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\bm{x}^{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> represents the 3D scene at time <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m5"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>. If the scene is static, <math alttext="\bm{x}^{k}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m6"><semantics><msup><mi>𝒙</mi><mi>k</mi></msup><annotation encoding="application/x-tex">\bm{x}^{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> should all be the same <math alttext="\bm{x}^{k}=\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m7"><semantics><mrow><msup><mi>𝒙</mi><mi>k</mi></msup><mo>=</mo><mi>𝒙</mi></mrow><annotation encoding="application/x-tex">\bm{x}^{k}=\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = bold_italic_x</annotation></semantics></math>. To simplify the notation, we may denote the set of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m8"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> equations as one:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Y}=H(\bm{x},\Theta)." class="ltx_Math" display="block" id="S5.E22.m1"><semantics><mrow><mrow><mi>𝒀</mi><mo>=</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Y}=H(\bm{x},\Theta).</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = italic_H ( bold_italic_x , roman_Θ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.5.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We may assume that we are given many samples of such stereo image sequences <math alttext="\{\bm{Y}_{i}\}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m9"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒀</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{Y}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math>. The problem is how to recover the associated motion sequence <math alttext="\{\Theta_{i}\}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m10"><semantics><mrow><mo stretchy="false">{</mo><msub><mi mathvariant="normal">Θ</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\Theta_{i}\}</annotation><annotation encoding="application/x-llamapun">{ roman_Θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> and learn the distribution of the scene <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p3.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (that is invariant to the motion). To our best knowledge, this remains an open challenging problem, probably as the final frontier for the 3D Vision problem.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.6 </span>Summary and Notes</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Measurement matching without clean samples.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p">In our development of
conditional sampling, we considered measurement matching under an observation
model (<a class="ltx_ref" href="#S3.E9" title="Equation 6.3.9 ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6.3.9</span></a>), where we assume that we have
paired data <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_y )</annotation></semantics></math>—i.e., ground truth for each observation <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m2"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>.
In many practically-relevant inverse problems, this is not the case: one of the
most fundamental examples is in the context of compressed sensing, which we
recalled in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>, where we need to reconstruct <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m4"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>
using prior knowledge about <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (i.e., sparsity).
In the setting of denoising-diffusion, we have access to an implicit prior for
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> via the learned denoisers <math alttext="\bar{\bm{x}}_{\theta}(t,\bm{\xi})" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m7"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>¯</mo></mover><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bar{\bm{x}}_{\theta}(t,\bm{\xi})</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_t , bold_italic_ξ )</annotation></semantics></math>. Can we still perform
conditional sampling without access to ground truth samples <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>?</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p">For intuition as to why this might be possible, we recall a classical example
from statistics known as Stein’s unbiased risk estimator (SURE).
Under an observation model <math alttext="\bm{x}_{t}=\bm{x}+t\bm{g}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m1"><semantics><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{t}=\bm{x}+t\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g</annotation></semantics></math> with <math alttext="\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m2"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\mathcal{N}(\mathbf{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> and <math alttext="t&gt;0" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m3"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t&gt;0</annotation><annotation encoding="application/x-llamapun">italic_t &gt; 0</annotation></semantics></math>, it turns out that for any weakly differentiable <math alttext="f:\mathbb{R}^{D}\to\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m4"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{D}\to\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}_{\bm{g}}\left[\left\|\bm{x}-f(\bm{x}+t\bm{g})\right\|_{2}^{2}\right]=\mathbb{E}_{\bm{g}}\left[\left\|\bm{x}+t\bm{g}-f(\bm{x}+t\bm{g})\right\|_{2}^{2}+2t^{2}\nabla\cdot f(\bm{x}+t\bm{g})\right]-t^{2}D," class="ltx_Math" display="block" id="S6.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>𝔼</mi><mi>𝒈</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msubsup><mrow><mo>‖</mo><mrow><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo>−</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mo>∇</mo></mrow><mo lspace="0em" rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{\bm{g}}\left[\left\|\bm{x}-f(\bm{x}+t\bm{g})\right\|_{2}^{2}\right]=\mathbb{E}_{\bm{g}}\left[\left\|\bm{x}+t\bm{g}-f(\bm{x}+t\bm{g})\right\|_{2}^{2}+2t^{2}\nabla\cdot f(\bm{x}+t\bm{g})\right]-t^{2}D,</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT [ ∥ bold_italic_x - italic_f ( bold_italic_x + italic_t bold_italic_g ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT [ ∥ bold_italic_x + italic_t bold_italic_g - italic_f ( bold_italic_x + italic_t bold_italic_g ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ ⋅ italic_f ( bold_italic_x + italic_t bold_italic_g ) ] - italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_D ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.6.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\nabla\cdot" class="ltx_math_unparsed" display="inline" id="S6.SS0.SSS0.Px1.p2.m5"><semantics><mrow><mo>∇</mo><mo lspace="0em">⋅</mo></mrow><annotation encoding="application/x-tex">\nabla\cdot</annotation><annotation encoding="application/x-llamapun">∇ ⋅</annotation></semantics></math> denotes the divergence operator:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla\cdot f=\sum_{i=1}^{D}\partial_{i}f_{i}." class="ltx_Math" display="block" id="S6.Ex1.m1"><semantics><mrow><mrow><mrow><mo>∇</mo><mo lspace="0em" rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><msub><mo lspace="0em" rspace="0em">∂</mo><mi>i</mi></msub><msub><mi>f</mi><mi>i</mi></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\nabla\cdot f=\sum_{i=1}^{D}\partial_{i}f_{i}.</annotation><annotation encoding="application/x-llamapun">∇ ⋅ italic_f = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ∂ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>-dependent part of the RHS of <a class="ltx_ref" href="#S6.E1" title="In Measurement matching without clean samples. ‣ 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.6.1</span></a> is called Stein’s unbiased risk
estimator (SURE). If we take expectations over <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in <a class="ltx_ref" href="#S6.E1" title="In Measurement matching without clean samples. ‣ 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.6.1</span></a>,
note that the RHS can be written as an expectation with respect to <math alttext="\bm{x}_{t}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m8"><semantics><msub><mi>𝒙</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{x}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>—in
particular, the mean-squared error of <span class="ltx_text ltx_font_italic">any denoiser <math alttext="f" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p2.m9"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math></span> can be estimated
<span class="ltx_text ltx_font_italic">solely from noisy samples</span>!
This remarkable fact, in refined forms, constitutes the basis for many practical
techniques for performing image restoration, denoising-diffusion, etc. using
only noisy data: notable examples include the “noise2noise” paradigm
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx155" title="">LMH+18</a>]</cite>
and Ambient Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx65" title="">DSD+23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p3">
<p class="ltx_p">As a fun aside, we point out that <a class="ltx_ref" href="#S6.E1" title="In Measurement matching without clean samples. ‣ 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.6.1</span></a> leads to an alternate
proof of Tweedie’s formula (<a class="ltx_ref" href="Ch3.html#Thmtheorem3" title="Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). At a high level, one takes
expectations over <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and expresses the main part of the RHS of
<a class="ltx_ref" href="#S6.E1" title="In Measurement matching without clean samples. ‣ 6.6 Summary and Notes ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.6.1</span></a> equivalently, via integration by parts, as</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}_{\bm{x}_{t}}\left[\left\|\bm{x}_{t}-f(\bm{x}_{t})\right\|_{2}^{2}+2t^{2}\nabla\cdot f(\bm{x}_{t})\right]=\mathbb{E}_{\bm{x}_{t}}\left[\left\|\bm{x}_{t}-f(\bm{x}_{t})\right\|_{2}^{2}\right]-2t^{2}\int\left\langle\nabla p_{\bm{x}_{t}}(\bm{\xi}),f(\bm{\xi})\right\rangle\mathrm{d}\bm{\xi}." class="ltx_Math" display="block" id="S6.E2.m1"><semantics><mrow><mrow><mrow><msub><mi>𝔼</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msubsup><mrow><mo>‖</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mo>∇</mo></mrow><mo lspace="0em" rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>𝔼</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msubsup><mrow><mo>‖</mo><mrow><msub><mi>𝒙</mi><mi>t</mi></msub><mo>−</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>]</mo></mrow></mrow><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>t</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">∫</mo><mrow><mrow><mo>⟨</mo><mrow><mrow><mo rspace="0.167em">∇</mo><msub><mi>p</mi><msub><mi>𝒙</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝝃</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⟩</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝝃</mi></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{\bm{x}_{t}}\left[\left\|\bm{x}_{t}-f(\bm{x}_{t})\right\|_{2}^{2}+2t^{2}\nabla\cdot f(\bm{x}_{t})\right]=\mathbb{E}_{\bm{x}_{t}}\left[\left\|\bm{x}_{t}-f(\bm{x}_{t})\right\|_{2}^{2}\right]-2t^{2}\int\left\langle\nabla p_{\bm{x}_{t}}(\bm{\xi}),f(\bm{\xi})\right\rangle\mathrm{d}\bm{\xi}.</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ ⋅ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] - 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∫ ⟨ ∇ italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ξ ) , italic_f ( bold_italic_ξ ) ⟩ roman_d bold_italic_ξ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6.6.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is a quadratic function of <math alttext="f" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p3.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, and formally taking derivatives gives
that the optimal <math alttext="f" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px1.p3.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> satisfies Tweedie’s formula (<a class="ltx_ref" href="Ch3.html#Thmtheorem3" title="Theorem 3.3 (Tweedie’s Formula). ‣ 3.2.1 Diffusion and Denoising Processes ‣ 3.2 Compression via Denoising ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). This
argument can be made rigorous using basic ideas from the calculus of variations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Corrections to the Diffusion Posterior Sampling (DPS) approximation.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p">In <a class="ltx_ref" href="#Thmexample2" title="Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">6.2</span></a> and in particular in
<a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>, we pointed out
a limitation of the DPS approximation
<a class="ltx_ref" href="#S3.E26" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.26</span></a> at
small levels of measurement noise.
This limitation is well-understood, and a principled approach to ameliorating it
has been proposed by Rozet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx233" title="">RAL+24</a>]</cite>.
The approach involves incorporating an additional estimate for the variance of
the noisy posterior <math alttext="p_{\bm{x}\mid\bm{x}_{t}}" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p1.m1"><semantics><msub><mi>p</mi><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow></msub><annotation encoding="application/x-tex">p_{\bm{x}\mid\bm{x}_{t}}</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> to
<a class="ltx_ref" href="#S3.E26" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">6.3.26</span></a>—we
refer to the paper for details.
Natural estimates for the posterior variance are slightly less scalable than DPS
itself, due to the need to invert an affine transformation of the Jacobian of
the posterior denoiser <math alttext="\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]" class="ltx_Math" display="inline" id="S6.SS0.SSS0.Px2.p1.m2"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>𝒙</mi><mo>∣</mo><msub><mi>𝒙</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>𝝃</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\bm{x}\mid\bm{x}_{t}=\bm{\xi}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ∣ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ξ ]</annotation></semantics></math> (a large matrix). This is done
relatively efficiently by Rozet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx233" title="">RAL+24</a>]</cite> using autodifferentiation and an
approximation for the inverse based on conjugate gradients. It seems that it
should be possible to improve further over this approach (say, using classical
ideas from second-order optimization).</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">More about measurement matching and diffusion models for inverse
problems.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p">Diffusion models have become an extremely popular tool for solving inverse
problems arising in scientific applications. Many more methods beyond the simple
DPS algorithm we have presented in <a class="ltx_ref" href="#alg1" title="In General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.1</span></a> have been
developed and continue to be developed, as the area is evolving rapidly.
Popular and performant classes of approaches beyond DPS, which we have presented
due to its generality, include variable splitting approaches like DAPS
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx318" title="">ZCB+24</a>]</cite>,
which allow for specific measurement constraints to be enforced much more
strongly than in DPS, and exact approaches that can avoid the use of
approximations like in DPS, such as TDS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx299" title="">WTN+23</a>]</cite>.
For more on this area, we recommend <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx322" title="">ZCZ+25</a>]</cite>, which
functions simultaneously as a survey and a benchmark of several popular methods
on specific scientific inverse problem datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6.7 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 6.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Posterior Variance Correction to DPS)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p">Using the code provided in the book GitHub for implementing
<a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>, implement the
posterior variance correction proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx233" title="">RAL+24</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p">Verify that it ameliorates the posterior collapse at low noise
variance issue observed in
<a class="ltx_ref" href="#F9" title="In Example 6.2. ‣ General nonlinear measurements. ‣ 6.3.2 Conditional Sampling with Measurement Matching ‣ 6.3 Conditional Inference with a Learned Data Representation ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6.9</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p">Discuss any issues of sampling correctness that are retained or
introduced by the corrected method, as well as its efficiency, relative to
diffusion posterior sampling (DPS).</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 6.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Conditional Sampling on MNIST)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<ol class="ltx_enumerate" id="S7.I2">
<li class="ltx_item" id="S7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I2.i1.p1">
<p class="ltx_p">Train a simple classifier for the MNIST dataset, using an architecture
of your choice. Additionally train a denoiser suitable for use in
conditional sampling (<a class="ltx_ref" href="#alg2" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>,
since this denoiser can be used for unconditional denoising as well).</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I2.i2.p1">
<p class="ltx_p">Integrate the classifier into a conditional sampler based on
classifier guidance, as described in the first part of <a class="ltx_ref" href="#S4.SS1" title="6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.1</span></a>.
Evaluate the resulting samples in terms of faithfulness to the
conditioning class (visually; in terms of nearest neighbor; in terms of
the output of the classifier).</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I2.i3.p1">
<p class="ltx_p">Integrate the classifier into a conditional sampler based on
classifier-free guidance, as described in <a class="ltx_ref" href="#S4.SS1" title="6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.4.1</span></a> and
<a class="ltx_ref" href="#alg2" title="In 6.4.1 Class Conditioned Image Generation ‣ 6.4 Conditional Inference with Paired Data and Measurements ‣ Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>. Perform the same
evaluation as in the previous step, and compare the results.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S7.I2.i4.p1">
<p class="ltx_p">Repeat the experiment on the CIFAR-10 dataset.</p>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 09:48:41 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
