<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 12:37:23 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Learning Consistent Representations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS1" title="In 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Linear Autoencoding via PCA</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px1" title="In 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Online PCA.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS2" title="In 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Nonlinear PCA and Autoencoding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px1" title="In 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Data on a Nonlinear Submanifold.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px2" title="In 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">A Classical Attempt via a Two-Layer Network.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS2.SSS0.Px3" title="In 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Manifold Flattening via a Deeper Network.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS3" title="In 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Sparse Autoencoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS4" title="In 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.4 </span>Variational Autoencoding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S1.SS4.SSSx1" title="In 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Probabilistic Perspective on Autoencoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S1.SS4.SSSx2" title="In 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">VAE Training as Probabilistic Autoencoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S1.SS4.SSSx3" title="In 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Training a VAE</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Learning Self-Consistent Representations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS1" title="In 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Closed-Loop Transcription via Stackelberg Games</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px1" title="In 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Measuring distance in the feature space.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px2" title="In 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Encoder and decoder as a two-player game.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px3" title="In 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Visualizing correlation of features <math alttext="\bm{Z}" class="ltx_Math" display="inline"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and decoded features <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px4" title="In 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Visualizing auto-encoding of the data <math alttext="\bm{X}" class="ltx_Math" display="inline"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="In 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>A Mixture of Low-Dimensional Gaussians</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Continuous Learning Self-Consistent Representations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS1" title="In 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>Class-wise Incremental Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px1" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">LDR memory sampling and replay.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px2" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Incremental learning LDR with an old-memory constraint.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px3" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Jointly optimal memory via incremental reviewing.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px4" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Experimental verification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px5" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Visualizing auto-encoding properties.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px6" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Principal subspaces of the learned features.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px7" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Replay images of samples from principal components.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSS0.Px8" title="In 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Effectiveness of incremental reviewing.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="In 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>Sample-wise Continuous Unsupervised Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px1" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Sample-wise constraints for unsupervised transcription.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px2" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Sample-wise self-consistency via closed-loop transcription.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px3" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Self-supervision via compressing augmented samples.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px4" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Unsupervised representation learning via closed-loop transcription.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px5" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Unsupervised CTRL.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSS0.Px6" title="In 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Unsupervised conditional image generation via rate reduction.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="In Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Summary and Notes</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS0.SSS0.Px1" title="In 5.4 Summary and Notes ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Shallow vs. deep neural networks, for autoencoding and more.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="In Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 5 </span>Consistent and Self-Consistent Representations</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">Learning Consistent Representations</a><div class="mini-toc-sub"><a href="#S1.SS1">Linear Autoencoding via PCA</a><a href="#S1.SS2">Nonlinear PCA and Autoencoding</a><a href="#S1.SS3">Sparse Autoencoding</a><a href="#S1.SS4">Variational Autoencoding</a></div></li><li><a href="#S2">Learning Self-Consistent Representations</a><div class="mini-toc-sub"><a href="#S2.SS1">Closed-Loop Transcription via Stackelberg Games</a><a href="#S2.SS2">A Mixture of Low-Dimensional Gaussians</a></div></li><li><a href="#S3">Continuous Learning Self-Consistent Representations</a><div class="mini-toc-sub"><a href="#S3.SS1">Class-wise Incremental Learning</a><a href="#S3.SS2">Sample-wise Continuous Unsupervised Learning</a></div></li><li><a href="#S4">Summary and Notes</a></li><li><a href="#S5">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">Everything should be made as simple as possible, but not any simpler</span>.”</p>
<p class="ltx_p">  – Albert Einstein</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">In the past chapters, we have established a basic fact that the
fundamental goal of learning is to learn a data
distribution with low-dimensional supports and transform it to a compact and structured
representation. Such a representation reveals intrinsic
low-dimensional structures of the data distribution and facilitates
subsequent tasks such as classification and generation.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">A fundamental approach to learning a good representation of such a
distribution is through <span class="ltx_text ltx_font_italic">compression</span>. To make the
goal of compression measurable and computable, it can be done
explicitly by learning a coding scheme that minimizes the coding rate (entropy) or maximizes the information gain (coding rate
reduction). In this context, the fundamental role of a deep neural
network is to realize a certain iterative optimization algorithm that
incrementally optimizes the learned representations in terms of those measures:</p>
<table class="ltx_equation ltx_eqn_table" id="S0.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f\colon\bm{X}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{Z}^{0}\rightarrow\cdots\rightarrow\bm{Z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{Z}^{\ell+1}\rightarrow\cdots\to\bm{Z}^{L}=\bm{Z}." class="ltx_Math" display="block" id="S0.E1.m1"><semantics><mrow><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mn>0</mn></msup></mover><msup><mi>𝒁</mi><mn>0</mn></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">→</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mover accent="true"><mo stretchy="false">→</mo><msup><mi>f</mi><mi mathvariant="normal">ℓ</mi></msup></mover><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">→</mo><mi mathvariant="normal">⋯</mi><mo stretchy="false">→</mo><msup><mi>𝒁</mi><mi>L</mi></msup><mo>=</mo><mi>𝒁</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">f\colon\bm{X}\xrightarrow{\hskip 2.84526ptf^{0}\hskip 2.84526pt}\bm{Z}^{0}\rightarrow\cdots\rightarrow\bm{Z}^{\ell}\xrightarrow{\hskip 2.84526ptf^{\ell}\hskip 2.84526pt}\bm{Z}^{\ell+1}\rightarrow\cdots\to\bm{Z}^{L}=\bm{Z}.</annotation><annotation encoding="application/x-llamapun">italic_f : bold_italic_X start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT → ⋯ → bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT → ⋯ → bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.0.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the preceding chapter, we have shown that main architectural
characteristics of almost all popular deep networks (ResNet, CNN, and
Transformer) can be derived and interpreted from this perspective.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">However, when we try to achieve a certain objective through
optimization, there is no guarantee that the solution <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="p4.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> found in
the end by incremental optimization is the correct solution. In fact, even if
the optimization process manages to find the globally optimal
solution <math alttext="\bm{Z}^{*}" class="ltx_Math" display="inline" id="p4.m2"><semantics><msup><mi>𝒁</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{*}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, there is no guarantee that the solution corresponds
to a complete representation of the data distribution.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This could be due to many reasons: for example, the data available for learning the distribution might not be sufficient, or formulation of the optimization program fails to consider some additional constraints or conditions.</span></span></span> Therefore, an outstanding question is how we can ensure that the learned representation of the data distribution is
correct or good enough?</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p">Of course, the only way to verify this is to see whether there exists a decoding map, say <math alttext="g" class="ltx_Math" display="inline" id="p5.m1"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>, that can decode the learned representation to reproduce the original data (distribution) well enough:</p>
<table class="ltx_equation ltx_eqn_table" id="S0.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}" class="ltx_Math" display="block" id="S0.E2.m1"><semantics><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>g</mi></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.0.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">in terms of some measure of similarity:</p>
<table class="ltx_equation ltx_eqn_table" id="S0.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(\bm{X},\hat{\bm{X}})." class="ltx_Math" display="block" id="S0.E3.m1"><semantics><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}}).</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.0.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This leads to the concept of <span class="ltx_text ltx_font_italic">consistent representation</span>. As we have briefly alluded to in
<a class="ltx_ref" href="Ch1.html" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">1</span></a> (see <a class="ltx_ref" href="Ch1.html#F23" title="In Bidirectional Autoencoding for Consistency. ‣ 1.4.3 Learning Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1.23</span></a>), <span class="ltx_text ltx_font_italic">autoencoding</span>, which integrates the
encoding and decoding processes, is a natural framework to learn such a representation. We have studied some important special cases in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>. In
this chapter, <a class="ltx_ref" href="#S1" title="5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>, we will study how to extend autoencoding to more general classes of distributions by enforcing the consistency between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="p5.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="p5.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">In many practical and natural learning scenarios, it can be difficult or even impossible to compare distributions of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="p6.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="p6.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>. We are left with the only option to compare in the learned feature <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="p6.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> with its image <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="p6.m4"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> under the encoder <math alttext="f" class="ltx_Math" display="inline" id="p6.m5"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S0.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\hat{\bm{Z}}?" class="ltx_Math" display="block" id="S0.E4.m1"><semantics><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>g</mi></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">?</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\hat{\bm{Z}}?</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_Z end_ARG ?</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.0.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This leads to the notion of a <span class="ltx_text ltx_font_italic">self-consistent representation</span>. In <a class="ltx_ref" href="#S2" title="5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>, we will study when and how we can learn a consistent representation by enforcing the self-consistency between <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="p6.m6"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="p6.m7"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> only through a closed-loop transcription framework.</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p">Furthermore, in many practical and natural learning scenarios, we normally do not have sufficient samples of the data distribution all at once. For example, animals and humans develop their visual memory through continuously taking in increments of observations all their life. In <a class="ltx_ref" href="#S3" title="5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>, we will study how to extend the closed-loop transcription framework to learn a self-consistent representation in a <span class="ltx_text ltx_font_italic">continuous learning</span> setting.</p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p">Of course, a fundamental motivation why we ever want to identify the
low-dimensional structures in a data distribution and find a good
representation is to make it easy to use the data for various tasks
of intelligence, such as classification, completion, and prediction.
Therefore, the resulting joint representation <math alttext="(\bm{x},\bm{z})" class="ltx_Math" display="inline" id="p8.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_x , bold_italic_z )</annotation></semantics></math> must be
structured in such a way that is best suited for these tasks. In
next chapter, we will
see how the learned representation can be structured to facilitate
conditioned completion or generation tasks.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.1 </span>Learning Consistent Representations</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Here we give a formal definition of consistent representations, which
are closely related to the concept of autoencoding.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 5.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Consistent Representations)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition1.p1">
<p class="ltx_p">Given data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, an <span class="ltx_text ltx_font_italic">consistent representation</span> is a pair
of functions <math alttext="(f\colon\mathcal{X}\to\mathcal{Z},g\colon\mathcal{Z}\to\mathcal{X})" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>,</mo><mi>g</mi></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f\colon\mathcal{X}\to\mathcal{Z},g\colon\mathcal{Z}\to\mathcal{X})</annotation><annotation encoding="application/x-llamapun">( italic_f : caligraphic_X → caligraphic_Z , italic_g : caligraphic_Z → caligraphic_X )</annotation></semantics></math>, such
that the <span class="ltx_text ltx_font_italic">features</span> <math alttext="\bm{Z}=f(\bm{X})" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m3"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X )</annotation></semantics></math> are compact and
structured, and the <span class="ltx_text ltx_font_italic">autoencoding</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{X}}\doteq g(\bm{Z})=g(f(\bm{X}))" class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>≐</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}\doteq g(\bm{Z})=g(f(\bm{X}))</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG ≐ italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is <span class="ltx_text ltx_font_italic">close</span> to <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> according to either the
following two measures:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">We say that it is <span class="ltx_text ltx_font_italic">sample-wise</span> consistent if <math alttext="\bm{X}\approx\hat{\bm{X}}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>≈</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{X}\approx\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ≈ over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> in certain norm with high probability.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">We say that the representation is <span class="ltx_text ltx_font_italic">distributionally
consistent</span> if <math alttext="\operatorname{Law}(\bm{X})\approx\operatorname{Law}(\hat{\bm{X}})" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1"><semantics><mrow><mrow><mi>Law</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mi>Law</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Law}(\bm{X})\approx\operatorname{Law}(\hat{\bm{X}})</annotation><annotation encoding="application/x-llamapun">roman_Law ( bold_italic_X ) ≈ roman_Law ( over^ start_ARG bold_italic_X end_ARG )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Acute readers may have noticed that if we do not impose certain
requirements on the representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p2.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> sought, the above problem has
a trivial solution: One may simply choose the functions <math alttext="f" class="ltx_Math" display="inline" id="S1.p2.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="S1.p2.m3"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>
to be the identity map! Hence, the true purpose of seeking for an
autoencoding is to try to ensure that so obtained <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p2.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is both more
compact and more structured than <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.p2.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. Firstly, for compactness, <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p2.m6"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>
should better reveal the intrinsic low-dimensionality of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.p2.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>.
Therefore, the representation should maximize a certain information
gain, say, measured by the rate reduction</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta R_{\epsilon}(\bm{Z})" class="ltx_Math" display="block" id="S1.E2.m1"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">introduced in <a class="ltx_ref" href="Ch3.html#S4.SS2" title="3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4.2</span></a>. Secondly, the main purpose of
learning a good representation of the data distribution is to
facilitate tasks that exploit the low-dimensionality of its
distribution. Hence, the distribution of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p2.m8"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> should be better
structured. For example, the distribution of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p2.m9"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is piecewise linear
or Gaussian, and its components are largely incoherent or independent
etc. These independent components can represent different clusters or
classes and can also be easily used as conditions for decoding the
corresponding data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.p2.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">From the definition of consistent representation, it requires that
the representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.p3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is sufficient to recover the original data
distribution <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to some degree of accuracy. For sample-wise
consistency, a typical choice is to minimize the expected reconstruction error:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(\bm{X},\hat{\bm{X}})=\mathbb{E}[\|\bm{X}-\hat{\bm{X}}\|_{2}^{2}]." class="ltx_Math" display="block" id="S1.E3.m1"><semantics><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}})=\mathbb{E}[\|\bm{X}-\hat{\bm{X}}\|_{2}^{2}].</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ∥ bold_italic_X - over^ start_ARG bold_italic_X end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For consistency in distribution, a typical choice is to minimize a
certain distributional distance such as the KL
divergence<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that for distributions without common
support, which is typical for degenerate distributions, KL divergence
may not even be well-defined. In fact, much of the distribution
learning literature is trying to address this technical difficulty by
replacing or approximating it with something well-defined and
efficiently computable.</span></span></span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(\bm{X},\hat{\bm{X}})=\mathcal{D}_{KL}(\bm{X}\|\hat{\bm{X}})." class="ltx_Math" display="block" id="S1.E4.m1"><semantics><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑿</mi><mo>∥</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}})=\mathcal{D}_{KL}(\bm{X}\|\hat{\bm{X}}).</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( bold_italic_X ∥ over^ start_ARG bold_italic_X end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Hence, computation aside, when we seek a good autoencoding for a data
distribution <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.p4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, conceptually we try to find an encoder <math alttext="f" class="ltx_Math" display="inline" id="S1.p4.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and
decoder <math alttext="g" class="ltx_Math" display="inline" id="S1.p4.m3"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{f,g}[-\Delta R_{\epsilon}(\bm{Z})+d(\bm{X},\hat{\bm{X}})]." class="ltx_Math" display="block" id="S1.E5.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow></munder><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mo>−</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{f,g}[-\Delta R_{\epsilon}(\bm{Z})+d(\bm{X},\hat{\bm{X}})].</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For the rest of this chapter, we will study how to solve such
autoencoding problems under different conditions, from simple and
ideal cases to increasingly more challenging and realistic conditions.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1.1 </span>Linear Autoencoding via PCA</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx13" title="">Bal11</a>]</cite>, the phrase “autoencoder” was first
introduced by Hinton and Rumelhart <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx235" title="">RHW86</a>]</cite> so that a
deep representation can be learned via back propagation (BP) in a self-supervision fashion—reconstructing the original data is the self-supervising task. However, the very same concept of seeking a compact and consistent representation has been rooted in many classic studies. As we have already seen in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the classical PCA, ICA, and sparse dictionary learning all share a similar goal. The only difference is when the underlying data distribution is simple (linear and
independent), the encoding or decoding mappings become easy to represent and
learn: they do not need to be deep and often can be computed in closed form or
with an explicit algorithm.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p">It is instructive to see how the notion of consistency we have
defined plays out in the simple case of PCA:
here, the consistent encoding and decoding mappings are given by a single-layer
linear transform:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}=\bm{U}^{\top}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}\hskip 5.69054pt}\hat{\bm{X}}," class="ltx_Math" display="block" id="S1.E6.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>𝑼</mi></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}=\bm{U}^{\top}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}\hskip 5.69054pt}\hat{\bm{X}},</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{U}\in\mathbb{R}^{D\times d}" class="ltx_Math" display="inline" id="S1.SS1.p2.m1"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathbb{R}^{D\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> typically with <math alttext="d\ll D" class="ltx_Math" display="inline" id="S1.SS1.p2.m2"><semantics><mrow><mi>d</mi><mo>≪</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d\ll D</annotation><annotation encoding="application/x-llamapun">italic_d ≪ italic_D</annotation></semantics></math>. Hence
<math alttext="\bm{U}^{\top}" class="ltx_Math" display="inline" id="S1.SS1.p2.m3"><semantics><msup><mi>𝑼</mi><mo>⊤</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> represents a projection from a higher-dimensional space
<math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.p2.m4"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> to a lower one <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.p2.m5"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, as illustrated in
<a class="ltx_ref" href="#F1" title="In 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="F1"><img alt="Figure 5.1 : Illustration of a typical autoencoder such as PCA, seeking a low-dimensional representation 𝒛 \bm{z} bold_italic_z of high-dimensional data 𝒙 \bm{x} bold_italic_x ." class="ltx_graphics ltx_img_landscape" height="159" id="F1.g1" src="chapters/chapter5/figs/autoencoder.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.1</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of a typical autoencoder such as PCA, seeking
a low-dimensional representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F1.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> of high-dimensional data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p">As we saw in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>, when the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is indeed
supported on a low-dimensional subspace <math alttext="\bm{U}_{o}" class="ltx_Math" display="inline" id="S1.SS1.p3.m2"><semantics><msub><mi>𝑼</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{U}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>, the compactness of
the representation
<math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS1.p3.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> produced by <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S1.SS1.p3.m4"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> is a direct consequence of correctly
estimating (and enforcing) the dimension of this subspace. Finally,
recall that gradient descent on the reconstruction criterion exactly
yields these sample-wise consistent mappings: indeed, the optimal
solution to the problem</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{U}^{\top}\bm{U}=\bm{I}}\,\mathbb{E}_{\bm{x}}\left[\left\|\bm{x}-\bm{U}\bm{U}^{\top}\bm{x}\right\|_{2}^{2}\right]" class="ltx_Math" display="block" id="S1.E7.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow></munder><mo lspace="0.337em">⁡</mo><msub><mi>𝔼</mi><mi>𝒙</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msubsup><mrow><mo>‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo>‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\min_{\bm{U}^{\top}\bm{U}=\bm{I}}\,\mathbb{E}_{\bm{x}}\left[\left\|\bm{x}-\bm{U}\bm{U}^{\top}\bm{x}\right\|_{2}^{2}\right]</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U = bold_italic_I end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ ∥ bold_italic_x - bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">precisely coincides with <math alttext="\bm{U}_{o}" class="ltx_Math" display="inline" id="S1.SS1.p3.m5"><semantics><msub><mi>𝑼</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{U}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> when the dimension of the representation is
sufficiently large. In this case, we obtain sample-wise consistency
for free, since this guarantees that <math alttext="\bm{U}_{o}\bm{U}_{o}^{\top}\bm{x}=\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.p3.m6"><semantics><mrow><mrow><msub><mi>𝑼</mi><mi>o</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>o</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>=</mo><mi>𝒙</mi></mrow><annotation encoding="application/x-tex">\bm{U}_{o}\bm{U}_{o}^{\top}\bm{x}=\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x = bold_italic_x</annotation></semantics></math>.
Notice that in the case of PCA, the rate reduction term in
(<a class="ltx_ref" href="#S1.E5" title="Equation 5.1.5 ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.5</span></a>) becomes void as the regularization
on the representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S1.SS1.p3.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> sought is explicit: It spans the entire
subspace of lower dimension<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>One may view <math alttext="\Delta R=0" class="ltx_Math" display="inline" id="footnote3.m1"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\Delta R=0</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R = 0</annotation></semantics></math> in this case.</span></span></span>.</p>
</div>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Online PCA.</h5>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p1">
<p class="ltx_p">Notice that in the above construction, the
linear transform <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>
used for the encoding and decoding is computed “offline” from all
the input data before hand. One question is whether this transform
can be learned “online” as the input data come in order? This
question was answered by the work of Oja in 1982 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx200" title="">Oja82</a>]</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 5.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Normalized Hebbian learning scheme for PCA)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">Consider a
sequence of i.i.d. random vectors <math alttext="\bm{x}_{1},\ldots,\bm{x}_{i},\ldots\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{1},\ldots,\bm{x}_{i},\ldots\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> with covariance <math alttext="\bm{\Sigma}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="Thmexample1.p1.m2"><semantics><mrow><mi>𝚺</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{\Sigma}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun">bold_Σ ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>. Let <math alttext="\bm{u}_{0}\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="Thmexample1.p1.m3"><semantics><mrow><msub><mi>𝒖</mi><mn>0</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\bm{u}_{0}\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> and define
the response of an input vector <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="Thmexample1.p1.m4"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> against a weight vector
<math alttext="\bm{u}_{i}" class="ltx_Math" display="inline" id="Thmexample1.p1.m5"><semantics><msub><mi>𝒖</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{u}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to be their inner product:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\eta_{i}=\bm{u}_{i}^{T}\bm{x}_{i}" class="ltx_Math" display="block" id="S1.E8.m1"><semantics><mrow><msub><mi>η</mi><mi>i</mi></msub><mo>=</mo><mrow><msubsup><mi>𝒖</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\eta_{i}=\bm{u}_{i}^{T}\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">italic_η start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and we update the weight vector according to the following scheme:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{u}_{i+1}=\frac{\bm{u}_{i}+\gamma\eta_{i}\bm{x}_{i}}{\|\bm{u}_{i}+\gamma\eta_{i}\bm{x}_{i}\|_{2}}" class="ltx_Math" display="block" id="S1.E9.m1"><semantics><mrow><msub><mi>𝒖</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>𝒖</mi><mi>i</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>η</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒖</mi><mi>i</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>η</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><annotation encoding="application/x-tex">\bm{u}_{i+1}=\frac{\bm{u}_{i}+\gamma\eta_{i}\bm{x}_{i}}{\|\bm{u}_{i}+\gamma\eta_{i}\bm{x}_{i}\|_{2}}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_γ italic_η start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_γ italic_η start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some small gain <math alttext="\gamma&gt;0" class="ltx_Math" display="inline" id="Thmexample1.p1.m6"><semantics><mrow><mi>γ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\gamma&gt;0</annotation><annotation encoding="application/x-llamapun">italic_γ &gt; 0</annotation></semantics></math>. This update scheme can be viewed
as a normalized Hebbian scheme, in which the weights of connections
between neurons become stronger if (products of) both the input
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample1.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and output <math alttext="\eta" class="ltx_Math" display="inline" id="Thmexample1.p1.m8"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation><annotation encoding="application/x-llamapun">italic_η</annotation></semantics></math> are strong. One may view the vector of
weights <math alttext="\bm{u}" class="ltx_Math" display="inline" id="Thmexample1.p1.m9"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> are “learned” based on a form of feedback from the
output <math alttext="\eta" class="ltx_Math" display="inline" id="Thmexample1.p1.m10"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation><annotation encoding="application/x-llamapun">italic_η</annotation></semantics></math>.
Then, under reasonable assumptions, Oja <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx200" title="">Oja82</a>]</cite> has
shown that <math alttext="\bm{u}_{i}" class="ltx_Math" display="inline" id="Thmexample1.p1.m11"><semantics><msub><mi>𝒖</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{u}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> converges to the eigenvector associated with
the large eigenvalue of <math alttext="\bm{\Sigma}" class="ltx_Math" display="inline" id="Thmexample1.p1.m12"><semantics><mi>𝚺</mi><annotation encoding="application/x-tex">\bm{\Sigma}</annotation><annotation encoding="application/x-llamapun">bold_Σ</annotation></semantics></math>.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p1.m13"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p2">
<p class="ltx_p">The normalized Hebbian scheme (<a class="ltx_ref" href="#S1.E9" title="Equation 5.1.9 ‣ Example 5.1 (Normalized Hebbian learning scheme for PCA). ‣ Online PCA. ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.9</span></a>) can be interpreted as
a first-order approximation to a <span class="ltx_text ltx_font_italic">stochastic</span> projected
gradient descent scheme on
the objective of the problem (<a class="ltx_ref" href="#S1.E7" title="Equation 5.1.7 ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.7</span></a>) (with batch size
<math alttext="1" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m1"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>, and with the number of columns of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m2"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> equal to <math alttext="1" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m3"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>) as long as
<math alttext="\|\bm{u}\|_{2}=1" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m4"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|\bm{u}\|_{2}=1</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1</annotation></semantics></math>, which is maintained by the projection operation in
(<a class="ltx_ref" href="#S1.E9" title="Equation 5.1.9 ‣ Example 5.1 (Normalized Hebbian learning scheme for PCA). ‣ Online PCA. ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.9</span></a>).
It is worth keeping its existence in the back of one’s mind, both as
a proof of correctness for the use of stochastic gradient methods for
optimizing reconstruction costs such as
(<a class="ltx_ref" href="#S1.E7" title="Equation 5.1.7 ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.7</span></a>), and for its
suggestion that <span class="ltx_text ltx_font_italic">simpler algorithms than (end-to-end) back
propagation can
succeed in learning consistent autoencoders</span>.</p>
</div>
<figure class="ltx_figure" id="F2"><svg class="ltx_picture" height="108.31" id="F2.pic1" overflow="visible" version="1.1" width="402.13"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,108.31) matrix(1 0 0 -1 0 0) translate(4.21,0) translate(0,29.29)"><path d="M -3.94 -7.87 C 39.37 9.84 39.37 9.84 82.68 -3.94" style="fill:none"></path><path d="M 82.68 -3.94 C 68.9 39.37 68.9 39.37 82.68 78.74" style="fill:none"></path><path d="M 82.68 78.74 C 39.37 68.9 39.37 68.9 -3.94 78.74" style="fill:none"></path><path d="M -3.94 78.74 C 9.84 39.37 9.84 39.37 -3.94 -7.87" style="fill:none"></path><path d="M 13.78 19.69 C 27.56 29.53 27.56 29.53 33.46 47.24" style="fill:none"></path><path d="M 33.46 47.24 C 39.37 55.12 39.37 55.12 45.28 47.24" style="fill:none"></path><path d="M 45.28 47.24 C 51.18 29.53 51.18 29.53 64.96 19.69" style="fill:none"></path><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 40.84 35.43 C 40.84 36.24 40.18 36.9 39.37 36.9 C 38.56 36.9 37.9 36.24 37.9 35.43 C 37.9 34.62 38.56 33.97 39.37 33.97 C 40.18 33.97 40.84 34.62 40.84 35.43 Z M 39.37 35.43" style="stroke:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 54.62 17.72 C 54.62 18.53 53.96 19.18 53.15 19.18 C 52.34 19.18 51.68 18.53 51.68 17.72 C 51.68 16.91 52.34 16.25 53.15 16.25 C 53.96 16.25 54.62 16.91 54.62 17.72 Z M 53.15 17.72" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 35.42 -22.66)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.91"><math alttext="\bm{x}" class="ltx_Math" display="inline" id="F2.pic1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math></foreignobject></g><path d="M 88.58 39.37 L 147.08 39.37" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 147.36 39.37)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 113.98 45.75)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="8.26"><math alttext="f" class="ltx_Math" display="inline" id="F2.pic1.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math></foreignobject></g><path d="M 161.42 3.94 L 232.28 3.94" style="fill:none"></path><path d="M 232.28 3.94 L 232.28 74.8" style="fill:none"></path><path d="M 232.28 74.8 L 161.42 74.8" style="fill:none"></path><path d="M 161.42 74.8 L 161.42 3.94" style="fill:none"></path><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 198.32 39.37 C 198.32 40.18 197.66 40.84 196.85 40.84 C 196.04 40.84 195.38 40.18 195.38 39.37 C 195.38 38.56 196.04 37.9 196.85 37.9 C 197.66 37.9 198.32 38.56 198.32 39.37 Z M 196.85 39.37" style="stroke:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 218 19.69 C 218 20.5 217.35 21.15 216.54 21.15 C 215.72 21.15 215.07 20.5 215.07 19.69 C 215.07 18.87 215.72 18.22 216.54 18.22 C 217.35 18.22 218 18.87 218 19.69 Z M 216.54 19.69" style="stroke:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 196.85 39.37 L 216.54 19.69" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 193.33 -22.66)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.04"><math alttext="\bm{z}" class="ltx_Math" display="inline" id="F2.pic1.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math></foreignobject></g><path d="M 246.06 39.37 L 304.56 39.37" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 304.84 39.37)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 272.04 47.58)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.1"><math alttext="g" class="ltx_Math" display="inline" id="F2.pic1.m4"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math></foreignobject></g><path d="M 311.02 -7.87 C 354.33 9.84 354.33 9.84 397.64 -3.94" style="fill:none"></path><path d="M 397.64 -3.94 C 383.86 39.37 383.86 39.37 397.64 78.74" style="fill:none"></path><path d="M 397.64 78.74 C 354.33 68.9 354.33 68.9 311.02 78.74" style="fill:none"></path><path d="M 311.02 78.74 C 324.8 39.37 324.8 39.37 311.02 -7.87" style="fill:none"></path><path d="M 328.74 19.69 C 342.52 29.53 342.52 29.53 348.43 47.24" style="fill:none"></path><path d="M 348.43 47.24 C 354.33 55.12 354.33 55.12 360.24 47.24" style="fill:none"></path><path d="M 360.24 47.24 C 366.14 29.53 366.14 29.53 379.92 19.69" style="fill:none"></path><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 355.8 35.43 C 355.8 36.24 355.14 36.9 354.33 36.9 C 353.52 36.9 352.86 36.24 352.86 35.43 C 352.86 34.62 353.52 33.97 354.33 33.97 C 355.14 33.97 355.8 34.62 355.8 35.43 Z M 354.33 35.43" style="stroke:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 369.58 17.72 C 369.58 18.53 368.92 19.18 368.11 19.18 C 367.3 19.18 366.64 18.53 366.64 17.72 C 366.64 16.91 367.3 16.25 368.11 16.25 C 368.92 16.25 369.58 16.91 369.58 17.72 Z M 368.11 17.72" style="stroke:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000"><path d="M 354.33 35.43 C 358.27 21.65 358.27 21.65 368.11 17.72" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 350.49 -24.68)"><foreignobject height="9.99" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.69"><math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="F2.pic1.m5"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.2</span>: </span><span class="ltx_text" style="font-size:90%;">A depiction of interpolation through manifold flattening
on a manifold in <math alttext="\mathbb{R}^{3}" class="ltx_Math" display="inline" id="F2.m5"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> of dimension <math alttext="d=2" class="ltx_Math" display="inline" id="F2.m6"><semantics><mrow><mi>d</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">d=2</annotation><annotation encoding="application/x-llamapun">italic_d = 2</annotation></semantics></math>. To interpolate
two points on the data manifold, map them through the flattening
map <math alttext="f" class="ltx_Math" display="inline" id="F2.m7"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> to the flattened space, take their convex interpolants,
and then map them back to the data manifold through the
reconstruction map <math alttext="g" class="ltx_Math" display="inline" id="F2.m8"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1.2 </span>Nonlinear PCA and Autoencoding</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p">Of course, one should expect that things will no longer be so simple
when we deal with more complex distributions whose underlying
low-dimensional structure could be nonlinear.</p>
</div>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data on a Nonlinear Submanifold.</h5>
<div class="ltx_para" id="S1.SS2.SSS0.Px1.p1">
<p class="ltx_p">So, to move beyond the
linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math>. The intrinsic dimension of the submanifold, say <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, is typically much lower than the
dimension of the ambient space <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m3"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. From this geometric
perspective, we typically want to find a nonlinear mapping <math alttext="f" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> such that
the resulting manifold
<math alttext="f(\mathcal{M})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m5"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\mathcal{M})</annotation><annotation encoding="application/x-llamapun">italic_f ( caligraphic_M )</annotation></semantics></math> is flattened, as illustrated by the example shown in
<a class="ltx_ref" href="#F2" title="In Online PCA. ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>. The resulting feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>-space
is typically
more compact (of lower-dimension) than the <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>-space, and the
manifold is flat.
From the statistical perspective, which is complementary to the geometric
perspective but distinct in general, we may also want to ensure that the data
distribution on <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m8"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> is mapped to a sufficiently regular
distribution, say a Gaussian or a uniform distribution (with a
very low-dimensional support), in the <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m9"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>-space. These two properties ensure that sampling and interpolation in the <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px1.p1.m10"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>-space are as easy as possible, and they are mathematical formalizations of the desirable
notions of compact and structured features in the low-dimensional manifold
model for the data distribution.
In general, the problem of learning such an autoencoding mapping for this class
of data distributions is known as <span class="ltx_text ltx_font_italic">nonlinear principal component analysis</span>
(NLPCA).</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">A Classical Attempt via a Two-Layer Network.</h5>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p1">
<p class="ltx_p">As we have
seen above, in the case of PCA, a one-layer linear neural
network is sufficient. That is no longer the case for NLPCA. In 1991, Kramer
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx143" title="">Kra91</a>]</cite> proposed to solve NLPCA by using a two-layer
neural network to represent the encoder mapping <math alttext="f" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> (or its inverse <math alttext="g" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m2"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>) based
on the universal representation property of two-layer networks with sigmoid
activation:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}=\bm{W}_{2}\sigma(\bm{W}_{1}\bm{x}+\bm{b})," class="ltx_Math" display="block" id="S1.E10.m1"><semantics><mrow><mrow><mi>𝒛</mi><mo>=</mo><mrow><msub><mi>𝑾</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑾</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>+</mo><mi>𝒃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{z}=\bm{W}_{2}\sigma(\bm{W}_{1}\bm{x}+\bm{b}),</annotation><annotation encoding="application/x-llamapun">bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_x + bold_italic_b ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\sigma(\,\cdot\,)" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m3"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(\,\cdot\,)</annotation><annotation encoding="application/x-llamapun">italic_σ ( ⋅ )</annotation></semantics></math> is the sigmoid function:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sigma(x)=\frac{1}{1+e^{-x}}." class="ltx_Math" display="block" id="S1.E11.m1"><semantics><mrow><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\sigma(x)=\frac{1}{1+e^{-x}}.</annotation><annotation encoding="application/x-llamapun">italic_σ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Cybenko <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx60" title="">Cyb89</a>]</cite> showed that functions of
the above form (with enough hidden nodes) can approximate any smooth
nonlinear function, say the encoder <math alttext="f(\,\cdot\,)" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p1.m4"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\,\cdot\,)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math>, to an arbitrary
precision. In particular, they can represent the flattening and reconstruction
maps for data distributions supported on (unions of) low-dimensional manifolds,
as in <a class="ltx_ref" href="#F2" title="In Online PCA. ‣ 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>. The overall architecture of the
original networks proposed by Kramer is illustrated in <a class="ltx_ref" href="#F3" title="In A Classical Attempt via a Two-Layer Network. ‣ 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<figure class="ltx_figure" id="F3"><img alt="Figure 5.3 : Nonlinear PCA by autoassociative neural networks of depth two for both the encoding and decoding mappings, suggested by Kramer [ Kra91 ] ." class="ltx_graphics ltx_img_landscape" height="263" id="F3.g1" src="chapters/chapter5/figs/kramer1991nonlinearPCA.png" width="359"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.3</span>: </span><span class="ltx_text" style="font-size:90%;">Nonlinear PCA by autoassociative neural networks of depth
two for both the encoding and decoding mappings, suggested by
Kramer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx143" title="">Kra91</a>]</cite>.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.SSS0.Px2.p2">
<p class="ltx_p">Unfortunately, unlike the above case of PCA, there is in general no
closed-form learning scheme for the parameters <math alttext="\bm{\theta}=(\bm{W},\bm{b})" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m1"><semantics><mrow><mi>𝜽</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>𝑾</mi><mo>,</mo><mi>𝒃</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\theta}=(\bm{W},\bm{b})</annotation><annotation encoding="application/x-llamapun">bold_italic_θ = ( bold_italic_W , bold_italic_b )</annotation></semantics></math> of these networks. Hence it was proposed to train the
network via back propagation with the supervision of reconstruction error:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{\theta}}\mathbb{E}[\|\bm{x}-\hat{\bm{x}}(\bm{\theta})\|_{2}^{2}]." class="ltx_Math" display="block" id="S1.E12.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{\theta}}\mathbb{E}[\|\bm{x}-\hat{\bm{x}}(\bm{\theta})\|_{2}^{2}].</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E [ ∥ bold_italic_x - over^ start_ARG bold_italic_x end_ARG ( bold_italic_θ ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Compared to the simple case of PCA, we utilize the same reconstruction objective
for learning, but a far more complex nonlinear class of models for
parameterizing and learning the encoder and decoder. Although universal
approximation properties such as Cybenko’s suggest that <span class="ltx_text ltx_font_italic">in principle</span>
learning consistent autoencoders via this framework is possible—because for
any random sample of data, given enough parameters, such autoencoding pairs
exist—one often finds it highly nontrivial to find them with gradient descent.
Moreover, to obtain an informative enough reconstruction objective
and model for the
distribution of high-dimensional real-world data such as images, the required
number of samples and hidden nodes can be huge.
In addition, as a measure of the compactness of the learned representation, the
(lower) dimension of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px2.p2.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> for the bottleneck layer is often chosen
heuristically.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>In the later work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx105" title="">HZ93</a>]</cite>, Hinton et.
al. suggested to use the minimum description length (MDL) principle to promote
the compactness of the learned coding scheme, in a spirit very similar to the
rate distortion measure introduced in this book.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Manifold Flattening via a Deeper Network.</h5>
<div class="ltx_para" id="S1.SS2.SSS0.Px3.p1">
<p class="ltx_p">Based on the modern practice of deep networks, such classical shallow
and wide network architectures are known to be rather difficult to
train effectively and efficiently via back propagation (BP), partly
due to the diminishing gradient of the sigmoid function. Hence, the
modern practice normally suggests to further
decompose the nonlinear transform <math alttext="f" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px3.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> (or <math alttext="g" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px3.p1.m2"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>) into a composition of
many more layers of simpler transforms, resulting a deeper network architecture
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx104" title="">HS06</a>]</cite>, as illustrated in <a class="ltx_ref" href="#F4" title="In Manifold Flattening via a Deeper Network. ‣ 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>.
In the modern context, further elaborations over the basic reconstruction cost
(<a class="ltx_ref" href="#S1.E12" title="Equation 5.1.12 ‣ A Classical Attempt via a Two-Layer Network. ‣ 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.12</span></a>) also prove necessary to achieve good performance on
complex real-world data distributions such as images.</p>
</div>
<figure class="ltx_figure" id="F4"><svg class="ltx_picture" height="106.29" id="F4.pic1" overflow="visible" version="1.1" width="362.76"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.29) matrix(1 0 0 -1 0 0) translate(4.21,0) translate(0,27.28)"><path d="M -3.94 -7.87 C 39.37 9.84 39.37 9.84 82.68 -3.94" style="fill:none"></path><path d="M 82.68 -3.94 C 68.9 39.37 68.9 39.37 82.68 78.74" style="fill:none"></path><path d="M 82.68 78.74 C 39.37 68.9 39.37 68.9 -3.94 78.74" style="fill:none"></path><path d="M -3.94 78.74 C 9.84 39.37 9.84 39.37 -3.94 -7.87" style="fill:none"></path><path d="M 13.78 19.69 C 27.56 29.53 27.56 29.53 33.46 47.24" style="fill:none"></path><path d="M 33.46 47.24 C 39.37 55.12 39.37 55.12 45.28 47.24" style="fill:none"></path><path d="M 45.28 47.24 C 51.18 29.53 51.18 29.53 64.96 19.69" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 35.42 -22.66)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.91"><math alttext="\bm{x}" class="ltx_Math" display="inline" id="F4.pic1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math></foreignobject></g><path d="M 88.58 49.21 L 127.4 49.21" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 127.68 49.21)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 102.2 55.6)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="12.14"><math alttext="f_{1}" class="ltx_Math" display="inline" id="F4.pic1.m2"><semantics><msub><mi>f</mi><mn>1</mn></msub><annotation encoding="application/x-tex">f_{1}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 137.8 49.21 L 176.61 49.21" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 176.89 49.21)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 151.41 55.6)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="12.14"><math alttext="f_{2}" class="ltx_Math" display="inline" id="F4.pic1.m3"><semantics><msub><mi>f</mi><mn>2</mn></msub><annotation encoding="application/x-tex">f_{2}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 187.01 49.21 L 225.82 49.21" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 226.1 49.21)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 201.5 55.6)"><foreignobject height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38"><math alttext="\cdots" class="ltx_Math" display="inline" id="F4.pic1.m4"><semantics><mi mathvariant="normal">⋯</mi><annotation encoding="application/x-tex">\cdots</annotation><annotation encoding="application/x-llamapun">⋯</annotation></semantics></math></foreignobject></g><path d="M 236.22 49.21 L 275.04 49.21" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(1.0 0.0 0.0 1.0 275.31 49.21)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 249.14 55.6)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.54"><math alttext="f_{L}" class="ltx_Math" display="inline" id="F4.pic1.m5"><semantics><msub><mi>f</mi><mi>L</mi></msub><annotation encoding="application/x-tex">f_{L}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 275.59 29.53 L 236.77 29.53" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(-1.0 0.0 0.0 -1.0 236.5 29.53)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 249.72 18.05)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="12.37"><math alttext="g_{L}" class="ltx_Math" display="inline" id="F4.pic1.m6"><semantics><msub><mi>g</mi><mi>L</mi></msub><annotation encoding="application/x-tex">g_{L}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 226.38 29.53 L 187.56 29.53" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(-1.0 0.0 0.0 -1.0 187.28 29.53)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 201.5 16.23)"><foreignobject height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.38"><math alttext="\cdots" class="ltx_Math" display="inline" id="F4.pic1.m7"><semantics><mi mathvariant="normal">⋯</mi><annotation encoding="application/x-tex">\cdots</annotation><annotation encoding="application/x-llamapun">⋯</annotation></semantics></math></foreignobject></g><path d="M 177.17 29.53 L 138.35 29.53" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(-1.0 0.0 0.0 -1.0 138.07 29.53)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 152 18.05)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.97"><math alttext="g_{2}" class="ltx_Math" display="inline" id="F4.pic1.m8"><semantics><msub><mi>g</mi><mn>2</mn></msub><annotation encoding="application/x-tex">g_{2}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 127.95 29.53 L 89.14 29.53" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(-1.0 0.0 0.0 -1.0 88.86 29.53)"><path d="M -2.88 3.32 C -2.35 1.33 -1.18 0.39 0 0 C -1.18 -0.39 -2.35 -1.33 -2.88 -3.32" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 102.78 18.05)"><foreignobject height="8.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="10.97"><math alttext="g_{1}" class="ltx_Math" display="inline" id="F4.pic1.m9"><semantics><msub><mi>g</mi><mn>1</mn></msub><annotation encoding="application/x-tex">g_{1}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><path d="M 287.4 3.94 L 358.27 3.94" style="fill:none"></path><path d="M 358.27 3.94 L 358.27 74.8" style="fill:none"></path><path d="M 358.27 74.8 L 287.4 74.8" style="fill:none"></path><path d="M 287.4 74.8 L 287.4 3.94" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 319.31 -22.66)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.04"><math alttext="\bm{z}" class="ltx_Math" display="inline" id="F4.pic1.m10"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.4</span>: </span><span class="ltx_text" style="font-size:90%;">A depiction of the construction process of the flattening
and reconstruction pair <math alttext="(f,g)" class="ltx_Math" display="inline" id="F4.m5"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>,</mo><mi>g</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f,g)</annotation><annotation encoding="application/x-llamapun">( italic_f , italic_g )</annotation></semantics></math>, where the encoder <math alttext="f=f_{L}\circ f_{L-1}\circ\cdots\circ f_{1}" class="ltx_Math" display="inline" id="F4.m6"><semantics><mrow><mi>f</mi><mo>=</mo><mrow><msub><mi>f</mi><mi>L</mi></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mn>1</mn></msub></mrow></mrow><annotation encoding="application/x-tex">f=f_{L}\circ f_{L-1}\circ\cdots\circ f_{1}</annotation><annotation encoding="application/x-llamapun">italic_f = italic_f start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT italic_L - 1 end_POSTSUBSCRIPT ∘ ⋯ ∘ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is
constructed from composing flattening layers, and the decoder <math alttext="g=g_{1}\circ g_{2}\circ\cdots\circ g_{L}" class="ltx_Math" display="inline" id="F4.m7"><semantics><mrow><mi>g</mi><mo>=</mo><mrow><msub><mi>g</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>g</mi><mn>2</mn></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>g</mi><mi>L</mi></msub></mrow></mrow><annotation encoding="application/x-tex">g=g_{1}\circ g_{2}\circ\cdots\circ g_{L}</annotation><annotation encoding="application/x-llamapun">italic_g = italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∘ italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∘ ⋯ ∘ italic_g start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> is composed of
inversions of each <math alttext="f_{\ell}" class="ltx_Math" display="inline" id="F4.m8"><semantics><msub><mi>f</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">f_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS2.SSS0.Px3.p2">
<p class="ltx_p">In light of universal approximation theorems such as Cybenko’s, one
may initially wonder why, conceptually, deeper autoencoders should be preferred
to shallow ones.
From purely an expressivity perspective, we can understand this phenomenon
through a geometric angle related to the task of flattening the nonlinear
manifold on which our hypothesized data distribution is supported. A purely
constructive approach to flattening the manifold proceeds incrementally, in
parallel to what we have seen in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapters</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a> with
the interaction between diffusion, denoising, and compression. In the geometric
setting, the incremental <span class="ltx_text ltx_font_italic">flattening process</span> corresponding to <math alttext="f_{\ell}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px3.p2.m1"><semantics><msub><mi>f</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">f_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math>
takes the form of transforming a neighborhood of one point of the manifold to be
closer to a flat manifold (i.e., a subspace), and enforcing local consistency
with the rest of the data samples; the corresponding incremental operation in
the decoder, <math alttext="g_{\ell}" class="ltx_Math" display="inline" id="S1.SS2.SSS0.Px3.p2.m2"><semantics><msub><mi>g</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">g_{\ell}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math>, undoes this transformation. This procedure precisely
incorporates curvature information about the underlying manifold, which is
estimated from data samples. Given enough samples from the manifold and
a careful instantiation of this conceptual process, it is possible to implement
this procedure as a computational procedure that verifiably flattens nonlinear
manifolds in a white-box fashion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx214" title="">PPR+24</a>]</cite>. However, the approach is
limited in its applicability to high-dimensional data distributions such as
images due to unfavorable scalability, motivating the development of more
flexible methods to incremental autoencoding.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1.3 </span>Sparse Autoencoding</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p">In the above autoencoding schemes, the dimension of the feature space
<math alttext="d" class="ltx_Math" display="inline" id="S1.SS3.p1.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> is typically chosen to be much lower than that the original data
space <math alttext="D" class="ltx_Math" display="inline" id="S1.SS3.p1.m2"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> so as to explicitly enforce or promote the learned
representation to be low-dimensional. However, in practice, we
normally do not know the intrinsic dimension of the data
distribution. Hence, the choice of the feature space dimension for
autoencoding is often done empirically. In more general situations,
the data distribution can be a mixture of a few low-dimensional
subspaces or submanifolds. In these cases, it is no longer feasible
to enforce a single low-dimensional space for all the features together.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p">The sparse autoencoder is meant to resolve some of these limitations. In
particular, the dimension of the feature space can be equal to or
even higher than that of the data space, as illustrated in
<a class="ltx_ref" href="#F5" title="In 5.1.3 Sparse Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>. However, the features are required to be highly
sparse in the feature space. So if we impose sparsity as the measure
of parsimony in addition to the rate reduction in the objective
(<a class="ltx_ref" href="#S1.E5" title="Equation 5.1.5 ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.5</span></a>), we obtain a new objective for
the sparse autoencoding:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{f,g}[\|\bm{Z}\|_{0}-\Delta R_{\epsilon}(\bm{Z})+d(\bm{X},\hat{\bm{X}})]," class="ltx_Math" display="block" id="S1.E13.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow></munder><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>−</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{f,g}[\|\bm{Z}\|_{0}-\Delta R_{\epsilon}(\bm{Z})+d(\bm{X},\hat{\bm{X}})],</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="S1.SS3.p2.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math>-“norm” <math alttext="\|\,\cdot\,\|_{0}" class="ltx_math_unparsed" display="inline" id="S1.SS3.p2.m2"><semantics><mrow><mo rspace="0em">∥</mo><mo>⋅</mo><msub><mo lspace="0em">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\,\cdot\,\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ ⋅ ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is known to promote sparsity.
This is very similar to the sparse rate reduction objective
(<a class="ltx_ref" href="Ch4.html#S2.E4" title="Equation 4.2.4 ‣ Sparse Rate Reduction. ‣ 4.2.1 Unrolled Optimization for Sparse Rate Reduction ‣ 4.2 White-Box Transformers from Unrolled Optimization ‣ Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>) which we used in the previous <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a> to derive the white-box CRATE architecture.</p>
</div>
<figure class="ltx_figure" id="F5"><img alt="Figure 5.5 : Illustration of a sparse autoencoder (SAE), compared to that of a typical autoencoder (AE) in Figure 5.1 ." class="ltx_graphics ltx_img_landscape" height="229" id="F5.g1" src="chapters/chapter5/figs/SAE_diagram.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.5</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of a sparse autoencoder (SAE), compared to
that of a typical autoencoder (AE) in <a class="ltx_ref" href="#F1" title="In 5.1.1 Linear Autoencoding via PCA ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>. </span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p">As a method for learning autoencoding pairs in an end-to-end fashion, sparse
autoencoding has been practiced in the past
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx224" title="">RPC+06</a>, <a class="ltx_ref" href="bib.html#bibx148" title="">LRM+12</a>]</cite>, but nearly all modern
autoencoding frameworks are instead based on a different, probabilistic
autoencoding framework, which we will study now.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1.4 </span>Variational Autoencoding</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p">In the classical conception of autoencoding, following Hinton and Rumelhart
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx235" title="">RHW86</a>]</cite>, the data distribution plays a very minor role in the
formulation, in spite of its centrality to the representation we ultimately
learn. Indeed, in the naive framework, one hopes that by training a deep network
to reconstruct samples from the data distribution with a suitably-configured
bottleneck for the representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS4.p1.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, the learned encoders <math alttext="f" class="ltx_Math" display="inline" id="S1.SS4.p1.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="S1.SS4.p1.m3"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> will
naturally end up corresponding to a compact and structured feature
representation for the data. This is rarely the case in practice.
An improved, more modern methodology for autoencoding that still finds
significant application to this day is <span class="ltx_text ltx_font_italic">variational autoencoding</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx136" title="">KW13</a>, <a class="ltx_ref" href="bib.html#bibx137" title="">KW19</a>]</cite>.
We will see how this framework, which trains a variational autoencoder (VAE)
derived through probabilistic modeling considerations, both generalizes the
classical autoencoder training (via minimization of the reconstruction loss),
and stabilizes it with appropriate regularization. Later, we will see how to
begin to go further and go beyond the black-box nature of the deep networks used
to represent the encoding and decoding mappings <math alttext="f" class="ltx_Math" display="inline" id="S1.SS4.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="S1.SS4.p1.m5"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsubsection" id="S1.SS4.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Probabilistic Perspective on Autoencoding</h4>
<div class="ltx_para" id="S1.SS4.SSSx1.p1">
<p class="ltx_p">In the manifold model for the data distribution, the key mathematical objects
are the <span class="ltx_text ltx_font_italic">support</span> of the data distribution, namely the manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math>,
and the density of the data on the support, say <math alttext="p" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m2"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>. When we formulate
autoencoding from the probabilistic perspective, we often think of the
high-dimensional input <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> as having a density <math alttext="p" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m4"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> with support on <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m5"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>; one
can think of adding a very small amount of noise to the (degenerate)
distribution supported on the manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m6"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> to obtain this density <math alttext="p" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m7"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>, in line
with our denoising-diffusion constructions in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
Then the goal of generative probabilistic modeling is to learn the density <math alttext="p" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m8"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math>
from samples <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, say from a class of models <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m10"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math> parameterized
by <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m11"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math>. As we have recalled in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>, a classical
approach to achieving this is via maximum likelihood estimation:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{\theta}}\,\mathbb{E}_{\bm{x}}[\log p(\bm{x};\,\bm{\theta})]." class="ltx_Math" display="block" id="S1.Ex1.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.337em">⁡</mo><msub><mi>𝔼</mi><mi>𝒙</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{\theta}}\,\mathbb{E}_{\bm{x}}[\log p(\bm{x};\,\bm{\theta})].</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ; bold_italic_θ ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">For certain representative classes of data distributions <math alttext="p" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m12"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> and sufficiently-expressive classes of models <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m13"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math>, even simpler
learning problems than the maximum likelihood estimation problem are known to be
statistically hard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx307" title="">YB99</a>]</cite>. Hence it is desirable to exploit the
knowledge that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m14"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has low-dimensional structure by seeking to factor the
distribution <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m15"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math> according to a low-dimensional “latent”
variable model <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m16"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. Indeed, we may write by conditioning <math alttext="p(\bm{x},\bm{z};\,\bm{\theta})=p(\bm{z};\,\bm{\theta})p(\bm{x}\mid\bm{z};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m17"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x},\bm{z};\,\bm{\theta})=p(\bm{z};\,\bm{\theta})p(\bm{x}\mid\bm{z};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) = italic_p ( bold_italic_z ; bold_italic_θ ) italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ )</annotation></semantics></math>, and</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx66">
<tbody id="S1.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(\bm{x};\bm{\theta})" class="ltx_Math" display="inline" id="S1.Ex2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle p(\bm{x};\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\int p(\bm{z};\,\bm{\theta})p(\bm{x}\mid\bm{z};\,\bm{\theta})\mathrm{d}\bm{z}" class="ltx_Math" display="inline" id="S1.Ex2.m2"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒛</mi></mrow></mrow></mrow></mstyle></mrow><annotation encoding="application/x-tex">\displaystyle=\int p(\bm{z};\,\bm{\theta})p(\bm{x}\mid\bm{z};\,\bm{\theta})\mathrm{d}\bm{z}</annotation><annotation encoding="application/x-llamapun">= ∫ italic_p ( bold_italic_z ; bold_italic_θ ) italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ ) roman_d bold_italic_z</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbb{E}_{\bm{z}\sim p(\,\cdot\,;\,\bm{\theta})}[p(\bm{x}\mid\bm{z};\,\bm{\theta})]." class="ltx_Math" display="inline" id="S1.Ex3.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\mathbb{E}_{\bm{z}\sim p(\,\cdot\,;\,\bm{\theta})}[p(\bm{x}\mid\bm{z};\,\bm{\theta})].</annotation><annotation encoding="application/x-llamapun">= blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_p ( ⋅ ; bold_italic_θ ) end_POSTSUBSCRIPT [ italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Classically, our model for the data distribution <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m18"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math> corresponds
to a choice of the latent distribution <math alttext="p(\bm{z};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m19"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z ; bold_italic_θ )</annotation></semantics></math> and the conditional
distribution <math alttext="p(\bm{x}\mid\bm{z};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m20"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{z};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ )</annotation></semantics></math>.
Even so, computing the model for the data distribution from these latent
distributions is intractable except for in special cases, analogous to those we
have studied in <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
By the same token, computing the posterior <math alttext="p(\bm{z}\mid\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m21"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z}\mid\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z ∣ bold_italic_x ; bold_italic_θ )</annotation></semantics></math> from
data, allowing us to <span class="ltx_text ltx_font_italic">encode</span> samples to their corresponding latents, is
generally intractable.
There is hence a tradeoff between the expressivity of our generative model,
its computational tractability, and the accuracy of any approximations we make
to the underlying probabilistic framework for the sake of
computational tractability.
In navigating this tradeoff, one also needs a flexible computational approach
for learning the model parameters <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p1.m22"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math> from data, analogous to the maximum
likelihood objective.</p>
</div>
<div class="ltx_para" id="S1.SS4.SSSx1.p2">
<p class="ltx_p">In the variational autoencoding framework, we navigate this tradeoff through
three key insights:</p>
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p">We posit <span class="ltx_text ltx_font_italic">simple</span> distributions for <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.I2.i1.p1.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I2.i1.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> conditional
on <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.I2.i1.p1.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, but make their parameters depend in a highly flexible way on the
input data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.I2.i1.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (where relevant) using deep networks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p">We replace the posterior <math alttext="p(\bm{z}\mid\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.I2.i2.p1.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z}\mid\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z ∣ bold_italic_x ; bold_italic_θ )</annotation></semantics></math>, used for encoding
and whose form is implied (by Bayes rule) by our modeling choices for <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.I2.i2.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>,
with a tractable approximation <math alttext="q(\bm{z}\mid\bm{x};\,\bm{\eta})" class="ltx_Math" display="inline" id="S1.I2.i2.p1.m3"><semantics><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\bm{z}\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math>, which has its own
parameters <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S1.I2.i2.p1.m4"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p">We jointly learn <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S1.I2.i3.p1.m1"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math> and <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S1.I2.i3.p1.m2"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math> via maximizing a tractable lower
bound for the likelihood <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.I2.i3.p1.m3"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math>, known as the evidence lower
bound (ELBO).</p>
</div>
</li>
</ol>
<p class="ltx_p">We will focus on the most useful instantation of the VAE framework
for practice, namely
where the prior <math alttext="p(\bm{z};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z ; bold_italic_θ )</annotation></semantics></math> and the conditional <math alttext="p(\bm{x}\mid\bm{z};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x}\mid\bm{z};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ )</annotation></semantics></math> are both Gaussian. Namely, we use the following Gaussian
distributions:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx67">
<tbody id="S1.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{z}" class="ltx_Math" display="inline" id="S1.Ex4.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\displaystyle\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sim\mathcal{N}(\mathbf{0},\bm{I})," class="ltx_Math" display="inline" id="S1.Ex4.m2"><semantics><mrow><mrow><mi></mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sim\mathcal{N}(\mathbf{0},\bm{I}),</annotation><annotation encoding="application/x-llamapun">∼ caligraphic_N ( bold_0 , bold_italic_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{x}\mid\bm{z}" class="ltx_Math" display="inline" id="S1.Ex5.m1"><semantics><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">\displaystyle\bm{x}\mid\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∣ bold_italic_z</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sim\mathcal{N}(g_{1}(\bm{z}),\operatorname{diag}(e^{g_{2}(\bm{z})})\bm{I})," class="ltx_Math" display="inline" id="S1.Ex5.m2"><semantics><mrow><mrow><mi></mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>g</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sim\mathcal{N}(g_{1}(\bm{z}),\operatorname{diag}(e^{g_{2}(\bm{z})})\bm{I}),</annotation><annotation encoding="application/x-llamapun">∼ caligraphic_N ( italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_z ) , roman_diag ( italic_e start_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_z ) end_POSTSUPERSCRIPT ) bold_italic_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="g=(g_{1},g_{2})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m3"><semantics><mrow><mi>g</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><msub><mi>g</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g=(g_{1},g_{2})</annotation><annotation encoding="application/x-llamapun">italic_g = ( italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math> are deep networks with parameters <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m4"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math>, which
correspond to the <span class="ltx_text ltx_font_italic">decoder</span> in the autoencoder.
Similarly, for the approximate posterior <math alttext="q(\bm{z}\mid\bm{x};\,\bm{\eta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m5"><semantics><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\bm{z}\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math>, we use
a special Gaussian distribution with parameters given by an encoder
MLP <math alttext="f=(f_{1},f_{2})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m6"><semantics><mrow><mi>f</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f=(f_{1},f_{2})</annotation><annotation encoding="application/x-llamapun">italic_f = ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math> with parameters <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m7"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}\mid\bm{x}\sim\mathcal{N}(f_{1}(\bm{x}),\operatorname{diag}(e^{f_{2}(\bm{x})})\bm{I})." class="ltx_Math" display="block" id="S1.Ex6.m1"><semantics><mrow><mrow><mrow><mi>𝒛</mi><mo>∣</mo><mi>𝒙</mi></mrow><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}\mid\bm{x}\sim\mathcal{N}(f_{1}(\bm{x}),\operatorname{diag}(e^{f_{2}(\bm{x})})\bm{I}).</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∣ bold_italic_x ∼ caligraphic_N ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) , roman_diag ( italic_e start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUPERSCRIPT ) bold_italic_I ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">This makes probabilistic encoding and decoding simple: we simply map our data
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to mean and variance parameters of a Gaussian distribution for
encoding, or vice versa.
For learning the encoder and decoder, we start from the maximum likelihood
objective <a class="ltx_ref" href="#S1.Ex1" title="Probabilistic Perspective on Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1.4</span></a>, and derive a convenient lower bound known as the
evidence lower bound, or ELBO. Starting from simple algebraic manipulations</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx68">
<tbody id="S1.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\log p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.Ex7.m1"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\log p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">roman_log italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{p(\bm{z}\mid\bm{x};\,\bm{\theta})}" class="ltx_Math" display="inline" id="S1.Ex7.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{p(\bm{z}\mid\bm{x};\,\bm{\theta})}</annotation><annotation encoding="application/x-llamapun">= roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) end_ARG start_ARG italic_p ( bold_italic_z ∣ bold_italic_x ; bold_italic_θ ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}+\log\frac{q(\bm{z}\mid\bm{x};\,\bm{\eta})}{p(\bm{z}\mid\bm{x};\,\bm{\theta})}," class="ltx_Math" display="inline" id="S1.Ex8.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>+</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}+\log\frac{q(\bm{z}\mid\bm{x};\,\bm{\eta})}{p(\bm{z}\mid\bm{x};\,\bm{\theta})},</annotation><annotation encoding="application/x-llamapun">= roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) end_ARG start_ARG italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η ) end_ARG + roman_log divide start_ARG italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η ) end_ARG start_ARG italic_p ( bold_italic_z ∣ bold_italic_x ; bold_italic_θ ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">we take expectations with respect to <math alttext="\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})" class="ltx_math_unparsed" display="inline" id="S1.SS4.SSSx1.p2.m9"><semantics><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math> and
use the Gibbs inequality (<a class="ltx_ref" href="Ch3.html#Thmtheorem1" title="Theorem 3.1 (Information Inequality). ‣ 3.1.3 Minimizing Coding Rate ‣ 3.1 Entropy Minimization and Compression ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) to get</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\log p(\bm{x};\,\bm{\theta})\geq\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right]." class="ltx_math_unparsed" display="block" id="S1.Ex9.m1"><semantics><mrow><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\log p(\bm{x};\,\bm{\theta})\geq\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right].</annotation><annotation encoding="application/x-llamapun">roman_log italic_p ( bold_italic_x ; bold_italic_θ ) ≥ blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) end_ARG start_ARG italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η ) end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The righthand side of this bound is the ELBO; as a lower bound for the pointwise
log-likelihood of <math alttext="p(\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m10"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ; bold_italic_θ )</annotation></semantics></math>, its maximization offers a
principled compromise
between the maximum likelihood objective and computational tractability.
Interestingly, the derivation above shows that its tightness depends on the KL
divergence between the approximate posterior <math alttext="q(\bm{z}\mid\bm{x};\,\bm{\eta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m11"><semantics><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\bm{z}\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math> and the
true posterior <math alttext="p(\bm{z}\mid\bm{x};\,\bm{\theta})" class="ltx_Math" display="inline" id="S1.SS4.SSSx1.p2.m12"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{z}\mid\bm{x};\,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_z ∣ bold_italic_x ; bold_italic_θ )</annotation></semantics></math>, implying that the more accurate our
approximate posterior is, the more maximization of the ELBO leads to
maximization of the underlying objective of interest, the likelihood of the
data. Thus, the VAE objective is:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{\theta},\bm{\eta}}\,\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right]." class="ltx_math_unparsed" display="block" id="S1.E14.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi></mrow></munder><mo lspace="0.337em">⁡</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{\theta},\bm{\eta}}\,\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right].</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ , bold_italic_η end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) end_ARG start_ARG italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η ) end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By our derivation, maximization of this objective corresponds to a tradeoff
between maximizing the likelihood function of the data and minimizing the KL
divergence between the approximate and true posterior, which is a highly
sensible objective given the VAE modeling assumptions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS4.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">VAE Training as Probabilistic Autoencoding</h4>
<div class="ltx_para" id="S1.SS4.SSSx2.p1">
<p class="ltx_p">There is a general methodology for maximizing the ELBO objective in
<a class="ltx_ref" href="#S1.E14" title="In Probabilistic Perspective on Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">5.1.14</span></a> using stochastic gradient descent and various tractable
Monte Carlo estimators for the associated gradients. However, the task is
simpler under the Gaussian assumptions we have made above. In this case, the
ELBO reads</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx69">
<tbody id="S1.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right]" class="ltx_math_unparsed" display="inline" id="S1.Ex10.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi></mrow></munder><mo lspace="0.337em">⁡</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>∣</mo><mrow><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log\frac{p(\bm{x},\bm{z};\,\bm{\theta})}{q(\bm{z}\mid\bm{x};\,\bm{\eta})}\right]</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ , bold_italic_η end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) end_ARG start_ARG italic_q ( bold_italic_z ∣ bold_italic_x ; bold_italic_η ) end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S1.Ex11.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x},\bm{z};\,\bm{\theta})\right]+\frac{d\log(2\pi e)}{2}+\frac{1}{2}\langle\mathbb{E}_{\bm{x}}[f_{2}(\bm{x})],\mathbf{1}\rangle\right\}" class="ltx_math_unparsed" display="inline" id="S1.Ex11.m2"><semantics><mrow><munder><mi>max</mi><mrow><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi></mrow></munder><mo lspace="0.170em">⁡</mo><mrow><mo>{</mo><mrow><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mn>2</mn></mfrac></mstyle><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">⟨</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x},\bm{z};\,\bm{\theta})\right]+\frac{d\log(2\pi e)}{2}+\frac{1}{2}\langle\mathbb{E}_{\bm{x}}[f_{2}(\bm{x})],\mathbf{1}\rangle\right\}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ , bold_italic_η end_POSTSUBSCRIPT { blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x , bold_italic_z ; bold_italic_θ ) ] + divide start_ARG italic_d roman_log ( 2 italic_π italic_e ) end_ARG start_ARG 2 end_ARG + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⟨ blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) ] , bold_1 ⟩ }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\equiv" class="ltx_Math" display="inline" id="S1.Ex12.m1"><semantics><mo>≡</mo><annotation encoding="application/x-tex">\displaystyle\equiv</annotation><annotation encoding="application/x-llamapun">≡</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})+\log p(\bm{z})\right]+\frac{1}{2}\langle\mathbb{E}_{\bm{x}}[f_{2}(\bm{x})],\mathbf{1}\rangle\right\}" class="ltx_math_unparsed" display="inline" id="S1.Ex12.m2"><semantics><mrow><munder><mi>max</mi><mrow><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi></mrow></munder><mo lspace="0.170em">⁡</mo><mrow><mo>{</mo><mrow><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">⟨</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})+\log p(\bm{z})\right]+\frac{1}{2}\langle\mathbb{E}_{\bm{x}}[f_{2}(\bm{x})],\mathbf{1}\rangle\right\}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ , bold_italic_η end_POSTSUBSCRIPT { blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ ) + roman_log italic_p ( bold_italic_z ) ] + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⟨ blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) ] , bold_1 ⟩ }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\equiv" class="ltx_Math" display="inline" id="S1.Ex13.m1"><semantics><mo>≡</mo><annotation encoding="application/x-tex">\displaystyle\equiv</annotation><annotation encoding="application/x-llamapun">≡</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{2\mathbb{E}_{\bm{x}}\left[\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})\right]+\langle f_{2}(\bm{x})-e^{f_{2}(\bm{x})},\mathbf{1}\rangle-\|f_{1}(\bm{x})\|_{2}^{2}\right]\right\}," class="ltx_math_unparsed" display="inline" id="S1.Ex13.m2"><semantics><mrow><mrow><munder><mi>max</mi><mrow><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi></mrow></munder><mo lspace="0.170em">⁡</mo><mrow><mo>{</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mo stretchy="false">⟨</mo><mrow><mrow><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><msup><mi>e</mi><mrow><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><mo>,</mo><mn>𝟏</mn><mo stretchy="false">⟩</mo></mrow></mrow><mo>−</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>]</mo></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta},\bm{\eta}}\,\left\{2\mathbb{E}_{\bm{x}}\left[\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})\right]+\langle f_{2}(\bm{x})-e^{f_{2}(\bm{x})},\mathbf{1}\rangle-\|f_{1}(\bm{x})\|_{2}^{2}\right]\right\},</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ , bold_italic_η end_POSTSUBSCRIPT { 2 blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ ) ] + ⟨ italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) - italic_e start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUPERSCRIPT , bold_1 ⟩ - ∥ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">following <a class="ltx_ref" href="A2.html#Thmtheorem1" title="Theorem B.1. ‣ B.1 Differential Entropy of Low-Dimensional Distributions ‣ Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">B.1</span></a> and the Gaussian entropy calculation done in <a class="ltx_ref" href="A2.html" title="Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>, where <math alttext="\equiv" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m1"><semantics><mo>≡</mo><annotation encoding="application/x-tex">\equiv</annotation><annotation encoding="application/x-llamapun">≡</annotation></semantics></math>
denotes equivalence of optimization objectives (in each case, we remove some
additive constants that do not change the optimization problem’s solutions).
The remaining term corresponds to the “autoencoding” part of the ELBO
objective: intuitively, it seeks to maximize the likelihood of data generated by
the decoder when the latents <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> are distribited according to the approximate
posterior (generated by the encoder applied to a data sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>), which is
a probabilistic form of autoencoding.
To see this more directly, consider the special case where the approximate
posterior concentrates on its mean <math alttext="f_{1}(\bm{x})" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m4"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{1}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math>, for every <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>: this
is the limit
where its log-variance <math alttext="f_{2,i}(\bm{x})\to-\infty" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m6"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mn>2</mn><mo>,</mo><mi>i</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">f_{2,i}(\bm{x})\to-\infty</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT 2 , italic_i end_POSTSUBSCRIPT ( bold_italic_x ) → - ∞</annotation></semantics></math> for each coordinate
<math alttext="i=1,\dots,d" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m7"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">i=1,\dots,d</annotation><annotation encoding="application/x-llamapun">italic_i = 1 , … , italic_d</annotation></semantics></math>.
For simplicity, assume moreover that <math alttext="g_{2}(\bm{x})=\mathbf{1}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m8"><semantics><mrow><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">g_{2}(\bm{x})=\mathbf{1}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) = bold_1</annotation></semantics></math>, giving the decoder
constant unit variance on each coordinate.
Then the loss term in question converges to</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx70">
<tbody id="S1.Ex14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})\right]" class="ltx_math_unparsed" display="inline" id="S1.Ex14.m1"><semantics><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mi>𝒛</mi><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{\bm{x}}\mathbb{E}_{\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})}\left[\log p(\bm{x}\mid\bm{z};\,\bm{\theta})\right]</annotation><annotation encoding="application/x-llamapun">blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η ) end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ∣ bold_italic_z ; bold_italic_θ ) ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\to\mathbb{E}_{\bm{x}}\left[\log p(\bm{x}\mid f_{1}(\bm{x});\,\bm{\theta})\right]" class="ltx_Math" display="inline" id="S1.Ex14.m2"><semantics><mrow><mi></mi><mo stretchy="false">→</mo><mrow><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mrow><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.337em">;</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\to\mathbb{E}_{\bm{x}}\left[\log p(\bm{x}\mid f_{1}(\bm{x});\,\bm{\theta})\right]</annotation><annotation encoding="application/x-llamapun">→ blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ∣ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) ; bold_italic_θ ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S1.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\equiv-\frac{1}{2}\mathbb{E}_{\bm{x}}\left[\|\bm{x}-g_{1}\circ f_{1}(\bm{x})\|_{2}^{2}\right]." class="ltx_Math" display="inline" id="S1.Ex15.m1"><semantics><mrow><mrow><mi></mi><mo>≡</mo><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mi>𝒙</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mrow><msub><mi>g</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">∘</mo><msub><mi>f</mi><mn>1</mn></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\equiv-\frac{1}{2}\mathbb{E}_{\bm{x}}\left[\|\bm{x}-g_{1}\circ f_{1}(\bm{x})\|_{2}^{2}\right].</annotation><annotation encoding="application/x-llamapun">≡ - divide start_ARG 1 end_ARG start_ARG 2 end_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ ∥ bold_italic_x - italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∘ italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">So with a highly confident encoder, which deterministically maps each sample
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to a single point <math alttext="f_{1}(\bm{x})" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m10"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{1}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x )</annotation></semantics></math> in <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS4.SSSx2.p1.m11"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>-space, and an isotropic decoder,
the “autoencoding” part of the ELBO maximization problem indeed becomes
a classical autoencoding objective!<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In the general case where <math alttext="g_{2}" class="ltx_Math" display="inline" id="footnote5.m1"><semantics><msub><mi>g</mi><mn>2</mn></msub><annotation encoding="application/x-tex">g_{2}</annotation><annotation encoding="application/x-llamapun">italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is
not fixed as <math alttext="\mathbf{1}" class="ltx_Math" display="inline" id="footnote5.m2"><semantics><mn>𝟏</mn><annotation encoding="application/x-tex">\mathbf{1}</annotation><annotation encoding="application/x-llamapun">bold_1</annotation></semantics></math>, the reader can verify that the autoencoding term in
the ELBO objective converges to a <span class="ltx_text ltx_font_italic">regularized</span> classical autoencoding
objective.</span></span></span>
At the same time, note that this special case is actually excluded by the
extra terms in <a class="ltx_ref" href="#S1.Ex13" title="In VAE Training as Probabilistic Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">5.1.15</span></a>—these effectively correspond to
regularization terms that discourage the encoder from collapsing.
The general ELBO loss (<a class="ltx_ref" href="#S1.E14" title="In Probabilistic Perspective on Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equations</span> <span class="ltx_text ltx_ref_tag">5.1.14</span></a> and <a class="ltx_ref" href="#S1.Ex13" title="Equation 5.1.15 ‣ VAE Training as Probabilistic Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1.15</span></a>) is
therefore a strict generalization of the classical autoencoding reconstruction
objective (<a class="ltx_ref" href="#S1.E12" title="In A Classical Attempt via a Two-Layer Network. ‣ 5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">5.1.12</span></a>), both in terms of its data fidelity term
and in terms of the inclusion of regularization terms.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS4.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Training a VAE</h4>
<div class="ltx_para" id="S1.SS4.SSSx3.p1">
<p class="ltx_p">VAEs are typically trained by alternating stochastic gradient ascent on the ELBO
objective (<a class="ltx_ref" href="#S1.Ex13" title="In VAE Training as Probabilistic Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">5.1.15</span></a>), given individual samples
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from the
true data distribution and from <math alttext="\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})" class="ltx_math_unparsed" display="inline" id="S1.SS4.SSSx3.p1.m2"><semantics><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math>. In
particular, it is standard to collect and train on many independently-generated
samples <math alttext="\bm{z}^{i}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m3"><semantics><msup><mi>𝒛</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{z}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> for each sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. To take gradients of
<a class="ltx_ref" href="#S1.Ex13" title="In VAE Training as Probabilistic Autoencoding ‣ 5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">5.1.15</span></a> with respect to the encoder parameters <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m5"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math>,
one makes use of the so-called reparameterization trick by writing <math alttext="\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})" class="ltx_math_unparsed" display="inline" id="S1.SS4.SSSx3.p1.m6"><semantics><mrow><mi>𝒛</mi><mo>∼</mo><mi>q</mi><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>𝒙</mi><mo rspace="0.337em">;</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\sim q(\,\cdot\,\mid\bm{x};\,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∼ italic_q ( ⋅ ∣ bold_italic_x ; bold_italic_η )</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S1.Ex16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}=_{\mathrm{d}}f_{1}(\bm{x})+\operatorname{diag}(e^{\tfrac{1}{2}f_{2}(\bm{x})})\bm{g},\quad\bm{g}\sim\mathcal{N}(0,\bm{I})," class="ltx_Math" display="block" id="S1.Ex16.m1"><semantics><mrow><mrow><mrow><mi>𝒛</mi><msub><mo>=</mo><mi mathvariant="normal">d</mi></msub><mrow><mrow><msub><mi>f</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mi>diag</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><mstyle scriptlevel="-1"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msub><mi>f</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{z}=_{\mathrm{d}}f_{1}(\bm{x})+\operatorname{diag}(e^{\tfrac{1}{2}f_{2}(\bm{x})})\bm{g},\quad\bm{g}\sim\mathcal{N}(0,\bm{I}),</annotation><annotation encoding="application/x-llamapun">bold_italic_z = start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) + roman_diag ( italic_e start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUPERSCRIPT ) bold_italic_g , bold_italic_g ∼ caligraphic_N ( 0 , bold_italic_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="=_{\mathrm{d}}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m7"><semantics><msub><mo>=</mo><mi mathvariant="normal">d</mi></msub><annotation encoding="application/x-tex">=_{\mathrm{d}}</annotation><annotation encoding="application/x-llamapun">= start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT</annotation></semantics></math> denotes equality in distribution. Then one can simply
take many independent standard Gaussian samples <math alttext="\bm{g}^{i}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m8"><semantics><msup><mi>𝒈</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{g}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_g start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> for each data sample
<math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, generate the corresponding samples from the approximate posterior
<math alttext="\bm{z}^{i}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m10"><semantics><msup><mi>𝒛</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{z}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>, and compute gradients with respect to <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S1.SS4.SSSx3.p1.m11"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math> using automatic
differentiation without any issues.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.2 </span>Learning Self-Consistent Representations</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">In earlier chapters, we have studied methods that would allow us to learn a low-dimensional distribution via (lossy) compression. As we have mentioned in <a class="ltx_ref" href="Ch1.html" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">1</span></a> and demonstrated in the previous chapters, the progresses made in machine intelligence largely rely on finding computationally feasible and efficient solutions to realize the desired compression, not only computable or tractable in theory, but also scalable in practice:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{{computable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}." class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mtext class="ltx_mathvariant_bold">computable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">tractable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">scalable</mtext></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{{computable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}.</annotation><annotation encoding="application/x-llamapun">computable ⟹ tractable ⟹ scalable .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It is even fair to say that the tremendous advancement in machine intelligence made in the past decade or so is largely attributed to the development of scalable models and methods, say by training deep networks via back propagation. Large models with billions of parameters trained with trillions of data points on tens of thousands of powerful GPUs have demonstrated super-human capabilities in memorizing existing knowledge. This has led many to believe that the “intelligence” of such models will continue to improve as their scale continues to go up.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">While we celebrate the engineering marvels of such large man-made machine learning systems, we also must admit that, compared to intelligence in nature, this approach to improve machine intelligence is unnecessarily resource demanding. Natural intelligent beings, including animals and humans, simply cannot afford such a brute force solution to learn because they must operate with a very limited budget in energy, space and time, subject to many strict physical constraints.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">Firstly, there is strong scientific evidence that our brain does not conduct global end-to-end back propagation to improve or correct its predictions. Instead, it was long known in neuroscience that our brain corrects errors with local closed-loop feedback, such as predictive coding. This was the scientific basis that had inspired Norbert Wiener to develop the theory of feedback control and the Cybernetics program back in the 1940s.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p">Secondly, we saw in the previous sections that in order to learn a consistent representation, one needs to learn a bi-directional autoencoding:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>g</mi></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It requires to enforce the observed input data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.p4.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> to be close by some measure of similarity <math alttext="d(\bm{X},\hat{\bm{X}})" class="ltx_Math" display="inline" id="S2.p4.m3"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}})</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG )</annotation></semantics></math>. In nature, animals or humans rarely have direct assess to the ground truth <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p4.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. For example, we never have direct access to true 3D shape, distance, or dynamics of objects in a scene. Yet, we have all learned to estimate and predict them very accurately and efficiently. Hence, an outstanding question is how we can learn the true distribution of the <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p4.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, even though we cannot directly compare our estimate <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.p4.m6"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> with the ground truth <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p4.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. As we will see in this chapter, the answer also lies in the closed-loop feedback, as well as the low-dimensionality of the data distribution.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p">As we know from the previous chapter, in order to ensure a representation is consistent, we need to compare the generated <math alttext="\hat{\bm{X}}\sim p(\hat{\bm{x}})" class="ltx_Math" display="inline" id="S2.p5.m1"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}\sim p(\hat{\bm{x}})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG ∼ italic_p ( over^ start_ARG bold_italic_x end_ARG )</annotation></semantics></math> and the original <math alttext="\bm{X}\sim p(\bm{x})" class="ltx_Math" display="inline" id="S2.p5.m2"><semantics><mrow><mi>𝑿</mi><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\sim p(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∼ italic_p ( bold_italic_x )</annotation></semantics></math>, at least in distribution. Even when we do have access to <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p5.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.p5.m4"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>, technically, computing and minimizing distance of two distributions can be problematic, especially when the support of the distributions is low-dimensional. The KL-divergence introduced in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a> is not even well-defined between two distributions that do not have overlap supports.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p">As an early attempt to alleviate the above difficulty in computing and minimizing the distance between two (low-dimensional) distributions, people had suggested to learn the generator/decoder <math alttext="g" class="ltx_Math" display="inline" id="S2.p6.m1"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> via discriminative approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx266" title="">Tu07</a>]</cite>. This line of thought has led to the idea of <span class="ltx_text ltx_font_italic">Generative Adversarial Nets (GAN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx94" title="">GPM+14a</a>]</cite>. It introduces a discriminator <math alttext="d" class="ltx_Math" display="inline" id="S2.p6.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, usually modeled by a deep network, to discern differences between the generated samples <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.p6.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> and the real ones <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p6.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\eta)\hskip 5.69054pt}\hat{\bm{X}},\,\bm{X}\xrightarrow{\hskip 5.69054ptd(\bm{x},\theta)\hskip 5.69054pt}\{\mathbf{0},\mathbf{1}\}." class="ltx_Math" display="block" id="S2.E3.m1"><semantics><mrow><mrow><mrow><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo rspace="0.337em">,</mo><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><mo stretchy="false">{</mo><mn>𝟎</mn><mo>,</mo><mn>𝟏</mn><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\eta)\hskip 5.69054pt}\hat{\bm{X}},\,\bm{X}\xrightarrow{\hskip 5.69054ptd(\bm{x},\theta)\hskip 5.69054pt}\{\mathbf{0},\mathbf{1}\}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , italic_η ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_d ( bold_italic_x , italic_θ ) end_OVERACCENT → end_ARROW { bold_0 , bold_1 } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It is suggested that we may attempt to align the distributions between <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.p6.m5"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> and <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p6.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> via a <span class="ltx_text ltx_font_italic">Stackelberg game</span> between the generator <math alttext="g" class="ltx_Math" display="inline" id="S2.p6.m7"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> and the discriminator <math alttext="d" class="ltx_Math" display="inline" id="S2.p6.m8"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\theta}\min_{\eta}\mathbb{E}_{p({\bm{x}})}\big{[}\log d(\bm{x},\theta)\big{]}+\mathbb{E}_{p({\bm{z}})}\big{[}1-\log d(\underbrace{g(\bm{z},\eta)}_{\hat{\bm{x}}\,\sim\,p_{g}},\theta)\big{]}." class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>θ</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mi>η</mi></munder><mo lspace="0.167em">⁡</mo><msub><mi>𝔼</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>𝔼</mi><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mn>1</mn><mo>−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><munder><munder accentunder="true"><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⏟</mo></munder><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo lspace="0.448em" rspace="0.448em">∼</mo><msub><mi>p</mi><mi>g</mi></msub></mrow></munder><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\theta}\min_{\eta}\mathbb{E}_{p({\bm{x}})}\big{[}\log d(\bm{x},\theta)\big{]}+\mathbb{E}_{p({\bm{z}})}\big{[}1-\log d(\underbrace{g(\bm{z},\eta)}_{\hat{\bm{x}}\,\sim\,p_{g}},\theta)\big{]}.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_p ( bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_d ( bold_italic_x , italic_θ ) ] + blackboard_E start_POSTSUBSCRIPT italic_p ( bold_italic_z ) end_POSTSUBSCRIPT [ 1 - roman_log italic_d ( under⏟ start_ARG italic_g ( bold_italic_z , italic_η ) end_ARG start_POSTSUBSCRIPT over^ start_ARG bold_italic_x end_ARG ∼ italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_θ ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, the discriminator <math alttext="d" class="ltx_Math" display="inline" id="S2.p6.m9"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> is trying to minimize the cross entropy between the true samples <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p6.m10"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the generated ones <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.p6.m11"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> while the generator <math alttext="g" class="ltx_Math" display="inline" id="S2.p6.m12"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> is trying to do the opposite.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p">One may show that finding an equilibrium for the above Stackelberg game is equivalent to minimizing the <span class="ltx_text ltx_font_italic">Jensen-Shannon divergence</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}_{JS}(p(\bm{x}),p_{g}(\hat{\bm{x}}))=\mathcal{D}_{KL}\big{(}p\|(p+p_{g})/{2}\big{)}+\mathcal{D}_{KL}\big{(}p_{g}\|(p+p_{g})/{2}\big{)}." class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>J</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>p</mi><mi>g</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>p</mi><mo>∥</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>p</mi><mi>g</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><msub><mi>p</mi><mi>g</mi></msub><mo>∥</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo>+</mo><msub><mi>p</mi><mi>g</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{JS}(p(\bm{x}),p_{g}(\hat{\bm{x}}))=\mathcal{D}_{KL}\big{(}p\|(p+p_{g})/{2}\big{)}+\mathcal{D}_{KL}\big{(}p_{g}\|(p+p_{g})/{2}\big{)}.</annotation><annotation encoding="application/x-llamapun">caligraphic_D start_POSTSUBSCRIPT italic_J italic_S end_POSTSUBSCRIPT ( italic_p ( bold_italic_x ) , italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_x end_ARG ) ) = caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_p ∥ ( italic_p + italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) / 2 ) + caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ∥ ( italic_p + italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) / 2 ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that, compared to the KL-divergence, the JS-divergence is well-defined even if the supports of the two distributions are non-overlapping. However, JS-divergence does not have a closed-form expression even between two Gaussians, whereas KL-divergence does. In addition, since the data distributions are low-dimensional, the JS-divergence can be highly ill-conditioned to optimize.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx6" title="">ACB17</a>]</cite>.</span></span></span> This may explain why many additional heuristics are typically used in many subsequent variants of GAN.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p">So, instead, it has also been suggested to replace JS-divergence with the earth mover (EM) distance or the Wasserstein distance.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Roughly speaking, for distributions with potentially non-overlapping low-dimensional supports, the JS-divergence behaves like the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="footnote7.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm, and the EM-distance behaves like the <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="footnote7.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm.</span></span></span> However, both the JS-divergence and Wasserstein distance can only be approximately computed between two general distributions.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>For instance, the Wasserstein distance requires one to compute the maximal difference between expectations of the two distributions over all 1-Lipschitz functions.</span></span></span> Furthermore, neither the JS-divergence nor the Wasserstein distance have closed-form formulae, even for the Gaussian distributions.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>The (<math alttext="\ell^{1}" class="ltx_Math" display="inline" id="footnote9.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm) Wasserstein distance can be bounded by the (<math alttext="\ell^{2}" class="ltx_Math" display="inline" id="footnote9.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm) Wasserstein distance which has a closed-form <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx69" title="">DDS22</a>]</cite>. However, as is well-known in high-dimensional geometry, <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="footnote9.m3"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm and <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="footnote9.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> norm deviate significantly in terms of their geometric and statistical properties as the dimension becomes high <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx294" title="">WM21</a>]</cite>. The bound can become very loose.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p">If it is difficult to compare distributions of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.p9.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.p9.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>, would it possible to compare in the learned feature <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.p9.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> with its image <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.p9.m4"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> under the encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.p9.m5"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\hat{\bm{Z}}?" class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>g</mi></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><mi>f</mi></mrow></mover><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">?</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\bm{Z}\xrightarrow{\hskip 2.84526pt\mathcal{D}=g\hskip 2.84526pt}\hat{\bm{X}}\xrightarrow{\hskip 2.84526pt\mathcal{E}=f\hskip 2.84526pt}\hat{\bm{Z}}?</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_Z end_ARG ?</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This leads to the notion of <span class="ltx_text ltx_font_italic">self-consistent representation</span>.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 5.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Self-Consistent Representation)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition2.p1">
<p class="ltx_p">Given data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we call an <span class="ltx_text ltx_font_italic">self-consistent representation</span> to be a pair of functions <math alttext="(f\colon\mathcal{X}\to\mathcal{Z},g\colon\mathcal{Z}\to\mathcal{X})" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>,</mo><mi>g</mi></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo stretchy="false">→</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f\colon\mathcal{X}\to\mathcal{Z},g\colon\mathcal{Z}\to\mathcal{X})</annotation><annotation encoding="application/x-llamapun">( italic_f : caligraphic_X → caligraphic_Z , italic_g : caligraphic_Z → caligraphic_X )</annotation></semantics></math>, such that the <span class="ltx_text ltx_font_italic">features</span> <math alttext="\bm{Z}=f(\bm{X})" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m3"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X )</annotation></semantics></math> are compact and structured, and the <span class="ltx_text ltx_font_italic">autoencoding features</span> <math alttext="\hat{\bm{Z}}\doteq f\circ g(\bm{Z})" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo>≐</mo><mrow><mrow><mi>f</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{Z}}\doteq f\circ g(\bm{Z})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG ≐ italic_f ∘ italic_g ( bold_italic_Z )</annotation></semantics></math> is <span class="ltx_text ltx_font_italic">close</span> to <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>.</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p">We say that it is <span class="ltx_text ltx_font_italic">sample-wise</span> self-consistent if <math alttext="\bm{Z}\approx\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m1"><semantics><mrow><mi>𝒁</mi><mo>≈</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{Z}\approx\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z ≈ over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> in a certain norm with high probability.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">We say that the representation is <span class="ltx_text ltx_font_italic">distributionally self-consistent</span> if <math alttext="\operatorname{Law}(\bm{Z})\approx\operatorname{Law}(\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m1"><semantics><mrow><mrow><mi>Law</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mi>Law</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Law}(\bm{Z})\approx\operatorname{Law}(\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_Law ( bold_italic_Z ) ≈ roman_Law ( over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2.1 </span>Closed-Loop Transcription via Stackelberg Games</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">How do we try to ensure a learned representation is self-consistent? As usual, let us assume <math alttext="\bm{X}=\cup_{k=1}^{K}\bm{X}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo rspace="0em">=</mo><mrow><msubsup><mo lspace="0em">∪</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><msub><mi>𝑿</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\cup_{k=1}^{K}\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = ∪ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> with each subset of samples <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.m2"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> belonging to a low-dimensional submanifold: <math alttext="\bm{X}_{k}\subset\mathcal{M}_{k},k=1,\ldots,K" class="ltx_Math" display="inline" id="S2.SS1.p1.m3"><semantics><mrow><mrow><msub><mi>𝑿</mi><mi>k</mi></msub><mo>⊂</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>k</mi></msub></mrow><mo>,</mo><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{X}_{k}\subset\mathcal{M}_{k},k=1,\ldots,K</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊂ caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k = 1 , … , italic_K</annotation></semantics></math>. Following the notation in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we use a matrix <math alttext="\bm{\Pi}_{k}(i,i)=1" class="ltx_Math" display="inline" id="S2.SS1.p1.m4"><semantics><mrow><mrow><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\bm{\Pi}_{k}(i,i)=1</annotation><annotation encoding="application/x-llamapun">bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_i , italic_i ) = 1</annotation></semantics></math> to denote the membership of sample <math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.p1.m5"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math> belonging to class <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.p1.m6"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> and <math alttext="\bm{\Pi}_{k}(i,j)=0" class="ltx_Math" display="inline" id="S2.SS1.p1.m7"><semantics><mrow><mrow><msub><mi>𝚷</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\bm{\Pi}_{k}(i,j)=0</annotation><annotation encoding="application/x-llamapun">bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_i , italic_j ) = 0</annotation></semantics></math> otherwise. One seeks a continuous mapping <math alttext="f(\cdot,\bm{\theta}):\bm{x}\mapsto\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.p1.m8"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta}):\bm{x}\mapsto\bm{z}</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ ) : bold_italic_x ↦ bold_italic_z</annotation></semantics></math> from <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.p1.m9"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to an optimal representation <math alttext="\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]\subset\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S2.SS1.p1.m10"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>⊂</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{N}]\subset\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z}," class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒁</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z},</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW bold_italic_Z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which maximizes the following coding rate-reduction (MCR<sup class="ltx_sup">2</sup>) objective:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\max_{\bm{Z}}\;\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})&amp;\doteq\underbrace{\frac{1}{2}\log\det\Big{(}\bm{I}+{\alpha}\bm{Z}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}(\bm{Z})}\;-\;\underbrace{\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\log\det\Big{(}\bm{I}+{\alpha_{k}}\bm{Z}\bm{\Pi}^{j}\bm{Z}^{\top}\Big{)}}_{R^{c}_{\epsilon}(\bm{Z}\mid\bm{\Pi})},\end{split}" class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mtable columnspacing="0pt" displaystyle="true"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mrow><munder><mi>max</mi><mi>𝒁</mi></munder><mo lspace="0.447em">⁡</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>≐</mo><mrow><munder><munder accentunder="true"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></munder><mo rspace="0.335em">−</mo><munder><munder accentunder="true"><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mfrac><msub><mi>γ</mi><mi>k</mi></msub><mn>2</mn></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">det</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mi>𝑰</mi><mo>+</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝚷</mi><mi>j</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>⊤</mo></msup></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow></mrow><mo>⏟</mo></munder><mrow><msubsup><mi>R</mi><mi>ϵ</mi><mi>c</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo>∣</mo><mi>𝚷</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></munder></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}\max_{\bm{Z}}\;\Delta R_{\epsilon}(\bm{Z}\mid\bm{\Pi})&amp;\doteq\underbrace{\frac{1}{2}\log\det\Big{(}\bm{I}+{\alpha}\bm{Z}\bm{Z}^{\top}\Big{)}}_{R_{\epsilon}(\bm{Z})}\;-\;\underbrace{\sum_{k=1}^{K}\frac{\gamma_{k}}{2}\log\det\Big{(}\bm{I}+{\alpha_{k}}\bm{Z}\bm{\Pi}^{j}\bm{Z}^{\top}\Big{)}}_{R^{c}_{\epsilon}(\bm{Z}\mid\bm{\Pi})},\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π ) end_CELL start_CELL ≐ under⏟ start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG start_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) end_POSTSUBSCRIPT - under⏟ start_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z bold_Π start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) end_ARG start_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ∣ bold_Π ) end_POSTSUBSCRIPT , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\alpha={d}/({N\epsilon^{2}})" class="ltx_Math" display="inline" id="S2.SS1.p1.m12"><semantics><mrow><mi>α</mi><mo>=</mo><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha={d}/({N\epsilon^{2}})</annotation><annotation encoding="application/x-llamapun">italic_α = italic_d / ( italic_N italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, <math alttext="\alpha_{k}=d/({\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}})" class="ltx_Math" display="inline" id="S2.SS1.p1.m13"><semantics><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\alpha_{k}=d/({\mathrm{tr}(\bm{\Pi}_{k})\epsilon^{2}})</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d / ( roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, <math alttext="\gamma_{k}={\mathrm{tr}(\bm{\Pi}_{k})}/{N}" class="ltx_Math" display="inline" id="S2.SS1.p1.m14"><semantics><mrow><msub><mi>γ</mi><mi>k</mi></msub><mo>=</mo><mrow><mrow><mi>tr</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝚷</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">\gamma_{k}={\mathrm{tr}(\bm{\Pi}_{k})}/{N}</annotation><annotation encoding="application/x-llamapun">italic_γ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_tr ( bold_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) / italic_N</annotation></semantics></math> for each <math alttext="k=1,\dots,K" class="ltx_Math" display="inline" id="S2.SS1.p1.m15"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow><annotation encoding="application/x-tex">k=1,\dots,K</annotation><annotation encoding="application/x-llamapun">italic_k = 1 , … , italic_K</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">One issue with learning such a one-sided mapping (<a class="ltx_ref" href="#S2.E7" title="Equation 5.2.7 ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.7</span></a>) via maximizing the above objective (<a class="ltx_ref" href="#S2.E8" title="Equation 5.2.8 ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.8</span></a>) is that it tends to expand the dimension of the learned subspace for features in each class<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>if the dimension of the feature space <math alttext="d" class="ltx_Math" display="inline" id="footnote10.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> is too high, maximizing the rate reduction may over-estimate the dimension of each class. Hence, to learn a good representation, one needs to pre-select a proper dimension for the feature space, as achieved in the experiments in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx313" title="">YCY+20</a>]</cite>. In fact the same “model selection” problem persists even in the simplest single-subspace case, which is the classic PCA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx125" title="">Jol86</a>]</cite>. To our best knowledge, selecting the correct number of principal components in a heterogeneous noisy situation remains an active research topic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx109" title="">HSD20</a>]</cite>.</span></span></span>, as we have seen in <a class="ltx_ref" href="Ch3.html#S4" title="3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a> of <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>. To verify whether the learned features are consistent, meaning neither over-estimating nor under-estimating the intrinsic data structure, we may consider learning a decoder <math alttext="g(\cdot,\eta):\bm{z}\mapsto\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p2.m1"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>η</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒛</mi><mo stretchy="false">↦</mo><mi>𝒙</mi></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\eta):\bm{z}\mapsto\bm{x}</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , italic_η ) : bold_italic_z ↦ bold_italic_x</annotation></semantics></math> from the representation <math alttext="\bm{Z}=f(\bm{X},\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.p2.m2"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=f(\bm{X},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = italic_f ( bold_italic_X , bold_italic_θ )</annotation></semantics></math> back to the data space <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>: <math alttext="\hat{\bm{X}}=g(\bm{Z},\bm{\eta})" class="ltx_Math" display="inline" id="S2.SS1.p2.m4"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}=g(\bm{Z},\bm{\eta})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG = italic_g ( bold_italic_Z , bold_italic_η )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}}," class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}},</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_η ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and check how close <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.p2.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.p2.m6"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> are. This forms an autoencoding which is what we have studied extensively in the previous <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></span>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Measuring distance in the feature space.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">However, as we have discussed above, if we do not have the option to compute the distance between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>, we are left with the option of comparing their corresponding features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}=f(\hat{\bm{X}},\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{Z}}=f(\hat{\bm{X}},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG = italic_f ( over^ start_ARG bold_italic_X end_ARG , bold_italic_θ )</annotation></semantics></math>. Notice that under the MCR<sup class="ltx_sup">2</sup> objective, the distributions of the resulting <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m6"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> or <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m7"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> tend to be piecewise linear so their distance can be computed more easily. More precisely, since the features of each class, <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m8"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m9"><semantics><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\hat{\bm{Z}}_{k}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, are close to be a low-dimensional subspace/Gaussian, their “distance” can be measured by the rate reduction, with (<a class="ltx_ref" href="#S2.E8" title="Equation 5.2.8 ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.8</span></a>) restricted to two sets of equal size:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}\doteq R_{\epsilon}\big{(}\bm{Z}_{k}\cup\hat{\bm{Z}}_{k}\big{)}-\frac{1}{2}\big{(}R_{\epsilon}\big{(}\bm{Z}_{k})+R_{\epsilon}\big{(}\hat{\bm{Z}}_{k})\big{)}." class="ltx_Math" display="block" id="S2.E10.m1"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>∪</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}\doteq R_{\epsilon}\big{(}\bm{Z}_{k}\cup\hat{\bm{Z}}_{k}\big{)}-\frac{1}{2}\big{(}R_{\epsilon}\big{(}\bm{Z}_{k})+R_{\epsilon}\big{(}\hat{\bm{Z}}_{k})\big{)}.</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ≐ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∪ over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The overall distance between <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m10"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m11"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(\bm{Z},\hat{\bm{Z}})\doteq\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},f(g(\bm{Z}_{k},\bm{\eta}),\bm{\theta})\big{)}." class="ltx_Math" display="block" id="S2.E11.m1"><semantics><mrow><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d(\bm{Z},\hat{\bm{Z}})\doteq\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},f(g(\bm{Z}_{k},\bm{\eta}),\bm{\theta})\big{)}.</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) ≐ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_f ( italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_η ) , bold_italic_θ ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.11)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p2">
<p class="ltx_p">If we are interested in discerning <span class="ltx_text ltx_font_italic">any</span> differences in the distributions of the original data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and their decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>, we may view the feature encoder <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> as a “discriminator” to <span class="ltx_text ltx_font_italic">magnify</span> any difference between all pairs of <math alttext="\bm{X}_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m4"><semantics><msub><mi>𝑿</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{X}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\hat{\bm{X}}_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m5"><semantics><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\hat{\bm{X}}_{k}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, by simply maximizing the distance <math alttext="d(\bm{X},\hat{\bm{X}})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m6"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}})</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{f}d(\bm{Z},\hat{\bm{Z}})=\max_{\bm{\theta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\max_{\bm{\theta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),f(\hat{\bm{X}}_{k},\bm{\theta})\big{)}." class="ltx_Math" display="block" id="S2.E12.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>f</mi></munder><mo lspace="0.167em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{f}d(\bm{Z},\hat{\bm{Z}})=\max_{\bm{\theta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\max_{\bm{\theta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),f(\hat{\bm{X}}_{k},\bm{\theta})\big{)}.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) = roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ ) , italic_f ( over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, a “distance” between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m8"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> can be measured as the maximally achievable rate reduction between all pairs of classes in these two sets. In a way, this measures how well or badly the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m9"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> aligns with the original data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m10"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>—hence measuring the goodness of “injectivity” of the encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m11"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>. Notice that such a discriminative measure is consistent with the idea of GAN (<a class="ltx_ref" href="#S2.E3" title="Equation 5.2.3 ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>) that tries to separate <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m12"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m13"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> into two classes, measured by the cross-entropy.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p3">
<p class="ltx_p">Nevertheless, here the discriminative encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p3.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> naturally generalizes to cases when the data distributions are multi-class and multi-modal, and the discriminativeness is measured with a more refined measure—the rate reduction—instead of the typical two-class loss (e.g., cross entropy) used in GANs. That is, we may view the encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p3.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> as a generalized discriminator that replaces the binary classifier <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p3.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> in (<a class="ltx_ref" href="#S2.E3" title="Equation 5.2.3 ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}},\,\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\{\hat{\bm{Z}},\bm{Z}\}." class="ltx_Math" display="block" id="S2.E13.m1"><semantics><mrow><mrow><mrow><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo rspace="0.337em">,</mo><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><mo stretchy="false">{</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo>,</mo><mi>𝒁</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}},\,\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\{\hat{\bm{Z}},\bm{Z}\}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_η ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW { over^ start_ARG bold_italic_Z end_ARG , bold_italic_Z } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The overall pipeline can be illustrated by the following “closed-loop” diagram:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\ \hat{\bm{Z}}," class="ltx_Math" display="block" id="S2.E14.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\ \hat{\bm{Z}},</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_η ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_Z end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the overall model has parameters: <math alttext="\bm{\Theta}=\{\bm{\theta},\bm{\eta}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p3.m4"><semantics><mrow><mi>𝚯</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{\Theta}=\{\bm{\theta},\bm{\eta}\}</annotation><annotation encoding="application/x-llamapun">bold_Θ = { bold_italic_θ , bold_italic_η }</annotation></semantics></math>. <a class="ltx_ref" href="#F6" title="In Measuring distance in the feature space. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.6</span></a> shows the overall process.</p>
</div>
<figure class="ltx_figure" id="F6"><img alt="Figure 5.6 : A Closed-loop Transcription. The encoder f f italic_f has dual roles: it learns a representation 𝒛 \bm{z} bold_italic_z for the data 𝒙 \bm{x} bold_italic_x via maximizing the rate reduction of 𝒛 \bm{z} bold_italic_z and it is also a “feedback sensor” for any discrepancy between the data 𝒙 \bm{x} bold_italic_x and the decoded 𝒙 ^ \hat{\bm{x}} over^ start_ARG bold_italic_x end_ARG . The decoder g g italic_g also has dual roles: it is a “controller” that corrects the discrepancy between 𝒙 \bm{x} bold_italic_x and 𝒙 ^ \hat{\bm{x}} over^ start_ARG bold_italic_x end_ARG and it also aims to minimize the overall coding rate for the learned representation." class="ltx_graphics" id="F6.g1" src="chapters/chapter5/figs/diagrams_redu_gan_2.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">A Closed-loop Transcription.<span class="ltx_text ltx_font_medium"> The encoder <math alttext="f" class="ltx_Math" display="inline" id="F6.m10"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> has dual roles: it learns a representation <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F6.m11"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> for the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m12"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> via maximizing the rate reduction of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F6.m13"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and it is also a “feedback sensor” for any discrepancy between the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m14"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="F6.m15"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>. The decoder <math alttext="g" class="ltx_Math" display="inline" id="F6.m16"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> also has dual roles: it is a “controller” that corrects the discrepancy between <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F6.m17"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="F6.m18"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> and it also aims to minimize the overall coding rate for the learned representation.</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Encoder and decoder as a two-player game.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p">Obviously, to ensure the learned auto-encoding to be self-consistent, the main goal of the decoder <math alttext="g(\cdot,\bm{\eta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m1"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , bold_italic_η )</annotation></semantics></math> is to <span class="ltx_text ltx_font_italic">minimize</span> the distance between <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m3"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>. That is, to learn <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m4"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>, we want to minimize the distance <math alttext="d(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m5"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{g}d(\bm{Z},\hat{\bm{Z}})\doteq\min_{\eta}\sum_{k=1}^{K}\Delta R\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\min_{\bm{\eta}}\sum_{k=1}^{K}\Delta R\big{(}\bm{Z}_{k},f(g(\bm{Z}_{k},\bm{\eta}),\bm{\theta})\big{)}," class="ltx_Math" display="block" id="S2.E15.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>g</mi></munder><mo lspace="0.167em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><munder><mi>min</mi><mi>η</mi></munder><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{g}d(\bm{Z},\hat{\bm{Z}})\doteq\min_{\eta}\sum_{k=1}^{K}\Delta R\big{(}\bm{Z}_{k},\hat{\bm{Z}}_{k}\big{)}=\min_{\bm{\eta}}\sum_{k=1}^{K}\Delta R\big{(}\bm{Z}_{k},f(g(\bm{Z}_{k},\bm{\eta}),\bm{\theta})\big{)},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) ≐ roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_f ( italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_η ) , bold_italic_θ ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{Z}_{k}=f(\bm{X}_{k},\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m6"><semantics><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{k}=f(\bm{X}_{k},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ )</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}_{k}=f(\hat{\bm{X}}_{k},\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m7"><semantics><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{Z}}_{k}=f(\hat{\bm{X}}_{k},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_f ( over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ )</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 5.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample2.p1">
<p class="ltx_p">One may wonder why we need the mapping <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="Thmexample2.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> to function as a discriminator between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmexample2.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> by maximizing <math alttext="\max_{\bm{\theta}}\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta}),f(\hat{\bm{X}},\bm{\theta})\big{)}" class="ltx_Math" display="inline" id="Thmexample2.p1.m4"><semantics><mrow><mrow><msub><mi>max</mi><mi>𝜽</mi></msub><mo lspace="0.167em">⁡</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\bm{\theta}}\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta}),f(\hat{\bm{X}},\bm{\theta})\big{)}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X , bold_italic_θ ) , italic_f ( over^ start_ARG bold_italic_X end_ARG , bold_italic_θ ) )</annotation></semantics></math>. <a class="ltx_ref" href="#F7" title="In Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.7</span></a> gives a simple illustration: there might be many decoders <math alttext="g" class="ltx_Math" display="inline" id="Thmexample2.p1.m5"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> such that <math alttext="f\circ g" class="ltx_Math" display="inline" id="Thmexample2.p1.m6"><semantics><mrow><mi>f</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>g</mi></mrow><annotation encoding="application/x-tex">f\circ g</annotation><annotation encoding="application/x-llamapun">italic_f ∘ italic_g</annotation></semantics></math> is an identity (Id) mapping. <math alttext="f\circ g(\bm{z})=\bm{z}" class="ltx_Math" display="inline" id="Thmexample2.p1.m7"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">f\circ g(\bm{z})=\bm{z}</annotation><annotation encoding="application/x-llamapun">italic_f ∘ italic_g ( bold_italic_z ) = bold_italic_z</annotation></semantics></math> for all <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmexample2.p1.m8"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> in the subspace <math alttext="S_{\bm{z}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m9"><semantics><msub><mi>S</mi><mi>𝒛</mi></msub><annotation encoding="application/x-tex">S_{\bm{z}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT bold_italic_z end_POSTSUBSCRIPT</annotation></semantics></math> in the feature space. However, <math alttext="g\circ f" class="ltx_Math" display="inline" id="Thmexample2.p1.m10"><semantics><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><annotation encoding="application/x-tex">g\circ f</annotation><annotation encoding="application/x-llamapun">italic_g ∘ italic_f</annotation></semantics></math> is not necessarily an auto-encoding map for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in the original distribution <math alttext="S_{\bm{x}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m12"><semantics><msub><mi>S</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">S_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> (here for simplicity drawn as a subspace). That is, <math alttext="g\circ f(S_{\bm{x}})\not\subset S_{\bm{x}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m13"><semantics><mrow><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>𝒙</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>⊄</mo><msub><mi>S</mi><mi>𝒙</mi></msub></mrow><annotation encoding="application/x-tex">g\circ f(S_{\bm{x}})\not\subset S_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_g ∘ italic_f ( italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ) ⊄ italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math>, let alone <math alttext="g\circ f(S_{\bm{x}})=S_{\bm{x}}" class="ltx_Math" display="inline" id="Thmexample2.p1.m14"><semantics><mrow><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>𝒙</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mi>S</mi><mi>𝒙</mi></msub></mrow><annotation encoding="application/x-tex">g\circ f(S_{\bm{x}})=S_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_g ∘ italic_f ( italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ) = italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> or <math alttext="g\circ f(\bm{x})=\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m15"><semantics><mrow><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>𝒙</mi></mrow><annotation encoding="application/x-tex">g\circ f(\bm{x})=\bm{x}</annotation><annotation encoding="application/x-llamapun">italic_g ∘ italic_f ( bold_italic_x ) = bold_italic_x</annotation></semantics></math>. One should expect, without careful control of the image of <math alttext="g" class="ltx_Math" display="inline" id="Thmexample2.p1.m16"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>, with high probability, this would be the case, especially when the support of the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m17"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is extremely low-dimensional in the original high-dimensional data space.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample2.p1.m18"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<figure class="ltx_figure" id="F7"><img alt="Figure 5.7 : Embeddings of low-dim submanifolds in a high-dim space. S 𝒙 S_{\bm{x}} italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT (blue) is the submanifold for the original data 𝒙 \bm{x} bold_italic_x ; S 𝒛 S_{\bm{z}} italic_S start_POSTSUBSCRIPT bold_italic_z end_POSTSUBSCRIPT (red) is the image of S 𝒙 S_{\bm{x}} italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT under the mapping f f italic_f , representing the learned feature 𝒛 \bm{z} bold_italic_z ; and the green curve is the image of the feature 𝒛 \bm{z} bold_italic_z under the decoding mapping g g italic_g ." class="ltx_graphics" id="F7.g1" src="chapters/chapter5/figs/diagrams_fig1.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Embeddings of low-dim submanifolds in a high-dim space.<span class="ltx_text ltx_font_medium"> <math alttext="S_{\bm{x}}" class="ltx_Math" display="inline" id="F7.m9"><semantics><msub><mi>S</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">S_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> (blue) is the submanifold for the original data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F7.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>; <math alttext="S_{\bm{z}}" class="ltx_Math" display="inline" id="F7.m11"><semantics><msub><mi>S</mi><mi>𝒛</mi></msub><annotation encoding="application/x-tex">S_{\bm{z}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT bold_italic_z end_POSTSUBSCRIPT</annotation></semantics></math> (red) is the image of <math alttext="S_{\bm{x}}" class="ltx_Math" display="inline" id="F7.m12"><semantics><msub><mi>S</mi><mi>𝒙</mi></msub><annotation encoding="application/x-tex">S_{\bm{x}}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT</annotation></semantics></math> under the mapping <math alttext="f" class="ltx_Math" display="inline" id="F7.m13"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, representing the learned feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F7.m14"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>; and the green curve is the image of the feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F7.m15"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> under the decoding mapping <math alttext="g" class="ltx_Math" display="inline" id="F7.m16"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>. </span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p2">
<p class="ltx_p">Comparing the contractive and contrastive nature of (<a class="ltx_ref" href="#S2.E15" title="Equation 5.2.15 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.15</span></a>) and (<a class="ltx_ref" href="#S2.E12" title="Equation 5.2.12 ‣ Measuring distance in the feature space. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.12</span></a>) on the same distance measure, we see the roles of the encoder <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> and the decoder <math alttext="g(\cdot,\bm{\eta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , bold_italic_η )</annotation></semantics></math> naturally as “<span class="ltx_text ltx_font_bold">a two-player game</span>”: <span class="ltx_text ltx_font_italic">while the encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> tries to magnify the difference between the original data and their transcribed data, the decoder <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m4"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> aims to minimize the difference.</span> Now for convenience, let us define the “closed-loop encoding” function:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{x},\bm{\theta},\bm{\eta})\doteq f\big{(}g\big{(}f(\bm{x},\bm{\theta}),\bm{\eta}\big{)},\bm{\theta}\big{)}:\;\bm{x}\mapsto\hat{\bm{z}}." class="ltx_Math" display="block" id="S2.E16.m1"><semantics><mrow><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝜼</mi><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>,</mo><mi>𝜽</mi><mo maxsize="120%" minsize="120%" rspace="0.278em">)</mo></mrow></mrow></mrow><mo rspace="0.558em">:</mo><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{x},\bm{\theta},\bm{\eta})\doteq f\big{(}g\big{(}f(\bm{x},\bm{\theta}),\bm{\eta}\big{)},\bm{\theta}\big{)}:\;\bm{x}\mapsto\hat{\bm{z}}.</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_x , bold_italic_θ , bold_italic_η ) ≐ italic_f ( italic_g ( italic_f ( bold_italic_x , bold_italic_θ ) , bold_italic_η ) , bold_italic_θ ) : bold_italic_x ↦ over^ start_ARG bold_italic_z end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.16)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p3">
<p class="ltx_p">Ideally, we want this function to be very close to <math alttext="f(\bm{x},\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p3.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\bm{x},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_x , bold_italic_θ )</annotation></semantics></math> or at least the distributions of their images should be close. With this notation, combining (<a class="ltx_ref" href="#S2.E15" title="Equation 5.2.15 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.15</span></a>) and (<a class="ltx_ref" href="#S2.E12" title="Equation 5.2.12 ‣ Measuring distance in the feature space. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.12</span></a>), a closed-loop notion of “distance” between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p3.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> can be computed as <span class="ltx_text ltx_font_italic">an equilibrium point</span> to the following Stackelberg game (cf <a class="ltx_ref" href="A1.html#S3" title="A.3 Game Theory and Minimax Optimization ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.3</span></a>) for the same utility in terms of rate reduction:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}(\bm{X},\hat{\bm{X}})\doteq\max_{\bm{\theta}}\min_{\bm{\eta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),h(\bm{X}_{k},\bm{\theta},\bm{\eta})\big{)}." class="ltx_Math" display="block" id="S2.E17.m1"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><munder><mi>min</mi><mi>𝜼</mi></munder></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}(\bm{X},\hat{\bm{X}})\doteq\max_{\bm{\theta}}\min_{\bm{\eta}}\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),h(\bm{X}_{k},\bm{\theta},\bm{\eta})\big{)}.</annotation><annotation encoding="application/x-llamapun">caligraphic_D ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ≐ roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ ) , italic_h ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ , bold_italic_η ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.17)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p4">
<p class="ltx_p">Notice that this only measures the difference between (features of) the original data and its transcribed version. It does not measure how good the representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> (or <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m2"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>) is for the multiple classes within <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> (or <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m4"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>). To this end, we may combine the above distance with the original MCR<sup class="ltx_sup">2</sup>-type objectives (<a class="ltx_ref" href="#S2.E8" title="Equation 5.2.8 ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.8</span></a>): namely, the rate reduction <math alttext="\Delta R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m6"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math> and <math alttext="\Delta R_{\epsilon}(\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m7"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> for the learned LDR <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m8"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m9"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m10"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> for the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m11"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>. Notice that although the encoder <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m12"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> tries to <span class="ltx_text ltx_font_italic">maximize</span> the multi-class rate reduction of the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m13"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m14"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, the decoder <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m15"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> should <span class="ltx_text ltx_font_italic">minimize</span> the rate reduction of the multi-class features <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m16"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> of the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m17"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>. That is, the decoder <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p4.m18"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> tries to use a minimal coding rate needed to achieve a good decoding quality.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p5">
<p class="ltx_p">Hence, the overall “multi-class” Stackelberg game for learning the closed-loop transcription, named CTRL-Multi, is</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx71">
<tbody id="S2.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta}}\min_{\bm{\eta}}\mathcal{T}_{\bm{X}}(\bm{\theta},\bm{\eta})" class="ltx_Math" display="inline" id="S2.E18.m1"><semantics><mrow><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo lspace="0.167em">⁡</mo><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>𝑿</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta}}\min_{\bm{\eta}}\mathcal{T}_{\bm{X}}(\bm{\theta},\bm{\eta})</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.18)</span></td>
</tr></tbody>
<tbody id="S2.E19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\doteq" class="ltx_Math" display="inline" id="S2.E19.m1"><semantics><mo>≐</mo><annotation encoding="application/x-tex">\displaystyle\doteq</annotation><annotation encoding="application/x-llamapun">≐</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\underbrace{\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta})\big{)}}_{\text{Expansive encode}}+\underbrace{\Delta R_{\epsilon}\big{(}h(\bm{X},\bm{\theta},\bm{\eta})\big{)}}_{\text{Compressive decode}}+\sum_{k=1}^{K}\underbrace{\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),h(\bm{X}_{k},\bm{\theta},\bm{\eta})\big{)}}_{\text{Contrastive encode \&amp; Contractive decode}}" class="ltx_Math" display="inline" id="S2.E19.m2"><semantics><mrow><munder><munder accentunder="true"><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>Expansive encode</mtext></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>Compressive decode</mtext></munder><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><munder><munder accentunder="true"><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mi>k</mi></msub><mo>,</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>⏟</mo></munder><mtext>Contrastive encode &amp; Contractive decode</mtext></munder></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\underbrace{\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta})\big{)}}_{\text{Expansive encode}}+\underbrace{\Delta R_{\epsilon}\big{(}h(\bm{X},\bm{\theta},\bm{\eta})\big{)}}_{\text{Compressive decode}}+\sum_{k=1}^{K}\underbrace{\Delta R_{\epsilon}\big{(}f(\bm{X}_{k},\bm{\theta}),h(\bm{X}_{k},\bm{\theta},\bm{\eta})\big{)}}_{\text{Contrastive encode \&amp; Contractive decode}}</annotation><annotation encoding="application/x-llamapun">under⏟ start_ARG roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X , bold_italic_θ ) ) end_ARG start_POSTSUBSCRIPT Expansive encode end_POSTSUBSCRIPT + under⏟ start_ARG roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_h ( bold_italic_X , bold_italic_θ , bold_italic_η ) ) end_ARG start_POSTSUBSCRIPT Compressive decode end_POSTSUBSCRIPT + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT under⏟ start_ARG roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ ) , italic_h ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_θ , bold_italic_η ) ) end_ARG start_POSTSUBSCRIPT Contrastive encode &amp; Contractive decode end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.19)</span></td>
</tr></tbody>
<tbody id="S2.E20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S2.E20.m1"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\Delta R_{\epsilon}\big{(}\bm{Z}(\bm{\theta})\big{)}+\Delta R_{\epsilon}\big{(}\hat{\bm{Z}}(\bm{\theta},\bm{\eta})\big{)}+\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k}(\bm{\theta}),\hat{\bm{Z}}_{k}(\bm{\theta},\bm{\eta})\big{)}," class="ltx_Math" display="inline" id="S2.E20.m2"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\Delta R_{\epsilon}\big{(}\bm{Z}(\bm{\theta})\big{)}+\Delta R_{\epsilon}\big{(}\hat{\bm{Z}}(\bm{\theta},\bm{\eta})\big{)}+\sum_{k=1}^{K}\Delta R_{\epsilon}\big{(}\bm{Z}_{k}(\bm{\theta}),\hat{\bm{Z}}_{k}(\bm{\theta},\bm{\eta})\big{)},</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ( bold_italic_θ ) ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ( bold_italic_θ , bold_italic_η ) ) + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_θ ) , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">subject to certain constraints (upper or lower bounds) on the first term and the second term.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p6">
<p class="ltx_p">Notice that, without the terms associated with the generative part <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p6.m1"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> or with all such terms fixed as constant, the above objective is precisely the original MCR<sup class="ltx_sup">2</sup> objective introduced in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>. In an unsupervised setting, if we view each sample (and its augmentations) as its own class, the above formulation remains exactly the same. The number of classes <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p6.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> is simply the number of independent samples. In addition, notice that the above game’s objective function depends only on (features of) the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p6.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, hence one can learn the encoder and decoder (parameters) without the need for sampling or matching any additional distribution (as typically needed in GANs or VAEs).</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p7">
<p class="ltx_p">As a special case, if <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> only has one class, the Stackelberg game reduces to a special “two-class” or “binary” form,<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>as the first two rate reduction terms automatically become zero</span></span></span> named CTRL-Binary,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{\theta}}\min_{\bm{\eta}}\mathcal{T}^{b}_{\bm{X}}(\bm{\theta},\bm{\eta})\doteq\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta}),h(\bm{X},\bm{\theta},\bm{\eta})\big{)}=\Delta R_{\epsilon}\big{(}\bm{Z}(\bm{\theta}),\hat{\bm{Z}}(\bm{\theta},\bm{\eta})\big{)}," class="ltx_Math" display="block" id="S2.E21.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo lspace="0.167em">⁡</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>𝑿</mi><mi>b</mi></msubsup></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{\theta}}\min_{\bm{\eta}}\mathcal{T}^{b}_{\bm{X}}(\bm{\theta},\bm{\eta})\doteq\Delta R_{\epsilon}\big{(}f(\bm{X},\bm{\theta}),h(\bm{X},\bm{\theta},\bm{\eta})\big{)}=\Delta R_{\epsilon}\big{(}\bm{Z}(\bm{\theta}),\hat{\bm{Z}}(\bm{\theta},\bm{\eta})\big{)},</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT caligraphic_T start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) ≐ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X , bold_italic_θ ) , italic_h ( bold_italic_X , bold_italic_θ , bold_italic_η ) ) = roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ( bold_italic_θ ) , over^ start_ARG bold_italic_Z end_ARG ( bold_italic_θ , bold_italic_η ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> by viewing <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m5"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> as two classes <math alttext="\{\bm{0},\bm{1}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p7.m6"><semantics><mrow><mo stretchy="false">{</mo><mn>𝟎</mn><mo>,</mo><mn>𝟏</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{0},\bm{1}\}</annotation><annotation encoding="application/x-llamapun">{ bold_0 , bold_1 }</annotation></semantics></math>. Notice that this binary case resembles the formulation of the original GAN (<a class="ltx_ref" href="#S2.E3" title="Equation 5.2.3 ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>). Instead of using cross entropy, our formulation adopts a more refined rate-reduction measure, which has been shown to promote diversity in the learned representation in <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p8">
<p class="ltx_p">Sometimes, even when <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p8.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> contains multiple classes/modes, one could still view all classes together as one class. Then, the above binary objective is to align the union distribution of all classes with their decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p8.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>. This is typically a simpler task to achieve than the multi-class one (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>), since it does not require learning of a more refined multi-class CTRL for the data, as we will later see in experiments. Notice that one good characteristic of the above formulation is that <span class="ltx_text ltx_font_italic">all quantities in the objectives are measured in terms of rate reduction for the learned features</span> (assuming features eventually become subspace Gaussians).</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p9">
<p class="ltx_p">One may notice that the above learning framework draws inspiration from closed-loop error correction widely practiced in feedback control systems. The closed-loop mechanism is used to form an overall feedback system between the two encoding and decoding networks for correcting any “error” in the distributions between the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m2"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>. Using terminology from control theory, one may view the encoding network <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> as a “sensor” for error feedback while the decoding network <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m4"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> as a “controller” for error correction. However, notice that here the “target” for control is not a scalar nor a finite dimensional vector, but a continuous distribution—in order for the distribution of <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m5"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> to match that of the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. This is in general a control problem in an infinite dimensional space. The space of possible diffeomorphisms of submanifolds that <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m7"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> tries to model is infinite-dimensional <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx154" title="">Lee02</a>]</cite>. Ideally, we hope when the sensor <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m8"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and the controller <math alttext="g" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m9"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> are optimal, the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> becomes a “fixed point” for the closed loop while the distribution of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p9.m11"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> reaches a compact linear discriminative representation. Hence, the minimax programs (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>) and (<a class="ltx_ref" href="#S2.E21" title="Equation 5.2.21 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.21</span></a>) can also be interpreted as games between an error-feedback sensor and an error-reducing controller.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p10">
<p class="ltx_p">The remaining question is whether the above framework can indeed learn a good (autoencoding) presentation of a given dataset? Before we give some formal theoretical justification (in the next subsection), we present some empirical results.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Visualizing correlation of features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and decoded features <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.m2"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p">We visualize the cosine similarity between <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m2"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> learned from the multi-class objective (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>) on MNIST, CIFAR-10 and ImageNet (10 classes), which indicates how close <math alttext="\hat{\bm{z}}=f\circ g(\bm{z})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m3"><semantics><mrow><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>f</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>g</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{z}}=f\circ g(\bm{z})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG = italic_f ∘ italic_g ( bold_italic_z )</annotation></semantics></math> is from <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. Results in <a class="ltx_ref" href="#F8" title="In Visualizing correlation of features 𝒁 and decoded features 𝒁̂. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.8</span></a> show that <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m6"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> are aligned very well within each class. The block-diagonal patterns for MNIST are sharper than those for CIFAR-10 and ImageNet, as images in CIFAR-10 and ImageNet have more diverse visual appearances.</p>
</div>
<figure class="ltx_figure" id="F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F8.sf1"><img alt="(a) MNIST" class="ltx_graphics ltx_img_square" height="502" id="F8.sf1.g1" src="chapters/chapter5/figs/MNIST_MNIST_ZZhat_heatmap_epo200.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">MNIST</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F8.sf2"><img alt="(a) MNIST" class="ltx_graphics ltx_img_square" height="496" id="F8.sf2.g1" src="chapters/chapter5/figs/cifar_heatmat_cifar.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR-10</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F8.sf3"><img alt="(a) MNIST" class="ltx_graphics ltx_img_square" height="501" id="F8.sf3.g1" src="chapters/chapter5/figs/Imagenet_heatmat_epoch200000.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">ImageNet</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.8</span>: </span><span class="ltx_text" style="font-size:90%;">Visualizing the alignment between <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="F8.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="F8.m5"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>: <math alttext="|\bm{Z}^{\top}\hat{\bm{Z}}|" class="ltx_Math" display="inline" id="F8.m6"><semantics><mrow><mo stretchy="false">|</mo><mrow><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\bm{Z}^{\top}\hat{\bm{Z}}|</annotation><annotation encoding="application/x-llamapun">| bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over^ start_ARG bold_italic_Z end_ARG |</annotation></semantics></math> and in the feature space for (<span class="ltx_text ltx_font_bold">a</span>) MNIST, (<span class="ltx_text ltx_font_bold">b</span>) CIFAR-10, and (<span class="ltx_text ltx_font_bold">c</span>) ImageNet-10-Class.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Visualizing auto-encoding of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the decoded <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p">We compare some representative <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> on MNIST, CIFAR-10 and ImageNet (10 classes) to verify how close <math alttext="\hat{\bm{x}}=g\circ f(\bm{x})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m3"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=g\circ f(\bm{x})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = italic_g ∘ italic_f ( bold_italic_x )</annotation></semantics></math> is to <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. The results are shown in <a class="ltx_ref" href="#F9" title="In Visualizing auto-encoding of the data 𝑿 and the decoded 𝑿̂. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.9</span></a>, and visualizations are created from training samples. Visually, the auto-encoded <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m5"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> faithfully captures major visual features from its respective training sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, especially the pose, shape, and layout. For the simpler dataset such as MNIST, auto-encoded images are almost identical to the original.</p>
</div>
<figure class="ltx_figure" id="F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf1"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="598" id="F9.sf1.g1" src="chapters/chapter5/figs/MNIST_MNIST_train_images_epoch200.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">MNIST <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F9.sf1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf2"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="603" id="F9.sf2.g1" src="chapters/chapter5/figs/cifar_input.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR-10 <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F9.sf2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf3"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="598" id="F9.sf3.g1" src="chapters/chapter5/figs/Imagenet_input.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">ImageNet <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F9.sf3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf4"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="598" id="F9.sf4.g1" src="chapters/chapter5/figs/MNIST_MNIST_train_recon_images_epoch200_multi.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">MNIST <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F9.sf4.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf5"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="605" id="F9.sf5.g1" src="chapters/chapter5/figs/cifar_reconstruct.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(e)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR-10 <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F9.sf5.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="F9.sf6"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="598" id="F9.sf6.g1" src="chapters/chapter5/figs/Imagenet_reconstruct.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(f)</span> </span><span class="ltx_text" style="font-size:90%;">ImageNet <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F9.sf6.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.9</span>: </span><span class="ltx_text" style="font-size:90%;">Visualizing the auto-encoding property of the learned closed-loop transcription (<math alttext="\bm{x}\approx\hat{\bm{x}}=g\circ f(\bm{x})" class="ltx_Math" display="inline" id="F9.m2"><semantics><mrow><mi>𝒙</mi><mo>≈</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\approx\hat{\bm{x}}=g\circ f(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_x ≈ over^ start_ARG bold_italic_x end_ARG = italic_g ∘ italic_f ( bold_italic_x )</annotation></semantics></math>) on MNIST, CIFAR-10, and ImageNet (zoom in for better visualization).</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2.2 </span>A Mixture of Low-Dimensional Gaussians</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">In the above, we have argued that it is possible to formulate the problem of learning a data distribution as a closed-loop autoencoding problem. We also saw empirically that such a scheme seems to work. The remaining question is when and why such a scheme should works. It is difficult to answer this question for the most general cases with arbitrary data distributions. Nevertheless, as usual, let us see if we can arrive at a rigorous justification for the ideal case when the data distribution is a mixture of low-dimensional subspaces or low-rank Gaussians. A clear characterization and understanding of this important special case would shed light on the more general cases.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>As most distributions with low-dimensional structures can be well-approximated by this family of distributions.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">To this end, let us first suppose that <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is distributed according to a mixture of low-dimensional Gaussians, and the label (i.e., subspace assignment) for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is given by <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S2.SS2.p2.m3"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation><annotation encoding="application/x-llamapun">bold_italic_y</annotation></semantics></math>. Then, let us set up a minimax optimization problem to learn the data distribution, say through learning an encoding of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p2.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> into representations <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS2.p2.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> which are supported on a mixture of <span class="ltx_text ltx_font_italic">orthogonal</span> subspaces, and a decoding of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS2.p2.m6"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> back into <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p2.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. Then, the representation we want to achieve is maximized by the earlier-discussed version of the information gain, i.e., <math alttext="\Delta R_{\epsilon}(\bm{Z})=R_{\epsilon}(\bm{Z})-\sum_{k=1}^{K}R_{\epsilon}(\bm{Z}_{k})" class="ltx_Math" display="inline" id="S2.SS2.p2.m8"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.055em">−</mo><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z})=R_{\epsilon}(\bm{Z})-\sum_{k=1}^{K}R_{\epsilon}(\bm{Z}_{k})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) = italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math>, which enforces that the representation <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.SS2.p2.m9"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> of each class <math alttext="k" class="ltx_Math" display="inline" id="S2.SS2.p2.m10"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> spans a subspace which is orthogonal to the supporting subspaces of other classes. The way to measure the consistency of the decoding is, as before, given by <math alttext="\sum_{k=1}^{K}\Delta R_{\epsilon}(\bm{Z}_{k},\hat{\bm{Z}}_{k})" class="ltx_Math" display="inline" id="S2.SS2.p2.m11"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>k</mi></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sum_{k=1}^{K}\Delta R_{\epsilon}(\bm{Z}_{k},\hat{\bm{Z}}_{k})</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math>, which enforces that the representation <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.SS2.p2.m12"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and its autoencoding <math alttext="\hat{\bm{Z}}_{k}" class="ltx_Math" display="inline" id="S2.SS2.p2.m13"><semantics><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\hat{\bm{Z}}_{k}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for each class <math alttext="k" class="ltx_Math" display="inline" id="S2.SS2.p2.m14"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> span the same subspace. Thus, we can set up a simplified Stackelberg game:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{\theta}}\min_{\bm{\eta}}\left\{\Delta R_{\epsilon}(\bm{Z}(\bm{\theta}))+\sum_{k=1}^{K}\Delta R_{\epsilon}(\bm{Z}_{k}(\bm{\theta}),\hat{\bm{Z}}_{k}(\bm{\theta},\bm{\eta}))\right\}" class="ltx_Math" display="block" id="S2.E22.m1"><semantics><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\max_{\bm{\theta}}\min_{\bm{\eta}}\left\{\Delta R_{\epsilon}(\bm{Z}(\bm{\theta}))+\sum_{k=1}^{K}\Delta R_{\epsilon}(\bm{Z}_{k}(\bm{\theta}),\hat{\bm{Z}}_{k}(\bm{\theta},\bm{\eta}))\right\}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT { roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ( bold_italic_θ ) ) + ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_θ ) , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) ) }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that this is a simpler setup than what’s used in practice—there’s no <math alttext="\Delta R_{\epsilon}(\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S2.SS2.p2.m15"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> term, for instance, and we work in the supervised setting with class labels (although the techniques used to prove the following result are easy to extend to unsupervised formulations). Also, the consistency of the representations is only measured in a distribution-wise sense via <math alttext="\Delta R_{\epsilon}" class="ltx_Math" display="inline" id="S2.SS2.p2.m16"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT</annotation></semantics></math> (though this may be substituted with a sample-wise distance metric such as the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="S2.SS2.p2.m17"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\ell_{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> norm if desired, and equivalent conclusions may be drawn, <span class="ltx_text ltx_font_italic">mutatis mutandis</span>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">Under mild conditions, in order to realize the desired encoder and decoder which realize <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S2.SS2.p3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> from a data source <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> that is already distributed according to a mixture of correlated low-dimensional Gaussians, we only require a linear encoder and decoder to disentangle and whiten the Gaussians. We then study this setting in the case where <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p3.m3"><semantics><mi>𝜽</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation><annotation encoding="application/x-llamapun">bold_italic_θ</annotation></semantics></math> and <math alttext="\bm{\eta}" class="ltx_Math" display="inline" id="S2.SS2.p3.m4"><semantics><mi>𝜼</mi><annotation encoding="application/x-tex">\bm{\eta}</annotation><annotation encoding="application/x-llamapun">bold_italic_η</annotation></semantics></math> parameterize matrices whose operator norm is constrained.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p">We want to understand what kinds of optima are learned in this setting. One suitable solution concept for this kind of game, where the decoder’s optimal solution is defined solely with respect to the encoder (and not, in particular, with respect to some other intrinsic property of the decoder), is a <span class="ltx_text ltx_font_italic">Stackelberg equilibrium</span> where the decoder follows the encoder. Namely, at such an equilibrium, the decoder should optimize its objective; meanwhile the encoder should optimize its objective, given that whatever it picks, the decoder will pick an optimal response (which may affect the encoder objective). In game-theoretic terms, it is like the decoder goes <span class="ltx_text ltx_font_italic">second</span>: it chooses its weights after the encoder, and both the encoder and decoder attempt to optimize their objective in light of this. It is computationally tractable to learn sequential equilibria via gradient methods via <span class="ltx_text ltx_font_italic">alternating optimization</span>, where each side uses different learning rates. Detailed exposition of sequential equilibria is beyond the scope of this book and we provide more technical details in <a class="ltx_ref" href="A1.html#S3" title="A.3 Game Theory and Minimax Optimization ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.3</span></a>. In this setting, we have the following result:</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 5.1</span></span><span class="ltx_text ltx_font_bold"> </span>(<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx206" title="">PPC+23</a>]</cite>, Abridged)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m1"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is distributed on a mixture of subspaces. Under certain realistic yet technical conditions, it holds that all sequential equilibria of (<a class="ltx_ref" href="#S2.E22" title="Equation 5.2.22 ‣ 5.2.2 A Mixture of Low-Dimensional Gaussians ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.22</span></a>) obey:</span></p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The </span><math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.m1"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> lie on orthogonal subspaces and are isotropic on those subspaces, i.e., maximizing the information gain.</span></p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The autoencoding is self-consistent, i.e., the subspaces spanned by </span><math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m1"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> and </span><math alttext="\hat{\bm{Z}}_{k}" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m2"><semantics><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mi>k</mi></msub><annotation encoding="application/x-tex">\hat{\bm{Z}}_{k}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic"> are the same for all </span><math alttext="k" class="ltx_Math" display="inline" id="S2.I2.i2.p1.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_italic">.</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p">This notion of self-consistency is the most one can expect if there are only geometric assumptions on the data, i.e., there are no statistical assumptions. If we assume that the columns of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.p5.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is drawn from a low-rank Gaussian mixture model, then analogous versions of this theorem certify that <math alttext="\bm{Z}_{k}" class="ltx_Math" display="inline" id="S2.SS2.p5.m2"><semantics><msub><mi>𝒁</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are also low-rank Gaussians whose covariance is isotropic, for instance. Essentially, this result validates, via the simple case of Gaussian mixtures on subspaces, that minimax games to optimize the information gain and self-consistency may achieve optimal solutions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.3 </span>Continuous Learning Self-Consistent Representations</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3.1 </span>Class-wise Incremental Learning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained offline, with uniformly sampled data from all classes simultaneously. It has been known that when an (open-loop) network is updated to learn new classes without data from the old ones, previously learned knowledge will fall victim to the problem of <span class="ltx_text ltx_font_italic">catastrophic forgetting</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx180" title="">MC89</a>]</cite>. This is known in neuroscience as the stability-plasticity dilemma: the challenge of ensuring that a neural system can learn from a new environment while retaining essential knowledge from previous ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx98" title="">Gro87</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">In contrast, natural neural systems (e.g. animal brains) do not seem to suffer from such catastrophic forgetting at all. They are capable of developing new memory of new objects while retaining memory of previously learned objects. This ability, for either natural or artificial neural systems, is often referred to as <span class="ltx_text ltx_font_italic">incremental learning, continual learning, sequential learning</span>, or <span class="ltx_text ltx_font_italic">life-long learning</span>  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx4" title="">AR20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">While many recent works have highlighted how artificial neural systems can be trained in more flexible ways, the strongest existing efforts toward answering the stability-plasticity dilemma for artificial neural networks typically require either storing raw exemplars <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx226" title="">RKS+17</a>, <a class="ltx_ref" href="bib.html#bibx44" title="">CRE+19</a>]</cite> or providing external mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx138" title="">KPR+17</a>]</cite>. Raw exemplars, particularly in the case of high-dimensional inputs like images, are costly and difficult to scale, while external mechanisms—which typically include secondary networks and representation spaces for generative replay, incremental allocation of network resources, network duplication, or explicit isolation of used and unused parts of the network—require heuristics and incur hidden costs.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p">Here we are interested in an incremental learning setting that is similar to nature. It counters these existing practices with two key qualities.</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">The first is that it should be <em class="ltx_emph ltx_font_italic">memory-based.</em> When learning new classes, no raw exemplars of old classes are available to train the network together with new data. This implies that one has to rely on a compact and thus structured “memory” learned for old classes, such as incrementally learned generative representations of the old classes, as well as the associated encoding and decoding mappings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx134" title="">KK18</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">The second is that it should be <em class="ltx_emph ltx_font_italic">self-contained.</em> Incremental learning takes place in a single neural system with a fixed capacity, and in a common representation space. The ability to minimize forgetting is implied by optimizing an overall learning objective, without external networks, architectural modifications, or resource allocation mechanisms.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p">The incoherent linear structures for features of different classes closely resemble how objects are encoded in different areas of the inferotemporal cortex of animal brains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx43" title="">CT17</a>, <a class="ltx_ref" href="bib.html#bibx16" title="">BSM+20</a>]</cite>. The closed-loop transcription <math alttext="\bm{X}\rightarrow\bm{Z}\rightarrow\hat{\bm{X}}\rightarrow\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S3.SS1.p5.m1"><semantics><mrow><mi>𝑿</mi><mo stretchy="false">→</mo><mi>𝒁</mi><mo stretchy="false">→</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">→</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{X}\rightarrow\bm{Z}\rightarrow\hat{\bm{X}}\rightarrow\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">bold_italic_X → bold_italic_Z → over^ start_ARG bold_italic_X end_ARG → over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> also resembles popularly hypothesized mechanisms for memory formation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx272" title="">VST+20</a>, <a class="ltx_ref" href="bib.html#bibx128" title="">JT20</a>]</cite>. This leads to a question: since memory in the brains is formed in an incremental fashion, can the above closed-loop transcription framework also support incremental learning?</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">LDR memory sampling and replay.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p">The simple linear <span class="ltx_text ltx_font_italic">structures</span> of LDR make it uniquely suited for incremental learning: the distribution of features <math alttext="\bm{Z}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m1"><semantics><msub><mi>𝒁</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> of each previously learned class can be explicitly and concisely represented by a principal subspace <math alttext="\mathcal{S}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{j}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in the feature space. To preserve the memory of an old class <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m3"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math>, we only need to preserve the subspace while learning new classes. To this end, we simply sample <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m4"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math> representative prototype features on the subspace along its top <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m5"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation><annotation encoding="application/x-llamapun">italic_r</annotation></semantics></math> principal components, and denote these features as <math alttext="\bm{Z}_{j,old}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m6"><semantics><msub><mi>𝒁</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{j,old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math>. Because of the simple linear structures of LDR, we can sample from <math alttext="\bm{Z}_{j,old}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m7"><semantics><msub><mi>𝒁</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{j,old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> by calculating the mean and covariance of <math alttext="\bm{Z}_{j,old}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m8"><semantics><msub><mi>𝒁</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{j,old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> after learning class <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m9"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation><annotation encoding="application/x-llamapun">italic_j</annotation></semantics></math>. The storage required is extremely small, since we only need to store means and covariances, which are sampled from as needed. Suppose a total of <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m10"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math> old classes have been learned so far. If prototype features, denoted <math alttext="\bm{Z}_{old}\doteq[\bm{Z}^{1}_{old},\ldots,\bm{Z}^{t}_{old}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m11"><semantics><mrow><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>≐</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mi>t</mi></msubsup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{old}\doteq[\bm{Z}^{1}_{old},\ldots,\bm{Z}^{t}_{old}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ≐ [ bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , … , bold_italic_Z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ]</annotation></semantics></math>, for all of these classes can be preserved when learning new classes, the subspaces <math alttext="\{\mathcal{S}_{j}\}_{j=1}^{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.m12"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">\{\mathcal{S}_{j}\}_{j=1}^{t}</annotation><annotation encoding="application/x-llamapun">{ caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> representing past memory will be preserved as well. Details about sampling and calculating mean and convariance can be found in the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx264" title="">TDW+23</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F10"><img alt="Figure 5.10 : Overall framework of our closed-loop transcription-based incremental learning for a structured LDR memory. Only a single, entirely self-contained, encoding-decoding network is needed: for a new data class 𝑿 n ​ e ​ w \bm{X}_{new} bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , a new LDR memory 𝒁 n ​ e ​ w \bm{Z}_{new} bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT is incrementally learned as a minimax game between the encoder and decoder subject to the constraint that old memory of past classes 𝒁 o ​ l ​ d \bm{Z}_{old} bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT is intact through the closed-loop transcription (or replay): 𝒁 o ​ l ​ d ≈ 𝒁 ^ o ​ l ​ d = f ​ ( g ​ ( 𝒁 o ​ l ​ d ) ) \bm{Z}_{old}\approx\hat{\bm{Z}}_{old}=f(g(\bm{Z}_{old})) bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ≈ over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT = italic_f ( italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) ) ." class="ltx_graphics ltx_img_landscape" height="202" id="F10.g1" src="chapters/chapter5/figs/framework-v7.png" width="538"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.10</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Overall framework<span class="ltx_text ltx_font_medium"> of our closed-loop transcription-based incremental learning for a structured LDR memory. Only a single, entirely self-contained, encoding-decoding network is needed: for a new data class <math alttext="\bm{X}_{new}" class="ltx_Math" display="inline" id="F10.m5"><semantics><msub><mi>𝑿</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{X}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math>, a new LDR memory <math alttext="\bm{Z}_{new}" class="ltx_Math" display="inline" id="F10.m6"><semantics><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> is incrementally learned as a minimax game between the encoder and decoder subject to the constraint that old memory of past classes <math alttext="\bm{Z}_{old}" class="ltx_Math" display="inline" id="F10.m7"><semantics><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> is intact through the closed-loop transcription (or replay): <math alttext="\bm{Z}_{old}\approx\hat{\bm{Z}}_{old}=f(g(\bm{Z}_{old}))" class="ltx_Math" display="inline" id="F10.m8"><semantics><mrow><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>≈</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{old}\approx\hat{\bm{Z}}_{old}=f(g(\bm{Z}_{old}))</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ≈ over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT = italic_f ( italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) )</annotation></semantics></math>.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Incremental learning LDR with an old-memory constraint.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p">Notice that, with the learned auto-encoding (<a class="ltx_ref" href="#S2.E9" title="Equation 5.2.9 ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.9</span></a>), one can replay and use the images, say <math alttext="\hat{\bm{X}}_{old}=g(\bm{Z}_{old},\bm{\eta})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.m1"><semantics><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}_{old}=g(\bm{Z}_{old},\bm{\eta})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT = italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , bold_italic_η )</annotation></semantics></math>, associated with the memory features to avoid forgetting while learning new classes. This is typically how generative models have been used for prior incremental learning methods. However, with the closed-loop framework, explicitly replaying images from the features is not necessary. Past memory can be effectively preserved through optimization exclusively on the features themselves.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p2">
<p class="ltx_p">Consider the task of incrementally learning a new class of objects.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Of course, one may also consider the more general setting where the task contains a small batch of new classes, without serious modification.</span></span></span> We denote a corresponding new sample set as <math alttext="\bm{X}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m1"><semantics><msub><mi>𝑿</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{X}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math>. The features of <math alttext="\bm{X}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m2"><semantics><msub><mi>𝑿</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{X}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> are denoted as <math alttext="\bm{Z}_{new}(\bm{\theta})=f(\bm{X}_{new},\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m3"><semantics><mrow><mrow><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑿</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{new}(\bm{\theta})=f(\bm{X}_{new},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ( bold_italic_θ ) = italic_f ( bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , bold_italic_θ )</annotation></semantics></math>. We concatenate them together with the prototype features of the old classes <math alttext="\bm{Z}_{old}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m4"><semantics><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> and form <math alttext="\bm{Z}=[\bm{Z}_{new}(\bm{\theta}),\bm{Z}_{old}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m5"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{Z}_{new}(\bm{\theta}),\bm{Z}_{old}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ( bold_italic_θ ) , bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ]</annotation></semantics></math>. We denote the replayed images from all features as <math alttext="\hat{\bm{X}}=[{\hat{\bm{X}}_{new}(\bm{\theta},\bm{\eta})},{\hat{\bm{X}}_{old}(\bm{\eta})}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m6"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}=[{\hat{\bm{X}}_{new}(\bm{\theta},\bm{\eta})},{\hat{\bm{X}}_{old}(\bm{\eta})}]</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG = [ over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) , over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ( bold_italic_η ) ]</annotation></semantics></math> although we do not actually need to compute or use them explicitly. We only need features of replayed images, denoted <math alttext="\hat{\bm{Z}}=f(\hat{\bm{X}},\bm{\theta})=[{\hat{\bm{Z}}_{new}(\bm{\theta},\bm{\eta})},{\hat{\bm{Z}}_{old}(\bm{\theta},\bm{\eta})}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.m7"><semantics><mrow><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{Z}}=f(\hat{\bm{X}},\bm{\theta})=[{\hat{\bm{Z}}_{new}(\bm{\theta},\bm{\eta})},{\hat{\bm{Z}}_{old}(\bm{\theta},\bm{\eta})}]</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG = italic_f ( over^ start_ARG bold_italic_X end_ARG , bold_italic_θ ) = [ over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ( bold_italic_θ , bold_italic_η ) ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p3">
<p class="ltx_p">Mirroring the motivation for the multi-class CTRL objective (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>), we would like the features of the new class <math alttext="\bm{Z}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m1"><semantics><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> to be incoherent to all of the old ones <math alttext="\bm{Z}_{old}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m2"><semantics><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{old}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math>. As <math alttext="\bm{Z}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m3"><semantics><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math> is the only new class whose features needs to be learned, the objective (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>) reduces to the case where <math alttext="k=1" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m4"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation><annotation encoding="application/x-llamapun">italic_k = 1</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{\eta}}\max_{\bm{\theta}}\Delta{R_{\epsilon}(\bm{Z})}+\Delta{R_{\epsilon}(\hat{\bm{Z}})}+\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}." class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{\eta}}\max_{\bm{\theta}}\Delta{R_{\epsilon}(\bm{Z})}+\Delta{R_{\epsilon}(\hat{\bm{Z}})}+\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, when we update the network parameters <math alttext="(\bm{\theta},\bm{\eta})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m5"><semantics><mrow><mo stretchy="false">(</mo><mi>𝜽</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{\theta},\bm{\eta})</annotation><annotation encoding="application/x-llamapun">( bold_italic_θ , bold_italic_η )</annotation></semantics></math> to optimize the features for the new class, the updated mappings <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m6"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m7"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> will change features of the old classes too. Hence, to minimize the distortion of the old class representations, we can try to enforce <math alttext="\mbox{Cov}(\bm{Z}_{j,old})=\mbox{Cov}(\hat{\bm{Z}}_{j,old})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.m8"><semantics><mrow><mrow><mtext>Cov</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Cov</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mbox{Cov}(\bm{Z}_{j,old})=\mbox{Cov}(\hat{\bm{Z}}_{j,old})</annotation><annotation encoding="application/x-llamapun">Cov ( bold_italic_Z start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT ) = Cov ( over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT )</annotation></semantics></math>. In other words, while learning new classes, we enforce the memory of old classes remain “self-consistent” through the transcription loop:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{old}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}}_{old}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\ \hat{\bm{Z}}_{old}." class="ltx_Math" display="block" id="S3.E2.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow></mover><msub><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mover><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{old}\xrightarrow{\hskip 5.69054ptg(\bm{z},\bm{\eta})\hskip 5.69054pt}\hat{\bm{X}}_{old}\xrightarrow{\hskip 5.69054ptf(\bm{x},\bm{\theta})\hskip 5.69054pt}\ \hat{\bm{Z}}_{old}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_η ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_θ ) end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Mathematically, this is equivalent to setting</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})\doteq\sum_{j=1}^{t}\Delta R_{\epsilon}(\bm{Z}_{j,old},\hat{\bm{Z}}_{j,old})=0." class="ltx_Math" display="block" id="S3.Ex1.m1"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≐</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>j</mi><mo>,</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})\doteq\sum_{j=1}^{t}\Delta R_{\epsilon}(\bm{Z}_{j,old},\hat{\bm{Z}}_{j,old})=0.</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) ≐ ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_j , italic_o italic_l italic_d end_POSTSUBSCRIPT ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, the above minimax program (<a class="ltx_ref" href="#S3.E1" title="Equation 5.3.1 ‣ Incremental learning LDR with an old-memory constraint. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.1</span></a>) is revised as a <span class="ltx_text ltx_font_italic">constrained</span> minimax game, which we refer to as <span class="ltx_text ltx_font_italic">incremental closed-loop transcription</span> (i-CTRL).
The objective of this game is identical to the standard multi-class CTRL objective (<a class="ltx_ref" href="#S2.E20" title="Equation 5.2.20 ‣ Encoder and decoder as a two-player game. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2.20</span></a>), but includes just one additional constraint:</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx72">
<tbody id="S3.E3">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{\bm{\eta}}\max_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.Ex2.m1"><semantics><mrow><munder><mi>min</mi><mi>𝜼</mi></munder><mo lspace="0.167em">⁡</mo><munder><mi>max</mi><mi>𝜽</mi></munder></mrow><annotation encoding="application/x-tex">\displaystyle\min_{\bm{\eta}}\max_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\Delta{R_{\epsilon}(\bm{Z})}+\Delta{R_{\epsilon}(\hat{\bm{Z}})}+\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}" class="ltx_Math" display="inline" id="S3.Ex2.m2"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\Delta{R_{\epsilon}(\bm{Z})}+\Delta{R_{\epsilon}(\hat{\bm{Z}})}+\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.3)</span></td>
</tr>
<tr class="ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mbox{subject to}\quad\Delta R(\bm{Z}_{old},\hat{\bm{Z}}_{old})=0." class="ltx_Math" display="inline" id="S3.E3.m1"><semantics><mrow><mrow><mrow><mtext>subject to</mtext><mspace width="1em"></mspace><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mbox{subject to}\quad\Delta R(\bm{Z}_{old},\hat{\bm{Z}}_{old})=0.</annotation><annotation encoding="application/x-llamapun">subject to roman_Δ italic_R ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p4">
<p class="ltx_p">In practice, the constrained minimax program can be solved by <span class="ltx_text ltx_font_italic">alternating</span> minimization and maximization between the encoder <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p4.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> and decoder <math alttext="g(\cdot,\bm{\eta})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p4.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , bold_italic_η )</annotation></semantics></math> as follows:</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx73">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.E4.m1"><semantics><munder><mi>max</mi><mi>𝜽</mi></munder><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\Delta{R_{\epsilon}(\bm{Z})}\!+\!\Delta{R_{\epsilon}(\hat{\bm{Z}})}\!+\!\lambda\cdot\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}-\gamma\cdot\Delta{R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})}," class="ltx_Math" display="inline" id="S3.E4.m2"><semantics><mrow><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mrow><mi>λ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi mathvariant="normal">Δ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mrow><mi>γ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi mathvariant="normal">Δ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\Delta{R_{\epsilon}(\bm{Z})}\!+\!\Delta{R_{\epsilon}(\hat{\bm{Z}})}\!+\!\lambda\cdot\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}-\gamma\cdot\Delta{R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})},</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + italic_λ ⋅ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ) - italic_γ ⋅ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\min_{\bm{\eta}}" class="ltx_Math" display="inline" id="S3.E5.m1"><semantics><munder><mi>min</mi><mi>𝜼</mi></munder><annotation encoding="application/x-tex">\displaystyle\min_{\bm{\eta}}</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\Delta{R_{\epsilon}(\bm{Z})}\!+\!\Delta{R_{\epsilon}(\hat{\bm{Z}})}\!+\!\lambda\cdot\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}+\gamma\cdot\Delta{R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})};" class="ltx_Math" display="inline" id="S3.E5.m2"><semantics><mrow><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0.052em" rspace="0.052em">+</mo><mrow><mrow><mi>λ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi mathvariant="normal">Δ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.052em">+</mo><mrow><mrow><mi>γ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi mathvariant="normal">Δ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>;</mo></mrow><annotation encoding="application/x-tex">\displaystyle\Delta{R_{\epsilon}(\bm{Z})}\!+\!\Delta{R_{\epsilon}(\hat{\bm{Z}})}\!+\!\lambda\cdot\Delta{R_{\epsilon}(\bm{Z}_{new},\hat{\bm{Z}}_{new})}+\gamma\cdot\Delta{R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})};</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + italic_λ ⋅ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT ) + italic_γ ⋅ roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) ;</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the constraint <math alttext="\Delta R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})=0" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p4.m3"><semantics><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo>,</mo><msub><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z}_{old},\hat{\bm{Z}}_{old})=0</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) = 0</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E3" title="Equation 5.3.3 ‣ Incremental learning LDR with an old-memory constraint. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.3</span></a>) has been converted (and relaxed) to a Lagrangian term with a corresponding coefficient <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p4.m4"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation><annotation encoding="application/x-llamapun">italic_γ</annotation></semantics></math> and sign. We additionally introduce another coefficient <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p4.m5"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_λ</annotation></semantics></math> for weighting the rate reduction term associated with the new data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Jointly optimal memory via incremental reviewing.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p">As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for <span class="ltx_text ltx_font_italic">all classes</span> cannot rely on graceful forgetting alone. Even for humans, if an object class is learned only once, we should expect the learned memory to fade as we continue to learn new others, unless the memory can be consolidated by reviewing old object classes.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p">To emulate this phase of memory forming, after incrementally learning a whole dataset, we may go back to review all classes again, one class at a time. We refer to going through all classes once as one reviewing “cycle”.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>to distinguish from the term “epoch” used in the conventional joint learning setting.</span></span></span> If needed, multiple reviewing cycles can be conducted. It is quite expected that reviewing can improve the learned (LDR) memory. But somewhat surprisingly, the closed-loop framework allows us to review even in a “class-unsupervised” manner: when reviewing data of an old class say <math alttext="\bm{X}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.m1"><semantics><msub><mi>𝑿</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{X}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, the system does not need the class label and can simply treat <math alttext="\bm{X}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.m2"><semantics><msub><mi>𝑿</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{X}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> as a new class <math alttext="\bm{X}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.m3"><semantics><msub><mi>𝑿</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{X}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math>. That is, the system optimizes the same constrained mini-max program (<a class="ltx_ref" href="#S3.E3" title="Equation 5.3.3 ‣ Incremental learning LDR with an old-memory constraint. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.3</span></a>) without any modification; after the system is optimized, one can identify the newly learned subspace spanned by <math alttext="\bm{Z}_{new}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.m4"><semantics><msub><mi>𝒁</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msub><annotation encoding="application/x-tex">\bm{Z}_{new}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT</annotation></semantics></math>, and use it to replace or merge with the old subspace <math alttext="\mathcal{S}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.m5"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathcal{S}_{j}</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>. As our experiments show, such an class-unsupervised incremental review process can gradually improve both discriminative and generative performance of the LDR memory, eventually converging to that of a jointly-learned memory.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Experimental verification.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p">We show some experimental results on the following datasets: MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx151" title="">LBB+98a</a>]</cite> and CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx145" title="">KNH14</a>]</cite>. All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFAR-10, the 10 classes are split into 5 tasks with 2 classes each or 10 tasks with 1 class each. For the encoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.m2"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>, we adopt a very simple network architecture modified from DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx221" title="">RMC16</a>]</cite>, which is merely a <span class="ltx_text ltx_font_italic">four-layer</span> convolutional network. Here we only show some qualitative visual results and more experiments and analytical analysis can be found in the work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx264" title="">TDW+23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Visualizing auto-encoding properties.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p">We begin by qualitatively visualizing some representative images <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the corresponding replayed <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> on MNIST and CIFAR-10. The model is learned incrementally with the datasets split into 5 tasks. Results are shown in <a class="ltx_ref" href="#F11" title="In Visualizing auto-encoding properties. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.11</span></a>, where we observe that the reconstructed <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> preserves the main visual characteristics of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> including shapes and textures. For a simpler dataset like MNIST, the replayed <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m5"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> are almost identical to the input <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>! This is rather remarkable given: (1) our method does not explicitly enforce <math alttext="\hat{\bm{x}}\approx\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px5.p1.m7"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>≈</mo><mi>𝒙</mi></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}\approx\bm{x}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ≈ bold_italic_x</annotation></semantics></math> for individual samples as most autoencoding methods do, and (2) after having incrementally learned all classes, the generator has not forgotten how to generate digits learned earlier, such as 0, 1, 2. For a more complex dataset like CIFAR-10, we also demonstrate good visual quality, faithfully capturing the essence of each image.</p>
</div>
<figure class="ltx_figure" id="F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf1"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="597" id="F11.sf1.g1" src="chapters/chapter5/figs/mnist_x.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">MNIST <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F11.sf1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf2"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="597" id="F11.sf2.g1" src="chapters/chapter5/figs/mnist_recon_x.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">MNIST <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F11.sf2.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf3"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="595" id="F11.sf3.g1" src="chapters/chapter5/figs/cifar10_x.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR-10 <math alttext="\bm{X}" class="ltx_Math" display="inline" id="F11.sf3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F11.sf4"><img alt="(a) MNIST 𝑿 \bm{X} bold_italic_X" class="ltx_graphics ltx_img_square" height="603" id="F11.sf4.g1" src="chapters/chapter5/figs/cifar10_x_recon.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">CIFAR-10 <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="F11.sf4.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.11</span>: </span><span class="ltx_text" style="font-size:90%;">Visualizing the auto-encoding property of the learned (<math alttext="\hat{\bm{X}}=g\circ f(\bm{X})" class="ltx_Math" display="inline" id="F11.m2"><semantics><mrow><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>g</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{X}}=g\circ f(\bm{X})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG = italic_g ∘ italic_f ( bold_italic_X )</annotation></semantics></math>). </span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Principal subspaces of the learned features.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px6.p1">
<p class="ltx_p">Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features <math alttext="\bm{Z}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px6.p1.m1"><semantics><msub><mi>𝒁</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> for each class is unclear in the feature space. The features <math alttext="\bm{Z}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px6.p1.m2"><semantics><msub><mi>𝒁</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> of the LDR memory, on the other hand, have a clear linear structure. <a class="ltx_ref" href="#F12" title="In Principal subspaces of the learned features. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.12</span></a> visualizes correlations among all learned features <math alttext="|\bm{Z}^{\top}\bm{Z}|" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px6.p1.m3"><semantics><mrow><mo stretchy="false">|</mo><mrow><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\bm{Z}^{\top}\bm{Z}|</annotation><annotation encoding="application/x-llamapun">| bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z |</annotation></semantics></math>, in which we observe clear block-diagonal patterns for both datasets.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Notice that these patterns closely resemble the similarity matrix of response profiles of object categories from different areas of the inferotemporal cortex, as shown in Extended DataFig.3 of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx16" title="">BSM+20</a>]</cite>.</span></span></span> This indicates the features for different classes <math alttext="\bm{Z}_{j}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px6.p1.m4"><semantics><msub><mi>𝒁</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> indeed lie on subspaces that are incoherent from one another. Hence, features of each class can be well modeled as a principal subspace in the feature space.</p>
</div>
<figure class="ltx_figure" id="F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 5.12 : Block diagonal structure of | 𝒁 ⊤ ​ 𝒁 | |\bm{Z}^{\top}\bm{Z}| | bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z | in the feature space for MNIST (left) and CIFAR-10 (right)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="161" id="F12.g1" src="chapters/chapter5/figs/Heatmap_MNIST.jpg" width="187"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 5.12 : Block diagonal structure of | 𝒁 ⊤ ​ 𝒁 | |\bm{Z}^{\top}\bm{Z}| | bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z | in the feature space for MNIST (left) and CIFAR-10 (right)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="157" id="F12.g2" src="chapters/chapter5/figs/Heatmap_CIFAR10.png" width="189"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.12</span>: </span><span class="ltx_text" style="font-size:90%;">Block diagonal structure of <math alttext="|\bm{Z}^{\top}\bm{Z}|" class="ltx_Math" display="inline" id="F12.m2"><semantics><mrow><mo stretchy="false">|</mo><mrow><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\bm{Z}^{\top}\bm{Z}|</annotation><annotation encoding="application/x-llamapun">| bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z |</annotation></semantics></math> in the feature space for MNIST (left) and CIFAR-10 (right).</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">Replay images of samples from principal components.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px7.p1">
<p class="ltx_p">Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. <a class="ltx_ref" href="#F13" title="In Replay images of samples from principal components. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.13</span></a> shows the images replayed from sampled features along the top-4 principal components for different classes, on MNIST and CIFAR-10 respectively. Each row represents samples along one principal component and they clearly show similar visual characteristics but distinctively different from those in other rows. We see that the model remembers different poses of ‘4’ after having learned all remaining classes. For CIFAR-10, the incrementally learned memory remembers representative poses and shapes of horses and ships.</p>
</div>
<figure class="ltx_figure" id="F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf1"><img alt="(a) sampled 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of ‘4’" class="ltx_graphics ltx_img_landscape" height="481" id="F13.sf1.g1" src="chapters/chapter5/figs/mnist_4.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">sampled <math alttext="\hat{\bm{x}}_{old}" class="ltx_Math" display="inline" id="F13.sf1.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{old}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> of ‘4’</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf2"><img alt="(a) sampled 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of ‘4’" class="ltx_graphics ltx_img_square" height="486" id="F13.sf2.g1" src="chapters/chapter5/figs/mnist_7.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">sampled <math alttext="\hat{\bm{x}}_{old}" class="ltx_Math" display="inline" id="F13.sf2.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{old}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> of ‘7’</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf3"><img alt="(a) sampled 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of ‘4’" class="ltx_graphics ltx_img_landscape" height="482" id="F13.sf3.g1" src="chapters/chapter5/figs/horse_z.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">sampled <math alttext="\hat{\bm{x}}_{old}" class="ltx_Math" display="inline" id="F13.sf3.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{old}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> of ‘horse’</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel" id="F13.sf4"><img alt="(a) sampled 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of ‘4’" class="ltx_graphics ltx_img_square" height="483" id="F13.sf4.g1" src="chapters/chapter5/figs/ship_z.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">sampled <math alttext="\hat{\bm{x}}_{old}" class="ltx_Math" display="inline" id="F13.sf4.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{old}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> of ‘ship’</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.13</span>: </span><span class="ltx_text" style="font-size:90%;">Visualization of 5 reconstructed <math alttext="\hat{\bm{x}}=g(\bm{z})" class="ltx_Math" display="inline" id="F13.m3"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=g(\bm{z})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = italic_g ( bold_italic_z )</annotation></semantics></math> from <math alttext="\bm{z}" class="ltx_Math" display="inline" id="F13.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>’s with the closest distance to (top-4) principal components of learned features for MNIST (class ‘4’ and class ‘7’) and CIFAR-10 (class ‘horse’ and ‘ship’).</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px8">
<h5 class="ltx_title ltx_title_paragraph">Effectiveness of incremental reviewing.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px8.p1">
<p class="ltx_p">We verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described before. Experiments are conducted on CIFAR-10, with 10 steps. <a class="ltx_ref" href="#F14" title="In Effectiveness of incremental reviewing. ‣ 5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.14</span></a> left shows replayed images of the first class ‘airplane’ at the end of incremental learning of all ten classes, sampled along the top-3 principal components – every two rows (16 images) are along one principal direction. Their visual quality remains very decent – observed almost no forgetting. The right figure shows replayed images after reviewing the first class once. We notice a significant improvement in visual quality after the reviewing, and principal components of the features in the subspace start to correspond to distinctively different visual attributes within the same class.</p>
</div>
<figure class="ltx_figure" id="F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 5.14 : Visualization of replayed images 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of class 1-‘airplane’ in CIFAR-10, before (left) and after (right) one reviewing cycle." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="188" id="F14.g1" src="chapters/chapter5/figs/memory_before_review_clip.png" width="246"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 5.14 : Visualization of replayed images 𝒙 ^ o ​ l ​ d \hat{\bm{x}}_{old} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT of class 1-‘airplane’ in CIFAR-10, before (left) and after (right) one reviewing cycle." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="185" id="F14.g2" src="chapters/chapter5/figs/memory_after_review_clip.png" width="240"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.14</span>: </span><span class="ltx_text" style="font-size:90%;">Visualization of replayed images <math alttext="\hat{\bm{x}}_{old}" class="ltx_Math" display="inline" id="F14.m2"><semantics><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">\hat{\bm{x}}_{old}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT</annotation></semantics></math> of class 1-‘airplane’ in CIFAR-10, before (left) and after (right) one reviewing cycle.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3.2 </span>Sample-wise Continuous Unsupervised Learning</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">As we know, the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information, with the CTRL-Binary program:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx74">
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\bm{\theta}}\min_{\bm{\eta}}\quad\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S3.E6.m1"><semantics><mrow><mrow><munder><mi>max</mi><mi>𝜽</mi></munder><mo lspace="0.167em">⁡</mo><munder><mi>min</mi><mi>𝜼</mi></munder></mrow><mspace width="1.167em"></mspace><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\bm{\theta}}\min_{\bm{\eta}}\quad\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_η end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, note that (<a class="ltx_ref" href="#S3.E6" title="Equation 5.3.6 ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.6</span></a>) is practically limited because it only aligns the dataset <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the regenerated <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> at the distribution level.
There is no guarantee that for each sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> would be close to the decoded <math alttext="\hat{\bm{x}}=g(f(\bm{x}))" class="ltx_Math" display="inline" id="S3.SS2.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=g(f(\bm{x}))</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = italic_g ( italic_f ( bold_italic_x ) )</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Sample-wise constraints for unsupervised transcription.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p">To improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game (<a class="ltx_ref" href="#S3.E6" title="Equation 5.3.6 ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.6</span></a>). For simplicity and uniformity, here these will be formulated as equality constraints over rate reduction measures, but in practice they can be enforced softly during optimization.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Sample-wise self-consistency via closed-loop transcription.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p">First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m1"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> to be close to <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> for each sample. In the CTRL framework, this can be achieved by enforcing their corresponding features <math alttext="\bm{z}=f(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m3"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}=f(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_z = italic_f ( bold_italic_x )</annotation></semantics></math> and <math alttext="\hat{\bm{z}}=f(\hat{\bm{x}})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{z}}=f(\hat{\bm{x}})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG = italic_f ( over^ start_ARG bold_italic_x end_ARG )</annotation></semantics></math> to be close.
To promote sample-wise self-consistency, where <math alttext="\hat{\bm{x}}=g(f(\bm{x}))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m5"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=g(f(\bm{x}))</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = italic_g ( italic_f ( bold_italic_x ) )</annotation></semantics></math> is close to <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> , we want the distance between <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\hat{\bm{z}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m8"><semantics><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG</annotation></semantics></math> to be zero or small, for all <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m9"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> samples.
This distance can be measured by the rate reduction:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx75">
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0.\vspace{-2mm}" class="ltx_Math" display="inline" id="S3.E7.m1"><semantics><mrow><mrow><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0.\vspace{-2mm}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that this again avoids measuring differences in the image space.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Self-supervision via compressing augmented samples.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p">Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusion etc) as one “class”—a basic idea behind almost all self-supervised learning methods. In the rate reduction framework, it is natural to compress the features of each sample and its augmentations. In this work, we adopt the standard transformations in SimCLR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx50" title="">CKN+20</a>]</cite> and denote such a transformation as <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m1"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math>. We denote each augmented sample <math alttext="\bm{x}_{a}=\tau(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m2"><semantics><mrow><msub><mi>𝒙</mi><mi>a</mi></msub><mo>=</mo><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{a}=\tau(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_τ ( bold_italic_x )</annotation></semantics></math>, and its corresponding feature as <math alttext="\bm{z}_{a}=f(\bm{x}_{a},\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m3"><semantics><mrow><msub><mi>𝒛</mi><mi>a</mi></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>a</mi></msub><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{a}=f(\bm{x}_{a},\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , bold_italic_θ )</annotation></semantics></math>. For discriminative purposes, we hope the classifier is <span class="ltx_text ltx_font_italic">invariant</span> to such transformations. Hence it is natural to enforce that the features <math alttext="\bm{z}_{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m4"><semantics><msub><mi>𝒛</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\bm{z}_{a}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> of all augmentations are the same as that <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m5"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> of the original sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. This is equivalent to requiring the distance between <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\bm{z}_{a}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m8"><semantics><msub><mi>𝒛</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\bm{z}_{a}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>, measured in terms of rate reduction again, to be zero (or small) for all <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m9"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> samples:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx76">
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0.\vspace{-3mm}" class="ltx_Math" display="inline" id="S3.E8.m1"><semantics><mrow><mrow><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0.\vspace{-3mm}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.8)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Unsupervised representation learning via closed-loop transcription.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p">So far, we know the CTRL-Binary objective <math alttext="\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m1"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E6" title="Equation 5.3.6 ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.6</span></a>) helps align the distributions while sample-wise self-consistency (<a class="ltx_ref" href="#S3.E7" title="Equation 5.3.7 ‣ Sample-wise self-consistency via closed-loop transcription. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.7</span></a>) and sample-wise augmentation (<a class="ltx_ref" href="#S3.E8" title="Equation 5.3.8 ‣ Self-supervision via compressing augmented samples. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.8</span></a>) help align and compress features associated with each sample. Besides consistency, we also want learned representations are maximally discriminative for different samples (here viewed as different “classes”). Notice that the rate distortion term <math alttext="R_{\epsilon}(\bm{Z})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m2"><semantics><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\epsilon}(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z )</annotation></semantics></math> measures the coding rate (hence volume) of all features.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Unsupervised CTRL.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p1">
<p class="ltx_p">Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as <span class="ltx_text ltx_font_italic">unsupervised CTRL</span> (u-CTRL):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx77">
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max_{\theta}\min_{\eta}\quad" class="ltx_Math" display="inline" id="S3.E9.m1"><semantics><mrow><mrow><munder><mi>max</mi><mi>θ</mi></munder><mo lspace="0.167em">⁡</mo><munder><mi>min</mi><mi>η</mi></munder></mrow><mspace width="1.167em"></mspace></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\theta}\min_{\eta}\quad</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle R_{\epsilon}(\bm{Z})+\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S3.E9.m2"><semantics><mrow><mrow><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle R_{\epsilon}(\bm{Z})+\Delta R_{\epsilon}(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.9)</span></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath">subject to</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0,\;\;\mbox{and}\;\;\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0." class="ltx_Math" display="inline" id="S3.Ex3.m2"><semantics><mrow><mrow><mrow><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo rspace="0.727em">,</mo><mrow><mrow><mtext>and</mtext><mo lspace="0.560em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0,\;\;\mbox{and}\;\;\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0.</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 , and ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><a class="ltx_ref" href="#F15" title="In Unsupervised CTRL. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.15</span></a> illustrate the overall architecture of the closed-loop system associated with this program.</p>
</div>
<figure class="ltx_figure" id="F15"><img alt="Figure 5.15 : Overall framework of closed-loop transcription for unsupervised learning. Two additional constraints are imposed on the Binary-CTRL method: 1) self-consistency for sample-wise features 𝒛 i \bm{z}^{i} bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and 𝒛 ^ i \hat{\bm{z}}^{i} over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , say 𝒛 i ≈ 𝒛 ^ i \bm{z}^{i}\approx\hat{\bm{z}}^{i} bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ≈ over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ; and 2) invariance/similarity among features of augmented samples 𝒛 i \bm{z}^{i} bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and 𝒛 a i \bm{z}_{a}^{i} bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , say 𝒛 i ≈ 𝒛 a i = f ​ ( τ ​ ( 𝒙 i ) , 𝜽 ) \bm{z}^{i}\approx\bm{z}_{a}^{i}=f(\tau(\bm{x}^{i}),\bm{\theta}) bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ≈ bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_f ( italic_τ ( bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) , bold_italic_θ ) , where 𝒙 a i = τ ​ ( 𝒙 i ) \bm{x}^{i}_{a}=\tau(\bm{x}^{i}) bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_τ ( bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) is an augmentation of sample 𝒙 i \bm{x}^{i} bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT via some transformation τ ​ ( ⋅ ) \tau(\cdot) italic_τ ( ⋅ ) ." class="ltx_graphics ltx_img_landscape" height="167" id="F15.g1" src="chapters/chapter5/figs/uCTRLv3.png" width="586"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Overall framework<span class="ltx_text ltx_font_medium"> of closed-loop transcription for unsupervised learning. Two additional constraints are imposed on the Binary-CTRL method: 1) self-consistency for sample-wise features <math alttext="\bm{z}^{i}" class="ltx_Math" display="inline" id="F15.m10"><semantics><msup><mi>𝒛</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{z}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\hat{\bm{z}}^{i}" class="ltx_Math" display="inline" id="F15.m11"><semantics><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><annotation encoding="application/x-tex">\hat{\bm{z}}^{i}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>, say <math alttext="\bm{z}^{i}\approx\hat{\bm{z}}^{i}" class="ltx_Math" display="inline" id="F15.m12"><semantics><mrow><msup><mi>𝒛</mi><mi>i</mi></msup><mo>≈</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}^{i}\approx\hat{\bm{z}}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ≈ over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>; and 2) invariance/similarity among features of augmented samples <math alttext="\bm{z}^{i}" class="ltx_Math" display="inline" id="F15.m13"><semantics><msup><mi>𝒛</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{z}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{z}_{a}^{i}" class="ltx_Math" display="inline" id="F15.m14"><semantics><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">\bm{z}_{a}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>, say <math alttext="\bm{z}^{i}\approx\bm{z}_{a}^{i}=f(\tau(\bm{x}^{i}),\bm{\theta})" class="ltx_Math" display="inline" id="F15.m15"><semantics><mrow><msup><mi>𝒛</mi><mi>i</mi></msup><mo>≈</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}^{i}\approx\bm{z}_{a}^{i}=f(\tau(\bm{x}^{i}),\bm{\theta})</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ≈ bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_f ( italic_τ ( bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) , bold_italic_θ )</annotation></semantics></math>, where <math alttext="\bm{x}^{i}_{a}=\tau(\bm{x}^{i})" class="ltx_Math" display="inline" id="F15.m16"><semantics><mrow><msubsup><mi>𝒙</mi><mi>a</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒙</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}^{i}_{a}=\tau(\bm{x}^{i})</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_τ ( bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT )</annotation></semantics></math> is an augmentation of sample <math alttext="\bm{x}^{i}" class="ltx_Math" display="inline" id="F15.m17"><semantics><msup><mi>𝒙</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{x}^{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> via some transformation <math alttext="\tau(\cdot)" class="ltx_Math" display="inline" id="F15.m18"><semantics><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\tau(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_τ ( ⋅ )</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p2">
<p class="ltx_p">In practice, the above program can be optimized by alternating maximization and minimization between the encoder <math alttext="f(\cdot,\bm{\theta})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\bm{\theta})</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , bold_italic_θ )</annotation></semantics></math> and the decoder <math alttext="g(\cdot,\bm{\eta})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m2"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>𝜼</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\bm{\eta})</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , bold_italic_η )</annotation></semantics></math>. We adopt the following optimization strategy that works well in practice, which is used for all subsequent experiments on real image datasets:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx78">
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\max_{\theta}\;R_{\epsilon}(\bm{Z})+\Delta{R_{\epsilon}(\bm{Z},\hat{\bm{Z}})-\lambda_{1}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})}-\lambda_{2}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i});" class="ltx_Math" display="inline" id="S3.E10.m1"><semantics><mrow><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>θ</mi></munder><mo lspace="0.447em">⁡</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>λ</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>;</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max_{\theta}\;R_{\epsilon}(\bm{Z})+\Delta{R_{\epsilon}(\bm{Z},\hat{\bm{Z}})-\lambda_{1}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})}-\lambda_{2}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i});</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) - italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) - italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ;</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.10)</span></td>
</tr></tbody>
<tbody id="S3.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\min_{\eta}\;R_{\epsilon}(\bm{Z})+\Delta{R_{\epsilon}(\bm{Z},\hat{\bm{Z}})+\lambda_{1}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})+\lambda_{2}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})}," class="ltx_Math" display="inline" id="S3.E11.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>η</mi></munder><mo lspace="0.447em">⁡</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></munder></mstyle><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\min_{\eta}\;R_{\epsilon}(\bm{Z})+\Delta{R_{\epsilon}(\bm{Z},\hat{\bm{Z}})+\lambda_{1}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})+\lambda_{2}\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) + italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) + italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the constraints <math alttext="\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m3"><semantics><mrow><mrow><msub><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></msub><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msup><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\hat{\bm{z}}^{i})=0</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0</annotation></semantics></math> and <math alttext="\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m4"><semantics><mrow><mrow><msub><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>N</mi></mrow></msub><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝒛</mi><mi>i</mi></msup><mo>,</mo><msubsup><mi>𝒛</mi><mi>a</mi><mi>i</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sum_{i\in N}\Delta R_{\epsilon}(\bm{z}^{i},\bm{z}_{a}^{i})=0</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E9" title="Equation 5.3.9 ‣ Unsupervised CTRL. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.9</span></a>) have been converted (and relaxed) to Lagrangian terms with corresponding coefficients <math alttext="\lambda_{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m5"><semantics><msub><mi>λ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\lambda_{1}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\lambda_{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px5.p2.m6"><semantics><msub><mi>λ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\lambda_{2}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>Notice that computing the rate reduction terms <math alttext="\Delta R" class="ltx_Math" display="inline" id="footnote16.m1"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\Delta R</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R</annotation></semantics></math> for all samples or a batch of samples requires computing the expensive <math alttext="\log\det" class="ltx_Math" display="inline" id="footnote16.m2"><semantics><mrow><mi>log</mi><mo lspace="0.167em" rspace="0em">​</mo><mo>det</mo></mrow><annotation encoding="application/x-tex">\log\det</annotation><annotation encoding="application/x-llamapun">roman_log roman_det</annotation></semantics></math> of large matrices. In practice, from the geometric meaning of <math alttext="\Delta R" class="ltx_Math" display="inline" id="footnote16.m3"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\Delta R</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R</annotation></semantics></math> for two vectors, <math alttext="\Delta R" class="ltx_Math" display="inline" id="footnote16.m4"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">\Delta R</annotation><annotation encoding="application/x-llamapun">roman_Δ italic_R</annotation></semantics></math> can be approximated with an <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="footnote16.m5"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> norm or the cosine distance between two vectors.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p3">
<p class="ltx_p">The above representation is learned without class information. In order to facilitate discriminative or generative tasks, it must be highly structured. It has been verified experimentally that this is indeed the case and u-CTRL demonstrates significant advantages over other incremental or unsupervised learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx263" title="">TDC+24</a>]</cite>. We here only illustrate some qualitative results with the experiment on the CIFAR-10 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx145" title="">KNH14</a>]</cite>, with standard augmentations for self-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx50" title="">CKN+20</a>]</cite>. One may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx263" title="">TDC+24</a>]</cite> for experiments on more and larger datasets and their quantitative evaluations.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p4">
<p class="ltx_p">As one can see from the experiments, specific and unique structure indeed emerges naturally in the representations learned using u-CTRL: globally, features of images in the same class tend to be clustered well together and separated from other classes, as shown in <a class="ltx_ref" href="#F16" title="In Unsupervised CTRL. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.16</span></a>; locally, features around individual samples exhibit approximately piecewise linear low-dimensional structures, as shown in <a class="ltx_ref" href="#F17" title="In Unsupervised CTRL. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.17</span></a>.</p>
</div>
<figure class="ltx_figure" id="F16"><img alt="Figure 5.16 : Emergence of block-diagonal structures of | 𝒁 ⊤ ​ 𝒁 | |\bm{Z}^{\top}\bm{Z}| | bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z | in the feature space for CIFAR-10." class="ltx_graphics ltx_img_square" height="259" id="F16.g1" src="chapters/chapter5/figs/CIFAR10_cifar10_heatmap_zz.png" width="299"/>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 5.16</span>: </span><span class="ltx_text" style="font-size:113%;">Emergence of block-diagonal structures of <math alttext="|\bm{Z}^{\top}\bm{Z}|" class="ltx_Math" display="inline" id="F16.m2"><semantics><mrow><mo stretchy="false">|</mo><mrow><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\bm{Z}^{\top}\bm{Z}|</annotation><annotation encoding="application/x-llamapun">| bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Z |</annotation></semantics></math> in the feature space for CIFAR-10.</span></figcaption>
</figure>
<figure class="ltx_figure" id="F17">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F17.sf1"><img alt="(a) u-CTRL" class="ltx_graphics ltx_img_landscape" height="316" id="F17.sf1.g1" src="chapters/chapter5/figs/uCTRL-tsne2.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">u-CTRL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="F17.sf2"><img alt="(a) u-CTRL" class="ltx_graphics ltx_img_landscape" height="322" id="F17.sf2.g1" src="chapters/chapter5/figs/MoCoV2-tsne3.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">MoCoV2</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5.17</span>: </span><span class="ltx_text" style="font-size:90%;">t-SNE visualizations of learned features of CIFAR-10 with different models.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Unsupervised conditional image generation via rate reduction.</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px6.p1">
<p class="ltx_p">The highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful clusters, and model them with low-dimensional (Gaussian) distributions or subspaces. By sampling from these compact models, we can conditionally regenerate meaningful samples from computed clusters. This is known as <span class="ltx_text ltx_font_italic">unsupervised conditional image generation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx116" title="">HKJ+21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px6.p2">
<p class="ltx_p">To cluster features, we exploit the fact that the rate reduction framework (<a class="ltx_ref" href="Ch3.html#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) is inspired by unsupervised clustering via compression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx173" title="">MDH+07a</a>]</cite>, which provides a principled way to find the membership <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m1"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math>.
Concretely, we maximize the same rate reduction objective (<a class="ltx_ref" href="Ch3.html#S4.E12" title="Equation 3.4.12 ‣ Coding rate of features. ‣ 3.4.2 The Principle of Maximal Coding Rate Reduction ‣ 3.4 Maximizing Information Gain ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.4.12</span></a>) over <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m2"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math>, but fix the learned representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> instead. We simply view the membership <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m4"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math> as a nonlinear function of the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, say <math alttext="h_{\bm{\pi}}(\cdot,\xi):\bm{Z}\mapsto\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m6"><semantics><mrow><mrow><msub><mi>h</mi><mi>𝝅</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>ξ</mi><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mi>𝒁</mi><mo stretchy="false">↦</mo><mi>𝚷</mi></mrow></mrow><annotation encoding="application/x-tex">h_{\bm{\pi}}(\cdot,\xi):\bm{Z}\mapsto\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">italic_h start_POSTSUBSCRIPT bold_italic_π end_POSTSUBSCRIPT ( ⋅ , italic_ξ ) : bold_italic_Z ↦ bold_Π</annotation></semantics></math> with parameters <math alttext="\xi" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m7"><semantics><mi>ξ</mi><annotation encoding="application/x-tex">\xi</annotation><annotation encoding="application/x-llamapun">italic_ξ</annotation></semantics></math>. In practice, we model this function with a simple neural network, such as an MLP head right after the output feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m8"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>.
To estimate a “pseudo” membership <math alttext="\hat{\bm{\Pi}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m9"><semantics><mover accent="true"><mi>𝚷</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{\Pi}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_Π end_ARG</annotation></semantics></math> of the samples, we solve the following optimization problem over <math alttext="\bm{\Pi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px6.p2.m10"><semantics><mi>𝚷</mi><annotation encoding="application/x-tex">\bm{\Pi}</annotation><annotation encoding="application/x-llamapun">bold_Π</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx79">
<tbody id="S3.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{\bm{\Pi}}=\arg\max_{\xi}\Delta R_{\epsilon}(\bm{Z}|\bm{\Pi}(\xi))." class="ltx_Math" display="inline" id="S3.E12.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝚷</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mi>ξ</mi></munder><mo lspace="0.167em">⁡</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>R</mi><mi>ϵ</mi></msub></mrow></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒁</mi><mo fence="false">|</mo><mrow><mi>𝚷</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ξ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\hat{\bm{\Pi}}=\arg\max_{\xi}\Delta R_{\epsilon}(\bm{Z}|\bm{\Pi}(\xi)).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_Π end_ARG = roman_arg roman_max start_POSTSUBSCRIPT italic_ξ end_POSTSUBSCRIPT roman_Δ italic_R start_POSTSUBSCRIPT italic_ϵ end_POSTSUBSCRIPT ( bold_italic_Z | bold_Π ( italic_ξ ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.3.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In <a class="ltx_ref" href="#F18" title="In Unsupervised conditional image generation via rate reduction. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.18</span></a>, we visualize images generated from the ten unsupervised clusters from (<a class="ltx_ref" href="#S3.E12" title="Equation 5.3.12 ‣ Unsupervised conditional image generation via rate reduction. ‣ 5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.3.12</span></a>). Each block represents one cluster and each row represents one principal component for each cluster. Despite learning and training without labels, the model not only organizes samples into correct clusters, but is also able to preserve statistical diversities within each cluster/class. We can easily recover the diversity within each cluster by computing different principal components and then sample and generate accordingly. While the experiments presented here are somewhat limited in scale, we will explore more direct and powerful methods that utilize the learned data distributions and representations for conditional generation and estimation in the next chapter.</p>
</div>
<figure class="ltx_figure" id="F18"><img alt="Figure 5.18 : Unsupervised conditional image generation from each cluster of CIFAR-10, using u-CTRL. Images from different rows mean generation from different principal components of each cluster." class="ltx_graphics ltx_img_landscape" height="126" id="F18.g1" src="chapters/chapter5/figs/CIFAR10_generatedfromcluster.png" width="568"/>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:113%;">Figure 5.18</span>: </span><span class="ltx_text" style="font-size:113%;">Unsupervised conditional image generation from each cluster of CIFAR-10, using u-CTRL. Images from different rows mean generation from different principal components of each cluster.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.4 </span>Summary and Notes</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">Historically, autoencoding has been one of the important drivers of research
innovation in neural networks for learning, although the most practically
impressive demonstrations of deep learning have probably been in other domains
(such as discriminative classification, with AlexNet
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>, or generative modeling with GPT architectures
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx32" title="">BMR+20</a>]</cite>).
Works we have featured throughout the chapter, especially the work of
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx104" title="">HS06</a>]</cite>, served as catalysts of research interest in neural networks
during times when they were otherwise not prominent in the machine learning
research landscape.
In modern practice, autoencoders remain core components of many large-scale
systems for generating highly structured data such as visual data, speech data,
and molecular data, especially the VQ-VAE approach
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx203" title="">OVK17</a>]</cite>, which builds on the variational autoencoder
methodology we discussed in <a class="ltx_ref" href="#S1.SS4" title="5.1.4 Variational Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1.4</span></a>.
The core problem of autoencoding remains of paramount intellectual importance
due to its close connection with representation learning, and we anticipate that
it will reappear on the radar of practical researchers in the future as
efficiency in training and deploying large models continue to become more
important.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">Materials presented in the second half of this chapter are based on a series of
recent work on the topic of closed-loop transcription: <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx63" title="">DTL+22</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx206" title="">PPC+23</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx264" title="">TDW+23</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx263" title="">TDC+24</a>]</cite>. In particular, <a class="ltx_ref" href="#S2.SS1" title="5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.1</span></a> is based on the pioneering work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx63" title="">DTL+22</a>]</cite>. After that, the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx206" title="">PPC+23</a>]</cite> has provided strong theoretical justifications for the closed-loop framework, at least for an ideal case. <a class="ltx_ref" href="#S3.SS1" title="5.3.1 Class-wise Incremental Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3.1</span></a> and <a class="ltx_ref" href="#S3.SS2" title="5.3.2 Sample-wise Continuous Unsupervised Learning ‣ 5.3 Continuous Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3.2</span></a> are based on the works of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx264" title="">TDW+23</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx263" title="">TDC+24</a>]</cite>, respectively. They demonstrate that the closed-loop framework naturally supports incremental and continuous learning, either in a class-wise or sample-wise setting. The reader may refer to these papers for more technical and experimental details.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Shallow vs. deep neural networks, for autoencoding and more.</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p">In <a class="ltx_ref" href="#S1.SS2" title="5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1.2</span></a>, we discussed Cybenko’s universal approximation
theorem and how it states that in principle, a neural network with a single
hidden layer (and suitable elementwise nonlinearities) is sufficient to
approximate any suitably regular target function. Of course, in practice, the
major architectural reason for the dominance of neural networks in practice has
been the refinement of techniques for training <span class="ltx_text ltx_font_italic">deeper</span> neural networks.
Why is depth necessary?
From a fundamental point of view, the issue of depth separations, which
construct settings where a deeper neural network can approximate a given class
of target functions with exponentially-superior efficiency relative to a shallow
network,
has been studied at great length in the theoretical literature:
examples include <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx259" title="">Tel16</a>, <a class="ltx_ref" href="bib.html#bibx29" title="">BN20</a>, <a class="ltx_ref" href="bib.html#bibx273" title="">VJO+21</a>]</cite>.
The ease of training deeper
networks in practice has not received as satisfying of an answer from the
perspective of theory.
ResNets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx103" title="">HZR+16a</a>]</cite> represent the pioneering empirical work of
making deeper networks more easily trainable, used in nearly all modern
architectures in some form. Theoretical studies have focused heavily on
trainability of very deep networks, quantified via the initial neural tangent
kernel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx34" title="">BGW21</a>, <a class="ltx_ref" href="bib.html#bibx179" title="">MBD+21</a>]</cite>, but these studies have not given
significant insight into the trainability benefits of deeper networks in middle
and late stages of training (but see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx306" title="">YH21</a>]</cite> for the root of a line
of research attempting to address this).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5.5 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 5.1</span></span><span class="ltx_text ltx_font_italic"> </span>(Conceptual Understanding of Manifold Flattening)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<p class="ltx_p">Consider data lying on a curved manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> embedded in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m2"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> (like
a curved surface in <math alttext="3" class="ltx_Math" display="inline" id="Thmexercise1.p1.m3"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation><annotation encoding="application/x-llamapun">3</annotation></semantics></math>-dimensional space), as discussed in the manifold
flattening subsection of <a class="ltx_ref" href="#S1.SS2" title="5.1.2 Nonlinear PCA and Autoencoding ‣ 5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1.2</span></a>.
In this exercise, we will describe the basic ingredients of the manifold
flattening algorithm from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx214" title="">PPR+24</a>]</cite>.
A manifold is called flat if it is an open set in Euclidean space (or
more generally, an open set in a subspace).</p>
</div>
<div class="ltx_para" id="Thmexercise1.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p">Suppose the manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic">graph</span>: this means that <math alttext="\mathcal{M}\subset\mathbb{R}^{D_{1}}\times\mathbb{R}^{D_{2}}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m2"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>⊂</mo><mrow><msup><mi>ℝ</mi><msub><mi>D</mi><mn>1</mn></msub></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mn>2</mn></msub></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{M}\subset\mathbb{R}^{D_{1}}\times\mathbb{R}^{D_{2}}</annotation><annotation encoding="application/x-llamapun">caligraphic_M ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> (say), and that there is a function <math alttext="F:\mathbb{R}^{D_{1}}\to\mathbb{R}^{D_{2}}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m3"><semantics><mrow><mi>F</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><msub><mi>D</mi><mn>1</mn></msub></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mn>2</mn></msub></msup></mrow></mrow><annotation encoding="application/x-tex">F:\mathbb{R}^{D_{1}}\to\mathbb{R}^{D_{2}}</annotation><annotation encoding="application/x-llamapun">italic_F : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{M}=\{(\bm{x},F(\bm{x})\mid\bm{x}\in\mathbb{R}^{D_{1}})\}." class="ltx_Math" display="block" id="S5.E1.m1"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝒙</mi><mo>,</mo><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∣</mo><mi>𝒙</mi></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mn>1</mn></msub></msup></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{M}=\{(\bm{x},F(\bm{x})\mid\bm{x}\in\mathbb{R}^{D_{1}})\}.</annotation><annotation encoding="application/x-llamapun">caligraphic_M = { ( bold_italic_x , italic_F ( bold_italic_x ) ∣ bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Give an integer <math alttext="D_{3}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m4"><semantics><msub><mi>D</mi><mn>3</mn></msub><annotation encoding="application/x-tex">D_{3}</annotation><annotation encoding="application/x-llamapun">italic_D start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math> and a map <math alttext="f:\mathbb{R}^{D_{1}}\times\mathbb{R}^{D_{2}}\to\mathbb{R}^{D_{3}}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m5"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><msup><mi>ℝ</mi><msub><mi>D</mi><mn>1</mn></msub></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mn>2</mn></msub></msup></mrow><mo stretchy="false">→</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mn>3</mn></msub></msup></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{D_{1}}\times\mathbb{R}^{D_{2}}\to\mathbb{R}^{D_{3}}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>
that flattens <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m6"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math>, and describe the corresponding (lossless)
reconstruction procedure from the flattened representation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p">Now suppose that <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> is a general smooth manifold. Smooth manifolds
have the property that they are locally well-approximated by subspaces
near each point. Describe in intuitive terms how to flatten the manifold
<math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> locally at each point, by relating it to a graph. (The algorithm of
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx214" title="">PPR+24</a>]</cite> performs a refined version of this local flattening
process in a way that allows them to be glued together to form a global
flattening.)</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 5.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Reproduce Closed-Loop Transcription)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<p class="ltx_p">Implement a closed-loop transcription pipeline for representation learning on
the CIFAR-10 dataset following
the methodology in <a class="ltx_ref" href="#S2.SS1" title="5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.1</span></a>.
Reference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx63" title="">DTL+22</a>]</cite> for useful hyperparameters and architecture
settings. Reproduce the result in <a class="ltx_ref" href="#F8" title="In Visualizing correlation of features 𝒁 and decoded features 𝒁̂. ‣ 5.2.1 Closed-Loop Transcription via Stackelberg Games ‣ 5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5.8</span></a>.</p>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 12:37:23 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
