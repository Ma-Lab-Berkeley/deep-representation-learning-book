<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 12:37:23 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>A Low-Dimensional Subspace</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S1.SS1" title="In 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Principal Components Analysis (PCA)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px1" title="In 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Problem formulation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px2" title="In 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Subspace encoding-decoding via denoising.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS1.SSS0.Px3" title="In 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Computing the subspace basis.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS2" title="In 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Pursuing Low-rank Structure via Power Iteration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS3" title="In 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Probabilistic PCA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS4" title="In 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Matrix Completion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>A Mixture of Complete Low-Dimensional Subspaces</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS1" title="In 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Mixtures of Subspaces and Sparsely-Used
Dictionaries</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px1" title="In 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Orthogonal dictionary for sparse coding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="In 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Complete Dictionary Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px1" title="In 2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Dictionary learning via the MSP algorithm.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS3" title="In 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Connection to ICA and Kurtosis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS3.SSS0.Px1" title="In 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Incremental ICA: correctness and FastICA algorithm.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>A Mixture of Overcomplete Low-Dimensional Subspaces</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="In 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Sparse Coding with an Overcomplete Dictionary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="In 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Overcomplete Dictionary Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS3" title="In 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>Learned Deep Sparse Coding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px1" title="In 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Learned ISTA.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px2" title="In 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Sparse Autoencoders.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px3" title="In 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Layerwise learned sparse coding?</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S4" title="In Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Summary and Notes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="In Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Exercises and Extensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 2 </span>Learning Linear and Independent Structures</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">A Low-Dimensional Subspace</a><div class="mini-toc-sub"><a href="#S1.SS1">Principal Components Analysis (PCA)</a><a href="#S1.SS2">Pursuing Low-rank Structure via Power Iteration</a><a href="#S1.SS3">Probabilistic PCA</a><a href="#S1.SS4">Matrix Completion</a></div></li><li><a href="#S2">A Mixture of Complete Low-Dimensional Subspaces</a><div class="mini-toc-sub"><a href="#S2.SS1">Mixtures of Subspaces and Sparsely-Used
Dictionaries</a><a href="#S2.SS2">Complete Dictionary Learning</a><a href="#S2.SS3">Connection to ICA and Kurtosis</a></div></li><li><a href="#S3">A Mixture of Overcomplete Low-Dimensional Subspaces</a><div class="mini-toc-sub"><a href="#S3.SS1">Sparse Coding with an Overcomplete Dictionary</a><a href="#S3.SS2">Overcomplete Dictionary Learning</a><a href="#S3.SS3">Learned Deep Sparse Coding</a></div></li><li><a href="#S4">Summary and Notes</a></li><li><a href="#S5">Exercises and Extensions</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">The art of doing mathematics consists in finding that special case which contains all the germs of generality</span>.”</p>
<p class="ltx_p">  – David Hilbert</p>
</blockquote>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Real data has low-dimensional structure.</span> To see why this is true, let us consider the unassuming case of static on a TV when the satellite isn’t working. At each frame (approximately every <math alttext="\frac{1}{30}" class="ltx_Math" display="inline" id="p2.m1"><semantics><mfrac><mn>1</mn><mn>30</mn></mfrac><annotation encoding="application/x-tex">\frac{1}{30}</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG 30 end_ARG</annotation></semantics></math> seconds), the RGB static on a screen of size <math alttext="H\times W" class="ltx_Math" display="inline" id="p2.m2"><semantics><mrow><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H\times W</annotation><annotation encoding="application/x-llamapun">italic_H × italic_W</annotation></semantics></math> is, roughly, sampled independently from a uniform distribution on <math alttext="[0,1]^{3\times H\times W}" class="ltx_Math" display="inline" id="p2.m3"><semantics><msup><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow></msup><annotation encoding="application/x-tex">[0,1]^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun">[ 0 , 1 ] start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>. In theory, the static <span class="ltx_text ltx_font_italic">could</span> resolve to a natural image on any given frame, but even if you spend a thousand years looking at the TV screen, it will not. This discrepancy is explained by the fact that the set of <math alttext="H\times W" class="ltx_Math" display="inline" id="p2.m4"><semantics><mrow><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H\times W</annotation><annotation encoding="application/x-llamapun">italic_H × italic_W</annotation></semantics></math> natural images takes up a vanishingly small fraction of the hypercube <math alttext="[0,1]^{3\times H\times W}" class="ltx_Math" display="inline" id="p2.m5"><semantics><msup><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>W</mi></mrow></msup><annotation encoding="application/x-tex">[0,1]^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun">[ 0 , 1 ] start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>. In particular, it is extremely low-dimensional, compared to the ambient space dimension. Similar phenomena occur for all other types of natural data, such as text, audio, and video. Thus, when we design systems and methods to process natural data and learn its structure or distribution, this is a central property of natural data which we need to take into account.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">Therefore, our central task is to learn a distribution that has low intrinsic dimension in a high-dimensional space. In the remainder of this section, we discuss several <span class="ltx_text ltx_font_italic">classical</span> methods to perform this task for several somewhat <span class="ltx_text ltx_font_italic">idealistic</span> models for the distribution, namely models that are geometrically linear or statistically independent. While these models and methods are important and useful in their own right, we discuss them here as they motivate, inspire, and serve as a predecessor or analogue to more modern methods for more general distributions that involve deep (representation) learning.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">Our main approach (and general problem formulation) can be summarized as:</p>
</div>
<div class="ltx_para ltx_noindent" id="p5">
<div class="tcbox"><p class="ltx_p">Problem: Given one or several (noisy or incomplete) observations of a ground truth sample from the data distribution, obtain an estimate of this sample.</p></div>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">This approach underpins several classical methods for data processing, which we discuss in this chapter.</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p">Section <a class="ltx_ref" href="#S1" title="2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1</span></a> — Principal Components Analysis (PCA): Given noisy samples from a distribution supported on <span class="ltx_text ltx_font_italic">one low-dimensional subspace</span>, obtain an estimate of the true sample that lies on this subspace.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p">Section <a class="ltx_ref" href="#S2" title="2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2</span></a> — Complete Dictionary Learning and Independent Components Analysis (ICA): Given noisy samples from a distribution supported on <span class="ltx_text ltx_font_italic">a union (not the span) of few low-dimensional subspaces</span>, obtain an estimate of the true samples.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i3.p1">
<p class="ltx_p">Section <a class="ltx_ref" href="#S3" title="2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3</span></a> — Sparse Coding and Overcomplete Dictionary Learning: Given noisy samples from <span class="ltx_text ltx_font_italic">a distribution supported on combinations of a few incoherent vectors</span>, such as the coordinate axes, obtain an estimate of the true sample, which also has this property.</p>
</div>
</li>
</ul>
<p class="ltx_p">As we will soon reveal in later chapters, in the deep learning era, modern methods essentially adopt the same approach to learn.</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p">In this chapter, as described above, we make simplifying modeling assumptions that essentially assume the data have geometrically (nearly, piece-wise) linear structures and statistically independent components. In Chapter <a class="ltx_ref" href="Ch1.html" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1</span></a>, we have referred to such models of the data as “analytical models”. These modeling assumptions allow us to derive efficient algorithms with provable efficiency guarantees<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Both in terms of data and computational complexity.</span></span></span> for processing data at scale. However, they are imperfect models for often-complex real-world data distributions, and so their underlying assumptions only approximately hold. This means that the guarantees afforded by detailed analysis of these algorithms also only approximately hold in the case of real data. Nonetheless, the techniques discussed in this chapter are useful in their own right, and beyond that, serve as the “special case with the germ of generality”, so to speak, in that they present a guiding motivation and intuition for the more general paradigms within (deep) learning of more general distributions that we later address.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.1 </span>A Low-Dimensional Subspace</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1.1 </span>Principal Components Analysis (PCA)</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p">Perhaps the simplest modeling assumption possible for low-dimensional structure
is the so-called <span class="ltx_text ltx_font_italic">low-rank</span> assumption. Letting <math alttext="D" class="ltx_Math" display="inline" id="S1.SS1.p1.m1"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> be the dimension of
our data space, we assume that our data belong to a low-dimensional subspace of dimension <math alttext="d\ll D" class="ltx_Math" display="inline" id="S1.SS1.p1.m2"><semantics><mrow><mi>d</mi><mo>≪</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d\ll D</annotation><annotation encoding="application/x-llamapun">italic_d ≪ italic_D</annotation></semantics></math>, possibly plus some small disturbances. This ends up being a nearly valid assumption for some surprisingly complex data, such as images of handwritten digits and face data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx219" title="">RD03</a>]</cite> as shown in Figure <a class="ltx_ref" href="#F1" title="Figure 2.1 ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1</span></a>, yet as we will see, it will lend itself extremely well to comprehensive analysis.</p>
</div>
<figure class="ltx_figure" id="F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 2.1 : Images of human faces and handwritten digits. Despite the seemingly large variety in their appearances, each set of those data spans (approximately) a very low-dimensional (nearly) linear subspace." class="ltx_graphics ltx_figure_panel ltx_img_square" height="238" id="F1.g1" src="chapters/chapter2/figs/faces.png" width="240"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 2.1 : Images of human faces and handwritten digits. Despite the seemingly large variety in their appearances, each set of those data spans (approximately) a very low-dimensional (nearly) linear subspace." class="ltx_graphics ltx_figure_panel ltx_img_square" height="238" id="F1.g2" src="chapters/chapter2/figs/handwritten-digits.png" width="237"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.1</span>: </span><span class="ltx_text" style="font-size:90%;">Images of human faces and handwritten digits. Despite the seemingly large variety in their appearances, each set of those data spans (approximately) a very low-dimensional (nearly) linear subspace.</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Problem formulation.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p1">
<p class="ltx_p">To write this in mathematical notation, we represent a subspace <math alttext="\mathcal{S}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m1"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_S ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> of dimension <math alttext="d" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> by an orthonormal matrix <math alttext="\bm{U}\in\mathsf{O}(D,d)\subseteq\mathbb{R}^{D\times d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m3"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,d)\subseteq\mathbb{R}^{D\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_d ) ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> such that the columns of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m4"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> span <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m5"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation><annotation encoding="application/x-llamapun">caligraphic_S</annotation></semantics></math>. Then, we say that our data <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m6"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> have (approximate) low-rank structure if there exists an orthonormal matrix <math alttext="\bm{U}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m7"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math>, vectors <math alttext="\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m8"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and <span class="ltx_text ltx_font_italic">small</span> vectors <math alttext="\{\bm{\varepsilon}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m9"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝜺</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{\varepsilon}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\quad\forall i\in[N]." class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi>𝜺</mi><mi>i</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\quad\forall i\in[N].</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ∀ italic_i ∈ [ italic_N ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m10"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are disturbances that prevent the data from being perfectly
low rank; their presence in our model allows us to quantify the degree to which
our analysis remains relevant in the presence of deviations from our model. The
true supporting subspace is <math alttext="\mathcal{S}\doteq\mathop{\mathrm{col}}(\bm{U})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m11"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo rspace="0.1389em">≐</mo><mrow><mo lspace="0.1389em" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><mi>𝑼</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}\doteq\mathop{\mathrm{col}}(\bm{U})</annotation><annotation encoding="application/x-llamapun">caligraphic_S ≐ roman_col ( bold_italic_U )</annotation></semantics></math>, the span of
the columns of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m12"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>. To process all we can from
this data, we need to recover <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m13"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation><annotation encoding="application/x-llamapun">caligraphic_S</annotation></semantics></math>; to do this it is sufficient to recover
<math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p1.m14"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>, also called the <span class="ltx_text ltx_font_italic">principal components</span>. Fortunately, this is
a computationally tractable task named <span class="ltx_text ltx_font_bold">Principal Component Analysis</span>, and
we discuss now how to solve it.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px1.p2">
<p class="ltx_p">Given data <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> distributed as in (<a class="ltx_ref" href="#S1.E1" title="Equation 2.1.1 ‣ Problem formulation. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>), we
aim to recover the model <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m2"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>. A natural approach is to find the subspace
<math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m3"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> and latent vectors <math alttext="\{\bm{z}_{i}^{\star}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m4"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝒛</mi><mi>i</mi><mo>⋆</mo></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}^{\star}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> which
yield the best approximation <math alttext="\bm{x}_{i}\approx\bm{U}^{\star}\bm{z}_{i}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m5"><semantics><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>≈</mo><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒛</mi><mi>i</mi><mo>⋆</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{i}\approx\bm{U}^{\star}\bm{z}_{i}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≈ bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. Namely, we aim to solve the problem</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}},\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}," class="ltx_Math" display="block" id="S1.E2.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></munder><mo lspace="0.167em">⁡</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}},\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px1.p2.m6"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math> is constrained to be an orthonormal matrix, as above. We will omit
this constraint in similar statements below for the sake of concision.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Subspace encoding-decoding via denoising.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p1">
<p class="ltx_p">To simplify this problem, for a fixed <math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m1"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math>, we have (proof as exercise):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx1">
<tbody id="S1.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}" class="ltx_Math" display="inline" id="S1.E3.m1"><semantics><mrow><mrow><munder><mi>min</mi><msubsup><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></munder><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\min_{\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{N}\sum_{i=1}^{N}\min_{\tilde{\bm{z}}_{i}}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}" class="ltx_Math" display="inline" id="S1.E3.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><munder><mi>min</mi><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub></munder><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{N}\sum_{i=1}^{N}\min_{\tilde{\bm{z}}_{i}}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.3)</span></td>
</tr></tbody>
<tbody id="S1.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}." class="ltx_Math" display="inline" id="S1.E4.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">= divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, the optimal solution <math alttext="(\bm{U}^{\star},\{\bm{z}_{i}^{\star}\}_{i=1}^{N})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝒛</mi><mi>i</mi><mo>⋆</mo></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{U}^{\star},\{\bm{z}_{i}^{\star}\}_{i=1}^{N})</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , { bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT )</annotation></semantics></math>
to the above optimization problem has <math alttext="\bm{z}_{i}^{\star}=(\bm{U}^{\star})^{\top}\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p1.m3"><semantics><mrow><msubsup><mi>𝒛</mi><mi>i</mi><mo>⋆</mo></msubsup><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{z}_{i}^{\star}=(\bm{U}^{\star})^{\top}\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT = ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p2">
<p class="ltx_p">Now, we can write the original optimization problem in <math alttext="\tilde{\bm{U}}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m1"><semantics><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⋆</mo></msup><annotation encoding="application/x-tex">\tilde{\bm{U}}^{\star}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>
and <math alttext="\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m2"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> as an optimization problem just over
<math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m3"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math>, i.e., to obtain the basis <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m4"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> and compact codes
<math alttext="\{\bm{z}_{i}^{\star}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m5"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>𝒛</mi><mi>i</mi><mo>⋆</mo></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{z}_{i}^{\star}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> it suffices to solve either of the two following equivalent problems:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}},\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}=\min_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}." class="ltx_Math" display="block" id="S1.E5.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>,</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow></munder><mo lspace="0.167em">⁡</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>min</mi><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}},\{\tilde{\bm{z}}_{i}\}_{i=1}^{N}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{z}}_{i}\|_{2}^{2}=\min_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that the problem on the right-hand side of (<a class="ltx_ref" href="#S1.E5" title="Equation 2.1.5 ‣ Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>)
is a <span class="ltx_text ltx_font_italic">denoising</span> problem: given noisy observations <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m6"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> of
low-rank data, we aim to find the <span class="ltx_text ltx_font_italic">noise-free</span> copy of <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m7"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, which
under the model (<a class="ltx_ref" href="#S1.E1" title="Equation 2.1.1 ‣ Problem formulation. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>) is <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m8"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. That is, the denoised input
<math alttext="\hat{\bm{x}}_{i}=\bm{U}\bm{U}^{\top}\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m9"><semantics><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{i}=\bm{U}\bm{U}^{\top}\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Notice that this is the point on the subspace
that is closest to <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m10"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, as visualized in <a class="ltx_ref" href="#F2" title="In Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>. Here by
solving the equivalent problem of finding the best subspace, parameterized by
the learned basis <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m11"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, we learn an approximation to the
<span class="ltx_text ltx_font_italic">denoiser</span>, i.e., the projection matrix <math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m12"><semantics><mrow><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>≈</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ≈ bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> that projects the noisy data point <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m13"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> onto the subspace <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p2.m14"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation><annotation encoding="application/x-llamapun">caligraphic_S</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F2"><svg class="ltx_picture" height="71.26" id="F2.pic1" overflow="visible" version="1.1" width="211.97"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,71.26) matrix(1 0 0 -1 0 0) translate(4.61,0) translate(0,4.61)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 190.95 37.64)"><foreignobject height="8.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="11.8"><math alttext="\bm{u}_{1}" class="ltx_Math" display="inline" id="F2.pic1.m1"><semantics><msub><mi>𝒖</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{u}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt"><path d="M 4.89 0.98 L 184.98 37" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(0.98058 0.19614 -0.19614 0.98058 185.52 37.11)"><path d="M -3.54 4.32 C -2.9 1.73 -1.45 0.5 0 0 C -1.45 -0.5 -2.9 -1.73 -3.54 -4.32" style="fill:none"></path></g></g><g fill="#FF0000" stroke="#000000"><path d="M 120.56 59.06 C 120.56 60.41 119.46 61.5 118.11 61.5 C 116.76 61.5 115.66 60.41 115.66 59.06 C 115.66 57.7 116.76 56.61 118.11 56.61 C 119.46 56.61 120.56 57.7 120.56 59.06 Z M 118.11 59.06"></path></g><g fill="#00FF00" stroke="#000000"><path d="M 128.43 25.2 C 128.43 26.55 127.34 27.64 125.98 27.64 C 124.63 27.64 123.54 26.55 123.54 25.2 C 123.54 23.85 124.63 22.75 125.98 22.75 C 127.34 22.75 128.43 23.85 128.43 25.2 Z M 125.98 25.2"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 133.84 56.08)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="7.91"><math alttext="\bm{x}" class="ltx_Math" display="inline" id="F2.pic1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 140.38 13.57)"><foreignobject height="12.49" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.95"><math alttext="\hat{\bm{x}}=\bm{u}_{1}\bm{u}_{1}^{\top}\bm{x}" class="ltx_Math" display="inline" id="F2.pic1.m3"><semantics><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><msub><mi>𝒖</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒖</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\bm{u}_{1}\bm{u}_{1}^{\top}\bm{x}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x</annotation></semantics></math></foreignobject></g><g color="#BF8040" fill="#BF8040" stroke="#BF8040" stroke-width="0.8pt"><path d="M 125.37 27.85 L 118.98 55.32" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" transform="matrix(-0.22652 0.97401 -0.97401 -0.22652 118.85 55.86)"><path d="M -3.54 4.32 C -2.9 1.73 -1.45 0.5 0 0 C -1.45 -0.5 -2.9 -1.73 -3.54 -4.32" style="fill:none"></path></g></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 105.04 36.39)"><foreignobject height="5.96" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.45"><math alttext="\varepsilon" class="ltx_Math" display="inline" id="F2.pic1.m4"><semantics><mi mathcolor="#BF8040">ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation><annotation encoding="application/x-llamapun">italic_ε</annotation></semantics></math></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Geometry of PCA.<span class="ltx_text ltx_font_medium"> A data point <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> (red) is projected onto the one-dimensional learned subspace spanned by the unit basis vector <math alttext="\bm{u}_{1}" class="ltx_Math" display="inline" id="F2.m8"><semantics><msub><mi>𝒖</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{u}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> (blue arrow). The projection <math alttext="\bm{U}\bm{U}^{\top}\bm{x}=\bm{u}_{1}\bm{u}_{1}^{\top}\bm{x}" class="ltx_Math" display="inline" id="F2.m9"><semantics><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>=</mo><mrow><msub><mi>𝒖</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒖</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{U}\bm{U}^{\top}\bm{x}=\bm{u}_{1}\bm{u}_{1}^{\top}\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x</annotation></semantics></math> (green) is the denoised version of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="F2.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> using the low-dimensional structure given by <math alttext="\bm{u}_{1}" class="ltx_Math" display="inline" id="F2.m11"><semantics><msub><mi>𝒖</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{u}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="F2.m12"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> (brown arrow) represents the projection residual or noise.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.SSS0.Px2.p3">
<p class="ltx_p">Putting the above process together, we essentially obtain a simple encoding-decoding scheme that maps a data point <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m2"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> to a lower-dimensional (latent) space <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m3"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and then back to <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m4"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}=(\bm{U}^{\star})^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}^{\star}\hskip 5.69054pt}\hat{\bm{x}}." class="ltx_Math" display="block" id="S1.E6.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup></mrow></mover><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}=(\bm{U}^{\star})^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}^{\star}\hskip 5.69054pt}\hat{\bm{x}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_x end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="\bm{z}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m5"><semantics><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{z}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> can be viewed as the low-dimensional compact code (or
a latent representation) of a data point <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m6"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and the learned subspace
basis <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m7"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> as the associated codebook whose columns are the (learned)
optimal code words. The process achieves the function of denoising <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> by
projecting it onto the subspace spanned by <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p3.m9"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Computing the subspace basis.</h4>
<div class="ltx_para" id="S1.SS1.SSS0.Px3.p1">
<p class="ltx_p">For now, we continue our calculation. Let <math alttext="\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> be the matrix whose columns are the observations <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m2"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. We have (proof as exercise)</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx2">
<tbody id="S1.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname*{arg\ min}_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}" class="ltx_Math" display="inline" id="S1.E7.m1"><semantics><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname*{arg\ min}_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{F}^{2}" class="ltx_Math" display="inline" id="S1.E7.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo>⁡</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{F}^{2}</annotation><annotation encoding="application/x-llamapun">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.7)</span></td>
</tr></tbody>
<tbody id="S1.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{tr}\left\{\tilde{\bm{U}}^{\top}\left(\frac{\bm{X}\bm{X}^{\top}}{N}\right)\tilde{\bm{U}}\right\}." class="ltx_Math" display="inline" id="S1.E8.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mi>tr</mi></mrow><mo>⁡</mo><mrow><mo>{</mo><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mi>N</mi></mfrac></mstyle><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></mrow><mo>}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{tr}\left\{\tilde{\bm{U}}^{\top}\left(\frac{\bm{X}\bm{X}^{\top}}{N}\right)\tilde{\bm{U}}\right\}.</annotation><annotation encoding="application/x-llamapun">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT roman_tr { over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( divide start_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG start_ARG italic_N end_ARG ) over~ start_ARG bold_italic_U end_ARG } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, to compute the principal components, we find the orthogonal matrix
<math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m3"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math> which maximizes the term
<math alttext="\operatorname{tr}(\tilde{\bm{U}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{U}})" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m4"><semantics><mrow><mi>tr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{tr}(\tilde{\bm{U}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{U}})</annotation><annotation encoding="application/x-llamapun">roman_tr ( over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_U end_ARG )</annotation></semantics></math>. We can prove via
induction that this matrix <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m5"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> has columns which are the <span class="ltx_text ltx_font_italic">top <math alttext="d" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m6"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> unit eigenvectors</span> of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m7"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>. We leave the whole proof to the reader in <a class="ltx_ref" href="#Thmexercise1" title="Exercise 2.1. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>, but we handle the base case of the induction here. Suppose that <math alttext="d=1" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m8"><semantics><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d=1</annotation><annotation encoding="application/x-llamapun">italic_d = 1</annotation></semantics></math>. Then we only have a single unit vector <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m9"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> to recover, so the above problem reduces to</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\tilde{\bm{u}}\colon\|\tilde{\bm{u}}\|_{2}=1}\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}." class="ltx_Math" display="block" id="S1.E9.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><msup><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo>⊤</mo></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\tilde{\bm{u}}\colon\|\tilde{\bm{u}}\|_{2}=1}\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT over~ start_ARG bold_italic_u end_ARG : ∥ over~ start_ARG bold_italic_u end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 end_POSTSUBSCRIPT over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is the so-called <span class="ltx_text ltx_font_italic">Rayleigh quotient</span> of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m10"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>. By invoking the spectral theorem we diagonalize <math alttext="\bm{X}\bm{X}^{\top}/N=\bm{V}\bm{\Lambda}\bm{V}^{\top}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m11"><semantics><mrow><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><mo>=</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N=\bm{V}\bm{\Lambda}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N = bold_italic_V bold_Λ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\bm{V}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m12"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math> is orthogonal and <math alttext="\bm{\Lambda}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m13"><semantics><mi>𝚲</mi><annotation encoding="application/x-tex">\bm{\Lambda}</annotation><annotation encoding="application/x-llamapun">bold_Λ</annotation></semantics></math> is diagonal with non-negative entries. Hence</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}=\tilde{\bm{u}}^{\top}\bm{V}\bm{\Lambda}\bm{V}^{\top}\bm{u}=(\bm{V}^{\top}\tilde{\bm{u}})^{\top}\bm{\Lambda}(\bm{V}^{\top}\tilde{\bm{u}})." class="ltx_Math" display="block" id="S1.E10.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><mo>=</mo><mrow><msup><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}=\tilde{\bm{u}}^{\top}\bm{V}\bm{\Lambda}\bm{V}^{\top}\bm{u}=(\bm{V}^{\top}\tilde{\bm{u}})^{\top}\bm{\Lambda}(\bm{V}^{\top}\tilde{\bm{u}}).</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG = over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_V bold_Λ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u = ( bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_u end_ARG ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Λ ( bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_u end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Since <math alttext="\bm{V}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m14"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math> is an invertible orthogonal transformation, <math alttext="\bm{V}^{\top}\tilde{\bm{u}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m15"><semantics><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\bm{V}^{\top}\tilde{\bm{u}}</annotation><annotation encoding="application/x-llamapun">bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_u end_ARG</annotation></semantics></math> is a unit vector, and optimizing over <math alttext="\tilde{\bm{u}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m16"><semantics><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{u}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_u end_ARG</annotation></semantics></math> is equivalent to optimizing over <math alttext="\tilde{\bm{w}}\doteq\bm{V}^{\top}\tilde{\bm{u}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m17"><semantics><mrow><mover accent="true"><mi>𝒘</mi><mo>~</mo></mover><mo>≐</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow></mrow><annotation encoding="application/x-tex">\tilde{\bm{w}}\doteq\bm{V}^{\top}\tilde{\bm{u}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_w end_ARG ≐ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_u end_ARG</annotation></semantics></math>. Hence, we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}=\tilde{\bm{w}}^{\top}\bm{\Lambda}\tilde{\bm{w}}," class="ltx_Math" display="block" id="S1.E11.m1"><semantics><mrow><mrow><mrow><msup><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover></mrow><mo>=</mo><mrow><msup><mover accent="true"><mi>𝒘</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝚲</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒘</mi><mo>~</mo></mover></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\tilde{\bm{u}}^{\top}(\bm{X}\bm{X}^{\top}/N)\tilde{\bm{u}}=\tilde{\bm{w}}^{\top}\bm{\Lambda}\tilde{\bm{w}},</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG = over~ start_ARG bold_italic_w end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Λ over~ start_ARG bold_italic_w end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">whose optimal solutions <math alttext="\bm{w}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m18"><semantics><msup><mi>𝒘</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{w}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> among unit vectors are one-hot vectors whose only nonzero (hence unit) entry is in one of the indices corresponding to the largest eigenvalue of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m19"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>. This means that <math alttext="\tilde{\bm{u}}=\bm{V}\tilde{\bm{w}}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m20"><semantics><mrow><mover accent="true"><mi>𝒖</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒘</mi><mo>~</mo></mover></mrow></mrow><annotation encoding="application/x-tex">\tilde{\bm{u}}=\bm{V}\tilde{\bm{w}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_u end_ARG = bold_italic_V over~ start_ARG bold_italic_w end_ARG</annotation></semantics></math>, the optimal solution to the original problem, corresponds to a unit eigenvector of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m21"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math> (i.e., column of <math alttext="\bm{V}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m22"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math>) which corresponds to the largest eigenvalue. Suitably generalizing this to the case <math alttext="d&gt;1" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p1.m23"><semantics><mrow><mi>d</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d&gt;1</annotation><annotation encoding="application/x-llamapun">italic_d &gt; 1</annotation></semantics></math>, and summarizing all the previous discussion, we have the following informal Theorem.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2.1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that our dataset <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m1"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> can be written as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\qquad\forall i\in[N]," class="ltx_Math" display="block" id="S1.E12.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi>𝜺</mi><mi>i</mi></msub></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\qquad\forall i\in[N],</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ∀ italic_i ∈ [ italic_N ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\bm{U}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m2"><semantics><mrow><mi>𝐔</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math> captures the low-rank structure,
<math alttext="\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m3"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝐳</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{z}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> are the compact codes
of the data, and <math alttext="\{\bm{\varepsilon}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m4"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝛆</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{\varepsilon}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> are small
vectors indicating the deviation of our data from the low-rank model. Then
the principal components <math alttext="\bm{U}^{\star}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m5"><semantics><mrow><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math> of our
dataset are given by the top <math alttext="d" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m6"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> eigenvectors of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m7"><semantics><mrow><mrow><mi>𝐗</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐗</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>,
where <math alttext="\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m8"><semantics><mrow><mi>𝐗</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝐱</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝐱</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, and
approximately correspond to the optimal linear denoiser:
<math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}" class="ltx_Math" display="inline" id="Thmtheorem1.p1.m9"><semantics><mrow><mrow><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>≈</mo><mrow><mi>𝐔</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐔</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ≈ bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S1.SS1.SSS0.Px3.p2">
<p class="ltx_p">We do not give explicit rates of approximation here as they can become rather
technical. In the special case that <math alttext="\bm{\varepsilon}_{i}=\bm{0}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m1"><semantics><mrow><msub><mi>𝜺</mi><mi>i</mi></msub><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}=\bm{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0</annotation></semantics></math> for all <math alttext="i" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m2"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>, the
learned <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m3"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> spans the support of the samples <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m4"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. If in addition the <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m5"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are sufficiently diverse (say,
spanning all of <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m6"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>) then we would have perfect recovery:
<math alttext="\bm{U}^{\star}(\bm{U}^{\star})=\bm{U}\bm{U}^{\top}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px3.p2.m7"><semantics><mrow><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})=\bm{U}\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark1.p1">
<p class="ltx_p">In some data analysis tasks, the data matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmremark1.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is formatted such that each data point is a <span class="ltx_text ltx_font_italic">row</span> rather than a <span class="ltx_text ltx_font_italic">column</span> as is presented here. In this case the principal components are the top <math alttext="d" class="ltx_Math" display="inline" id="Thmremark1.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> eigenvectors of <math alttext="\bm{X}^{\top}\bm{X}/N" class="ltx_Math" display="inline" id="Thmremark1.p1.m3"><semantics><mrow><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}^{\top}\bm{X}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X / italic_N</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.2</span></span><span class="ltx_text ltx_font_italic"> </span>(Basis Selection via Denoising Eigenvalues)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark2.p1">
<p class="ltx_p">In many cases, either our data will not truly be distributed according to a subspace-plus-noise model, or we will not know the true underlying dimension <math alttext="d" class="ltx_Math" display="inline" id="Thmremark2.p1.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> of the subspace. In this case, we have to choose <math alttext="d" class="ltx_Math" display="inline" id="Thmremark2.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>; this problem is called <span class="ltx_text ltx_font_italic">model selection</span>. In the restricted case of PCA, one way to perform model selection is to compute <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="Thmremark2.p1.m3"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math> and look for instances where adjacent eigenvalues sharply decrease; this is one indicator that the index of the larger eigenvalue is the “true dimension <math alttext="d" class="ltx_Math" display="inline" id="Thmremark2.p1.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>”, and the rest of the eigenvalues of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="Thmremark2.p1.m5"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math> are contributed by the noise or disturbances <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="Thmremark2.p1.m6"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Model selection is a difficult problem and, nowadays in the era of deep learning where it is called “hyperparameter optimization”, is usually done via brute force or Bayesian optimization.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.3</span></span><span class="ltx_text ltx_font_italic"> </span>(Denoising Samples)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark3.p1">
<p class="ltx_p">The expression on the right-hand side of (<a class="ltx_ref" href="#S1.E5" title="Equation 2.1.5 ‣ Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>), that is,</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2}," class="ltx_Math" display="block" id="S1.E13.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}}}\frac{1}{N}\sum_{i=1}^{N}\|\bm{x}_{i}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}_{i}\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is what’s known as a <span class="ltx_text ltx_font_italic">denoising problem</span>, thusly named because it is an optimization problem whose solution <span class="ltx_text ltx_font_italic">removes the noise from the samples so it fits on the subspace</span>. Denoising—learning a map which removes noise from noisy samples so that it fits on the data structure (such as in (<a class="ltx_ref" href="#S1.E1" title="Equation 2.1.1 ‣ Problem formulation. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>), but maybe more complicated)—is a common method for learning distributions that will be discussed in the sequel and throughout the manuscript. Note that we have already discussed this notion, but it bears repeating due to its central importance in later Chapters.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.4</span></span><span class="ltx_text ltx_font_italic"> </span>(Neural Network Interpretation)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark4.p1">
<p class="ltx_p">If we do a PCA, we approximately recover the support of the distribution
encoded by the parameter <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="Thmremark4.p1.m1"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. The learned denoising map then
takes the form <math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}" class="ltx_Math" display="inline" id="Thmremark4.p1.m2"><semantics><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>. On top of being
a denoiser, this can be viewed as a <span class="ltx_text ltx_font_italic">simple two-layer weight-tied
linear neural network</span>: the first layer multiplies by
<math alttext="(\bm{U}^{\star})^{\top}" class="ltx_Math" display="inline" id="Thmremark4.p1.m3"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><annotation encoding="application/x-tex">(\bm{U}^{\star})^{\top}</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, and the second layer multiplies by <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="Thmremark4.p1.m4"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, namely</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{denoise}(\bm{x})=\underbrace{\bm{U}^{\star}\circ\underbrace{\operatorname{id}\circ\underbrace{(\bm{U}^{\star})^{\top}\bm{x}}_{\text{first ``layer''}}}_{\text{post-activation of first ``layer''}}}_{\text{output of ``NN''}}" class="ltx_Math" display="block" id="S1.E14.m1"><semantics><mrow><mrow><mi>denoise</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><munder><munder accentunder="true"><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0.222em" rspace="0.222em">∘</mo><munder><munder accentunder="true"><mrow><mi>id</mi><mo lspace="0em" rspace="0.222em">∘</mo><munder><munder accentunder="true"><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>⏟</mo></munder><mtext>first “layer”</mtext></munder></mrow><mo>⏟</mo></munder><mtext>post-activation of first “layer”</mtext></munder></mrow><mo>⏟</mo></munder><mtext>output of “NN”</mtext></munder></mrow><annotation encoding="application/x-tex">\operatorname{denoise}(\bm{x})=\underbrace{\bm{U}^{\star}\circ\underbrace{\operatorname{id}\circ\underbrace{(\bm{U}^{\star})^{\top}\bm{x}}_{\text{first ``layer''}}}_{\text{post-activation of first ``layer''}}}_{\text{output of ``NN''}}</annotation><annotation encoding="application/x-llamapun">roman_denoise ( bold_italic_x ) = under⏟ start_ARG bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∘ under⏟ start_ARG roman_id ∘ under⏟ start_ARG ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x end_ARG start_POSTSUBSCRIPT first “layer” end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT post-activation of first “layer” end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT output of “NN” end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Contrasting this to a standard two-layer neural network, we see a structural similarity:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{NN}(\bm{x})=\underbrace{\bm{W}^{\star}\circ\underbrace{\mathrm{ReLU}\circ\underbrace{(\bm{U}^{\star})^{\top}\bm{x}}_{\text{first layer}}}_{\text{post-activation of first layer}}}_{\text{output of NN}}" class="ltx_Math" display="block" id="S1.E15.m1"><semantics><mrow><mrow><mi>NN</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><munder><munder accentunder="true"><mrow><msup><mi>𝑾</mi><mo>⋆</mo></msup><mo lspace="0.222em" rspace="0.222em">∘</mo><munder><munder accentunder="true"><mrow><mi>ReLU</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><munder><munder accentunder="true"><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>⏟</mo></munder><mtext>first layer</mtext></munder></mrow><mo>⏟</mo></munder><mtext>post-activation of first layer</mtext></munder></mrow><mo>⏟</mo></munder><mtext>output of NN</mtext></munder></mrow><annotation encoding="application/x-tex">\operatorname{NN}(\bm{x})=\underbrace{\bm{W}^{\star}\circ\underbrace{\mathrm{ReLU}\circ\underbrace{(\bm{U}^{\star})^{\top}\bm{x}}_{\text{first layer}}}_{\text{post-activation of first layer}}}_{\text{output of NN}}</annotation><annotation encoding="application/x-llamapun">roman_NN ( bold_italic_x ) = under⏟ start_ARG bold_italic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∘ under⏟ start_ARG roman_ReLU ∘ under⏟ start_ARG ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x end_ARG start_POSTSUBSCRIPT first layer end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT post-activation of first layer end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT output of NN end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In particular, PCA can be interpreted as <span class="ltx_text ltx_font_italic">learning a simple two-layer
denoising autoencoder</span>,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In fact, as we have mentioned in the
previous chapter, PCA was one of the first problems that neural networks
were used to solve <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx200" title="">Oja82</a>, <a class="ltx_ref" href="bib.html#bibx14" title="">BH89</a>]</cite>.</span></span></span> one of the simplest
examples of a non-trivial neural network. In this framework, the
<span class="ltx_text ltx_font_italic">learned representations</span> would just be <math alttext="(\bm{U}^{\star})^{\top}\bm{x}\approx\bm{z}" class="ltx_Math" display="inline" id="Thmremark4.p1.m5"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo>≈</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">(\bm{U}^{\star})^{\top}\bm{x}\approx\bm{z}</annotation><annotation encoding="application/x-llamapun">( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ≈ bold_italic_z</annotation></semantics></math>. In this way, PCA serves as a model problem for (deep) representation learning, which we will draw upon further in the monograph. Notice that in this analogy, the representations reflect, or are projections of, the input data towards a learned low-dimensional structure. This property will be particularly relevant in the future.</p>
</div>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1.2 </span>Pursuing Low-rank Structure via Power Iteration</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p">There is a computationally efficient way to estimate the top eigenvectors of <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS2.p1.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math> or any symmetric positive semidefinite matrix <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p1.m2"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>, called <span class="ltx_text ltx_font_italic">power iteration</span>. This method is the building block of several algorithmic approaches to high-dimensional data analysis that we discuss later in the Chapter, so we discuss it here.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p">Let <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p2.m1"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math> be a symmetric positive semidefinite matrix. There exists an orthonormal basis for <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p2.m2"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> consisting of eigenvectors <math alttext="(\bm{w}_{i})_{i=1}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p2.m3"><semantics><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>𝒘</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><annotation encoding="application/x-tex">(\bm{w}_{i})_{i=1}^{D}</annotation><annotation encoding="application/x-llamapun">( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p2.m4"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>, with corresponding eigenvalues <math alttext="\lambda_{1}\geq\cdots\geq\lambda_{D}\geq 0" class="ltx_Math" display="inline" id="S1.SS2.p2.m5"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>≥</mo><mi mathvariant="normal">⋯</mi><mo>≥</mo><msub><mi>λ</mi><mi>D</mi></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_{1}\geq\cdots\geq\lambda_{D}\geq 0</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≥ ⋯ ≥ italic_λ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ≥ 0</annotation></semantics></math>. By definition, any eigenvector <math alttext="\bm{w}_{i}" class="ltx_Math" display="inline" id="S1.SS2.p2.m6"><semantics><msub><mi>𝒘</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> satisfies <math alttext="\lambda_{i}\bm{w}_{i}=\bm{M}\bm{w}_{i}" class="ltx_Math" display="inline" id="S1.SS2.p2.m7"><semantics><mrow><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\lambda_{i}\bm{w}_{i}=\bm{M}\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_M bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Therefore, for any <math alttext="\lambda_{i}&gt;0" class="ltx_Math" display="inline" id="S1.SS2.p2.m8"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_{i}&gt;0</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; 0</annotation></semantics></math>, <math alttext="\bm{w}_{i}" class="ltx_Math" display="inline" id="S1.SS2.p2.m9"><semantics><msub><mi>𝒘</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a “fixed point” to the following equation:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{w}=\frac{\bm{M}\bm{w}}{\|\bm{M}\bm{w}\|_{2}}." class="ltx_Math" display="block" id="S1.E16.m1"><semantics><mrow><mrow><mi>𝒘</mi><mo>=</mo><mfrac><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒘</mi></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒘</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{w}=\frac{\bm{M}\bm{w}}{\|\bm{M}\bm{w}\|_{2}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_w = divide start_ARG bold_italic_M bold_italic_w end_ARG start_ARG ∥ bold_italic_M bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.16)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Power Iteration)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Assume that <math alttext="\lambda_{1}&gt;\lambda_{i}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m1"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{1}&gt;\lambda_{i}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &gt; italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for all <math alttext="i&gt;1" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m2"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i&gt;1</annotation><annotation encoding="application/x-llamapun">italic_i &gt; 1</annotation></semantics></math>. If we compute the fixed point of (<a class="ltx_ref" href="#S1.E16" title="Equation 2.1.16 ‣ 2.1.2 Pursuing Low-rank Structure via Power Iteration ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.16</span></a>) using the following iteration:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{0}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{1}),\qquad\bm{v}_{t+1}\leftarrow\frac{\bm{M}\bm{v}_{t}}{\|\bm{M}\bm{v}_{t}\|_{2}}," class="ltx_Math" display="block" id="S1.E17.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒗</mi><mn>0</mn></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mn>𝟏</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">←</mo><mfrac><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>t</mi></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{0}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{1}),\qquad\bm{v}_{t+1}\leftarrow\frac{\bm{M}\bm{v}_{t}}{\|\bm{M}\bm{v}_{t}\|_{2}},</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_1 ) , bold_italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← divide start_ARG bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">then, in the limit, <math alttext="\bm{v}_{t}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m3"><semantics><msub><mi>𝐯</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{v}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> will converge to a top unit-norm eigenvector of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="Thmtheorem2.p1.m4"><semantics><mi>𝐌</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p">First, note that for all <math alttext="t" class="ltx_Math" display="inline" id="S1.SS2.p3.m1"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{t}=\frac{\bm{M}\bm{v}_{t-1}}{\|\bm{M}\bm{v}_{t-1}\|_{2}}=\frac{\bm{M}^{2}\bm{v}_{t-2}}{\|\bm{M}\bm{v}_{t-1}\|_{2}\|\bm{M}\bm{v}_{t-2}\|_{2}}=\cdots=\frac{\bm{M}^{t}\bm{v}_{0}}{\prod_{s=1}^{t}\|\bm{M}\bm{v}_{s}\|_{2}}." class="ltx_Math" display="block" id="S1.E18.m1"><semantics><mrow><mrow><msub><mi>𝒗</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac><mo>=</mo><mfrac><mrow><msup><mi>𝑴</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mfrac><mo>=</mo><mi mathvariant="normal">⋯</mi><mo>=</mo><mfrac><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><mrow><msubsup><mo>∏</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mrow><mo lspace="0em" stretchy="false">‖</mo><mrow><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>s</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{t}=\frac{\bm{M}\bm{v}_{t-1}}{\|\bm{M}\bm{v}_{t-1}\|_{2}}=\frac{\bm{M}^{2}\bm{v}_{t-2}}{\|\bm{M}\bm{v}_{t-1}\|_{2}\|\bm{M}\bm{v}_{t-2}\|_{2}}=\cdots=\frac{\bm{M}^{t}\bm{v}_{0}}{\prod_{s=1}^{t}\|\bm{M}\bm{v}_{s}\|_{2}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_t - 2 end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∥ bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 2 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG = ⋯ = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG ∏ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∥ bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, <math alttext="\bm{v}_{t}" class="ltx_Math" display="inline" id="S1.SS2.p3.m2"><semantics><msub><mi>𝒗</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{v}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> has the same direction as <math alttext="\bm{M}^{t}\bm{v}_{0}" class="ltx_Math" display="inline" id="S1.SS2.p3.m3"><semantics><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\bm{M}^{t}\bm{v}_{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and is unit norm, so we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{t}=\frac{\bm{M}^{t}\bm{v}_{0}}{\|\bm{M}^{t}\bm{v}_{0}\|_{2}}." class="ltx_Math" display="block" id="S1.E19.m1"><semantics><mrow><mrow><msub><mi>𝒗</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{t}=\frac{\bm{M}^{t}\bm{v}_{0}}{\|\bm{M}^{t}\bm{v}_{0}\|_{2}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Because all the eigenvectors <math alttext="\bm{w}_{i}" class="ltx_Math" display="inline" id="S1.SS2.p3.m4"><semantics><msub><mi>𝒘</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{w}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p3.m5"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math> form an orthonormal basis for <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p3.m6"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{0}=\sum_{i=1}^{D}\alpha_{i}\bm{w}_{i}," class="ltx_Math" display="block" id="S1.E20.m1"><semantics><mrow><mrow><msub><mi>𝒗</mi><mn>0</mn></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><msub><mi>α</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{0}=\sum_{i=1}^{D}\alpha_{i}\bm{w}_{i},</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where because <math alttext="\bm{v}_{0}" class="ltx_Math" display="inline" id="S1.SS2.p3.m7"><semantics><msub><mi>𝒗</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\bm{v}_{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is Gaussian the <math alttext="\alpha_{i}" class="ltx_Math" display="inline" id="S1.SS2.p3.m8"><semantics><msub><mi>α</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\alpha_{i}</annotation><annotation encoding="application/x-llamapun">italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are all nonzero with probability <math alttext="1" class="ltx_Math" display="inline" id="S1.SS2.p3.m9"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>. Thus, we can use our earlier expression for <math alttext="\bm{v}_{t}" class="ltx_Math" display="inline" id="S1.SS2.p3.m10"><semantics><msub><mi>𝒗</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{v}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to write</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{t}=\frac{\bm{M}^{t}\bm{v}_{0}}{\|\bm{M}^{t}\bm{v}_{0}\|_{2}}=\frac{\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}}{\|\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}\|_{2}}=\frac{\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}}{\sum_{i=1}^{D}\lambda_{i}^{t}\lvert\alpha_{i}\rvert}." class="ltx_Math" display="block" id="S1.E21.m1"><semantics><mrow><mrow><msub><mi>𝒗</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑴</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>0</mn></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><mrow><msubsup><mi>λ</mi><mi>i</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mo lspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><mrow><msubsup><mi>λ</mi><mi>i</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><mrow><msubsup><mi>λ</mi><mi>i</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><mrow><msubsup><mi>λ</mi><mi>i</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">|</mo><msub><mi>α</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{t}=\frac{\bm{M}^{t}\bm{v}_{0}}{\|\bm{M}^{t}\bm{v}_{0}\|_{2}}=\frac{\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}}{\|\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}\|_{2}}=\frac{\sum_{i=1}^{D}\lambda_{i}^{t}\alpha_{i}\bm{w}_{i}}{\sum_{i=1}^{D}\lambda_{i}^{t}\lvert\alpha_{i}\rvert}.</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ∥ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT | italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, let us consider the case where <math alttext="\lambda_{1}&gt;\lambda_{2}\geq\cdots\geq\lambda_{D}\geq 0" class="ltx_Math" display="inline" id="S1.SS2.p3.m11"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>≥</mo><mi mathvariant="normal">⋯</mi><mo>≥</mo><msub><mi>λ</mi><mi>D</mi></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_{1}&gt;\lambda_{2}\geq\cdots\geq\lambda_{D}\geq 0</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &gt; italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≥ ⋯ ≥ italic_λ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ≥ 0</annotation></semantics></math>. (The case with repeated top eigenvalues goes similarly.) Then we can write</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{v}_{t}=\frac{\alpha_{1}\bm{w}_{1}+\sum_{i=2}^{D}(\lambda_{i}/\lambda_{1})^{t}\alpha_{i}\bm{w}_{i}}{\lvert\alpha_{1}\rvert+\sum_{i=2}^{D}(\lambda_{i}/\lambda_{1})^{t}\lvert\alpha_{i}\rvert}." class="ltx_Math" display="block" id="S1.E22.m1"><semantics><mrow><mrow><msub><mi>𝒗</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mn>1</mn></msub></mrow><mo rspace="0.055em">+</mo><mrow><msubsup><mo rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>D</mi></msubsup><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>/</mo><msub><mi>λ</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>α</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mi>i</mi></msub></mrow></mrow></mrow><mrow><mrow><mo stretchy="false">|</mo><msub><mi>α</mi><mn>1</mn></msub><mo stretchy="false">|</mo></mrow><mo rspace="0.055em">+</mo><mrow><msubsup><mo rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>D</mi></msubsup><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>/</mo><msub><mi>λ</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">|</mo><msub><mi>α</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{v}_{t}=\frac{\alpha_{1}\bm{w}_{1}+\sum_{i=2}^{D}(\lambda_{i}/\lambda_{1})^{t}\alpha_{i}\bm{w}_{i}}{\lvert\alpha_{1}\rvert+\sum_{i=2}^{D}(\lambda_{i}/\lambda_{1})^{t}\lvert\alpha_{i}\rvert}.</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ∑ start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG | italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | + ∑ start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT | italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Because <math alttext="\lambda_{1}&gt;\lambda_{i}" class="ltx_Math" display="inline" id="S1.SS2.p3.m12"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{1}&gt;\lambda_{i}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT &gt; italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for all <math alttext="i&gt;1" class="ltx_Math" display="inline" id="S1.SS2.p3.m13"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i&gt;1</annotation><annotation encoding="application/x-llamapun">italic_i &gt; 1</annotation></semantics></math>, the terms inside the summation go to <math alttext="0" class="ltx_Math" display="inline" id="S1.SS2.p3.m14"><mn>0</mn></math> exponentially fast, and the remainder is the limit</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lim_{t\to\infty}\bm{v}_{t}=\frac{\alpha_{1}}{\lvert\alpha_{1}\rvert}\bm{w}_{1}=\operatorname{sign}(\alpha_{1})\bm{w}_{1}," class="ltx_Math" display="block" id="S1.E23.m1"><semantics><mrow><mrow><mrow><munder><mo movablelimits="false">lim</mo><mrow><mi>t</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></munder><msub><mi>𝒗</mi><mi>t</mi></msub></mrow><mo>=</mo><mrow><mfrac><msub><mi>α</mi><mn>1</mn></msub><mrow><mo stretchy="false">|</mo><msub><mi>α</mi><mn>1</mn></msub><mo stretchy="false">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mn>1</mn></msub></mrow><mo>=</mo><mrow><mrow><mi>sign</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>α</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒘</mi><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\lim_{t\to\infty}\bm{v}_{t}=\frac{\alpha_{1}}{\lvert\alpha_{1}\rvert}\bm{w}_{1}=\operatorname{sign}(\alpha_{1})\bm{w}_{1},</annotation><annotation encoding="application/x-llamapun">roman_lim start_POSTSUBSCRIPT italic_t → ∞ end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG | italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | end_ARG bold_italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = roman_sign ( italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) bold_italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is a top unit eigenvector of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p3.m15"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>. The top eigenvalue <math alttext="\lambda_{1}" class="ltx_Math" display="inline" id="S1.SS2.p3.m16"><semantics><msub><mi>λ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\lambda_{1}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p3.m17"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math> can be estimated by <math alttext="\bm{v}_{t}^{\top}\bm{M}\bm{v}_{t}" class="ltx_Math" display="inline" id="S1.SS2.p3.m18"><semantics><mrow><msubsup><mi>𝒗</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝑴</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\bm{v}_{t}^{\top}\bm{M}\bm{v}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, which converges similarly rapidly to <math alttext="\lambda_{1}" class="ltx_Math" display="inline" id="S1.SS2.p3.m19"><semantics><msub><mi>λ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\lambda_{1}</annotation><annotation encoding="application/x-llamapun">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.
∎</p>
</div>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p">To find the second top eigenvector, we apply the power iteration algorithm to <math alttext="\bm{M}-\lambda_{1}\bm{v}_{1}\bm{v}_{1}^{\top}" class="ltx_Math" display="inline" id="S1.SS2.p4.m1"><semantics><mrow><mi>𝑴</mi><mo>−</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">\bm{M}-\lambda_{1}\bm{v}_{1}\bm{v}_{1}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_M - italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, which has eigenvectors <math alttext="(\bm{w}_{i})_{i=2}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p4.m2"><semantics><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>𝒘</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>D</mi></msubsup><annotation encoding="application/x-tex">(\bm{w}_{i})_{i=2}^{D}</annotation><annotation encoding="application/x-llamapun">( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and corresponding eigenvalues <math alttext="(\lambda_{i})_{i=2}^{D}" class="ltx_Math" display="inline" id="S1.SS2.p4.m3"><semantics><msubsup><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>D</mi></msubsup><annotation encoding="application/x-tex">(\lambda_{i})_{i=2}^{D}</annotation><annotation encoding="application/x-llamapun">( italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. By repeating this procedure <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.p4.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> times in sequence, we can very efficiently estimate the top <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.p4.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> eigenvectors of <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p4.m6"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math> very quickly, for any symmetric positive semidefinite matrix <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS2.p4.m7"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>. Thus we can also apply it to <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS2.p4.m8"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math> to recover the top <math alttext="d" class="ltx_Math" display="inline" id="S1.SS2.p4.m9"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> principal components, which is what we were after in the first place. Notice that this approach recovers one principal component at a time; we will contrast this to other algorithmic approaches, such as gradient descent on global objective functions, in future sections.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1.3 </span>Probabilistic PCA</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p">Notice that the above formulation makes no statistical assumptions on the
data-generating process. However, it is common to include statistical elements
within a given data model, as it may add further enlightening interpretations
about the result of the analysis. As such, we ask the natural question:
<span class="ltx_text ltx_font_italic">what is the statistical analogue to low-dimensional structure?</span> Our answer is that a low-dimensional <span class="ltx_text ltx_font_italic">distribution</span> is one whose support is concentrated around a low-dimensional geometric structure.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p">To illustrate this point, we discuss <span class="ltx_text ltx_font_italic">probabilistic principal component analysis (PPCA).</span> This formulation can be viewed as a statistical variant of regular PCA. Mathematically, we now consider our data as samples from a random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS3.p2.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> taking values in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS3.p2.m2"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> (also sometimes called a <span class="ltx_text ltx_font_italic">random vector</span>). We say that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS3.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has (approximate) low-rank statistical structure if and only if there exists an orthonormal matrix <math alttext="\bm{U}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="S1.SS3.p2.m4"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math>, a random variable <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS3.p2.m5"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> taking values in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S1.SS3.p2.m6"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and a <span class="ltx_text ltx_font_italic">small</span> random variable <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="S1.SS3.p2.m7"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> taking values in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S1.SS3.p2.m8"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> such that <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS3.p2.m9"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="S1.SS3.p2.m10"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> are independent, and</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}." class="ltx_Math" display="block" id="S1.E24.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Our goal is again to recover <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S1.SS3.p2.m11"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>. Towards this end, we set up the analogous problem as in Subsection (<a class="ltx_ref" href="#S1.SS1" title="2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>), i.e., optimizing over subspace supports <math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="S1.SS3.p2.m12"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math> and random variables <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS3.p2.m13"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> to solve the following problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}},\tilde{\bm{z}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}\|_{2}^{2}." class="ltx_Math" display="block" id="S1.E25.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>,</mo><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}},\tilde{\bm{z}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , over~ start_ARG bold_italic_z end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Since we are finding the best such random variable <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S1.SS3.p2.m14"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> we can find its realization <math alttext="\bm{z}(\bm{x})" class="ltx_Math" display="inline" id="S1.SS3.p2.m15"><semantics><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}(\bm{x})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ( bold_italic_x )</annotation></semantics></math> separately for each value of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S1.SS3.p2.m16"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. Performing the same calculations as in Subsection (<a class="ltx_ref" href="#S1.SS1" title="2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>), we obtain</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}},\tilde{\bm{z}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}\|_{2}^{2}=\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\min_{\tilde{\bm{z}}(\bm{x})}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}(\bm{x})\|_{2}^{2}=\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}," class="ltx_Math" display="block" id="S1.E26.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>,</mo><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover></mrow></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></munder><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒛</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}},\tilde{\bm{z}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}\|_{2}^{2}=\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\min_{\tilde{\bm{z}}(\bm{x})}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{z}}(\bm{x})\|_{2}^{2}=\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , over~ start_ARG bold_italic_z end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_z end_ARG ( bold_italic_x ) end_POSTSUBSCRIPT ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG ( bold_italic_x ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">again re-emphasizing the fact that the estimated subspace with principal components <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="S1.SS3.p2.m17"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> corresponds to a denoiser <math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}" class="ltx_Math" display="inline" id="S1.SS3.p2.m18"><semantics><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> which projects onto that subspace. As before, we obtain</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx3">
<tbody id="S1.E27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname*{arg\ min}_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}" class="ltx_Math" display="inline" id="S1.E27.m1"><semantics><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname*{arg\ min}_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}" class="ltx_Math" display="inline" id="S1.E27.m2"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ∥ over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.27)</span></td>
</tr></tbody>
<tbody id="S1.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{tr}(\tilde{\bm{U}}^{\top}\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]\tilde{\bm{U}})," class="ltx_Math" display="inline" id="S1.E28.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mi>tr</mi></mrow><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname*{arg\ max}_{\tilde{\bm{U}}}\operatorname{tr}(\tilde{\bm{U}}^{\top}\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]\tilde{\bm{U}}),</annotation><annotation encoding="application/x-llamapun">= start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT roman_tr ( over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] over~ start_ARG bold_italic_U end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.28)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and the solution to the latter problem is just the top <math alttext="d" class="ltx_Math" display="inline" id="S1.SS3.p2.m19"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> principal components of the second moment matrix <math alttext="\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m20"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math>. Actually, the above problems are visually very similar to the equations for computing the principal components in the previous Subsection, except with <math alttext="\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m21"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math> replacing <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS3.p2.m22"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>. In fact, the latter quantity is an estimate for the former. Both formulations effectively do the same thing, and have the same practical solution—compute the left singular vectors of the data matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS3.p2.m23"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, or equivalently the top eigenvectors of the estimated covariance matrix <math alttext="\bm{X}\bm{X}^{\top}/N" class="ltx_Math" display="inline" id="S1.SS3.p2.m24"><semantics><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo>/</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\bm{X}\bm{X}^{\top}/N</annotation><annotation encoding="application/x-llamapun">bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT / italic_N</annotation></semantics></math>. The statistical formulation, however, has an additional interpretation. Suppose that <math alttext="\operatorname{\mathbb{E}}[\bm{z}]=\bm{0}" class="ltx_Math" display="inline" id="S1.SS3.p2.m25"><semantics><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒛</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{z}]=\bm{0}</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_z ] = bold_0</annotation></semantics></math> and <math alttext="\operatorname{\mathbb{E}}[\bm{\varepsilon}]=\bm{0}" class="ltx_Math" display="inline" id="S1.SS3.p2.m26"><semantics><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝜺</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{\varepsilon}]=\bm{0}</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_ε ] = bold_0</annotation></semantics></math>. We have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{\mathbb{E}}[\bm{x}]=\bm{U}\operatorname{\mathbb{E}}[\bm{z}]+\operatorname{\mathbb{E}}[\bm{\varepsilon}]=\bm{0}," class="ltx_Math" display="block" id="S1.E29.m1"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒛</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝜺</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}]=\bm{U}\operatorname{\mathbb{E}}[\bm{z}]+\operatorname{\mathbb{E}}[\bm{\varepsilon}]=\bm{0},</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x ] = bold_italic_U blackboard_E [ bold_italic_z ] + blackboard_E [ bold_italic_ε ] = bold_0 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">so that <math alttext="\operatorname{Cov}[\bm{x}]=\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m27"><semantics><mrow><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{x}]=\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_x ] = blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math>. Now working out <math alttext="\operatorname{Cov}[\bm{x}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m28"><semantics><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{x}]</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_x ]</annotation></semantics></math> we have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{Cov}[\bm{x}]=\bm{U}\operatorname{Cov}[\bm{z}]\bm{U}^{\top}+\operatorname{Cov}[\bm{\varepsilon}]=\bm{U}\operatorname{\mathbb{E}}[\bm{z}\bm{z}^{\top}]\bm{U}^{\top}+\operatorname{Cov}[\bm{\varepsilon}]." class="ltx_Math" display="block" id="S1.E30.m1"><semantics><mrow><mrow><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒛</mi><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝜺</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒛</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝜺</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{x}]=\bm{U}\operatorname{Cov}[\bm{z}]\bm{U}^{\top}+\operatorname{Cov}[\bm{\varepsilon}]=\bm{U}\operatorname{\mathbb{E}}[\bm{z}\bm{z}^{\top}]\bm{U}^{\top}+\operatorname{Cov}[\bm{\varepsilon}].</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_x ] = bold_italic_U roman_Cov [ bold_italic_z ] bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + roman_Cov [ bold_italic_ε ] = bold_italic_U blackboard_E [ bold_italic_z bold_italic_z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + roman_Cov [ bold_italic_ε ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In particular, if <math alttext="\operatorname{Cov}[\bm{\varepsilon}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m29"><semantics><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝜺</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{\varepsilon}]</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_ε ]</annotation></semantics></math> is small, it holds that <math alttext="\operatorname{Cov}[\bm{x}]=\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m30"><semantics><mrow><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{x}]=\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_x ] = blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math> is approximately a low-rank matrix, in particular rank <math alttext="d" class="ltx_Math" display="inline" id="S1.SS3.p2.m31"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>. Thus the top <math alttext="d" class="ltx_Math" display="inline" id="S1.SS3.p2.m32"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> eigenvectors of <math alttext="\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m33"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒙</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math> essentially summarize the whole covariance matrix. But they are also the principal components, so we can interpret principal component analysis as performing a low-rank decomposition of <math alttext="\operatorname{Cov}[\bm{x}]" class="ltx_Math" display="inline" id="S1.SS3.p2.m34"><semantics><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mi>𝒙</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}[\bm{x}]</annotation><annotation encoding="application/x-llamapun">roman_Cov [ bold_italic_x ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark5.p1">
<p class="ltx_p">By using the probabilistic viewpoint of PCA, we achieve a clearer and more quantitative understanding of how it relates to denoising. First, consider the denoising problem in (<a class="ltx_ref" href="#S1.E26" title="Equation 2.1.26 ‣ 2.1.3 Probabilistic PCA ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.26</span></a>), namely</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}." class="ltx_Math" display="block" id="S1.E31.m1"><semantics><mrow><mrow><munder><mi>min</mi><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover></munder><mo lspace="0.167em">⁡</mo><mrow><mi>𝔼</mi><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msup><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{U}}}\operatorname{\mathbb{E}}\|\bm{x}-\tilde{\bm{U}}\tilde{\bm{U}}^{\top}\bm{x}\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ∥ bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.31)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It is not too hard to prove that if <math alttext="\tilde{\bm{U}}" class="ltx_Math" display="inline" id="Thmremark5.p1.m1"><semantics><mover accent="true"><mi>𝑼</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{U}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_U end_ARG</annotation></semantics></math> has <math alttext="d" class="ltx_Math" display="inline" id="Thmremark5.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> columns and if
<math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="Thmremark5.p1.m3"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> is an isotropic Gaussian
random variable, i.e., with distribution <math alttext="\bm{\varepsilon}\sim\operatorname{\mathcal{N}}(\bm{0},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="Thmremark5.p1.m4"><semantics><mrow><mi>𝜺</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}\sim\operatorname{\mathcal{N}}(\bm{0},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_ε ∼ caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Other distributions work so long as they support
all of <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="footnote3.m1"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, but the Gaussian is the easiest to work with here.</span></span></span> then
for <span class="ltx_text ltx_font_italic">any</span> optimal solution
<math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="Thmremark5.p1.m5"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> to this problem, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}=\bm{U}\bm{U}^{\top}" class="ltx_Math" display="block" id="S1.E32.m1"><semantics><mrow><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}=\bm{U}\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.32)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and so the true supporting subspace, say <math alttext="\mathcal{S}\doteq\mathop{\mathrm{col}}(\bm{U})" class="ltx_Math" display="inline" id="Thmremark5.p1.m6"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo rspace="0.1389em">≐</mo><mrow><mo lspace="0.1389em" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><mi>𝑼</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}\doteq\mathop{\mathrm{col}}(\bm{U})</annotation><annotation encoding="application/x-llamapun">caligraphic_S ≐ roman_col ( bold_italic_U )</annotation></semantics></math>, is
recovered as the span of columns of <math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="Thmremark5.p1.m7"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, since</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{S}=\mathop{\mathrm{col}}(\bm{U})=\mathop{\mathrm{col}}(\bm{U}\bm{U}^{\top})=\mathop{\mathrm{col}}(\bm{U}^{\star}(\bm{U}^{\star})^{\top})=\mathop{\mathrm{col}}(\bm{U}^{\star})." class="ltx_Math" display="block" id="S1.E33.m1"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" movablelimits="false" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><mi>𝑼</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" movablelimits="false" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" movablelimits="false" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" movablelimits="false" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{S}=\mathop{\mathrm{col}}(\bm{U})=\mathop{\mathrm{col}}(\bm{U}\bm{U}^{\top})=\mathop{\mathrm{col}}(\bm{U}^{\star}(\bm{U}^{\star})^{\top})=\mathop{\mathrm{col}}(\bm{U}^{\star}).</annotation><annotation encoding="application/x-llamapun">caligraphic_S = roman_col ( bold_italic_U ) = roman_col ( bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) = roman_col ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) = roman_col ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In particular, the learned <span class="ltx_text ltx_font_italic">denoising map</span> <math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}" class="ltx_Math" display="inline" id="Thmremark5.p1.m8"><semantics><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> is an orthogonal projection onto <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="Thmremark5.p1.m9"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation><annotation encoding="application/x-llamapun">caligraphic_S</annotation></semantics></math>, pushing noisy points onto the ground truth supporting subspace. We can establish a similar technical result in the case where we only have finite samples, as in <a class="ltx_ref" href="#Thmtheorem1" title="Theorem 2.1. ‣ Computing the subspace basis. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Theorem</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>, but this takes more effort and technicality. Summarizing this discussion, we have the following informal Theorem.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="Thmtheorem3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2.3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmtheorem3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that the random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m1"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be written as</span></p>
<table class="ltx_equation ltx_eqn_table" id="S1.E34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}" class="ltx_Math" display="block" id="S1.E34.m1"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.34)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\bm{U}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m2"><semantics><mrow><mi>𝐔</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math> captures the low-rank structure, <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m3"><semantics><mi>𝐳</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>
is a random vector taking values in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m4"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m5"><semantics><mi>𝛆</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> is a random
vector taking values in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m6"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> such that <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m7"><semantics><mi>𝐳</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m8"><semantics><mi>𝛆</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> are
independent, and <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m9"><semantics><mi>𝛆</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> is small. Then the principal components
<math alttext="\bm{U}^{\star}\in\mathsf{O}(D,d)" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m10"><semantics><mrow><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}\in\mathsf{O}(D,d)</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ sansserif_O ( italic_D , italic_d )</annotation></semantics></math> of our dataset are given by the top <math alttext="d" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m11"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>
eigenvectors of <math alttext="\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m12"><semantics><mrow><mi>𝔼</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>𝐱</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐱</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathbb{E}}[\bm{x}\bm{x}^{\top}]</annotation><annotation encoding="application/x-llamapun">blackboard_E [ bold_italic_x bold_italic_x start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math>, and approximately correspond to the
optimal linear denoiser: <math alttext="\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}" class="ltx_Math" display="inline" id="Thmtheorem3.p1.m13"><semantics><mrow><mrow><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝐔</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>≈</mo><mrow><mi>𝐔</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐔</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}(\bm{U}^{\star})^{\top}\approx\bm{U}\bm{U}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ≈ bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1.4 </span>Matrix Completion</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p">In the previous Subsections, we discussed the problem of <span class="ltx_text ltx_font_italic">learning a low-rank geometric or statistical distribution</span>, where the data were sampled from a subspace with additive noise. But this is not the only type of disturbance from a low-dimensional distribution that is worthwhile to study. In this subsection, we introduce one more class of non-additive errors which become increasingly important in deep learning. Let us consider the case where we have some data <math alttext="\{\bm{x}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S1.SS4.p1.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{n}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> generated according to (<a class="ltx_ref" href="#S1.E1" title="Equation 2.1.1 ‣ Problem formulation. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>). Now we arrange them into a matrix <math alttext="\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S1.SS4.p1.m2"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. Unlike before, we do not observe <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> directly; we instead imagine that our observation was corrupted en route and we obtained</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E35">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Y}=\bm{M}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{X}," class="ltx_Math" display="block" id="S1.E35.m1"><semantics><mrow><mrow><mi>𝒀</mi><mo>=</mo><mrow><mi>𝑴</mi><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><mi>𝑿</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Y}=\bm{M}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{X},</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = bold_italic_M ⊙ bold_italic_X ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.35)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{M}\in\{0,1\}^{D\times N}" class="ltx_Math" display="inline" id="S1.SS4.p1.m4"><semantics><mrow><mi>𝑴</mi><mo>∈</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{M}\in\{0,1\}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_M ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic">mask</span> that is known by us,
and <math alttext="\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}" class="ltx_Math" display="inline" id="S1.SS4.p1.m5"><semantics><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><annotation encoding="application/x-tex">\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}</annotation><annotation encoding="application/x-llamapun">⊙</annotation></semantics></math> is element-wise multiplication. In this case, our goal is to recover <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p1.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> (from which point we can use PCA to recover <math alttext="\bm{U}_{o}" class="ltx_Math" display="inline" id="S1.SS4.p1.m7"><semantics><msub><mi>𝑼</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{U}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>, etc), given only the corrupted observation <math alttext="\bm{Y}" class="ltx_Math" display="inline" id="S1.SS4.p1.m8"><semantics><mi>𝒀</mi><annotation encoding="application/x-tex">\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y</annotation></semantics></math>, the mask <math alttext="\bm{M}" class="ltx_Math" display="inline" id="S1.SS4.p1.m9"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math>, and the knowledge that <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p1.m10"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is (approximately) low-rank. This is called <span class="ltx_text ltx_font_italic">low-rank matrix completion</span>.</p>
</div>
<div class="ltx_para" id="S1.SS4.p2">
<p class="ltx_p">There are many excellent resources discussing algorithms and approaches to solve this problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>. Indeed, this and similar generalizations of this low-rank structure recovery problem are resolved by “robust PCA”. We will not go into the solution method here. Instead, we will discuss under what conditions this problem is <span class="ltx_text ltx_font_italic">plausible</span> to solve. On one hand, in the most absurd case, suppose that each entry of the matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> were chosen independently from all the others. Then there would be no hope of recovering <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> exactly even if only one entry were missing and we had <math alttext="DN-1" class="ltx_Math" display="inline" id="S1.SS4.p2.m3"><semantics><mrow><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">DN-1</annotation><annotation encoding="application/x-llamapun">italic_D italic_N - 1</annotation></semantics></math> entries. On the other hand, suppose that we knew that <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p2.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> has rank 1 exactly, which is an extremely strong condition on the low-dimensional structure of the data, and we were dealing with the mask</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E36">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{M}=\begin{bmatrix}\bm{1}_{(D-1)\times 1}&amp;\bm{0}_{(D-1)\times(N-1)}\\
1&amp;\bm{1}_{1\times(N-1)}\end{bmatrix}." class="ltx_Math" display="block" id="S1.E36.m1"><semantics><mrow><mrow><mi>𝑴</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mn>𝟏</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mn>1</mn></mrow></msub></mtd><mtd><msub><mn>𝟎</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo>−</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><msub><mn>𝟏</mn><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{M}=\begin{bmatrix}\bm{1}_{(D-1)\times 1}&amp;\bm{0}_{(D-1)\times(N-1)}\\
1&amp;\bm{1}_{1\times(N-1)}\end{bmatrix}.</annotation><annotation encoding="application/x-llamapun">bold_italic_M = [ start_ARG start_ROW start_CELL bold_1 start_POSTSUBSCRIPT ( italic_D - 1 ) × 1 end_POSTSUBSCRIPT end_CELL start_CELL bold_0 start_POSTSUBSCRIPT ( italic_D - 1 ) × ( italic_N - 1 ) end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL bold_1 start_POSTSUBSCRIPT 1 × ( italic_N - 1 ) end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.1.36)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then we know that the data were distributed on a line, and we know a vector on
that line—it is just the first column of the matrix <math alttext="\bm{Y}=\bm{M}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p2.m5"><semantics><mrow><mi>𝒀</mi><mo>=</mo><mrow><mi>𝑴</mi><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><mi>𝑿</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{Y}=\bm{M}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y = bold_italic_M ⊙ bold_italic_X</annotation></semantics></math>. From the last coordinate of each column, also revealed to us by the mask, we can solve for each column since for each final coordinate there is only one vector on the line with this coordinate. Thus we can reconstruct <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S1.SS4.p2.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> with perfect accuracy, and we only need a linear number of observations <math alttext="D+N-1" class="ltx_Math" display="inline" id="S1.SS4.p2.m7"><semantics><mrow><mrow><mi>D</mi><mo>+</mo><mi>N</mi></mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">D+N-1</annotation><annotation encoding="application/x-llamapun">italic_D + italic_N - 1</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.SS4.p3">
<p class="ltx_p">In the real world, actual problems are somewhere in between the two limiting cases discussed above. Yet the differences between these two extremes, as well as the earlier discussion of PCA, reveal a general kernel of truth:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The lower-dimensional and more structured the data distribution is, the easier it is to process, and the fewer observations are needed—provided that the algorithm effectively utilizes this low-dimensional structure.</span></p>
</blockquote>
<p class="ltx_p">As is perhaps predictable, we will encounter this motif repeatedly in the remainder of the manuscript, starting in the very next section.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.2 </span>A Mixture of Complete Low-Dimensional Subspaces</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">As we have seen, low-rank signal models are rich enough to provide a full picture of the interplay between low-dimensionality in data and efficient and scalable computational algorithms for representation and recovery under errors.
These models imply a <span class="ltx_text ltx_font_italic">linear</span> and symmetric representation learning pipeline (<a class="ltx_ref" href="#S1.E6" title="Equation 2.1.6 ‣ Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.6</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}=\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x},\quad\hat{\bm{x}}=\mathcal{D}(\bm{z})=\bm{U}\bm{z}," class="ltx_Math" display="block" id="S2.Ex1.m1"><semantics><mrow><mrow><mrow><mi>𝒛</mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{z}=\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x},\quad\hat{\bm{x}}=\mathcal{D}(\bm{z})=\bm{U}\bm{z},</annotation><annotation encoding="application/x-llamapun">bold_italic_z = caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x , over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">which can be provably learned from finite samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> with principal component analysis (solved efficiently, say, with the power method) whenever the distribution of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> truly is linear.
This is a restrictive assumption—for as Harold Hotelling, the distinguished 20th century statistican,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>By coincidence, also famous for his contributions to the development and naming of Principal Component Analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx110" title="">Hot33</a>]</cite>.</span></span></span> objected following George Dantzig’s presentation of his theory of linear programming for the first time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx64" title="">Dan02</a>]</cite>,</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">“…we all know the world is nonlinear.”</span></p>
</blockquote>
</div>
<figure class="ltx_figure" id="F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 2.3 : Left: features tracked on independently moving objects in a scene. Right: image patches associated with different regions of an image." class="ltx_graphics ltx_figure_panel ltx_img_square" height="210" id="F3.g1" src="chapters/chapter2/figs/motion.png" width="241"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 2.3 : Left: features tracked on independently moving objects in a scene. Right: image patches associated with different regions of an image." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="209" id="F3.g2" src="chapters/chapter2/figs/segment.png" width="299"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Left:<span class="ltx_text ltx_font_medium"> features tracked on independently moving objects in a scene. </span>Right:<span class="ltx_text ltx_font_medium"> image patches associated with different regions of an image.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">Even accounting for its elegance and simplicity, the low-rank assumption is too restrictive to be broadly applicable to modeling real-world data.
A key limitation is the assumption of a <span class="ltx_text ltx_font_italic">single</span> linear subspace that is responsible for generating the structured observations.
In many practical applications, structure generated by a <span class="ltx_text ltx_font_italic">mixture</span> of
distinct low-dimensional subspaces provides a more realistic model.
For example, consider a video sequence that captures the motion of several distinct objects, each subject to its own independent displacement (Fig. <a class="ltx_ref" href="#F3" title="Figure 2.3 ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3</span></a> left).
Under suitable assumptions on the individual motions, each object becomes responsible for an independent low-dimensional subspace in the concatenated sequence of video frames <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx275" title="">VM04</a>]</cite>.
As another example, consider modeling natural images via learning a model for
the distribution of <span class="ltx_text ltx_font_italic">patches</span>, spatially-contiguous collections of
pixels, within an image (Fig. <a class="ltx_ref" href="#F3" title="Figure 2.3 ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3</span></a> right). Unlike in
the Eigenface example we saw previously, where images of faces with matched
poses can be well-approximated by a single low-dimensional subspace, the patch
at a specific location in a natural image can correspond to objects with very
different properties—for example, distinct color or shape due to occlusion
boundaries. Therefore, modeling the distribution of patches with a single
subspace is futile, but a <span class="ltx_text ltx_font_italic">mixture</span> of subspaces, one for each region,
performs surprisingly well in practice, say for segmenting or compressing
purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx190" title="">MRY+11</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We will return to this observation in
Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, where we will show it can be significantly
generalized to learn representations for large-scale modern datasets.</span></span></span> We will see a concrete example in the next chapter (Example <a class="ltx_ref" href="Ch3.html#Thmexample12" title="Example 3.12 (Image Segmentation). ‣ Optimization strategies to cluster. ‣ 3.3.4 Clustering a Mixture of Low-Dimensional Gaussians ‣ 3.3 Compression via Lossy Coding ‣ Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3.12</span></a>).</p>
</div>
<figure class="ltx_figure" id="F4"><img alt="Figure 2.4 : Data on a mixture of low-dimensional subspaces, say 𝒮 j = col ( 𝑼 j ) \mathcal{S}_{j}=\mathop{\mathrm{col}}(\bm{U}_{j}) caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_col ( bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ." class="ltx_graphics ltx_img_square" height="210" id="F4.g1" src="chapters/chapter2/figs/subspaces.png" width="240"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.4</span>: </span><span class="ltx_text" style="font-size:90%;">Data on a mixture of low-dimensional subspaces, say <math alttext="\mathcal{S}_{j}=\mathop{\mathrm{col}}(\bm{U}_{j})" class="ltx_Math" display="inline" id="F4.m2"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>j</mi></msub><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑼</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}_{j}=\mathop{\mathrm{col}}(\bm{U}_{j})</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = roman_col ( bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">In this section, we will begin by discussing the conceptual and algorithmic
foundations for structured representation learning when the data distribution
can be modeled by <span class="ltx_text ltx_font_italic">a mixture of low-dimensional subspaces</span>, as illustrated in Figure <a class="ltx_ref" href="#F4" title="Figure 2.4 ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.4</span></a>. In this setting, the decoder mapping will be almost as simple as the case of a single subspace: we simply represent via</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}=\mathcal{D}(\bm{z})=\left(\sum_{k=1}^{K}\pi_{k}(\bm{z})\bm{U}_{k}\right)\bm{z}," class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑼</mi><mi>k</mi></msub></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}=\mathcal{D}(\bm{z})=\left(\sum_{k=1}^{K}\pi_{k}(\bm{z})\bm{U}_{k}\right)\bm{z},</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = ( ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_z ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\pi_{k}:\mathbb{R}^{d}\to\{0,1\}" class="ltx_Math" display="inline" id="S2.p3.m1"><semantics><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\pi_{k}:\mathbb{R}^{d}\to\{0,1\}</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → { 0 , 1 }</annotation></semantics></math> are a set of <span class="ltx_text ltx_font_italic">sparse</span> weighting random
variables, such that only a single subspace <math alttext="\mathcal{S}_{k}=\mathop{\mathrm{col}}(\bm{U}_{k})" class="ltx_Math" display="inline" id="S2.p3.m2"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><mi>k</mi></msub><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{S}_{k}=\mathop{\mathrm{col}}(\bm{U}_{k})</annotation><annotation encoding="application/x-llamapun">caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_col ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> is selected in the sum.
However, the task of encoding such data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.p3.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to suitable representations <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.p3.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, and learning such an encoder-decoder pair from data, will prove to be more involved.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p">We will see how ideas from the rich literature on <span class="ltx_text ltx_font_italic">sparse representation</span>
and <span class="ltx_text ltx_font_italic">independent component analysis</span> lead to a natural reformulation of
the above decoder architecture through the lens of sparsity, the corresponding
encoder architecture (obtained through a power-method-like algorithm analogous
to that of principal component analysis), and strong guarantees of correctness
and efficiency for learning such encoder-decoder pairs from data. In this sense,
the case of mixed linear low-dimensional structure already leads to many of the
key conceptual components of structured representation learning that we will develop in
far greater generality in this book.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2.1 </span>Mixtures of Subspaces and Sparsely-Used
Dictionaries</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Let <math alttext="\bm{U}_{1},\dots,\bm{U}_{K}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1"><semantics><mrow><msub><mi>𝑼</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑼</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">\bm{U}_{1},\dots,\bm{U}_{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math>, each of size <math alttext="D\times d" class="ltx_Math" display="inline" id="S2.SS1.p1.m2"><semantics><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">D\times d</annotation><annotation encoding="application/x-llamapun">italic_D × italic_d</annotation></semantics></math>, denote a collection of orthonormal bases for <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p1.m3"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> subspaces of dimension <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p1.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS1.p1.m5"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.
To say that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> follows a mixture-of-subspaces distribution parameterized by <math alttext="\bm{U}_{1},\dots,\bm{U}_{K}" class="ltx_Math" display="inline" id="S2.SS1.p1.m7"><semantics><mrow><msub><mi>𝑼</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑼</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">\bm{U}_{1},\dots,\bm{U}_{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math> means, geometrically speaking,
that</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{U}_{k}\bm{z}\quad\text{for some}\enspace k\in[K],\enspace\bm{z}\in\mathbb{R}^{d}." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><mrow><mtext>for some</mtext><mo lspace="0.500em" rspace="0em">​</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}_{k}\bm{z}\quad\text{for some}\enspace k\in[K],\enspace\bm{z}\in\mathbb{R}^{d}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z for some italic_k ∈ [ italic_K ] , bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The statistical analogue of this geometric model, as we saw for the case of PCA and linear structure,
is that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> follows a <span class="ltx_text ltx_font_italic">mixture of Gaussians</span> distribution: that is,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\sim\sum_{k=1}^{K}\pi_{k}\mathcal{N}(\mathbf{0},\bm{U}_{k}\bm{U}_{k}^{\top}),\quad\text{for some}\enspace\pi_{k}\geq 0,\enspace\sum_{k=1}^{K}\pi_{k}=1." class="ltx_Math" display="block" id="S2.E3.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo rspace="0.111em">∼</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><msub><mi>π</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msub><mi>𝑼</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>k</mi><mo>⊤</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><mtext>for some</mtext><mo lspace="0.500em" rspace="0em">​</mo><msub><mi>π</mi><mi>k</mi></msub></mrow><mo>≥</mo><mn>0</mn></mrow><mo rspace="0.500em">,</mo><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>π</mi><mi>k</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\sim\sum_{k=1}^{K}\pi_{k}\mathcal{N}(\mathbf{0},\bm{U}_{k}\bm{U}_{k}^{\top}),\quad\text{for some}\enspace\pi_{k}\geq 0,\enspace\sum_{k=1}^{K}\pi_{k}=1.</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) , for some italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≥ 0 , ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 1 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In other words, for each <math alttext="k\in[K]" class="ltx_Math" display="inline" id="S2.SS1.p1.m9"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">k\in[K]</annotation><annotation encoding="application/x-llamapun">italic_k ∈ [ italic_K ]</annotation></semantics></math>, <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p1.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is Gaussian on the low-dimensional
subspace <math alttext="\mathop{\mathrm{col}}(\bm{U}_{k})" class="ltx_Math" display="inline" id="S2.SS1.p1.m11"><semantics><mrow><mo rspace="0em">col</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑼</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{col}}(\bm{U}_{k})</annotation><annotation encoding="application/x-llamapun">roman_col ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> with probability <math alttext="\pi_{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.m12"><semantics><msub><mi>π</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\pi_{k}</annotation><annotation encoding="application/x-llamapun">italic_π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.6</span></span><span class="ltx_text ltx_font_italic"> </span>(A Mixture of Gaussians v.s. A Superposition of Gaussians)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark6.p1">
<p class="ltx_p">One should be aware that the above model
(<a class="ltx_ref" href="#S2.E3" title="Equation 2.2.3 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>) is a <span class="ltx_text ltx_font_italic">mixture</span> of
Gaussian distributions, not to be confused with a mixture of Gaussian
variables by superposition, say</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\sum_{i=1}^{n}w_{i}\bm{x}_{i},\quad\bm{x}_{i}\sim\mathcal{N}(\mathbf{0},\bm{U}_{i}\bm{U}_{i}^{\top})," class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msub><mi>𝑼</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑼</mi><mi>i</mi><mo>⊤</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\sum_{i=1}^{n}w_{i}\bm{x}_{i},\quad\bm{x}_{i}\sim\mathcal{N}(\mathbf{0},\bm{U}_{i}\bm{U}_{i}^{\top}),</annotation><annotation encoding="application/x-llamapun">bold_italic_x = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="Thmremark6.p1.m1"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are independent random Gaussian vectors and <math alttext="w_{i}" class="ltx_Math" display="inline" id="Thmremark6.p1.m2"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_{i}</annotation><annotation encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are a set of fixed weights. As we know from the properties of Gaussian vectors, such a superposition <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmremark6.p1.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> will remain a Gaussian distribution.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">For now, we focus on the geometric perspective offered by (<a class="ltx_ref" href="#S2.E2" title="Equation 2.2.2 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>).
There is an algebraically convenient alternative to this conditional representation. Consider a <span class="ltx_text ltx_font_italic">lifted</span> representation vector <math alttext="\bm{z}=[\bm{z}_{1}^{\top},\dots,\bm{z}_{K}^{\top}]^{\top}\in\mathbb{R}^{dK}" class="ltx_Math" display="inline" id="S2.SS1.p2.m1"><semantics><mrow><mi>𝒛</mi><mo>=</mo><msup><mrow><mo stretchy="false">[</mo><msubsup><mi>𝒛</mi><mn>1</mn><mo>⊤</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒛</mi><mi>K</mi><mo>⊤</mo></msubsup><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{z}=[\bm{z}_{1}^{\top},\dots,\bm{z}_{K}^{\top}]^{\top}\in\mathbb{R}^{dK}</annotation><annotation encoding="application/x-llamapun">bold_italic_z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>, such that <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.p2.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math><span class="ltx_text ltx_font_italic">-sparse</span> with support on one of the <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p2.m4"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> consecutive non-overlapping blocks of <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> coordinates out of <math alttext="dK" class="ltx_Math" display="inline" id="S2.SS1.p2.m6"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">dK</annotation><annotation encoding="application/x-llamapun">italic_d italic_K</annotation></semantics></math>.
Then (<a class="ltx_ref" href="#S2.E2" title="Equation 2.2.2 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>) can be written equivalently as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\underbrace{\begin{bmatrix}|&amp;\ldots&amp;|\\
\bm{U}_{1}&amp;\ldots&amp;\bm{U}_{K}\\
|&amp;\ldots&amp;|\end{bmatrix}}_{\bm{U}}\underbrace{\begin{bmatrix}\bm{z}_{1}\\
\vdots\\
\bm{z}_{K}\end{bmatrix}}_{\bm{z}},\quad\left\|\begin{bmatrix}\left\|\bm{z}_{1}\right\|_{2}\\
\vdots\\
\left\|\bm{z}_{K}\right\|_{2}\end{bmatrix}\right\|_{0}=1." class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><mo>[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd><mo fence="false" stretchy="false">|</mo></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><mo fence="false" stretchy="false">|</mo></mtd></mtr><mtr><mtd><msub><mi>𝑼</mi><mn>1</mn></msub></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><msub><mi>𝑼</mi><mi>K</mi></msub></mtd></mtr><mtr><mtd><mo fence="false" stretchy="false">|</mo></mtd><mtd><mi mathvariant="normal">…</mi></mtd><mtd><mo fence="false" stretchy="false">|</mo></mtd></mtr></mtable><mo>]</mo></mrow><mo>⏟</mo></munder><mi>𝑼</mi></munder><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mi>𝒛</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msub><mi>𝒛</mi><mi>K</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>⏟</mo></munder><mi>𝒛</mi></munder></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mrow><mo>‖</mo><mrow><mo>[</mo><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd><msub><mrow><mo>‖</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>‖</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd><msub><mrow><mo>‖</mo><msub><mi>𝒛</mi><mi>K</mi></msub><mo>‖</mo></mrow><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>‖</mo></mrow><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\underbrace{\begin{bmatrix}|&amp;\ldots&amp;|\\
\bm{U}_{1}&amp;\ldots&amp;\bm{U}_{K}\\
|&amp;\ldots&amp;|\end{bmatrix}}_{\bm{U}}\underbrace{\begin{bmatrix}\bm{z}_{1}\\
\vdots\\
\bm{z}_{K}\end{bmatrix}}_{\bm{z}},\quad\left\|\begin{bmatrix}\left\|\bm{z}_{1}\right\|_{2}\\
\vdots\\
\left\|\bm{z}_{K}\right\|_{2}\end{bmatrix}\right\|_{0}=1.</annotation><annotation encoding="application/x-llamapun">bold_italic_x = under⏟ start_ARG [ start_ARG start_ROW start_CELL | end_CELL start_CELL … end_CELL start_CELL | end_CELL end_ROW start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL | end_CELL start_CELL … end_CELL start_CELL | end_CELL end_ROW end_ARG ] end_ARG start_POSTSUBSCRIPT bold_italic_U end_POSTSUBSCRIPT under⏟ start_ARG [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] end_ARG start_POSTSUBSCRIPT bold_italic_z end_POSTSUBSCRIPT , ∥ [ start_ARG start_ROW start_CELL ∥ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ⋮ end_CELL end_ROW start_ROW start_CELL ∥ bold_italic_z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="S2.SS1.p2.m7"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> “norm” <math alttext="\|\,\cdot\,\|_{0}" class="ltx_math_unparsed" display="inline" id="S2.SS1.p2.m8"><semantics><mrow><mo rspace="0em">∥</mo><mo>⋅</mo><msub><mo lspace="0em">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\,\cdot\,\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ ⋅ ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> measures sparsity by counting the number of nonzero entries:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\|\bm{z}\|_{0}=\left\lvert\{i\mid z_{i}\neq 0\}\right\rvert," class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>=</mo><mrow><mo>|</mo><mrow><mo stretchy="false">{</mo><mi>i</mi><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>≠</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow><mo>|</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\|\bm{z}\|_{0}=\left\lvert\{i\mid z_{i}\neq 0\}\right\rvert,</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = | { italic_i ∣ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≠ 0 } | ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and the matrix <math alttext="\bm{U}\in\mathbb{R}^{D\times Kd}" class="ltx_Math" display="inline" id="S2.SS1.p2.m9"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathbb{R}^{D\times Kd}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_K italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is called a <span class="ltx_text ltx_font_italic">dictionary</span> with all the <math alttext="\{\bm{U}_{i}\}_{i=1}^{K}" class="ltx_Math" display="inline" id="S2.SS1.p2.m10"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝑼</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\bm{U}_{i}\}_{i=1}^{K}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math> as code words. In general, if the number of subspaces in the mixture <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p2.m11"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> is large enough, there is no bound on the number of columns contained in the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.p2.m12"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>. In the case where <math alttext="Kd&lt;D" class="ltx_Math" display="inline" id="S2.SS1.p2.m13"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo>&lt;</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">Kd&lt;D</annotation><annotation encoding="application/x-llamapun">italic_K italic_d &lt; italic_D</annotation></semantics></math>, <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.p2.m14"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is called <span class="ltx_text ltx_font_italic">undercomplete</span>;
when <math alttext="Kd=D" class="ltx_Math" display="inline" id="S2.SS1.p2.m15"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo>=</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">Kd=D</annotation><annotation encoding="application/x-llamapun">italic_K italic_d = italic_D</annotation></semantics></math>, it is called <span class="ltx_text ltx_font_italic">complete</span>; and when <math alttext="Kd&gt;D" class="ltx_Math" display="inline" id="S2.SS1.p2.m16"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo>&gt;</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">Kd&gt;D</annotation><annotation encoding="application/x-llamapun">italic_K italic_d &gt; italic_D</annotation></semantics></math>, it is called <span class="ltx_text ltx_font_italic">overcomplete</span>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">Now, (<a class="ltx_ref" href="#S2.E5" title="Equation 2.2.5 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.5</span></a>) suggests a convenient relaxation for tractability of analysis: rather than modeling <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> as coming from a mixture of <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p3.m2"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> <span class="ltx_text ltx_font_italic">specific</span> subspaces <math alttext="\bm{U}_{1},\dots,\bm{U}_{K}" class="ltx_Math" display="inline" id="S2.SS1.p3.m3"><semantics><mrow><msub><mi>𝑼</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑼</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">\bm{U}_{1},\dots,\bm{U}_{K}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math>, we may instead start with a dictionary <math alttext="\bm{U}\in\mathbb{R}^{D\times m}" class="ltx_Math" display="inline" id="S2.SS1.p3.m4"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathbb{R}^{D\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="m" class="ltx_Math" display="inline" id="S2.SS1.p3.m5"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math> may be smaller or larger than <math alttext="D" class="ltx_Math" display="inline" id="S2.SS1.p3.m6"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>, and simply seek to represent <math alttext="\bm{x}=\bm{U}\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.p3.m7"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z</annotation></semantics></math> with the sparsity <math alttext="\|\bm{z}\|_{0}" class="ltx_Math" display="inline" id="S2.SS1.p3.m8"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><annotation encoding="application/x-tex">\|\bm{z}\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> sufficiently small.
This leads to the <span class="ltx_text ltx_font_italic">sparse dictionary model</span> for <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p3.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon},\quad\|\bm{z}\|_{0}\ll d," class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>≪</mo><mi>d</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon},\quad\|\bm{z}\|_{0}\ll d,</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≪ italic_d ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="S2.SS1.p3.m10"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> represents an unknown noise vector.
Geometrically, this implies that <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.p3.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> lies close to the span of a subset of <math alttext="\|\bm{z}\|_{0}" class="ltx_Math" display="inline" id="S2.SS1.p3.m12"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><annotation encoding="application/x-tex">\|\bm{z}\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> columns of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.p3.m13"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>,
making this an instantiation of the mixture-of-subspaces model (<a class="ltx_ref" href="#S2.E2" title="Equation 2.2.2 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>) with a very large value of <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p3.m14"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math>, and specific correlations between the subspaces <math alttext="\bm{U}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p3.m15"><semantics><msub><mi>𝑼</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\bm{U}_{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Orthogonal dictionary for sparse coding.</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">Now we can formulate the structured representation learning problem for mixtures of low-dimensional subspaces that we will study in this section.
We assume that we have samples <math alttext="\bm{X}=[\bm{x}_{1},\dots\bm{x}_{N}]" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> from an unknown sparse dictionary model (<a class="ltx_ref" href="#S2.E7" title="Equation 2.2.7 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.7</span></a>), possibly with added noises <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m2"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
Let us begin from the assumption that the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> in the sparse
dictionary model (<a class="ltx_ref" href="#S2.E7" title="Equation 2.2.7 ‣ 2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.7</span></a>) is complete and
orthogonal,<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>It can be shown that for the complete case, we do not lose any generality by making the orthogonal assumption (Exercise <a class="ltx_ref" href="#Thmexercise4" title="Exercise 2.4. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.4</span></a>).</span></span></span> and that each coefficient vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-sparse, with <math alttext="d\ll D" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m6"><semantics><mrow><mi>d</mi><mo>≪</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d\ll D</annotation><annotation encoding="application/x-llamapun">italic_d ≪ italic_D</annotation></semantics></math>.
In this setting, representation learning amounts to correctly learning the orthogonal dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m7"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> via optimization: we
can then take <math alttext="\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m8"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x}</annotation><annotation encoding="application/x-llamapun">caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x</annotation></semantics></math> as the encoder and <math alttext="\mathcal{D}(\bm{z})=\bm{U}\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m9"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}(\bm{z})=\bm{U}\bm{z}</annotation><annotation encoding="application/x-llamapun">caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z</annotation></semantics></math>
as the decoder, and <math alttext="\mathcal{D}=\mathcal{E}^{-1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m10"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><msup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}=\mathcal{E}^{-1}</annotation><annotation encoding="application/x-llamapun">caligraphic_D = caligraphic_E start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>. In diagram form:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}=\bm{U}^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}\hskip 5.69054pt}\hat{\bm{x}}." class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>=</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>𝑼</mi></mrow></mover><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}=\bm{U}^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{U}\hskip 5.69054pt}\hat{\bm{x}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_x end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We see that the autoencoding pair <math alttext="(\mathcal{E},\mathcal{D})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m11"><semantics><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{E},\mathcal{D})</annotation><annotation encoding="application/x-llamapun">( caligraphic_E , caligraphic_D )</annotation></semantics></math> for complete dictionary learning is symmetric, as in the case of a single linear subspace, making the computational task of encoding and decoding no more difficult than in the linear case. On the other hand, the task of learning the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m12"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is strictly more difficult than learning a single linear subspace by PCA.
To see why we cannot simply use PCA to learn the orthogonal dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m13"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> correctly, note that the
loss function that gave rise to PCA, namely (<a class="ltx_ref" href="#S1.E5" title="Equation 2.1.5 ‣ Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>), is
completely invariant to rotations of the rows of the matrix <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m14"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>: that is, if
<math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m15"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> is any <math alttext="d\times d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m16"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d\times d</annotation><annotation encoding="application/x-llamapun">italic_d × italic_d</annotation></semantics></math> orthogonal matrix, then <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m17"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> and <math alttext="\bm{U}\bm{Q}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m18"><semantics><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><annotation encoding="application/x-tex">\bm{U}\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_U bold_italic_Q</annotation></semantics></math> are both
feasible and have an identical loss for (<a class="ltx_ref" href="#S1.E5" title="Equation 2.1.5 ‣ Subspace encoding-decoding via denoising. ‣ 2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>). The
sparse dictionary model is decidedly not invariant to such transformations: if
we replaced <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m19"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> by <math alttext="\bm{U}\bm{Q}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m20"><semantics><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><annotation encoding="application/x-tex">\bm{U}\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_U bold_italic_Q</annotation></semantics></math> and made a corresponding rotation <math alttext="\bm{Q}^{\top}\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m21"><semantics><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">\bm{Q}^{\top}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_z</annotation></semantics></math>
of the representation coefficients <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m22"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, we would in general destroy the sparsity structure of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m23"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, violating the modeling assumption. Thus, we need new algorithms for learning orthogonal dictionaries.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2.2 </span>Complete Dictionary Learning</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">In this section, we will derive algorithms for solving the orthogonal dictionary learning problem. To be more precise, we assume that the observed vector <math alttext="\bm{x}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS2.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> follows a statistical model</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon}," class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}+\bm{\varepsilon},</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z + bold_italic_ε ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{U}\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S2.SS2.p1.m2"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is an unknown orthogonal dictionary, <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS2.p1.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is a random vector with statistically independent components <math alttext="z_{i}" class="ltx_Math" display="inline" id="S2.SS2.p1.m4"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, each with zero mean, and <math alttext="\bm{\varepsilon}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS2.p1.m5"><semantics><mrow><mi>𝜺</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\varepsilon}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is an independent random vector of small (Gaussian) noises. The goal is to recover <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS2.p1.m6"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> (and hence <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS2.p1.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>) from samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">Here we assume that each independent component <math alttext="z_{i}" class="ltx_Math" display="inline" id="S2.SS2.p2.m1"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is distributed as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{i}\sim\mathrm{Bern}(\theta)\cdot\mathcal{N}(0,1/\theta)." class="ltx_Math" display="block" id="S2.Ex2.m1"><semantics><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∼</mo><mrow><mrow><mrow><mi>Bern</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mrow><mn>1</mn><mo>/</mo><mi>θ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">z_{i}\sim\mathrm{Bern}(\theta)\cdot\mathcal{N}(0,1/\theta).</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ roman_Bern ( italic_θ ) ⋅ caligraphic_N ( 0 , 1 / italic_θ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, it is the product of a Bernoulli random variable with probability <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.m2"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> of being <math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.p2.m3"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> and <math alttext="1-\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.m4"><semantics><mrow><mn>1</mn><mo>−</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">1-\theta</annotation><annotation encoding="application/x-llamapun">1 - italic_θ</annotation></semantics></math> of being <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.p2.m5"><mn>0</mn></math>, and an independent Gaussian random variable with variance <math alttext="1/\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.m6"><semantics><mrow><mn>1</mn><mo>/</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">1/\theta</annotation><annotation encoding="application/x-llamapun">1 / italic_θ</annotation></semantics></math>. This distribution is formally known as the <span class="ltx_text ltx_font_italic">Bernoulli-Gaussian</span> distribution.
The normalization is chosen so that <math alttext="\operatorname{Var}(z_{i})=1" class="ltx_Math" display="inline" id="S2.SS2.p2.m7"><semantics><mrow><mrow><mi>Var</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\operatorname{Var}(z_{i})=1</annotation><annotation encoding="application/x-llamapun">roman_Var ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 1</annotation></semantics></math> and hence <math alttext="\mathbb{E}[\|\bm{z}\|_{2}^{2}]=d" class="ltx_Math" display="inline" id="S2.SS2.p2.m8"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\mathbb{E}[\|\bm{z}\|_{2}^{2}]=d</annotation><annotation encoding="application/x-llamapun">blackboard_E [ ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = italic_d</annotation></semantics></math>.
This modeling assumption implies that the vector of independent components <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S2.SS2.p2.m9"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is typically very sparse:
we calculate <math alttext="\mathbb{E}\left[\|\bm{z}\|_{0}\right]=d\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.m10"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>θ</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}\left[\|\bm{z}\|_{0}\right]=d\theta</annotation><annotation encoding="application/x-llamapun">blackboard_E [ ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] = italic_d italic_θ</annotation></semantics></math>, which is small when <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.m11"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation><annotation encoding="application/x-llamapun">italic_θ</annotation></semantics></math> is inversely proportional to <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.p2.m12"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.7</span></span><span class="ltx_text ltx_font_italic"> </span>(The Orthogonal Assumption)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark7.p1">
<p class="ltx_p">At first sight, the assumption that the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="Thmremark7.p1.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is orthogonal might seem to be somewhat restrictive. But there is actually no loss of generality. One may consider a complete dictionary to be any square invertible matrix <math alttext="\bm{U}" class="ltx_Math" display="inline" id="Thmremark7.p1.m2"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>. With samples generated from this dictionary: <math alttext="\bm{X}=\bm{U}\bm{Z}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="Thmremark7.p1.m3"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{Z}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, it is easy to show that with some preconditioning of the data matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="Thmremark7.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{X}}=\Big{(}\frac{1}{N\theta}\bm{X}\bm{X}^{\top}\Big{)}^{-\frac{1}{2}}\bm{X}," class="ltx_Math" display="block" id="S2.E10.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝑿</mi><mo>¯</mo></mover><mo>=</mo><mrow><msup><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mfrac><mn>1</mn><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mi>θ</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{X}}=\Big{(}\frac{1}{N\theta}\bm{X}\bm{X}^{\top}\Big{)}^{-\frac{1}{2}}\bm{X},</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_X end_ARG = ( divide start_ARG 1 end_ARG start_ARG italic_N italic_θ end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT bold_italic_X ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">then there exists an orthogonal matrix <math alttext="\bm{U}_{o}\in\mathsf{O}(D)" class="ltx_Math" display="inline" id="Thmremark7.p1.m5"><semantics><mrow><msub><mi>𝑼</mi><mi>o</mi></msub><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}_{o}\in\mathsf{O}(D)</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∈ sansserif_O ( italic_D )</annotation></semantics></math> such that</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bar{\bm{X}}=\bm{U}_{o}\bm{Z}." class="ltx_Math" display="block" id="S2.E11.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝑿</mi><mo>¯</mo></mover><mo>=</mo><mrow><msub><mi>𝑼</mi><mi>o</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bar{\bm{X}}=\bm{U}_{o}\bm{Z}.</annotation><annotation encoding="application/x-llamapun">over¯ start_ARG bold_italic_X end_ARG = bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">See <a class="ltx_ref" href="#Thmexercise4" title="Exercise 2.4. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Exercise</span> <span class="ltx_text ltx_ref_tag">2.4</span></a> or <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx255" title="">SQW17a</a>]</cite> for more details.</p>
</div>
</div>
<figure class="ltx_figure" id="F5"><img alt="Figure 2.5 : Maximizing ℓ 4 \ell^{4} roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT norm or minimizing ℓ 1 \ell^{1} roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm promotes sparsity (for vectors on the sphere)." class="ltx_graphics ltx_img_landscape" height="269" id="F5.g1" src="chapters/chapter2/figs/2DL4Sphere.png" width="359"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.5</span>: </span><span class="ltx_text" style="font-size:90%;">Maximizing <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="F5.m3"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> norm or minimizing <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="F5.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm promotes sparsity (for vectors on the sphere).</span></figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dictionary learning via the MSP algorithm.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p">Now suppose that we are given a set of observations:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\ \forall i\in[N]." class="ltx_Math" display="block" id="S2.E12.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi>𝜺</mi><mi>i</mi></msub></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{i}=\bm{U}\bm{z}_{i}+\bm{\varepsilon}_{i},\ \forall i\in[N].</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ∀ italic_i ∈ [ italic_N ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let <math alttext="\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="\bm{Z}=[\bm{z}_{1},\dots,\bm{z}_{N}]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m2"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\dots,\bm{z}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>. The goal is to recover <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> from the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. Therefore, given any orthogonal matrix <math alttext="\bm{A}\in\mathsf{O}(D)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m5"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathsf{O}(D)</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ sansserif_O ( italic_D )</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}\bm{x}_{i}=\bm{A}\bm{U}\bm{z}_{i}+\bm{A}\bm{\varepsilon}_{i}" class="ltx_Math" display="block" id="S2.E13.m1"><semantics><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝜺</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{A}\bm{x}_{i}=\bm{A}\bm{U}\bm{z}_{i}+\bm{A}\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_A bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">would be nearly sparse if <math alttext="\bm{A}=\bm{U}^{T}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m6"><semantics><mrow><mi>𝑨</mi><mo>=</mo><msup><mi>𝑼</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\bm{A}=\bm{U}^{T}</annotation><annotation encoding="application/x-llamapun">bold_italic_A = bold_italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> (as by assumption the noise <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m7"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is of small magnitude).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p">Also, given <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is orthogonal and the fact <math alttext="\bm{\varepsilon}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m2"><semantics><mi>𝜺</mi><annotation encoding="application/x-tex">\bm{\varepsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε</annotation></semantics></math> is small, the vector <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> has a predictable expected norm, i.e., <math alttext="\mathbb{E}[\|\bm{x}\|_{2}^{2}]\approx\mathbb{E}[\|\bm{z}\|_{2}^{2}]=d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m4"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒙</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>≈</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\mathbb{E}[\|\bm{x}\|_{2}^{2}]\approx\mathbb{E}[\|\bm{z}\|_{2}^{2}]=d</annotation><annotation encoding="application/x-llamapun">blackboard_E [ ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≈ blackboard_E [ ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = italic_d</annotation></semantics></math>. It is a known fact that for vectors on a sphere, maximizing the <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m5"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> norm is equivalent to minimizing the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m6"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> norm (for promoting sparsity),</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname*{arg\ max}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{4}\quad\Leftrightarrow\quad\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{0}." class="ltx_Math" display="block" id="S2.E14.m1"><semantics><mrow><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>max</mi></mrow><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>𝕊</mi><mi>n</mi></msup></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>4</mn></msub></mrow><mspace width="1em"></mspace><mo stretchy="false">⇔</mo><mspace width="1em"></mspace><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>𝕊</mi><mi>n</mi></msup></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname*{arg\ max}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{4}\quad\Leftrightarrow\quad\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{0}.</annotation><annotation encoding="application/x-llamapun">start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ⇔ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is illustrated in Figure <a class="ltx_ref" href="#F5" title="Figure 2.5 ‣ 2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p3">
<p class="ltx_p">An orthogonal matrix <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> preserves the Euclidean (<math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>) norm: <math alttext="\|\bm{A}\bm{x}\|_{2}^{2}=\|\bm{x}\|_{2}^{2}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m3"><semantics><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒙</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\|\bm{A}\bm{x}\|_{2}^{2}=\|\bm{x}\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_A bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. Therefore, to find the correct orthogonal dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m4"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> from <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we may try to solve the following optimization program:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\tilde{\bm{A}}\in\mathsf{O}(D)}\,\frac{1}{4}\left\|\tilde{\bm{A}}\bm{X}\right\|_{4}^{4}=\frac{1}{4}\sum_{i=1}^{N}\left\|\tilde{\bm{A}}\bm{x}_{i}\right\|_{4}^{4}" class="ltx_Math" display="block" id="S2.E15.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo lspace="0.337em">⁡</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo>‖</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mrow><mo>‖</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>i</mi></msub></mrow><mo>‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\max_{\tilde{\bm{A}}\in\mathsf{O}(D)}\,\frac{1}{4}\left\|\tilde{\bm{A}}\bm{X}\right\|_{4}^{4}=\frac{1}{4}\sum_{i=1}^{N}\left\|\tilde{\bm{A}}\bm{x}_{i}\right\|_{4}^{4}</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG ∈ sansserif_O ( italic_D ) end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∥ over~ start_ARG bold_italic_A end_ARG bold_italic_X ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∥ over~ start_ARG bold_italic_A end_ARG bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is known as the <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m6"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> maximization problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx317" title="">ZMZ+20</a>]</cite>. After we
find the solution <math alttext="\bm{A}^{\star}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m7"><semantics><msup><mi>𝑨</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{A}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, we can take the transpose <math alttext="\bm{U}^{\star}=(\bm{A}^{\star})^{\top}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p3.m8"><semantics><mrow><msup><mi>𝑼</mi><mo>⋆</mo></msup><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mo>⋆</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><annotation encoding="application/x-tex">\bm{U}^{\star}=(\bm{A}^{\star})^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT = ( bold_italic_A start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.8</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark8.p1">
<p class="ltx_p">It is also known that for vectors on a sphere, minimizing the <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="Thmremark8.p1.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm is equivalent to minimizing the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="Thmremark8.p1.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> norm (for promoting sparsity),</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{1}\quad\Leftrightarrow\quad\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{0}," class="ltx_Math" display="block" id="S2.Ex3.m1"><semantics><mrow><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>𝕊</mi><mi>n</mi></msup></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mspace width="1em"></mspace><mo stretchy="false">⇔</mo><mspace width="1em"></mspace><mrow><munder><mrow><mi>arg</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>min</mi></mrow><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>𝕊</mi><mi>n</mi></msup></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{1}\quad\Leftrightarrow\quad\operatorname*{arg\ min}_{\bm{z}\in\mathbb{S}^{n}}\|\bm{z}\|_{0},</annotation><annotation encoding="application/x-llamapun">start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⇔ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">which is also illustrated in Figure <a class="ltx_ref" href="#F5" title="Figure 2.5 ‣ 2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5</span></a>. This fact can also be exploited to learn the dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmremark8.p1.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> effectively and efficiently. This was actually explored earlier than the <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark8.p1.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> norm used here. Interested readers may refer to the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx218" title="">QZL+20a</a>]</cite>.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p4">
<p class="ltx_p">Note that the above problem is equivalent to the following constrained optimization problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\,-\frac{1}{4}\left\|\tilde{\bm{A}}\bm{X}\right\|_{4}^{4}\quad\mbox{subject to}\quad\tilde{\bm{A}}^{\top}\tilde{\bm{A}}=\bm{I}." class="ltx_Math" display="block" id="S2.E16.m1"><semantics><mrow><mrow><mrow><mrow><mi>min</mi><mo lspace="0.170em">−</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo>‖</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow></mrow><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><mrow><msup><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>=</mo><mi>𝑰</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min\,-\frac{1}{4}\left\|\tilde{\bm{A}}\bm{X}\right\|_{4}^{4}\quad\mbox{subject to}\quad\tilde{\bm{A}}^{\top}\tilde{\bm{A}}=\bm{I}.</annotation><annotation encoding="application/x-llamapun">roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∥ over~ start_ARG bold_italic_A end_ARG bold_italic_X ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT subject to over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_A end_ARG = bold_italic_I .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>, using the Lagrange multiplier method, one can derive that the optimal solution to the problem should satisfy the following
“fixed point” condition:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}^{\star}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{A}^{\star}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}]," class="ltx_Math" display="block" id="S2.E17.m1"><semantics><mrow><mrow><msup><mi>𝑨</mi><mo>⋆</mo></msup><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑨</mi><mo>⋆</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{A}^{\star}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{A}^{\star}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}],</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ ( bold_italic_A start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{P}_{\mathrm{O}(D)}[\,\cdot\,]" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p4.m1"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mo>⋅</mo><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{\mathrm{O}(D)}[\,\cdot\,]</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ ⋅ ]</annotation></semantics></math> is a projection onto the space of
orthogonal matrices <math alttext="\mathrm{O}(D)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p4.m2"><semantics><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{O}(D)</annotation><annotation encoding="application/x-llamapun">roman_O ( italic_D )</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>For any matrix <math alttext="\bm{M}" class="ltx_Math" display="inline" id="footnote7.m1"><semantics><mi>𝑴</mi><annotation encoding="application/x-tex">\bm{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_M</annotation></semantics></math> with SVD <math alttext="\bm{M}=\bm{U}\bm{\Sigma}\bm{V}^{\top}" class="ltx_Math" display="inline" id="footnote7.m2"><semantics><mrow><mi>𝑴</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝚺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{M}=\bm{U}\bm{\Sigma}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_M = bold_italic_U bold_Σ bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\mathcal{P}_{\mathrm{O}(D)}[\bm{M}]=\bm{U}\bm{V}^{\top}" class="ltx_Math" display="inline" id="footnote7.m3"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>𝑴</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{\mathrm{O}(D)}[\bm{M}]=\bm{U}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ bold_italic_M ] = bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>. We leave this as an exercise for the reader.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p5">
<p class="ltx_p">To compute the fixed point for the above equation, similar to how we computed
eigenvectors for PCA (<a class="ltx_ref" href="#S1.E16" title="Equation 2.1.16 ‣ 2.1.2 Pursuing Low-rank Structure via Power Iteration ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.16</span></a>), we may take the following
power iteration:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{A}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{A}_{t}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}]." class="ltx_Math" display="block" id="S2.E18.m1"><semantics><mrow><mrow><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑨</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{A}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{A}_{t}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}].</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ ( bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is known as the <span class="ltx_text ltx_font_italic">matching, stretching, and projection</span> (MSP) algorithm proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx317" title="">ZMZ+20</a>]</cite>. It was shown that under broad conditions such a greedy algorithm indeed converges to the correct solution at a superlinear rate.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.9</span></span><span class="ltx_text ltx_font_italic"> </span>(Global Optimality of <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark9.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> Maximization)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark9.p1">
<p class="ltx_p">The constrained <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark9.p1.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> maximization problem is a nonconvex program. In general one should <span class="ltx_text ltx_font_italic">not</span> expect that any greedy (say gradient-descent type) algorithms would converge to the globally optimal solution. Surprisingly, one can show that, unlike general nonconvex programs, the landscape of <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark9.p1.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> maximization over a sphere</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\,-\frac{1}{4}\left\|\bm{q}^{\top}\bm{X}\right\|_{4}^{4}\quad\mbox{subject to}\quad\bm{q}^{\top}\bm{q}=1." class="ltx_Math" display="block" id="S2.E19.m1"><semantics><mrow><mrow><mrow><mrow><mi>min</mi><mo lspace="0.170em">−</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo>‖</mo><mrow><msup><mi>𝒒</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow></mrow><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><mrow><msup><mi>𝒒</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒒</mi></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min\,-\frac{1}{4}\left\|\bm{q}^{\top}\bm{X}\right\|_{4}^{4}\quad\mbox{subject to}\quad\bm{q}^{\top}\bm{q}=1.</annotation><annotation encoding="application/x-llamapun">roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∥ bold_italic_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT subject to bold_italic_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_q = 1 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is very benign: All local minima are close to the global optima and all critical points are saddle points with a direction of negative curvature. Hence, any descent method with the ability of escaping strict saddle points provably finds global optimal solutions. For more precise statements, interested readers may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx217" title="">QZL+20</a>]</cite>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark10">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.10</span></span><span class="ltx_text ltx_font_italic"> </span>(Stable Deep Linear Network)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark10.p1">
<p class="ltx_p">The above iterative process of computing the dictionary has a natural incremental “deep learning” interpretation. Let us define
<math alttext="\delta\bm{A}_{t+1}=\bm{A}_{t+1}\bm{A}_{t}^{\top}" class="ltx_Math" display="inline" id="Thmremark10.p1.m1"><semantics><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑨</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">\delta\bm{A}_{t+1}=\bm{A}_{t+1}\bm{A}_{t}^{\top}</annotation><annotation encoding="application/x-llamapun">italic_δ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{Z}_{t}=\bm{A}_{t}\bm{X}" class="ltx_Math" display="inline" id="Thmremark10.p1.m2"><semantics><mrow><msub><mi>𝒁</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝑨</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{t}=\bm{A}_{t}\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_X</annotation></semantics></math>, then it is easy to show that</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\delta\bm{A}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[(\bm{Z}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{Z}_{t}^{\top}]." class="ltx_Math" display="block" id="S2.Ex4.m1"><semantics><mrow><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝒁</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\delta\bm{A}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[(\bm{Z}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{Z}_{t}^{\top}].</annotation><annotation encoding="application/x-llamapun">italic_δ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">If <math alttext="\bm{A}_{t}" class="ltx_Math" display="inline" id="Thmremark10.p1.m3"><semantics><msub><mi>𝑨</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{A}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> converges to the correct dictionary <math alttext="\bm{D}_{o}" class="ltx_Math" display="inline" id="Thmremark10.p1.m4"><semantics><msub><mi>𝑫</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{D}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>, then the above iterative encoding process is essentially equivalent to a “deep linear network”:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}\;\longleftarrow\;\bm{Z}_{t+1}=\underbrace{\delta\bm{A}_{t+1}\delta\bm{A}_{t}\ldots\delta\bm{A}_{1}}_{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{forward constructed layers}}\bm{X}." class="ltx_Math" display="block" id="S2.Ex5.m1"><semantics><mrow><mrow><mi>𝒁</mi><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟵</mo><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mn>1</mn></msub></mrow><mo>⏟</mo></munder><mtext mathcolor="#FF0000">forward constructed layers</mtext></munder><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}\;\longleftarrow\;\bm{Z}_{t+1}=\underbrace{\delta\bm{A}_{t+1}\delta\bm{A}_{t}\ldots\delta\bm{A}_{1}}_{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{forward constructed layers}}\bm{X}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z ⟵ bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = under⏟ start_ARG italic_δ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT italic_δ bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT … italic_δ bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT forward constructed layers end_POSTSUBSCRIPT bold_italic_X .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that the computation of the increment transforms <math alttext="\delta\bm{A}_{t+1}" class="ltx_Math" display="inline" id="Thmremark10.p1.m5"><semantics><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\delta\bm{A}_{t+1}</annotation><annotation encoding="application/x-llamapun">italic_δ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math> at each layer depends only on the feature output from the previous layer <math alttext="\bm{Z}_{t}" class="ltx_Math" display="inline" id="Thmremark10.p1.m6"><semantics><msub><mi>𝒁</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\bm{Z}_{t}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. The network is naturally stable as each layer is a norm-preserving orthogonal transform. Despite its resemblance to a linear deep network, backpropagation is unnecessary to learn each layer. All layers are learned in one forward pass!</p>
</div>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2.3 </span>Connection to ICA and Kurtosis</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">With the Bernoulli-Gaussian model, the variables <math alttext="z_{i}" class="ltx_Math" display="inline" id="S2.SS3.p1.m1"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are independent and non-Gaussian. Then, there is a clear correspondence between the dictionary learning and the classic independent component analysis (ICA), to the extent that algorithms to solve one problem can be used to solve the other.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We explore this issue in more depth in Exercise <a class="ltx_ref" href="#Thmexercise3" title="Exercise 2.3. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3</span></a>, where a connection between non-Gaussianity of the independent components and the purely geometric notion of symmetry is made. This issue is related to our observation above that PCA does not work for recovering sparsely-used orthogonal dictionaries: in the statistical setting, it can be related to rotational invariance of the Gaussian distribution (Exercise <a class="ltx_ref" href="#Thmexercise2" title="Exercise 2.2. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2</span></a>).</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p">Towards deriving an algorithm based on ICA, we focus on an objective function known as <span class="ltx_text ltx_font_italic">kurtosis</span>, which is used in ICA as a direct consequence of the non-Gaussianity of the components. The <span class="ltx_text ltx_font_italic">kurtosis</span>, or fourth-order cumulant, of a zero-mean random variable <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.p2.m1"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation><annotation encoding="application/x-llamapun">italic_X</annotation></semantics></math> is defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathop{\mathrm{kurt}}(X)=\operatorname{\mathbb{E}}{X^{4}}-3(\operatorname{\mathbb{E}}{X^{2}})^{2}." class="ltx_Math" display="block" id="S2.E20.m1"><semantics><mrow><mrow><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝔼</mi><mo lspace="0.167em">⁡</mo><msup><mi>X</mi><mn>4</mn></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝔼</mi><mo lspace="0.167em">⁡</mo><msup><mi>X</mi><mn>2</mn></msup></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(X)=\operatorname{\mathbb{E}}{X^{4}}-3(\operatorname{\mathbb{E}}{X^{2}})^{2}.</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_X ) = blackboard_E italic_X start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ( blackboard_E italic_X start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we have only finite samples from the random variable <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.p2.m2"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation><annotation encoding="application/x-llamapun">italic_X</annotation></semantics></math> arranged into a vector <math alttext="\bm{x}=[x_{1},\dots,x_{N}]" class="ltx_Math" display="inline" id="S2.SS3.p2.m3"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=[x_{1},\dots,x_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_x = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>, we define kurtosis through their empirical average, which yields</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathop{\mathrm{kurt}}(\bm{x})=\frac{1}{N}\|\bm{x}\|_{4}^{4}-\frac{3}{N^{2}}\|\bm{x}\|_{2}^{4}." class="ltx_Math" display="block" id="S2.E21.m1"><semantics><mrow><mrow><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒙</mi><mo stretchy="false">‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow><mo>−</mo><mrow><mfrac><mn>3</mn><msup><mi>N</mi><mn>2</mn></msup></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒙</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>4</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(\bm{x})=\frac{1}{N}\|\bm{x}\|_{4}^{4}-\frac{3}{N^{2}}\|\bm{x}\|_{2}^{4}.</annotation><annotation encoding="application/x-llamapun">roman_kurt ( bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - divide start_ARG 3 end_ARG start_ARG italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∥ bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, for random vectors, we define their kurtosis as the sum of each component’s scalar kurtosis.
Kurtosis is a natural loss function for ICA because for Gaussian <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.p2.m4"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation><annotation encoding="application/x-llamapun">italic_X</annotation></semantics></math>, kurtosis is zero; the reader can verify further that the Bernoulli-Gaussian distribution has positive kurtosis.
Thus a natural procedure for seeking non-Gaussian independent components is to search for a set of mutually-orthogonal directions <math alttext="\bm{V}\in\mathbb{R}^{d\times k}" class="ltx_Math" display="inline" id="S2.SS3.p2.m5"><semantics><mrow><mi>𝑽</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{V}\in\mathbb{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun">bold_italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> such that <math alttext="\bm{V}^{\top}\bm{X}" class="ltx_Math" display="inline" id="S2.SS3.p2.m6"><semantics><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">\bm{V}^{\top}\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X</annotation></semantics></math> has maximal kurtosis, where <math alttext="\bm{X}=\bm{U}\bm{Z}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S2.SS3.p2.m7"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{Z}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is the Bernoulli-Gaussian ICA data matrix.
Formally, we seek to solve the problem</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{V}^{\top}\bm{V}=\bm{I}}\mathop{\mathrm{kurt}}(\bm{V}^{\top}\bm{X})." class="ltx_Math" display="block" id="S2.E22.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑽</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow></munder><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑽</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{V}^{\top}\bm{V}=\bm{I}}\mathop{\mathrm{kurt}}(\bm{V}^{\top}\bm{X}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_V = bold_italic_I end_POSTSUBSCRIPT roman_kurt ( bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">At one extreme, we may set <math alttext="k=D" class="ltx_Math" display="inline" id="S2.SS3.p2.m8"><semantics><mrow><mi>k</mi><mo>=</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">k=D</annotation><annotation encoding="application/x-llamapun">italic_k = italic_D</annotation></semantics></math> and seek to recover the entire dictionary
<math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS3.p2.m9"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> in a single shot. It can be shown that this problem can be solved with
the MSP algorithm we have seen previously.
At the other extreme, we may set <math alttext="k=1" class="ltx_Math" display="inline" id="S2.SS3.p2.m10"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation><annotation encoding="application/x-llamapun">italic_k = 1</annotation></semantics></math> and seek to recover a single direction (column of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS3.p2.m11"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>) at a time, performing <span class="ltx_text ltx_font_italic">deflation</span>, i.e., replacing the data matrix <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S2.SS3.p2.m12"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> by <math alttext="(\bm{I}-\bm{u}\bm{u}^{\top})\bm{X}" class="ltx_Math" display="inline" id="S2.SS3.p2.m13"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">(\bm{I}-\bm{u}\bm{u}^{\top})\bm{X}</annotation><annotation encoding="application/x-llamapun">( bold_italic_I - bold_italic_u bold_italic_u start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) bold_italic_X</annotation></semantics></math>, after each step before finding another direction.
There is a natural tradeoff between the scalability of the <math alttext="k=1" class="ltx_Math" display="inline" id="S2.SS3.p2.m14"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation><annotation encoding="application/x-llamapun">italic_k = 1</annotation></semantics></math> incremental approach and the efficiency and robustness of the <math alttext="k=D" class="ltx_Math" display="inline" id="S2.SS3.p2.m15"><semantics><mrow><mi>k</mi><mo>=</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">k=D</annotation><annotation encoding="application/x-llamapun">italic_k = italic_D</annotation></semantics></math> approach.</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Incremental ICA: correctness and FastICA algorithm.</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p">The FastICA algorithm, advanced by Hyvärinen and Oja <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx118" title="">HO97</a>]</cite>, is a fast fixed-point algorithm for solving the <math alttext="k=1" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m1"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation><annotation encoding="application/x-llamapun">italic_k = 1</annotation></semantics></math> kurtosis maximization scheme for ICA.
The problem at hand is</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\|\bm{v}\|_{2}^{2}=1}\,\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{v})." class="ltx_Math" display="block" id="S2.E23.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒗</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow></munder><mo lspace="0.337em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\|\bm{v}\|_{2}^{2}=1}\,\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{v}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT ∥ bold_italic_v ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">First, we will perform some very basic analysis of this objective to verify its correctness. Notice by the change of variables <math alttext="\bm{w}=\bm{U}^{\top}\bm{v}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m2"><semantics><mrow><mi>𝒘</mi><mo>=</mo><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒗</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{w}=\bm{U}^{\top}\bm{v}</annotation><annotation encoding="application/x-llamapun">bold_italic_w = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_v</annotation></semantics></math> that this problem is equivalent to</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\|\bm{w}\|_{2}^{2}=1}\,\mathrm{kurt}(\bm{Z}^{\top}\bm{w})." class="ltx_Math" display="block" id="S2.Ex6.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒘</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow></munder><mo lspace="0.337em">⁡</mo><mi>kurt</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝒁</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒘</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\|\bm{w}\|_{2}^{2}=1}\,\mathrm{kurt}(\bm{Z}^{\top}\bm{w}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_Z start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_w ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">This objective is simple enough that we can make strong statements about its correctness as a formulation for recovering the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>.
For example, in the population setting where <math alttext="N\to\infty" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m4"><semantics><mrow><mi>N</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N\to\infty</annotation><annotation encoding="application/x-llamapun">italic_N → ∞</annotation></semantics></math>,
we may use additivity properties of the kurtosis (Exercise <a class="ltx_ref" href="#Thmexercise5" title="Exercise 2.5. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5</span></a>) and our assumed normalization on the independent components to write the previous problem equivalently as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\|\bm{w}\|_{2}^{2}=1}\,\sum_{i=1}^{d}\mathrm{kurt}(z_{i})w_{i}^{4}." class="ltx_Math" display="block" id="S2.E24.m1"><semantics><mrow><mrow><munder><mi>max</mi><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒘</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow></munder><mo lspace="0.337em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>kurt</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>w</mi><mi>i</mi><mn>4</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\|\bm{w}\|_{2}^{2}=1}\,\sum_{i=1}^{d}\mathrm{kurt}(z_{i})w_{i}^{4}.</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It can be shown that under the Bernoulli-Gaussian assumption, the optimization landscape of this problem is “benign” (Exercise <a class="ltx_ref" href="#Thmexercise7" title="Exercise 2.7. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.7</span></a>)—meaning that all local maxima of the objective function correspond to the recovery of one of the independent components.
One efficient and scalable way to compute one of these maxima is via first-order optimization algorithms, which iteratively follow the gradient of the objective function and project onto the constraint set <math alttext="\{\bm{w}\mid\|\bm{w}\|_{2}^{2}=1\}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m5"><semantics><mrow><mo stretchy="false">{</mo><mi>𝒘</mi><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒘</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{w}\mid\|\bm{w}\|_{2}^{2}=1\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_w ∣ ∥ bold_italic_w ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 }</annotation></semantics></math>.
Since we have assumed that each <math alttext="z_{i}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m6"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> satisfies <math alttext="\operatorname{Var}(z_{i})=1" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m7"><semantics><mrow><mrow><mi>Var</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\operatorname{Var}(z_{i})=1</annotation><annotation encoding="application/x-llamapun">roman_Var ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 1</annotation></semantics></math>, we
have for large <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m8"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{u})\approx\tfrac{1}{N}\|\bm{X}^{\top}\bm{u}\|_{4}^{4}-3\|\bm{u}\|_{2}^{4}." class="ltx_Math" display="block" id="S2.E25.m1"><semantics><mrow><mrow><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><mrow><mstyle displaystyle="false"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>4</mn></msubsup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{u})\approx\tfrac{1}{N}\|\bm{X}^{\top}\bm{u}\|_{4}^{4}-3\|\bm{u}\|_{2}^{4}.</annotation><annotation encoding="application/x-llamapun">roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) ≈ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∥ bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We can then derive a corresponding approximation to the gradient:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\bm{u}}\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{u})\approx\tfrac{4}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-12\|\bm{u}\|_{2}^{2}\bm{u}." class="ltx_Math" display="block" id="S2.Ex7.m1"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝒖</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo movablelimits="false" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≈</mo><mrow><mrow><mstyle displaystyle="false"><mfrac><mn>4</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>12</mn><mo lspace="0em" rspace="0em">​</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\nabla_{\bm{u}}\mathop{\mathrm{kurt}}(\bm{X}^{\top}\bm{u})\approx\tfrac{4}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-12\|\bm{u}\|_{2}^{2}\bm{u}.</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) ≈ divide start_ARG 4 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 12 ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_u .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The FastICA algorithm uses a fixed-point method to compute directions of maximum kurtosis. It starts from the first-order optimality conditions for the kurtosis maximization problem, given the preceding gradient approximation and the constraint set, which read</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx4">
<tbody id="S2.E26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}=\underbrace{\left\langle\bm{u},\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\right\rangle}_{\lambda}\bm{u}," class="ltx_Math" display="inline" id="S2.E26.m1"><semantics><mrow><mrow><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>=</mo><mrow><munder><munder accentunder="true"><mrow><mo>⟨</mo><mi>𝒖</mi><mo>,</mo><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>⟩</mo></mrow><mo>⏟</mo></munder><mi>λ</mi></munder><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}=\underbrace{\left\langle\bm{u},\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\right\rangle}_{\lambda}\bm{u},</annotation><annotation encoding="application/x-llamapun">bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT = under⏟ start_ARG ⟨ bold_italic_u , bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT ⟩ end_ARG start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT bold_italic_u ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the specific value of <math alttext="\lambda" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m9"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_λ</annotation></semantics></math> is determined using the unit norm constraint on <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m10"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math>.
Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a> describes the mathematical background necessary to derive these optimality conditions from first principles.
Equation (<a class="ltx_ref" href="#S2.E26" title="Equation 2.2.26 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.26</span></a>) is satisfied by <span class="ltx_text ltx_font_italic">any</span> critical point of the kurtosis maximization problem; we want to derive an equation satisfied by only the maximizers.
After noticing that <math alttext="\lambda=\|\bm{X}^{\top}\bm{u}\|_{4}^{4}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m11"><semantics><mrow><mi>λ</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>4</mn><mn>4</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda=\|\bm{X}^{\top}\bm{u}\|_{4}^{4}</annotation><annotation encoding="application/x-llamapun">italic_λ = ∥ bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ∥ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, we equivalently re-express (<a class="ltx_ref" href="#S2.E26" title="Equation 2.2.26 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.26</span></a>) as the modified equation</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx5">
<tbody id="S2.E27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}=\left(\frac{\lambda}{N}-3\right)\bm{u}," class="ltx_Math" display="inline" id="S2.E27.m1"><semantics><mrow><mrow><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mi>λ</mi><mi>N</mi></mfrac></mstyle><mo>−</mo><mn>3</mn></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}=\left(\frac{\lambda}{N}-3\right)\bm{u},</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u = ( divide start_ARG italic_λ end_ARG start_ARG italic_N end_ARG - 3 ) bold_italic_u ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and realize that any maximizer of (<a class="ltx_ref" href="#S2.E23" title="Equation 2.2.23 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.23</span></a>)
must satisfy <math alttext="\lambda/N-3&gt;0" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m12"><semantics><mrow><mrow><mrow><mi>λ</mi><mo>/</mo><mi>N</mi></mrow><mo>−</mo><mn>3</mn></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda/N-3&gt;0</annotation><annotation encoding="application/x-llamapun">italic_λ / italic_N - 3 &gt; 0</annotation></semantics></math>,
assuming that <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m13"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is sufficiently large.
Hence, we may <span class="ltx_text ltx_font_italic">normalize</span> both sides of (<a class="ltx_ref" href="#S2.E27" title="Equation 2.2.27 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.27</span></a>),
giving the following fixed-point equation satisfied by every maximizer of (<a class="ltx_ref" href="#S2.E23" title="Equation 2.2.23 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.23</span></a>):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx6">
<tbody id="S2.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\frac{\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}}{\left\|\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}\right\|_{2}}=\bm{u}." class="ltx_Math" display="inline" id="S2.E28.m1"><semantics><mrow><mrow><mstyle displaystyle="true"><mfrac><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow><msub><mrow><mo>‖</mo><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow><mo>‖</mo></mrow><mn>2</mn></msub></mfrac></mstyle><mo>=</mo><mi>𝒖</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\frac{\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}}{\left\|\frac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}\right\|_{2}}=\bm{u}.</annotation><annotation encoding="application/x-llamapun">divide start_ARG divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u end_ARG start_ARG ∥ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG = bold_italic_u .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.28)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Iterating the mapping defined by the lefthand side of this fixed point expression then yields the FastICA algorithm of Hyvärinen and Oja <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx118" title="">HO97</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\bm{v}^{+}&amp;=\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u},\\
\bm{u}^{+}&amp;=\bm{v}^{+}/\left\|\bm{v}^{+}\right\|_{2}.\end{split}" class="ltx_Math" display="block" id="S2.E29.m1"><semantics><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msup><mi>𝒗</mi><mo>+</mo></msup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mstyle displaystyle="false"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msup><mi>𝒖</mi><mo>+</mo></msup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mi>𝒗</mi><mo>+</mo></msup><mo>/</mo><msub><mrow><mo>‖</mo><msup><mi>𝒗</mi><mo>+</mo></msup><mo>‖</mo></mrow><mn>2</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}\bm{v}^{+}&amp;=\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u},\\
\bm{u}^{+}&amp;=\bm{v}^{+}/\left\|\bm{v}^{+}\right\|_{2}.\end{split}</annotation><annotation encoding="application/x-llamapun">start_ROW start_CELL bold_italic_v start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT end_CELL start_CELL = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u , end_CELL end_ROW start_ROW start_CELL bold_italic_u start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT end_CELL start_CELL = bold_italic_v start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT / ∥ bold_italic_v start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.2.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">It turns out that the FastICA algorithm converges extremely rapidly (actually at
a <span class="ltx_text ltx_font_italic">cubic</span> rate) to columns of the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.m14"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>; interested readers
may consult <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx118" title="">HO97</a>]</cite> for details.
Comparing the FastICA algorithm (<a class="ltx_ref" href="#S2.E29" title="Equation 2.2.29 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.29</span></a>) to the power method studied
in <a class="ltx_ref" href="#S1.SS1" title="2.1.1 Principal Components Analysis (PCA) ‣ 2.1 A Low-Dimensional Subspace ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.1.1</span></a> for the PCA problem and the MSP algorithm
(<a class="ltx_ref" href="#S2.E18" title="Equation 2.2.18 ‣ Dictionary learning via the MSP algorithm. ‣ 2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.18</span></a>), we notice a striking similarity. Indeed, FastICA is essentially a modified power method, involving the gradient of the empirical kurtosis rather than the simpler linear gradient of the PCA objective.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.3 </span>A Mixture of Overcomplete Low-Dimensional Subspaces</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">As we have seen, complete dictionary learning enjoys an elegant computational theory in which we maintain a symmetric autoencoding structure <math alttext="\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x}" class="ltx_Math" display="inline" id="S3.p1.m1"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}(\bm{x})=\bm{U}^{\top}\bm{x}</annotation><annotation encoding="application/x-llamapun">caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x</annotation></semantics></math>, <math alttext="\mathcal{D}(\bm{z})=\bm{U}\bm{z}" class="ltx_Math" display="inline" id="S3.p1.m2"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}(\bm{z})=\bm{U}\bm{z}</annotation><annotation encoding="application/x-llamapun">caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z</annotation></semantics></math>, with a scalable power-method-like algorithm (the MSP algorithm) for learning an orthogonal dictionary/codebook <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.p1.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> from data. Nevertheless, for learning representations of general high-dimensional data distributions, one must expand the size of the codebook beyond the orthogonality requirement—meaning that we must have <math alttext="\bm{A}\in\mathbb{R}^{D\times m}" class="ltx_Math" display="inline" id="S3.p1.m4"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{D\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="m\gg D" class="ltx_Math" display="inline" id="S3.p1.m5"><semantics><mrow><mi>m</mi><mo>≫</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">m\gg D</annotation><annotation encoding="application/x-llamapun">italic_m ≫ italic_D</annotation></semantics></math>, corresponding to the case of an <span class="ltx_text ltx_font_italic">overcomplete</span> dictionary/codebook,<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>We change the notation here from <math alttext="\bm{U}" class="ltx_Math" display="inline" id="footnote9.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> to <math alttext="\bm{A}" class="ltx_Math" display="inline" id="footnote9.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> in order to emphasize the non-orthogonality and non-square-shape of the overcomplete dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="footnote9.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.</span></span></span> and the signal model</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{A}\bm{z}+\bm{\varepsilon},\quad\|\bm{z}\|_{0}=d\ll m." class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝜺</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>=</mo><mi>d</mi><mo>≪</mo><mi>m</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{A}\bm{z}+\bm{\varepsilon},\quad\|\bm{z}\|_{0}=d\ll m.</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ε , ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_d ≪ italic_m .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">There are both geometric and physical/modeling motivations for passing to the overcomplete case.
Geometrically, recall that in our original reduction from the mixture of subspace data model to the sparse dictionary model, a mixture of <math alttext="K" class="ltx_Math" display="inline" id="S3.p1.m6"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation><annotation encoding="application/x-llamapun">italic_K</annotation></semantics></math> subspaces in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S3.p1.m7"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, each of dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.p1.m8"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, led to a dictionary of shape <math alttext="\bm{A}\in\mathbb{R}^{D\times Kd}" class="ltx_Math" display="inline" id="S3.p1.m9"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{D\times Kd}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_K italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
In other words, overcomplete dictionaries correspond to <span class="ltx_text ltx_font_italic">richer</span> mixtures of subspaces, with more distinct modes of variability for modeling the high-dimensional data distribution.
On the modeling side, we may run a computational experiment on real-world data that reveals the additional modeling power conferred by an overcomplete representation.</p>
</div>
<figure class="ltx_figure" id="F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="F6.sf1"><img alt="(a)" class="ltx_graphics ltx_img_landscape" height="150" id="F6.sf1.g1" src="chapters/chapter2/figs/msp_atoms_patches_new.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="F6.sf2"><img alt="(a)" class="ltx_graphics ltx_img_landscape" height="150" id="F6.sf2.g1" src="chapters/chapter2/figs/palm_atoms_patches_new.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2.6</span>: </span><span class="ltx_text" style="font-size:90%;">Comparison of learned dictionary atoms for complete (orthogonal)
and overcomplete dictionaries, trained to reconstruct <math alttext="8" class="ltx_Math" display="inline" id="F6.m13"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> by <math alttext="8" class="ltx_Math" display="inline" id="F6.m14"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> patches taken from
MNIST digits. Both dictionaries are trained for <math alttext="6000" class="ltx_Math" display="inline" id="F6.m15"><semantics><mn>6000</mn><annotation encoding="application/x-tex">6000</annotation><annotation encoding="application/x-llamapun">6000</annotation></semantics></math> epochs on <math alttext="10^{4}" class="ltx_Math" display="inline" id="F6.m16"><semantics><msup><mn>10</mn><mn>4</mn></msup><annotation encoding="application/x-tex">10^{4}</annotation><annotation encoding="application/x-llamapun">10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> random patches with
nontrivial content, and sparse codes are computed with the LASSO objective
and <math alttext="\lambda=0.1" class="ltx_Math" display="inline" id="F6.m17"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\lambda=0.1</annotation><annotation encoding="application/x-llamapun">italic_λ = 0.1</annotation></semantics></math> (see (<a class="ltx_ref" href="#S3.E5" title="Equation 2.3.5 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.5</span></a>)). Colormaps have black for negative values, and white for
positive values. <span class="ltx_text ltx_font_bold">Top:</span> An orthogonal dictionary learned with the
MSP algorithm (<a class="ltx_ref" href="#S2.E18" title="Equation 2.2.18 ‣ Dictionary learning via the MSP algorithm. ‣ 2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.18</span></a>) is constrained to
have no more than <math alttext="64" class="ltx_Math" display="inline" id="F6.m18"><semantics><mn>64</mn><annotation encoding="application/x-tex">64</annotation><annotation encoding="application/x-llamapun">64</annotation></semantics></math> atoms; the learned atoms roughly correspond to
a “spike and slab” dictionary, and achieve relatively poor reconstruction sparsity
levels on held-out test data (codes are approximately <math alttext="17" class="ltx_Math" display="inline" id="F6.m19"><semantics><mn>17</mn><annotation encoding="application/x-tex">17</annotation><annotation encoding="application/x-llamapun">17</annotation></semantics></math>-sparse on
average, with respect to a threshold of <math alttext="10^{-1}" class="ltx_Math" display="inline" id="F6.m20"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">10^{-1}</annotation><annotation encoding="application/x-llamapun">10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>).
<span class="ltx_text ltx_font_bold">Bottom:</span> In contrast, an overcomplete dictionary (here, with <math alttext="8^{3}" class="ltx_Math" display="inline" id="F6.m21"><semantics><msup><mn>8</mn><mn>3</mn></msup><annotation encoding="application/x-tex">8^{3}</annotation><annotation encoding="application/x-llamapun">8 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>
atoms; we visualize a random subset of <math alttext="64" class="ltx_Math" display="inline" id="F6.m22"><semantics><mn>64</mn><annotation encoding="application/x-tex">64</annotation><annotation encoding="application/x-llamapun">64</annotation></semantics></math>) learns
semantically-meaningful dictionary atoms corresponding to signed oriented
edges, which can be pieced together to create digit patches and achieve
superior reconstruction and sparisty levels. Codes are approximately
<math alttext="20" class="ltx_Math" display="inline" id="F6.m23"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation><annotation encoding="application/x-llamapun">20</annotation></semantics></math>-sparse on
average, while being <math alttext="8" class="ltx_Math" display="inline" id="F6.m24"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation><annotation encoding="application/x-llamapun">8</annotation></semantics></math> times larger than those of the orthogonal
dictionary. To compute the
dictionary, we use an optimizer based on proximal alternating linearized
minimization on a suitably-regularized version of
(<a class="ltx_ref" href="#S3.E17" title="Equation 2.3.17 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.17</span></a>).</span></figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 2.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">Given sampled images of hand-written digits, Figure <a class="ltx_ref" href="#F6" title="Figure 2.6 ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>(a) shows the result of fitting an orthogonal dictionary to the dataset.
In contrast, Figure <a class="ltx_ref" href="#F6" title="Figure 2.6 ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>(b) shows the result of running an
optimization algorithm for learning overcomplete dictionaries (which we will
present in detail later in the Chapter) on these samples.
Notice that the representations become far sparser and the codebooks far more interpretable—they consist of fundamental primitives for the strokes composing the digits, i.e. oriented edges.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">In fact, overcomplete dictionary learning was originally proposed as a biologically plausible algorithm for image representation based on empirical evidence of how early stages of the visual cortex represent visual stimuli <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx202" title="">OF96</a>, <a class="ltx_ref" href="bib.html#bibx201" title="">OF97</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p">In the remainder of this section, we will overview the conceptual and computational foundations of overcomplete dictionary learning.
Supposing that the model (<a class="ltx_ref" href="#S3.E1" title="Equation 2.3.1 ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>) is satisfied with
sparse codes <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.p3.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, overcomplete dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p3.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, and sparsity level <math alttext="d" class="ltx_Math" display="inline" id="S3.p3.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>,
and given samples <math alttext="\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S3.p3.m4"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p3.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, we want to learn
an encoder <math alttext="\mathcal{E}:\mathbb{R}^{D}\to\mathbb{R}^{m}" class="ltx_Math" display="inline" id="S3.p3.m6"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}:\mathbb{R}^{D}\to\mathbb{R}^{m}</annotation><annotation encoding="application/x-llamapun">caligraphic_E : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> mapping each <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p3.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to its sparse code
<math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.p3.m8"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, and a decoder <math alttext="\mathcal{D}(\bm{z})=\bm{A}\bm{z}" class="ltx_Math" display="inline" id="S3.p3.m9"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}(\bm{z})=\bm{A}\bm{z}</annotation><annotation encoding="application/x-llamapun">caligraphic_D ( bold_italic_z ) = bold_italic_A bold_italic_z</annotation></semantics></math> reconstructing each <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p3.m10"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> from
its sparse code. In diagram form:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 11.38109pt\mathcal{E}\hskip 11.38109pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{A}\hskip 5.69054pt}\hat{\bm{x}}." class="ltx_math_unparsed" display="block" id="S3.E2.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mspace width="1.14em"></mspace></mrow></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mi>𝑨</mi></mrow></mover><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 11.38109pt\mathcal{E}\hskip 11.38109pt}\bm{z}\xrightarrow{\hskip 5.69054pt\mathcal{D}=\bm{A}\hskip 5.69054pt}\hat{\bm{x}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_A end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_x end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p">We will start from the construction of the encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.p4.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math>.
We will work incrementally: first, <span class="ltx_text ltx_font_italic">given the true dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p4.m2"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math></span>, we will show how the problem of <span class="ltx_text ltx_font_italic">sparse coding</span> gives an elegant, scalable, and provably-correct algorithm for recovering the sparse code <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.p4.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.p4.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.
Although this problem is NP-hard in the worst case, it can be solved efficiently and scalably for dictionaries <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p4.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> which are <span class="ltx_text ltx_font_italic">incoherent</span>, i.e. having columns that are not too correlated.
The encoder architecture encompassed by this solution will no longer be symmetric: we will see it has the form of a primitive deep network, which depends on the dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p4.m6"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p">Then we will proceed to the task of learning the decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.p5.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math>, or equivalently the overcomplete dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p5.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.
We will derive an algorithm for overcomplete dictionary learning that allows us
to simultaneously learn the codebook <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.p5.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> and the sparse codes <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.p5.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, using ideas from sparse coding.
Finally, we will discuss a more modern perspective on learnable sparse coding that leads us to a fully asymmetric encoder-decoder structure, as an alternative to (<a class="ltx_ref" href="#S3.E2" title="Equation 2.3.2 ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>).
Here, the decoder will correspond to an incremental solution to the sparse dictionary learning problem, and yield a pair of deep network-like encoder decoders for sparse dictionary learning.
This structure will foreshadow many developments to come in the remainder of the
monograph, as we progress from analytical models to modern neural networks.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3.1 </span>Sparse Coding with an Overcomplete Dictionary</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">In this section, we will consider the data model
(<a class="ltx_ref" href="#S3.E1" title="Equation 2.3.1 ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>), which accommodates sparse linear combinations
of many motifs, or <span class="ltx_text ltx_font_italic">atoms</span>. Given data <math alttext="\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>⊆</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}_{i=1}^{N}\subseteq\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ⊆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> satisfying this model, i.e. expressible as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}=\bm{A}\bm{z}_{i}+\bm{\varepsilon}_{i},\qquad\forall i\in[N]" class="ltx_Math" display="block" id="S3.E3.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒛</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi>𝜺</mi><mi>i</mi></msub></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>i</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>N</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{i}=\bm{A}\bm{z}_{i}+\bm{\varepsilon}_{i},\qquad\forall i\in[N]</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ∀ italic_i ∈ [ italic_N ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some dictionary <math alttext="\bm{A}\in\mathbb{R}^{D\times m}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{D\times m}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.p1.m3"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math> atoms, sparse codes
<math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m4"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> such that <math alttext="\|\bm{z}_{i}\|_{0}\leq d" class="ltx_Math" display="inline" id="S3.SS1.p1.m5"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>≤</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\|\bm{z}_{i}\|_{0}\leq d</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≤ italic_d</annotation></semantics></math>, and small errors <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>,
the sparse coding problem is to recover the codes <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m7"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as accurately as
possible from the data <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, given the dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.p1.m9"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.
Efficient algorithms to solve this problem succeed when
the dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.p1.m10"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is <span class="ltx_text ltx_font_italic">incoherent</span> in the sense that the inner
products <math alttext="\bm{a}_{i}^{\top}\bm{a}_{j}" class="ltx_Math" display="inline" id="S3.SS1.p1.m11"><semantics><mrow><msubsup><mi>𝒂</mi><mi>i</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒂</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\bm{a}_{i}^{\top}\bm{a}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are uniformly small, hence the atoms are
nearly orthogonal.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>As it turns out, in a high-dimensional space, it is
rather easy to pack a number of nearly orthogonal vectors that is much larger
than the ambient dimension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>. </span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">Note that we can collect the <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.m1"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into <math alttext="\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S3.SS1.p2.m2"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}=\begin{bmatrix}\bm{x}_{1},\dots,\bm{x}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, collect the <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.m3"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into <math alttext="\bm{Z}=\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{N}\end{bmatrix}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S3.SS1.p2.m4"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Z}=\begin{bmatrix}\bm{z}_{1},\dots,\bm{z}_{N}\end{bmatrix}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, and collect the <math alttext="\bm{\varepsilon}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.m5"><semantics><msub><mi>𝜺</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{\varepsilon}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into <math alttext="\bm{E}=\begin{bmatrix}\bm{\varepsilon}_{1},\dots,\bm{\varepsilon}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S3.SS1.p2.m6"><semantics><mrow><mi>𝑬</mi><mo>=</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝜺</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝜺</mi><mi>N</mi></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{E}=\begin{bmatrix}\bm{\varepsilon}_{1},\dots,\bm{\varepsilon}_{N}\end{bmatrix}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_E = [ start_ARG start_ROW start_CELL bold_italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_ε start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, to rewrite (<a class="ltx_ref" href="#S3.E3" title="Equation 2.3.3 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.3</span></a>) as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}=\bm{A}\bm{Z}+\bm{E}." class="ltx_Math" display="block" id="S3.E4.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>+</mo><mi>𝑬</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{A}\bm{Z}+\bm{E}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">A natural approach to solving the sparse coding problem is to seek the sparsest
signals that are consistent with our observations, and this naturally leads to
the following optimization problem:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{Z}\in\mathbb{R}^{d\times N}}\left\{\|\bm{X}-\bm{A}\bm{Z}\|_{F}^{2}+\lambda\|\bm{Z}\|_{1}\right\}," class="ltx_Math" display="block" id="S3.E5.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mi>𝒁</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{Z}\in\mathbb{R}^{d\times N}}\left\{\|\bm{X}-\bm{A}\bm{Z}\|_{F}^{2}+\lambda\|\bm{Z}\|_{1}\right\},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { ∥ bold_italic_X - bold_italic_A bold_italic_Z ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="S3.SS1.p2.m7"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm <math alttext="\|\bm{Z}\|_{1}" class="ltx_Math" display="inline" id="S3.SS1.p2.m8"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><annotation encoding="application/x-tex">\|\bm{Z}\|_{1}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, taken elementwise, is known to promote sparsity of the solution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.
This optimization problem is known as the LASSO problem.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">However, unlike PCA or the
complete dictionary learning problem, there is no clear power iteration-type
algorithm to recover <math alttext="\bm{Z}^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p3.m1"><semantics><msup><mi>𝒁</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. A natural alternative is to consider
solving the above optimization problem with gradient descent type algorithms.
Let <math alttext="f(\bm{Z})=\|\bm{X}-\bm{A}\bm{Z}\|_{2}^{2}+\lambda\|\bm{Z}\|_{1}" class="ltx_Math" display="inline" id="S3.SS1.p3.m2"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">f(\bm{Z})=\|\bm{X}-\bm{A}\bm{Z}\|_{2}^{2}+\lambda\|\bm{Z}\|_{1}</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_Z ) = ∥ bold_italic_X - bold_italic_A bold_italic_Z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.
Conceptually, we could try to find <math alttext="\bm{Z}^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p3.m3"><semantics><msup><mi>𝒁</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> with the following iterations:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{t+1}\leftarrow\bm{Z}_{t}+\eta\nabla f(\bm{Z}_{t})." class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝒁</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>η</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{t+1}\leftarrow\bm{Z}_{t}+\eta\nabla f(\bm{Z}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_η ∇ italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, because the term associated with the <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="S3.SS1.p3.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm <math alttext="\|\bm{Z}\|_{1}" class="ltx_Math" display="inline" id="S3.SS1.p3.m5"><semantics><msub><mrow><mo stretchy="false">‖</mo><mi>𝒁</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><annotation encoding="application/x-tex">\|\bm{Z}\|_{1}</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_Z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>
is non-smooth, we cannot just run gradient descent. For this type of function,
we need to replace the gradient <math alttext="\nabla f(\bm{Z})" class="ltx_Math" display="inline" id="S3.SS1.p3.m6"><semantics><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla f(\bm{Z})</annotation><annotation encoding="application/x-llamapun">∇ italic_f ( bold_italic_Z )</annotation></semantics></math> with something that
generalizes the notion of gradient, known as the subgradient <math alttext="\partial f(\bm{Z})" class="ltx_Math" display="inline" id="S3.SS1.p3.m7"><semantics><mrow><mo rspace="0em">∂</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\partial f(\bm{Z})</annotation><annotation encoding="application/x-llamapun">∂ italic_f ( bold_italic_Z )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{t+1}\leftarrow\bm{Z}_{t}+\eta\partial f(\bm{Z}_{t})." class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">←</mo><mrow><msub><mi>𝒁</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">∂</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{t+1}\leftarrow\bm{Z}_{t}+\eta\partial f(\bm{Z}_{t}).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ← bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_η ∂ italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, it is known that the convergence of subgradient descent is usually very
slow. Hence, for this type of optimization problems, it is conventional to adopt
a so-called <span class="ltx_text ltx_font_italic">proximal gradient descent</span>-type algorithm. We give a technical
overview of this method in <a class="ltx_ref" href="A1.html#S1.SS3" title="A.1.3 Proximal Gradient Descent for Non-Smooth Problems ‣ A.1 Steepest Descent ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1.3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p">Applying proximal gradient to the LASSO objective function
(<a class="ltx_ref" href="#S3.E5" title="Equation 2.3.5 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.5</span></a>) leads to the classic <span class="ltx_text ltx_font_italic">iterative
shrinkage-thresholding algorithm</span> (ISTA), which implements the iteration</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A2.S3.EGx7">
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{1}" class="ltx_Math" display="inline" id="S3.E8.m1"><semantics><msub><mi>𝒁</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\sim" class="ltx_Math" display="inline" id="S3.E8.m2"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\displaystyle\sim</annotation><annotation encoding="application/x-llamapun">∼</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{\mathcal{N}}(\bm{0},\bm{I})," class="ltx_Math" display="inline" id="S3.E8.m3"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\operatorname{\mathcal{N}}(\bm{0},\bm{I}),</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , bold_italic_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}_{t+1}" class="ltx_Math" display="inline" id="S3.E9.m1"><semantics><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\displaystyle\bm{Z}_{t+1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.E9.m2"><semantics><mo>=</mo><annotation encoding="application/x-tex">\displaystyle=</annotation><annotation encoding="application/x-llamapun">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle S_{\eta\lambda}\left(\bm{Z}_{t}-2\eta\bm{A}^{\top}(\bm{A}\bm{Z}_{t}-\bm{X})\right),\quad\forall t\geq 1," class="ltx_Math" display="inline" id="S3.E9.m3"><semantics><mrow><mrow><mrow><mrow><msub><mi>S</mi><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>𝒁</mi><mi>t</mi></msub><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒁</mi><mi>t</mi></msub></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mo rspace="0.167em">∀</mo><mi>t</mi></mrow></mrow><mo>≥</mo><mn>1</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle S_{\eta\lambda}\left(\bm{Z}_{t}-2\eta\bm{A}^{\top}(\bm{A}\bm{Z}_{t}-\bm{X})\right),\quad\forall t\geq 1,</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_η italic_λ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - 2 italic_η bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_X ) ) , ∀ italic_t ≥ 1 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with step size <math alttext="\eta\geq 0" class="ltx_Math" display="inline" id="S3.SS1.p4.m1"><semantics><mrow><mi>η</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\eta\geq 0</annotation><annotation encoding="application/x-llamapun">italic_η ≥ 0</annotation></semantics></math>, and the soft-thresholding operator <math alttext="S_{\alpha}" class="ltx_Math" display="inline" id="S3.SS1.p4.m2"><semantics><msub><mi>S</mi><mi>α</mi></msub><annotation encoding="application/x-tex">S_{\alpha}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT</annotation></semantics></math> defined on scalars by</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx8">
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle S_{\alpha}(x)" class="ltx_Math" display="inline" id="S3.E10.m1"><semantics><mrow><msub><mi>S</mi><mi>α</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle S_{\alpha}(x)</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\doteq\begin{cases}x-\alpha,&amp;x\geq\alpha,\\
0,&amp;-\alpha&lt;x&lt;\alpha,\\
x+\alpha,&amp;x\leq-\alpha\end{cases}" class="ltx_Math" display="inline" id="S3.E10.m2"><semantics><mrow><mi></mi><mo>≐</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi>x</mi><mo>−</mo><mi>α</mi></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi>x</mi><mo>≥</mo><mi>α</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mo>−</mo><mi>α</mi></mrow><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mi>α</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi>x</mi><mo>+</mo><mi>α</mi></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi>x</mi><mo>≤</mo><mrow><mo>−</mo><mi>α</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\doteq\begin{cases}x-\alpha,&amp;x\geq\alpha,\\
0,&amp;-\alpha&lt;x&lt;\alpha,\\
x+\alpha,&amp;x\leq-\alpha\end{cases}</annotation><annotation encoding="application/x-llamapun">≐ { start_ROW start_CELL italic_x - italic_α , end_CELL start_CELL italic_x ≥ italic_α , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL - italic_α &lt; italic_x &lt; italic_α , end_CELL end_ROW start_ROW start_CELL italic_x + italic_α , end_CELL start_CELL italic_x ≤ - italic_α end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.10)</span></td>
</tr></tbody>
<tbody id="S3.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname{sign}(x)\max\{\lvert x\rvert-\alpha,0\}," class="ltx_Math" display="inline" id="S3.E11.m1"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>sign</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>α</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname{sign}(x)\max\{\lvert x\rvert-\alpha,0\},</annotation><annotation encoding="application/x-llamapun">= roman_sign ( italic_x ) roman_max { | italic_x | - italic_α , 0 } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and applied element-wise to the input matrix. As a proximal gradient descent
algorithm applied to a convex problem, convergence to a global optimum is
assured, and a precise convergence rate can be derived straightforwardly
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p">We now take a moment to remark on the functional form of the update operator in (<a class="ltx_ref" href="#S3.E9" title="Equation 2.3.9 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.9</span></a>). It takes the form</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{t+1}=\texttt{nonlinearity}(\bm{Z}_{t}+\texttt{linear}^{\top}(\texttt{linear}(\bm{Z}_{t})+\texttt{bias}))." class="ltx_Math" display="block" id="S3.E12.m1"><semantics><mrow><mrow><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mtext class="ltx_mathvariant_monospace">nonlinearity</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒁</mi><mi>t</mi></msub><mo>+</mo><mrow><msup><mtext class="ltx_mathvariant_monospace">linear</mtext><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mtext class="ltx_mathvariant_monospace">linear</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mtext class="ltx_mathvariant_monospace">bias</mtext></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}_{t+1}=\texttt{nonlinearity}(\bm{Z}_{t}+\texttt{linear}^{\top}(\texttt{linear}(\bm{Z}_{t})+\texttt{bias})).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = nonlinearity ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + linear start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( linear ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bias ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This functional form is very similar to that of a residual network layer, namely,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}_{t+1}=\bm{Z}_{t}+\texttt{linear}_{1}^{\top}(\texttt{nonlinearity}(\texttt{linear}_{2}(\bm{Z}_{t})+\texttt{bias}_{1})+\texttt{bias}_{2}," class="ltx_math_unparsed" display="block" id="S3.E13.m1"><semantics><mrow><msub><mi>𝒁</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo>+</mo><msubsup><mtext class="ltx_mathvariant_monospace">linear</mtext><mn>1</mn><mo>⊤</mo></msubsup><mrow><mo stretchy="false">(</mo><mtext class="ltx_mathvariant_monospace">nonlinearity</mtext><mrow><mo stretchy="false">(</mo><msub><mtext class="ltx_mathvariant_monospace">linear</mtext><mn>2</mn></msub><mrow><mo stretchy="false">(</mo><msub><mi>𝒁</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>+</mo><msub><mtext class="ltx_mathvariant_monospace">bias</mtext><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>+</mo><msub><mtext class="ltx_mathvariant_monospace">bias</mtext><mn>2</mn></msub><mo>,</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}_{t+1}=\bm{Z}_{t}+\texttt{linear}_{1}^{\top}(\texttt{nonlinearity}(\texttt{linear}_{2}(\bm{Z}_{t})+\texttt{bias}_{1})+\texttt{bias}_{2},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + linear start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( nonlinearity ( linear start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bias start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + bias start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">only decoupling the linear maps (i.e., matrix multiplications), adding a bias, and moving the nonlinearity.
The nonlinearity in (<a class="ltx_ref" href="#S3.E9" title="Equation 2.3.9 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.9</span></a>) is notably similar to the
commonly-used ReLU activation function <math alttext="x\mapsto\max\{x,0\}" class="ltx_Math" display="inline" id="S3.SS1.p5.m1"><semantics><mrow><mi>x</mi><mo stretchy="false">↦</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mi>x</mi><mo>,</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x\mapsto\max\{x,0\}</annotation><annotation encoding="application/x-llamapun">italic_x ↦ roman_max { italic_x , 0 }</annotation></semantics></math> in deep
learning—in particular, the soft-thresholding operator is like a “signed”
ReLU activation, which is also shifted by a bias.
The ISTA, then, can be viewed as a forward pass of a primitive (recurrent one-layer) neural network. We argue in Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a> that such operations are essential to deep representation learning.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3.2 </span>Overcomplete Dictionary Learning</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">Recall that we have the data model</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}=\bm{A}\bm{Z}+\bm{E}," class="ltx_Math" display="block" id="S3.E14.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>+</mo><mi>𝑬</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{A}\bm{Z}+\bm{E},</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is sparse, and our goal previously was to estimate <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> given
knowledge of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the dictionary atoms <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p1.m4"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>. Now we turn to
the more practical and more difficult case where we do not know
either <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> or <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p1.m6"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and seek to learn them from a large dataset.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">A direct generalization of <a class="ltx_ref" href="#S3.E5" title="In 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2.3.5</span></a> suggests solving the problem</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{A}},\tilde{\bm{Z}}}\left\{\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+\lambda\|\tilde{\bm{Z}}\|_{1}\right\}." class="ltx_Math" display="block" id="S3.E15.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{A}},\tilde{\bm{Z}}}\left\{\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+\lambda\|\tilde{\bm{Z}}\|_{1}\right\}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG , over~ start_ARG bold_italic_Z end_ARG end_POSTSUBSCRIPT { ∥ bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, the bilinear term in <a class="ltx_ref" href="#S3.E15" title="In 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2.3.15</span></a> introduces a scale
ambiguity that hinders convergence: given any point <math alttext="(\tilde{\bm{A}},\tilde{\bm{Z}})" class="ltx_Math" display="inline" id="S3.SS2.p2.m1"><semantics><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\tilde{\bm{A}},\tilde{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">( over~ start_ARG bold_italic_A end_ARG , over~ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> and any constant <math alttext="c&gt;0" class="ltx_Math" display="inline" id="S3.SS2.p2.m2"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c&gt;0</annotation><annotation encoding="application/x-llamapun">italic_c &gt; 0</annotation></semantics></math>, the substitution
<math alttext="(c^{-1}\tilde{\bm{A}},c\tilde{\bm{Z}})" class="ltx_Math" display="inline" id="S3.SS2.p2.m3"><semantics><mrow><mo stretchy="false">(</mo><mrow><msup><mi>c</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover></mrow><mo>,</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(c^{-1}\tilde{\bm{A}},c\tilde{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">( italic_c start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT over~ start_ARG bold_italic_A end_ARG , italic_c over~ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> gives loss value</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+c\lambda\|\tilde{\bm{Z}}\|_{1}." class="ltx_Math" display="block" id="S3.E16.m1"><semantics><mrow><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+c\lambda\|\tilde{\bm{Z}}\|_{1}.</annotation><annotation encoding="application/x-llamapun">∥ bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_c italic_λ ∥ over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This loss is evidently minimized over <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.p2.m4"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation><annotation encoding="application/x-llamapun">italic_c</annotation></semantics></math> by taking <math alttext="c\to 0" class="ltx_Math" display="inline" id="S3.SS2.p2.m5"><semantics><mrow><mi>c</mi><mo stretchy="false">→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c\to 0</annotation><annotation encoding="application/x-llamapun">italic_c → 0</annotation></semantics></math>, which corresponds
to making the rescaled dictionary <math alttext="c^{-1}\tilde{\bm{A}}" class="ltx_Math" display="inline" id="S3.SS2.p2.m6"><semantics><mrow><msup><mi>c</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">c^{-1}\tilde{\bm{A}}</annotation><annotation encoding="application/x-llamapun">italic_c start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT over~ start_ARG bold_italic_A end_ARG</annotation></semantics></math> go ‘to infinity’. In
particular, the iterates of any optimization algorithm solving
(<a class="ltx_ref" href="#S3.E15" title="Equation 2.3.15 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.15</span></a>) will not converge.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">This issue with (<a class="ltx_ref" href="#S3.E15" title="Equation 2.3.15 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.15</span></a>) is dealt with by adding
a constraint on the scale of the columns of the dictionary <math alttext="\tilde{\bm{A}}" class="ltx_Math" display="inline" id="S3.SS2.p3.m1"><semantics><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{\bm{A}}</annotation><annotation encoding="application/x-llamapun">over~ start_ARG bold_italic_A end_ARG</annotation></semantics></math>.
For example, it is common to assume that each column <math alttext="\bm{A}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p3.m2"><semantics><msub><mi>𝑨</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\bm{A}_{j}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> of the dictionary
<math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p3.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E14" title="Equation 2.3.14 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.14</span></a>) has bounded <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S3.SS2.p3.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>
norm—say, without loss of generality, by <math alttext="1" class="ltx_Math" display="inline" id="S3.SS2.p3.m5"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>.
We then enforce this as a constraint, giving the objective</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\tilde{\bm{Z}},\tilde{\bm{A}}\,:\,\|\tilde{\bm{A}}_{j}\|_{2}\leq 1}\left\{\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+\lambda\|\tilde{\bm{Z}}\|_{1}\right\}." class="ltx_Math" display="block" id="S3.E17.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mrow><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo>,</mo><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover></mrow><mo lspace="0.278em" rspace="0.448em">:</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><msub><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn></mrow></mrow></munder><mo>⁡</mo><mrow><mo>{</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mover accent="true"><mi>𝑨</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mover accent="true"><mi>𝒁</mi><mo>~</mo></mover><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\tilde{\bm{Z}},\tilde{\bm{A}}\,:\,\|\tilde{\bm{A}}_{j}\|_{2}\leq 1}\left\{\|\bm{X}-\tilde{\bm{A}}\tilde{\bm{Z}}\|_{F}^{2}+\lambda\|\tilde{\bm{Z}}\|_{1}\right\}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_Z end_ARG , over~ start_ARG bold_italic_A end_ARG : ∥ over~ start_ARG bold_italic_A end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ 1 end_POSTSUBSCRIPT { ∥ bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ over~ start_ARG bold_italic_Z end_ARG ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Constrained optimization problems such as (<a class="ltx_ref" href="#S3.E17" title="Equation 2.3.17 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.17</span></a>)
can be solved by a host of sophisticated algorithms
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx198" title="">NW06</a>]</cite>. However, a simple and scalable one is actually
furnished by the same proximal gradient descent algorithm that we used to solve
the sparse coding problem in the previous section.
We can encode each constraint as additional regularization term, via the
characteristic function for the constraint set—details are given in
<a class="ltx_ref" href="A1.html#Thmexample1" title="Example A.1. ‣ A.1.3 Proximal Gradient Descent for Non-Smooth Problems ‣ A.1 Steepest Descent ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>.
Applying proximal gradient descent to the resulting regularized problem is
equivalent to <span class="ltx_text ltx_font_italic">projected gradient descent</span>, in which, at each iteration,
the iterates after taking a gradient descent step are projected onto the
constraint set.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark11">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 2.11</span></span><span class="ltx_text ltx_font_italic"> </span>(<math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark11.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> maximization versus <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="Thmremark11.m2"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> minimization)<span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark11.p1">
<p class="ltx_p">Note that the above problem formulation follows naturally from the LASSO
formulation (<a class="ltx_ref" href="#S3.E5" title="Equation 2.3.5 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.5</span></a>) for sparse coding. We promote the
sparsity of the solution via the <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="Thmremark11.p1.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm. Nevertheless, if we are only interested in recovering the over-complete dictionary <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmremark11.p1.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, the <math alttext="\ell^{4}" class="ltx_Math" display="inline" id="Thmremark11.p1.m3"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>4</mn></msup><annotation encoding="application/x-tex">\ell^{4}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math> maximization scheme introduced in Section <a class="ltx_ref" href="#S2.SS2" title="2.2.2 Complete Dictionary Learning ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.2</span></a> also generalizes to the over-complete case, without any significant modification. Interested readers may refer to the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx217" title="">QZL+20</a>]</cite>.</p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p">The above problem (<a class="ltx_ref" href="#S3.E17" title="Equation 2.3.17 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.17</span></a>), which we call
<span class="ltx_text ltx_font_italic">overcomplete dictionary learning</span>, is nonconvex as here both <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p4.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> and
<math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p4.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> are unknown. It cannot be solved easily by the standard convex
optimization toolkit. Nevertheless, because it is interesting, simple to state,
and practically important, there have been many important works dedicated to
different algorithms and theoretical analysis for this problem. Here, for the
interest of this manuscript, we present an idiomatic approach to solve this problem
which is closer to the spirit of deep learning.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p">From our experience with the LASSO problem above, it is easy to see that, for
the two unknowns <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p5.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> and <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p5.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, if we fix one and optimize the other, each
subproblem is in fact convex and easy to solve. This naturally suggests that we
could attempt to solve the above program (<a class="ltx_ref" href="#S3.E17" title="Equation 2.3.17 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.17</span></a>) by
minimizing against <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.p5.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> or <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p5.m4"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> alternatively, say using gradient descent.
Coupled with a natural choice of initialization, this leads to the following
iterative scheme:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx9">
<tbody id="S3.E18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta\bm{A}_{+}^{\top}(\bm{A}_{+}\bm{Z}^{\ell}-\bm{X})\right),\quad\bm{Z}^{1}=\mathbf{0},\quad\forall\ell\in[L]" class="ltx_Math" display="inline" id="S3.E18.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msub><mi>S</mi><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑨</mi><mo>+</mo><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑨</mi><mo>+</mo></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>=</mo><mn>𝟎</mn></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi mathvariant="normal">ℓ</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta\bm{A}_{+}^{\top}(\bm{A}_{+}\bm{Z}^{\ell}-\bm{X})\right),\quad\bm{Z}^{1}=\mathbf{0},\quad\forall\ell\in[L]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = italic_S start_POSTSUBSCRIPT italic_η italic_λ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - 2 italic_η bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_X ) ) , bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = bold_0 , ∀ roman_ℓ ∈ [ italic_L ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.18)</span></td>
</tr></tbody>
<tbody id="S3.E19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{+}=\bm{Z}^{L}," class="ltx_Math" display="inline" id="S3.E19.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mo>+</mo></msup><mo>=</mo><msup><mi>𝒁</mi><mi>L</mi></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{+}=\bm{Z}^{L},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.19)</span></td>
</tr></tbody>
<tbody id="S3.E20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{A}_{t+1}=\operatorname{proj}_{\|(\,\cdot\,)_{j}\|_{2}\leq 1,\,\forall j}\left(\bm{A}_{t}-2\nu(\bm{A}_{t}\bm{Z}^{+}-\bm{X})(\bm{Z}^{+})^{\top}\right),\quad(\bm{A}_{1})_{j}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\tfrac{1}{D}\bm{I}),\enspace\forall j\in[m],\quad\;\;\forall t\in[T]," class="ltx_math_unparsed" display="inline" id="S3.E20.m1"><semantics><mrow><mrow><mrow><msub><mi>𝑨</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>proj</mi><mrow><msub><mrow><mo stretchy="false">‖</mo><msub><mrow><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≤</mo><mrow><mn>1</mn><mo rspace="0.337em">,</mo><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow></mrow></mrow></msub><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>𝑨</mi><mi>t</mi></msub><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>ν</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑨</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mo>+</mo></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝑨</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mi>j</mi></msub><mover><mo>∼</mo><mrow><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">d</mi><mo lspace="0em">.</mo></mrow></mover><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mfrac><mn>1</mn><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo></mrow></mrow><mo rspace="1.727em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>T</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{A}_{t+1}=\operatorname{proj}_{\|(\,\cdot\,)_{j}\|_{2}\leq 1,\,\forall j}\left(\bm{A}_{t}-2\nu(\bm{A}_{t}\bm{Z}^{+}-\bm{X})(\bm{Z}^{+})^{\top}\right),\quad(\bm{A}_{1})_{j}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\tfrac{1}{D}\bm{I}),\enspace\forall j\in[m],\quad\;\;\forall t\in[T],</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = roman_proj start_POSTSUBSCRIPT ∥ ( ⋅ ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ 1 , ∀ italic_j end_POSTSUBSCRIPT ( bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - 2 italic_ν ( bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT - bold_italic_X ) ( bold_italic_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) , ( bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG ∼ end_ARG start_ARG roman_i . roman_i . roman_d . end_ARG end_RELOP caligraphic_N ( bold_0 , divide start_ARG 1 end_ARG start_ARG italic_D end_ARG bold_italic_I ) , ∀ italic_j ∈ [ italic_m ] , ∀ italic_t ∈ [ italic_T ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.20)</span></td>
</tr></tbody>
<tbody id="S3.E21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{A}_{+}=\bm{A}_{T}," class="ltx_Math" display="inline" id="S3.E21.m1"><semantics><mrow><mrow><msub><mi>𝑨</mi><mo>+</mo></msub><mo>=</mo><msub><mi>𝑨</mi><mi>T</mi></msub></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{A}_{+}=\bm{A}_{T},</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the projection operation in the update for <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS2.p5.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> ensures each column has
at most unit <math alttext="\ell^{2}" class="ltx_Math" display="inline" id="S3.SS2.p5.m6"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^{2}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> norm, via <math alttext="\bm{A}_{j}\mapsto\bm{A}_{j}/\max\{\|\bm{A}_{j}\|_{2},1\}" class="ltx_Math" display="inline" id="S3.SS2.p5.m7"><semantics><mrow><msub><mi>𝑨</mi><mi>j</mi></msub><mo stretchy="false">↦</mo><mrow><msub><mi>𝑨</mi><mi>j</mi></msub><mo>/</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝑨</mi><mi>j</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{A}_{j}\mapsto\bm{A}_{j}/\max\{\|\bm{A}_{j}\|_{2},1\}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ↦ bold_italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / roman_max { ∥ bold_italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , 1 }</annotation></semantics></math>, and where <math alttext="\bm{A}_{+}" class="ltx_Math" display="inline" id="S3.SS2.p5.m8"><semantics><msub><mi>𝑨</mi><mo>+</mo></msub><annotation encoding="application/x-tex">\bm{A}_{+}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT</annotation></semantics></math> is initialized with each column i.i.d. <math alttext="\mathcal{N}(\mathbf{0},\tfrac{1}{D}\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.p5.m9"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mfrac><mn>1</mn><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\mathbf{0},\tfrac{1}{D}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , divide start_ARG 1 end_ARG start_ARG italic_D end_ARG bold_italic_I )</annotation></semantics></math>.
The above consists of one ‘block’ of alternating minimization, and we repeatedly
perform such blocks, each with independent initializations, until convergence.
Above, we have used two separate indices <math alttext="\{t\}" class="ltx_Math" display="inline" id="S3.SS2.p5.m10"><semantics><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t\}</annotation><annotation encoding="application/x-llamapun">{ italic_t }</annotation></semantics></math> and <math alttext="\{\ell\}" class="ltx_Math" display="inline" id="S3.SS2.p5.m11"><semantics><mrow><mo stretchy="false">{</mo><mi mathvariant="normal">ℓ</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\ell\}</annotation><annotation encoding="application/x-llamapun">{ roman_ℓ }</annotation></semantics></math> to indicate the
iterations. As we will see later, this allows us to interpret the two updates
separately in the context of deep learning.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p">Despite the dictionary learning problem being a nonconvex problem, it has been
shown that alternating minimization type algorithms indeed converge to
the correct solution, at least locally. See, for example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx2" title="">AAJ+16</a>]</cite>.
As a practical demonstration, the above algorithm (with <math alttext="L=T=1" class="ltx_Math" display="inline" id="S3.SS2.p6.m1"><semantics><mrow><mi>L</mi><mo>=</mo><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L=T=1</annotation><annotation encoding="application/x-llamapun">italic_L = italic_T = 1</annotation></semantics></math>) was used to
generate the results for overcomplete dictionary learning in
<a class="ltx_ref" href="#F6" title="In 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2.6</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3.3 </span>Learned Deep Sparse Coding</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">The main insight from the alternating minimization algorithm for
overcomplete dictionary learning in the previous section
(<a class="ltx_ref" href="#S3.E18" title="In 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Equations</span> <span class="ltx_text ltx_ref_tag">2.3.18</span></a> and <a class="ltx_ref" href="#S3.E20" title="Equation 2.3.20 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.20</span></a>) is to notice that <span class="ltx_text ltx_font_italic">when we
fix <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.m1"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, the ISTA update for <math alttext="\bm{Z}^{\ell}" class="ltx_Math" display="inline" id="S3.SS3.p1.m2"><semantics><msup><mi>𝐙</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> (<a class="ltx_ref" href="#S3.E18" title="Equation 2.3.18 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.18</span></a>) looks like
the forward pass of a deep neural network with weights given by <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.m3"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> (and
<math alttext="\bm{A}^{\top}" class="ltx_Math" display="inline" id="S3.SS3.p1.m4"><semantics><msup><mi>𝐀</mi><mo>⊤</mo></msup><annotation encoding="application/x-tex">\bm{A}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>)</span>. But in general, we do not know the true <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, and the current
estimate <math alttext="\bm{A}_{+}" class="ltx_Math" display="inline" id="S3.SS3.p1.m6"><semantics><msub><mi>𝑨</mi><mo>+</mo></msub><annotation encoding="application/x-tex">\bm{A}_{+}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT</annotation></semantics></math> could be erroneous. Hence it needs to be further updated using
(<a class="ltx_ref" href="#S3.E20" title="Equation 2.3.20 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.20</span></a>) based on the residual of using the current estimate of the
sparse codes <math alttext="\bm{Z}^{+}" class="ltx_Math" display="inline" id="S3.SS3.p1.m7"><semantics><msup><mi>𝒁</mi><mo>+</mo></msup><annotation encoding="application/x-tex">\bm{Z}^{+}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math> to reconstruct <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.p1.m8"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. The alternating minimization
algorithm iterates these two procedures until convergence. But we can instead
extrapolate, and design other learning procedures by combining these insights
with techniques from deep learning. This leads to more interpretable network
architectures, which will be a recurring theme throughout this manuscript.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Learned ISTA.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p">The above deep-network interpretation of the
alternating minimization is more conceptual than practical, as the process could
be rather inefficient and take many layers or iterations to converge.
But this
is mainly because we try to infer both <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> from <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>.
The problem can be significantly simplified and the above iterations can be made
much more efficient in the <span class="ltx_text ltx_font_italic">supervised</span> setting, where we have a dataset of input
and output pairs <math alttext="(\bm{X},\bm{Z})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m4"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{Z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_Z )</annotation></semantics></math> distributed according to
(<a class="ltx_ref" href="#S3.E14" title="Equation 2.3.14 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.14</span></a>) and we only seek to learn
<math alttext="\bm{A}^{\ell}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m5"><semantics><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{A}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> for the layerwise learnable sparse coding iterations
(<a class="ltx_ref" href="#S3.E29" title="Equation 2.3.29 ‣ Layerwise learned sparse coding? ‣ 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.29</span></a>):</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx10">
<tbody id="S3.E22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta(\bm{A}^{\ell})^{\top}(\bm{A}^{\ell}\bm{Z}^{\ell}-\bm{X})\right),\quad\forall\ell\in[L]." class="ltx_Math" display="inline" id="S3.E22.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msub><mi>S</mi><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi mathvariant="normal">ℓ</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta(\bm{A}^{\ell})^{\top}(\bm{A}^{\ell}\bm{Z}^{\ell}-\bm{X})\right),\quad\forall\ell\in[L].</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = italic_S start_POSTSUBSCRIPT italic_η italic_λ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - 2 italic_η ( bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_X ) ) , ∀ roman_ℓ ∈ [ italic_L ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we denote the operator for each iteration as <math alttext="\bm{Z}^{\ell+1}=f(\bm{A}^{\ell},\bm{Z}^{\ell})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m6"><semantics><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><mo>,</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{\ell+1}=f(\bm{A}^{\ell},\bm{Z}^{\ell})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT , bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT )</annotation></semantics></math>, the above iteration can be illustrated in terms of a diagram:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X},\bm{Z}^{1}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{1},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{2}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{2},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{3}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{3},\,\cdot\,)\hskip 2.84526pt}\cdots\bm{Z}^{L}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{L},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{L+1}\approx\bm{Z}." class="ltx_Math" display="block" id="S3.Ex1.m1"><semantics><mrow><mrow><mrow><mi>𝑿</mi><mo>,</mo><msup><mi>𝒁</mi><mn>1</mn></msup></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo rspace="0em">,</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mover><msup><mi>𝒁</mi><mn>2</mn></msup><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>2</mn></msup><mo rspace="0em">,</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mover><msup><mi>𝒁</mi><mn>3</mn></msup><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>3</mn></msup><mo rspace="0em">,</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><mi mathvariant="normal">⋯</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi>L</mi></msup></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mi>L</mi></msup><mo rspace="0em">,</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mover><msup><mi>𝒁</mi><mrow><mi>L</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>≈</mo><mi>𝒁</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X},\bm{Z}^{1}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{1},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{2}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{2},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{3}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{3},\,\cdot\,)\hskip 2.84526pt}\cdots\bm{Z}^{L}\xrightarrow{\hskip 2.84526ptf(\bm{A}^{L},\,\cdot\,)\hskip 2.84526pt}\bm{Z}^{L+1}\approx\bm{Z}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X , bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ⋅ ) end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ⋅ ) end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_A start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , ⋅ ) end_OVERACCENT → end_ARROW ⋯ bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_A start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , ⋅ ) end_OVERACCENT → end_ARROW bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT ≈ bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, given the sequential architecture, to learn the operator <math alttext="\bm{A}^{\ell}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m7"><semantics><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><annotation encoding="application/x-tex">\bm{A}^{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT</annotation></semantics></math> at
each layer, it is completely natural to learn it, say via back propagation
(BP),<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>See Appendix <a class="ltx_ref" href="A1.html#S2" title="A.2 Computing Gradients via Automatic Differentiation ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">A.2</span></a> for a brief description of BP.</span></span></span> by minimizing the error between the final code <math alttext="\bm{Z}^{L}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m8"><semantics><msup><mi>𝒁</mi><mi>L</mi></msup><annotation encoding="application/x-tex">\bm{Z}^{L}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> and the ground truth <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m9"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\{\bm{A}^{\ell}\}}\big{\|}\bm{Z}^{L}(\bm{A}^{1},\ldots,\bm{A}^{L})-\bm{Z}\big{\|}_{2}^{2}." class="ltx_Math" display="block" id="S3.E23.m1"><semantics><mrow><mrow><munder><mi>min</mi><mrow><mo stretchy="false">{</mo><msup><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msup><mo stretchy="false">}</mo></mrow></munder><mo>⁡</mo><msubsup><mrow><mo maxsize="120%" minsize="120%">‖</mo><mrow><mrow><msup><mi>𝒁</mi><mi>L</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>𝑨</mi><mi>L</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>𝒁</mi></mrow><mo maxsize="120%" minsize="120%">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\{\bm{A}^{\ell}\}}\big{\|}\bm{Z}^{L}(\bm{A}^{1},\ldots,\bm{A}^{L})-\bm{Z}\big{\|}_{2}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT { bold_italic_A start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT } end_POSTSUBSCRIPT ∥ bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , bold_italic_A start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ) - bold_italic_Z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is the basis of the Learned ISTA (LISTA) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx95" title="">GL10</a>]</cite>, which can be viewed as the learning algorithm for a deep neural network, which tries to emulate the sparse encoding process from <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m10"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.m11"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>. In particular, it can be viewed as a <span class="ltx_text ltx_font_italic">simple representation learning algorithm</span>.
In fact, this same methodology can be used as a basis to understand the
representations computed in more powerful network architectures, such as
transformers. We develop these implications in detail in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Sparse Autoencoders.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p">The original motivation for overcomplete dictionary learning was to provide
a simple generative model for high-dimensional data. We have seen with LISTA that, in
addition, iterative algorithms for learning sparsely-used overcomplete
dictionaries provide an interpretation for ReLU-like deep networks, which we
will generalize in later chapters to more complex data distributions than
(<a class="ltx_ref" href="#S3.E14" title="Equation 2.3.14 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.14</span></a>).
But it is also worth noting that even in the modern era of large models, the data
generating model (<a class="ltx_ref" href="#S3.E14" title="Equation 2.3.14 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.14</span></a>) provides a useful
practical basis for
<span class="ltx_text ltx_font_italic">interpreting features in pretrained large-scale deep networks</span>, such as
transformers, following the hypothesis that the (non-interpretable, <span class="ltx_text ltx_font_italic">a
priori</span>) features in these networks consist of sparse “superpositions” of
underlying features, which are themselves interpretable
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx80" title="">EHO+22a</a>]</cite>. These <span class="ltx_text ltx_font_italic">unsupervised</span> learning paradigms
are generally more data friendly than LISTA, as well, which requires large amounts
of labeled <math alttext="(\bm{X},\bm{Z})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\bm{Z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , bold_italic_Z )</annotation></semantics></math> pairs for supervised training.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p2">
<p class="ltx_p">We can use our development of the LISTA algorithm above to understand common
practices in this field of research.
In the most straightforward instantiation (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx114" title="">HCS+24</a>, <a class="ltx_ref" href="bib.html#bibx89" title="">GTT+25</a>]</cite>), a large number of features from a pretrained deep network <math alttext="h" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m1"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math>
are collected from different inputs <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m2"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, which themselves are chosen based
on a desired interpretation task.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>For example, the inputs <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="footnote12.m1"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> could
correspond to texts containing samples of computer code in different programming
languages, with our task being to try to identify interpretable features in
a transformer feature map <math alttext="h" class="ltx_Math" display="inline" id="footnote12.m2"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> corresponding to different salient aspects of the
input, such as the specific programming language (distinct across input
“classes”) or the need to insert a matching parenthesis at the current
position (common across input “classes”). We discuss the use of deep networks,
and in particular transformers, for text representation
learning in greater detail in <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">7</span></a>.</span></span></span> For
simplicity, we will use <math alttext="h" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m3"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> to denote the pre-selected feature map in question,
with <math alttext="D" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m4"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>-dimensional features; given <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m5"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> sample inputs, let <math alttext="\bm{H}\in\mathbb{R}^{D\times N}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m6"><semantics><mrow><mi>𝑯</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{H}\in\mathbb{R}^{D\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_H ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> denote the full matrix of features of <math alttext="h" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m7"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math>.
Then a so-called sparse autoencoder <math alttext="f:\mathbb{R}^{D}\to\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m8"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>D</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{D}\to\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, with decoder <math alttext="g:\mathbb{R}^{d}\to\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m9"><semantics><mrow><mi>g</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow></mrow><annotation encoding="application/x-tex">g:\mathbb{R}^{d}\to\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">italic_g : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, is trained via the LASSO loss (<a class="ltx_ref" href="#S3.E5" title="Equation 2.3.5 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.5</span></a>):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{f,g}\|\bm{H}-g(f(\bm{H})))\|_{F}^{2}+\lambda\|f(\bm{H})\|_{1}," class="ltx_math_unparsed" display="block" id="S3.E24.m1"><semantics><mrow><munder><mi>min</mi><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow></munder><mo lspace="0em" rspace="0.167em">∥</mo><mi>𝑯</mi><mo>−</mo><mi>g</mi><mrow><mo stretchy="false">(</mo><mi>f</mi><mrow><mo stretchy="false">(</mo><mi>𝑯</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo><mo lspace="0em" rspace="0.167em">∥</mo><msub><mi></mi><mi>F</mi></msub><msup><mi></mi><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo lspace="0em" rspace="0.167em">∥</mo><mi>f</mi><mo stretchy="false">(</mo><mi>𝑯</mi><mo stretchy="false">)</mo><mo lspace="0em" rspace="0.167em">∥</mo><msub><mi></mi><mn>1</mn></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{f,g}\|\bm{H}-g(f(\bm{H})))\|_{F}^{2}+\lambda\|f(\bm{H})\|_{1},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ∥ bold_italic_H - italic_g ( italic_f ( bold_italic_H ) ) ) ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_λ ∥ italic_f ( bold_italic_H ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the sparse autoencoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m10"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> takes the form of a one-layer neural
network, i.e. <math alttext="f(\bm{h}_{i})=\sigma(\bm{W}_{\mathrm{enc}}(\bm{h}_{i}-\bm{b})+\bm{b}_{\mathrm{enc}})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m11"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒉</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑾</mi><mi>enc</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒉</mi><mi>i</mi></msub><mo>−</mo><mi>𝒃</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msub><mi>𝒃</mi><mi>enc</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f(\bm{h}_{i})=\sigma(\bm{W}_{\mathrm{enc}}(\bm{h}_{i}-\bm{b})+\bm{b}_{\mathrm{enc}})</annotation><annotation encoding="application/x-llamapun">italic_f ( bold_italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_σ ( bold_italic_W start_POSTSUBSCRIPT roman_enc end_POSTSUBSCRIPT ( bold_italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_italic_b ) + bold_italic_b start_POSTSUBSCRIPT roman_enc end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="\sigma(x)=\max\{x,0\}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m12"><semantics><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mi>x</mi><mo>,</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sigma(x)=\max\{x,0\}</annotation><annotation encoding="application/x-llamapun">italic_σ ( italic_x ) = roman_max { italic_x , 0 }</annotation></semantics></math> is the ReLU activation
function, and the decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m13"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> is linear, so that <math alttext="g(\bm{z}_{i})=\bm{W}_{\mathrm{dec}}\bm{z}+\bm{b}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p2.m14"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒛</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>𝑾</mi><mi>dec</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi>𝒃</mi></mrow></mrow><annotation encoding="application/x-tex">g(\bm{z}_{i})=\bm{W}_{\mathrm{dec}}\bm{z}+\bm{b}</annotation><annotation encoding="application/x-llamapun">italic_g ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = bold_italic_W start_POSTSUBSCRIPT roman_dec end_POSTSUBSCRIPT bold_italic_z + bold_italic_b</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p3">
<p class="ltx_p">The parameterization and training procedure (<a class="ltx_ref" href="#S3.E24" title="Equation 2.3.24 ‣ Sparse Autoencoders. ‣ 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.24</span></a>) may initially
seem to be an arbitrary application of deep learning to the sparse coding
problem, but it is actually highly aligned with the algorithms we have studied
above for layerwise sparse coding with a learned dictionary.
In particular, recall the LISTA architecture <math alttext="\bm{Z}^{L}=f(\bm{A}^{L},f(\bm{A}^{L-1},\cdots,f(\bm{A}^{1},\bm{X})\cdots))" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m1"><semantics><mrow><msup><mi>𝒁</mi><mi>L</mi></msup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mi>L</mi></msup><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo>,</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">⋯</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{L}=f(\bm{A}^{L},f(\bm{A}^{L-1},\cdots,f(\bm{A}^{1},\bm{X})\cdots))</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , italic_f ( bold_italic_A start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT , ⋯ , italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) ⋯ ) )</annotation></semantics></math>.
In the special case <math alttext="L=2" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m2"><semantics><mrow><mi>L</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">L=2</annotation><annotation encoding="application/x-llamapun">italic_L = 2</annotation></semantics></math>, we have</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=S_{\eta\lambda}\left(\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}(\bm{A}^{1}\bm{Z}^{1}-\bm{X})\right)." class="ltx_Math" display="block" id="S3.E25.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mn>2</mn></msup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo>,</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>S</mi><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑨</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mn>1</mn></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=S_{\eta\lambda}\left(\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}(\bm{A}^{1}\bm{Z}^{1}-\bm{X})\right).</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = italic_S start_POSTSUBSCRIPT italic_η italic_λ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - 2 italic_η ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - bold_italic_X ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Let us assume that the sparse codes <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> in question are nonnegative, i.e.,
that <math alttext="\bm{Z}\geq\mathbf{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m4"><semantics><mrow><mi>𝒁</mi><mo>≥</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{Z}\geq\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z ≥ bold_0</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>In the data generating model
(<a class="ltx_ref" href="#S3.E14" title="Equation 2.3.14 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.14</span></a>), an arbitrary
dictionary-and-sparse-code pair <math alttext="(\bm{A},\bm{Z})" class="ltx_Math" display="inline" id="footnote13.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑨</mi><mo>,</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{A},\bm{Z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_A , bold_italic_Z )</annotation></semantics></math> can be replaced by one in which
<math alttext="\bm{Z}\geq\mathbf{0}" class="ltx_Math" display="inline" id="footnote13.m2"><semantics><mrow><mi>𝒁</mi><mo>≥</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{Z}\geq\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z ≥ bold_0</annotation></semantics></math> simply by doubling the number of columns in <math alttext="\bm{A}" class="ltx_Math" display="inline" id="footnote13.m3"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, so from
a modeling perspective, this is a very mild assumption.</span></span></span>
Then (see <a class="ltx_ref" href="A1.html#Thmexample3" title="Example A.3. ‣ A.1.3 Proximal Gradient Descent for Non-Smooth Problems ‣ A.1 Steepest Descent ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Example</span> <span class="ltx_text ltx_ref_tag">A.3</span></a>), we can consider an equivalent
LISTA architecture obtained from the sparse coding objective with an additional
nonnegativity constraint on <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=\max\left\{\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}(\bm{A}^{1}\bm{Z}^{1}-\bm{X})-\lambda\eta\mathbf{1},0\right\}," class="ltx_Math" display="block" id="S3.E26.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mn>2</mn></msup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo>,</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>𝑨</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mn>1</mn></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn></mrow></mrow><mo>,</mo><mn>0</mn><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=\max\left\{\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}(\bm{A}^{1}\bm{Z}^{1}-\bm{X})-\lambda\eta\mathbf{1},0\right\},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = roman_max { bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - 2 italic_η ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - bold_italic_X ) - italic_λ italic_η bold_1 , 0 } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and after some algebra, express this as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=\max\left\{2\eta(\bm{A}^{1})^{\top}+\left(\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}\bm{A}^{1}\bm{Z}^{1}-\lambda\eta\mathbf{1}\right),0\right\}." class="ltx_Math" display="block" id="S3.E27.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mn>2</mn></msup><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo>,</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑨</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mn>1</mn></msup></mrow><mo>−</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn></mrow></mrow><mo>)</mo></mrow></mrow><mo>,</mo><mn>0</mn><mo>}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}^{2}=f(\bm{A}^{1},\bm{X})=\max\left\{2\eta(\bm{A}^{1})^{\top}+\left(\bm{Z}^{1}-2\eta(\bm{A}^{1})^{\top}\bm{A}^{1}\bm{Z}^{1}-\lambda\eta\mathbf{1}\right),0\right\}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = roman_max { 2 italic_η ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - 2 italic_η ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - italic_λ italic_η bold_1 ) , 0 } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Given the ability to change the sparse code initialization <math alttext="\bm{Z}^{1}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m6"><semantics><msup><mi>𝒁</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> as
a learnable parameter (which, in the current framework, must have all columns
equal to the same learnable vector), this has the form of a ReLU neural network
with learnable bias—identical to the sparse autoencoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m7"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>!
Moreover, to <span class="ltx_text ltx_font_italic">decode</span> the learned sparse codes <math alttext="\bm{Z}^{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m8"><semantics><msup><mi>𝒁</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\bm{Z}^{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, it is natural to
apply the learned dictionary <math alttext="\bm{Z}^{2}\mapsto\bm{A}^{1}\bm{Z}^{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m9"><semantics><mrow><msup><mi>𝒁</mi><mn>2</mn></msup><mo stretchy="false">↦</mo><mrow><msup><mi>𝑨</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mn>2</mn></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}^{2}\mapsto\bm{A}^{1}\bm{Z}^{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ↦ bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. Then the only
difference between this and the SAE decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m10"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> is the additional bias <math alttext="\bm{b}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m11"><semantics><mi>𝒃</mi><annotation encoding="application/x-tex">\bm{b}</annotation><annotation encoding="application/x-llamapun">bold_italic_b</annotation></semantics></math>,
which can technically be absorbed into <math alttext="\bm{H}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m12"><semantics><mi>𝑯</mi><annotation encoding="application/x-tex">\bm{H}</annotation><annotation encoding="application/x-llamapun">bold_italic_H</annotation></semantics></math> and <math alttext="f" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p3.m13"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> in the training objective
(<a class="ltx_ref" href="#S3.E24" title="Equation 2.3.24 ‣ Sparse Autoencoders. ‣ 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.24</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p4">
<p class="ltx_p">Thus, the SAE parameterization and training procedure coincides with
LISTA training with <math alttext="L=1" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p4.m1"><semantics><mrow><mi>L</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L=1</annotation><annotation encoding="application/x-llamapun">italic_L = 1</annotation></semantics></math>, and a modified training objective—using the
LASSO objective (<a class="ltx_ref" href="#S3.E5" title="Equation 2.3.5 ‣ 2.3.1 Sparse Coding with an Overcomplete Dictionary ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.5</span></a>), which remains <span class="ltx_text ltx_font_italic">unsupervised</span>, instead
of the supervised reconstruction loss (<a class="ltx_ref" href="#S3.E23" title="Equation 2.3.23 ‣ Learned ISTA. ‣ 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.23</span></a>) used in vanilla
LISTA. In particular, we can understand the SAE architecture in terms of our
interpretation of the LISTA architecture in terms of layerwise sparse coding in
(<a class="ltx_ref" href="#S3.E29" title="Equation 2.3.29 ‣ Layerwise learned sparse coding? ‣ 2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.29</span></a>). This connection is suggestive of a host of new
design strategies for improving practical interpretability methodology, many of
which remain tantalizingly unexplored. We begin to lay out some connections to
broader autoencoding methodology in <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Layerwise learned sparse coding?</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p">In the supervised setting, LISTA provides a deep neural network analogue of
the sparse coding iteration, with layerwise-learned dictionaries, inspired by
alternating minimization; even in the unsupervised setting, the same methodology
can be applied to learning, as with sparse autoencoders.
But the connection between low-dimensional-structure-seeking optimization algorithms
and deep network architectures goes much deeper than this, and suggests an array
of scalable and natural neural learning architectures which may even be usable
without backpropagation.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p2">
<p class="ltx_p">As a simple illustration, we return the alternating minimization iterations
(<a class="ltx_ref" href="#S3.E18" title="Equation 2.3.18 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.18</span></a>) and (<a class="ltx_ref" href="#S3.E20" title="Equation 2.3.20 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.20</span></a>).
This scheme randomly re-initializes the dictionary <math alttext="\bm{A}_{1}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m1"><semantics><msub><mi>𝑨</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{A}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> on every such update.
An improvement uses instead <span class="ltx_text ltx_font_italic">warm starting</span>, where the residual is
generated using the previous estimate <math alttext="\bm{A}_{+}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m2"><semantics><msub><mi>𝑨</mi><mo>+</mo></msub><annotation encoding="application/x-tex">\bm{A}_{+}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT</annotation></semantics></math> for the dictionary.
If we then view each ISTA update (<a class="ltx_ref" href="#S3.E18" title="Equation 2.3.18 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.18</span></a>) as a layer and allow
the associated dictionary, now coupled with the sparse code updates as
<math alttext="\bm{A}_{\ell}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m3"><semantics><msub><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msub><annotation encoding="application/x-tex">\bm{A}_{\ell}</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT</annotation></semantics></math>, to update in time, this leads to a “layerwise-learnable” sparse
coding scheme:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx11">
<tbody id="S3.E28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{1}=\mathbf{0},\quad(\bm{A}_{1})_{j}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\tfrac{1}{D}\bm{I}),\enspace\forall j\in[m]," class="ltx_math_unparsed" display="inline" id="S3.E28.m1"><semantics><mrow><mrow><mrow><msup><mi>𝒁</mi><mn>1</mn></msup><mo>=</mo><mn>𝟎</mn></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mrow><mo stretchy="false">(</mo><msub><mi>𝑨</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mi>j</mi></msub><mover><mo>∼</mo><mrow><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">i</mi><mo lspace="0em" rspace="0.167em">.</mo><mi mathvariant="normal">d</mi><mo lspace="0em">.</mo></mrow></mover><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mfrac><mn>1</mn><mi>D</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mrow><mo rspace="0.167em">∀</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{1}=\mathbf{0},\quad(\bm{A}_{1})_{j}\stackrel{{\scriptstyle\mathrm{i.i.d.}}}{{\sim}}\operatorname{\mathcal{N}}(\bm{0},\tfrac{1}{D}\bm{I}),\enspace\forall j\in[m],</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = bold_0 , ( bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG ∼ end_ARG start_ARG roman_i . roman_i . roman_d . end_ARG end_RELOP caligraphic_N ( bold_0 , divide start_ARG 1 end_ARG start_ARG italic_D end_ARG bold_italic_I ) , ∀ italic_j ∈ [ italic_m ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.28)</span></td>
</tr></tbody>
<tbody id="S3.E29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta(\bm{A}_{\ell})^{\top}(\bm{A}_{\ell}\bm{Z}^{\ell}-\bm{X})\right)," class="ltx_Math" display="inline" id="S3.E29.m1"><semantics><mrow><mrow><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msub><mi>S</mi><mrow><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>λ</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msub><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mi mathvariant="normal">ℓ</mi></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Z}^{\ell+1}=S_{\eta\lambda}\left(\bm{Z}^{\ell}-2\eta(\bm{A}_{\ell})^{\top}(\bm{A}_{\ell}\bm{Z}^{\ell}-\bm{X})\right),</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT = italic_S start_POSTSUBSCRIPT italic_η italic_λ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - 2 italic_η ( bold_italic_A start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT - bold_italic_X ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.29)</span></td>
</tr></tbody>
<tbody id="S3.E30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{A}_{\ell+1}=\bm{A}_{\ell}-2\nu(\bm{A}_{\ell}\bm{Z}^{\ell+1}-\bm{X})(\bm{Z}^{\ell+1})^{\top}." class="ltx_Math" display="inline" id="S3.E30.m1"><semantics><mrow><mrow><msub><mi>𝑨</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msub><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>ν</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>𝑨</mi><mi mathvariant="normal">ℓ</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>𝒁</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{A}_{\ell+1}=\bm{A}_{\ell}-2\nu(\bm{A}_{\ell}\bm{Z}^{\ell+1}-\bm{X})(\bm{Z}^{\ell+1})^{\top}.</annotation><annotation encoding="application/x-llamapun">bold_italic_A start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT - 2 italic_ν ( bold_italic_A start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT - bold_italic_X ) ( bold_italic_Z start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.3.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that this iteration corresponds to a relabeling of (<a class="ltx_ref" href="#S3.E18" title="Equation 2.3.18 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.18</span></a>) and (<a class="ltx_ref" href="#S3.E20" title="Equation 2.3.20 ‣ 2.3.2 Overcomplete Dictionary Learning ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.3.20</span></a>)
for <math alttext="T=L=1" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m4"><semantics><mrow><mi>T</mi><mo>=</mo><mi>L</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=L=1</annotation><annotation encoding="application/x-llamapun">italic_T = italic_L = 1</annotation></semantics></math>, over infinitely many blocks.
Each of the ‘inner’ steps updating <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> can be considered as a one-layer
forward pass, while each of the ‘outer’ steps updating <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.m6"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> can be considered
as a one-layer backward pass, of a primitive deep neural network. In
particular, this algorithm is the simplest case in which a clear divide between
forward optimization and backward learning manifests. This divide is still
observed in current neural networks and autoencoders—we will have much more to say about it in <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">4</span></a> and in
<a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p3">
<p class="ltx_p">Notice that the above layer-wise scheme also suggests a plausible alternative to the current end-to-end optimization strategy that primarily relies on back propagation (BP) detailed in the Appendix <a class="ltx_ref" href="A1.html#S2.SS3" title="A.2.3 Back Propagation ‣ A.2 Computing Gradients via Automatic Differentiation ‣ Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">A.2.3</span></a>. Freeing training large networks from BP would be one of the biggest challenges and opportunities in the future, as we will discuss more at the end of the book in <a class="ltx_ref" href="Ch8.html" title="Chapter 8 Future Study of Intelligence ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Chapter</span> <span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.4 </span>Summary and Notes</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">The idealistic models we have presented in this chapter—PCA, ICA, and
dictionary learning—were developed over the course of the twentieth century.
Many books have been written solely about each method, so we will only attempt
here to give a broad overview of the key works and history.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">Jolliffe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx125" title="">Jol86</a>]</cite> attributes principal component analysis to Pearson
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx212" title="">Pea01</a>]</cite>, and independently Hotelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx110" title="">Hot33</a>]</cite>. In
mathematics, the main result on the related problem of low-rank approximation in
unitarily invariant norms is attributed to Eckart and Young
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx76" title="">EY36</a>]</cite>, and to Mirsky for full generality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx187" title="">Mir60</a>]</cite>.
PCA continues to play an important role in research as perhaps the simplest
model problem for unsupervised representation learning: as early as the 1980s,
works such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx200" title="">Oja82</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx14" title="">BH89</a>]</cite> used the
problem to understand learning in primitive neural networks, and more recently,
it has served as a tool for understanding more complex representation learning
frameworks, such as diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx285" title="">WZZ+24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">Independent component analysis was proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx9" title="">BJC85</a>]</cite> and pioneered by
Aapo Hyvärinen in the 1990s and early 2000s in a series of influential
works: see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx120" title="">HO00a</a>]</cite> for a summary. As a simple model for
structure that arises in practical data, it initially saw significant use in
applications such as blind source separation, where each independent component
<math alttext="z_{i}" class="ltx_Math" display="inline" id="S4.p3.m1"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents an independent source (such as sound associated to a distinct
instrument in a musical recording) that is superimposed to produce the
observation <math alttext="\bm{x}=\bm{U}\bm{z}" class="ltx_Math" display="inline" id="S4.p3.m2"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p">The problem of
dictionary learning can, in the complete or orthogonal case, be seen as one of
the foundational problems of twentieth-century signal processing, particularly
in linear systems theory, where the Fourier basis plays the key role; from the
1980s onward, the field of computational harmonic analysis crystallized around
the study of alternate such dictionaries for classes of signals in which optimal
approximation could only be realized in a basis other than Fourier (e.g., wavelets)
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx73" title="">DVD+98</a>]</cite>. However, the importance of the case of redundant bases, or
overcomplete dictionaries, was only highlighted following the pioneering work of
Olshausen and Field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx202" title="">OF96</a>, <a class="ltx_ref" href="bib.html#bibx201" title="">OF97</a>]</cite>. Early subsequent work
established the conceptual and algorithmic foundations for learning
sparsely-used overcomplete dictionaries, often aimed at representing natural
images
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx72" title="">Don01</a>, <a class="ltx_ref" href="bib.html#bibx62" title="">DM03</a>, <a class="ltx_ref" href="bib.html#bibx78" title="">EA06</a>, <a class="ltx_ref" href="bib.html#bibx195" title="">MK07</a>, <a class="ltx_ref" href="bib.html#bibx3" title="">AEB06</a>, <a class="ltx_ref" href="bib.html#bibx176" title="">MBP14</a>, <a class="ltx_ref" href="bib.html#bibx97" title="">GJB15</a>]</cite>.
Later, a significant amount of theoretical interest in the problem, as an
important and nontrivial model problem for unsupervised representation learning,
led to its study by the signal processing, theoretical machine learning, and
theoretical computer science communities, in particular focused on conditions
under which the problem could be provably and efficiently solved.
A non-exhaustive list of notable works in this line include those of
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx251" title="">SWW12</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx254" title="">SQW17</a>]</cite> on the complete
case; <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx7" title="">AGM+15</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx17" title="">BKS15</a>]</cite>; and
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx217" title="">QZL+20</a>]</cite>. Many deep theoretical questions about this
simple-to-state problem remain open, perhaps in part due to a tension with the
problem’s worst-case NP-hardness (e.g., see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx260" title="">Til15</a>]</cite>).</p>
</div>
<figure class="ltx_table" id="T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 2.1</span>: </span><span class="ltx_text" style="font-size:90%;">Summary of (generalized) power methods presented in the Chapter </span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Problem</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Algorithm</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Iteration</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Type of Structure Enforced</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PCA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Power Method</th>
<td class="ltx_td ltx_align_left ltx_border_t"><math alttext="\bm{u}_{t}=\frac{\bm{X}\bm{X}^{\top}\bm{u}_{t}}{\|\bm{X}\bm{X}^{\top}\bm{u}_{t}\|_{2}}" class="ltx_Math" display="inline" id="T1.m1"><semantics><mrow><msub><mi>𝒖</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><annotation encoding="application/x-tex">\bm{u}_{t}=\frac{\bm{X}\bm{X}^{\top}\bm{u}_{t}}{\|\bm{X}\bm{X}^{\top}\bm{u}_{t}\|_{2}}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t">
<math alttext="1" class="ltx_Math" display="inline" id="T1.m2"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>-dim. subspace (unit vector)</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">ICA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">FastICA</th>
<td class="ltx_td ltx_align_left"><math alttext="\bm{u}_{t+1}=\frac{\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}_{t}}{\left\|\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}_{t}\right\|_{2}}" class="ltx_Math" display="inline" id="T1.m3"><semantics><mrow><msub><mi>𝒖</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><mrow><mstyle scriptlevel="-1"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.1pt" height="0.6pt" voffset="0.6pt" width="1.2pt"><mo class="ltx_markedasmath" mathsize="26%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow></mrow><msub><mrow><mo>‖</mo><mrow><mrow><mstyle scriptlevel="-1"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.1pt" height="0.6pt" voffset="0.6pt" width="1.2pt"><mo class="ltx_markedasmath" mathsize="26%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow><mo>−</mo><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow></mrow><mo>‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><annotation encoding="application/x-tex">\bm{u}_{t+1}=\frac{\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}_{t}}{\left\|\tfrac{1}{N}\bm{X}(\bm{X}^{\top}\bm{u}_{t})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}-3\bm{u}_{t}\right\|_{2}}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∥ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT - 3 bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left">
<math alttext="1" class="ltx_Math" display="inline" id="T1.m4"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math>-dim. subspace (unit vector)</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Complete DL</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MSP Algorithm</th>
<td class="ltx_td ltx_align_left ltx_border_bb"><math alttext="\bm{U}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{U}_{t}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}]" class="ltx_Math" display="inline" id="T1.m5"><semantics><mrow><msub><mi>𝑼</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mi mathvariant="normal">O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑼</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑿</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{U}_{t+1}=\mathcal{P}_{\mathrm{O}(D)}[({\bm{U}_{t}\bm{X}})^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}\bm{X}^{\top}]</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_O ( italic_D ) end_POSTSUBSCRIPT [ ( bold_italic_U start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb">
<math alttext="D" class="ltx_Math" display="inline" id="T1.m6"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>-dim. subspace (orthogonal matrix)</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p">One point that we wish to highlight from the study of these classical
analytical models for low-dimensional structure is the common role played by
various <span class="ltx_text ltx_font_italic">generalized power methods</span>—algorithms that very rapidly
converge, at
least locally, to various types of low-dimensional structures. The terminology
for this class of algorithms follows the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx170" title="">MYP+10</a>]</cite>.
At a high level, modeled on the classical power iteration for computation of the top
eigenvector of a semidefinite matrix <math alttext="\bm{A}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="S4.p5.m1"><semantics><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{A}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, that is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{u}_{t+1}=\frac{\bm{A}\bm{u}_{t}}{\|\bm{A}\bm{u}_{t}\|_{2}}," class="ltx_Math" display="block" id="S4.E1.m1"><semantics><mrow><mrow><msub><mi>𝒖</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>t</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{u}_{t+1}=\frac{\bm{A}\bm{u}_{t}}{\|\bm{A}\bm{u}_{t}\|_{2}},</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_A bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ∥ bold_italic_A bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.4.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">this class of algorithms consists of a “powering” operation involving a matrix
<math alttext="\bm{A}" class="ltx_Math" display="inline" id="S4.p5.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> associated to the data, along with a “projection” operation that
enforces a desired type of structure. <a class="ltx_ref" href="#T1" title="In 2.4 Summary and Notes ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2.1</span></a> presents
a summary of the algorithms we have studied in this Chapter that follow this
structure. The reader may appreciate the applicability of this methodology to
different types of low-dimensional structure, and different losses (i.e., both
the quadratic loss from PCA, and the kurtosis-type losses from ICA), as well as
the lack of such an algorithm for overcomplete dictionary learning, despite the
breadth of the literature on these algorithms. We see the development of power
methods for further families of low-dimensional structures, particularly those
relevant to applications where deep learning is prevalent, as one of the more
important (and open) research questions suggested by this chapter.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p">The connection we make in <a class="ltx_ref" href="#S2.SS1" title="2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2.1</span></a> between the
geometric mixture-of-subspaces distributional assumption and the more
analytically-convenient sparse dictionary assumption has been mentioned in prior
work, especially by those focused on generalized principal component analysis
and applications such as subspace clustering, e.g. work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx276" title="">VMS16</a>]</cite>.
The mixture of subspaces assumption will continue to play a significant role
throughout this manuscript, both as an analytical test case for different
algorithmic paradigms, and as a foundation for deriving different deep network
architectures, as with LISTA in <a class="ltx_ref" href="#S3.SS3" title="2.3.3 Learned Deep Sparse Coding ‣ 2.3 A Mixture of Overcomplete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3.3</span></a>, but which can scale to more
complex data distributions.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.5 </span>Exercises and Extensions</h2>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise1.p1">
<p class="ltx_p">Prove that, for any symmetric matrix <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, the solution to the problem
<math alttext="\max_{\bm{U}\in\mathsf{O}(D,d)}\operatorname{tr}\left(\bm{U}^{\top}\bm{A}\bm{U}\right)" class="ltx_Math" display="inline" id="Thmexercise1.p1.m2"><semantics><mrow><msub><mi>max</mi><mrow><mi>𝑼</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0.167em">⁡</mo><mrow><mi>tr</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\max_{\bm{U}\in\mathsf{O}(D,d)}\operatorname{tr}\left(\bm{U}^{\top}\bm{A}\bm{U}\right)</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_U ∈ sansserif_O ( italic_D , italic_d ) end_POSTSUBSCRIPT roman_tr ( bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_A bold_italic_U )</annotation></semantics></math> is the matrix
<math alttext="\bm{U}^{\star}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m3"><semantics><msup><mi>𝑼</mi><mo>⋆</mo></msup><annotation encoding="application/x-tex">\bm{U}^{\star}</annotation><annotation encoding="application/x-llamapun">bold_italic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> whose columns are the top <math alttext="d" class="ltx_Math" display="inline" id="Thmexercise1.p1.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> unit eigenvectors of <math alttext="\bm{A}" class="ltx_Math" display="inline" id="Thmexercise1.p1.m5"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise2.p1">
<p class="ltx_p">Let <math alttext="\bm{z}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="Thmexercise2.p1.m1"><semantics><mrow><mi>𝒛</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_z ∼ caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math> be a Gaussian random variable with independent components, each with variance <math alttext="\sigma^{2}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m2"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^{2}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Prove that for any orthogonal matrix <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m3"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> (i.e., <math alttext="\bm{Q}^{\top}\bm{Q}=\bm{I}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m4"><semantics><mrow><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">\bm{Q}^{\top}\bm{Q}=\bm{I}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I</annotation></semantics></math>), the random variable <math alttext="\bm{Q}\bm{z}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m5"><semantics><mrow><mi>𝑸</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">\bm{Q}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q bold_italic_z</annotation></semantics></math> is distributed identically to <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmexercise2.p1.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>.
<span class="ltx_text ltx_font_italic">(Hint: recall the formula for the Gaussian probability density function, and the formula for the density of a linear function of a random variable.) </span></p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.3</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise3.p1">
<p class="ltx_p">The notion of statistical identifiability discussed above can be related to <span class="ltx_text ltx_font_italic">symmetries</span> of the model class, allowing estimation to be understood in a purely deterministic fashion without any statistical assumptions.</p>
</div>
<div class="ltx_para" id="Thmexercise3.p2">
<p class="ltx_p">Consider the model <math alttext="\bm{X}=\bm{U}\bm{Z}" class="ltx_Math" display="inline" id="Thmexercise3.p2.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_italic_Z</annotation></semantics></math> for matrices <math alttext="\bm{X},\bm{U},\bm{Z}" class="ltx_Math" display="inline" id="Thmexercise3.p2.m2"><semantics><mrow><mi>𝑿</mi><mo>,</mo><mi>𝑼</mi><mo>,</mo><mi>𝒁</mi></mrow><annotation encoding="application/x-tex">\bm{X},\bm{U},\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_X , bold_italic_U , bold_italic_Z</annotation></semantics></math> of compatible sizes.</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p">Show that if <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is any square invertible matrix of compatible size, then the pair
<math alttext="(\bm{U}\bm{A},\bm{A}^{-1}\bm{Z})" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑨</mi></mrow><mo>,</mo><mrow><msup><mi>𝑨</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{U}\bm{A},\bm{A}^{-1}\bm{Z})</annotation><annotation encoding="application/x-llamapun">( bold_italic_U bold_italic_A , bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Z )</annotation></semantics></math> also equals <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> under the model. We call this a <math alttext="\mathsf{GL}(d)" class="ltx_Math" display="inline" id="S5.I1.i1.p1.m4"><semantics><mrow><mi>𝖦𝖫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{GL}(d)</annotation><annotation encoding="application/x-llamapun">sansserif_GL ( italic_d )</annotation></semantics></math><span class="ltx_text ltx_font_italic"> symmetry</span>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p">Suppose each column of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is an independent and identically distributed observation from a common statistical model <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, which moreover has zero mean and independent components <math alttext="z_{i}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m3"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with positive variance.
Show that for any square invertible matrix <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m4"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, if <math alttext="\bm{A}\bm{z}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m5"><semantics><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><annotation encoding="application/x-tex">\bm{A}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_A bold_italic_z</annotation></semantics></math> has uncorrelated components, then <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m6"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> can be written as <math alttext="\bm{D}_{1}\bm{Q}\bm{D}_{2}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m7"><semantics><mrow><msub><mi>𝑫</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝑫</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\bm{D}_{1}\bm{Q}\bm{D}_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_Q bold_italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m8"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> is an orthogonal matrix and <math alttext="\bm{D}_{1}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m9"><semantics><msub><mi>𝑫</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\bm{D}_{1}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\bm{D}_{2}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.m10"><semantics><msub><mi>𝑫</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\bm{D}_{2}</annotation><annotation encoding="application/x-llamapun">bold_italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> are diagonal matrices.
<span class="ltx_text ltx_font_italic">This links the “independence” assumption in ICA to a “symmetry breaking” effect, which only allows scale and rotational symmetries.</span></p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.4</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise4.p1">
<p class="ltx_p">Consider the model <math alttext="\bm{x}=\bm{U}\bm{z}" class="ltx_Math" display="inline" id="Thmexercise4.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{U}\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_U bold_italic_z</annotation></semantics></math>, where <math alttext="\bm{U}\in\mathbb{R}^{D\times d}" class="ltx_Math" display="inline" id="Thmexercise4.p1.m2"><semantics><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{U}\in\mathbb{R}^{D\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="D\geq d" class="ltx_Math" display="inline" id="Thmexercise4.p1.m3"><semantics><mrow><mi>D</mi><mo>≥</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">D\geq d</annotation><annotation encoding="application/x-llamapun">italic_D ≥ italic_d</annotation></semantics></math> is fixed and has rank <math alttext="d" class="ltx_Math" display="inline" id="Thmexercise4.p1.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmexercise4.p1.m5"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> is a zero-mean random variable. Let <math alttext="\bm{x}_{1},\dots\bm{x}_{N}" class="ltx_Math" display="inline" id="Thmexercise4.p1.m6"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>N</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{1},\dots\bm{x}_{N}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math> denote i.i.d. observations from this model.</p>
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p">Show that the matrix <math alttext="\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]" class="ltx_Math" display="inline" id="S5.I2.i1.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\dots,\bm{x}_{N}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math> has rank no larger than <math alttext="d" class="ltx_Math" display="inline" id="S5.I2.i1.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, and therefore there is an orthonormal matrix <math alttext="\bm{V}\in\mathbb{R}^{D\times d}" class="ltx_Math" display="inline" id="S5.I2.i1.p1.m3"><semantics><mrow><mi>𝑽</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{V}\in\mathbb{R}^{D\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> so that <math alttext="\bm{X}=\bm{V}\bm{Y}" class="ltx_Math" display="inline" id="S5.I2.i1.p1.m4"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑽</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒀</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{V}\bm{Y}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_V bold_italic_Y</annotation></semantics></math>, where <math alttext="\bm{Y}\in\mathbb{R}^{d\times N}" class="ltx_Math" display="inline" id="S5.I2.i1.p1.m5"><semantics><mrow><mi>𝒀</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Y}\in\mathbb{R}^{d\times N}</annotation><annotation encoding="application/x-llamapun">bold_italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. (<span class="ltx_text ltx_font_italic">Hint: use PCA.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p">Show that the <span class="ltx_text ltx_font_italic">whitened matrix</span> <math alttext="(\bm{Y}\bm{Y}^{\top})^{-1/2}\bm{Y}" class="ltx_Math" display="inline" id="S5.I2.i2.p1.m1"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝒀</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒀</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒀</mi></mrow><annotation encoding="application/x-tex">(\bm{Y}\bm{Y}^{\top})^{-1/2}\bm{Y}</annotation><annotation encoding="application/x-llamapun">( bold_italic_Y bold_italic_Y start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT bold_italic_Y</annotation></semantics></math> exists in expectation whenever <math alttext="\operatorname{Cov}(\bm{z})" class="ltx_Math" display="inline" id="S5.I2.i2.p1.m2"><semantics><mrow><mi>Cov</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Cov}(\bm{z})</annotation><annotation encoding="application/x-llamapun">roman_Cov ( bold_italic_z )</annotation></semantics></math> is nonsingular, and that it has identity empirical covariance.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>In particular, it can be proved mathematically that this is enough to guarantee that the whitened matrix exists with high probability whenever <math alttext="\bm{z}" class="ltx_Math" display="inline" id="footnote14.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> satisfies a suitable concentration inequality and <math alttext="N" class="ltx_Math" display="inline" id="footnote14.m2"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> is sufficiently large.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p">Show, by using the singular value decomposition of <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S5.I2.i3.p1.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>, that the matrix <math alttext="\bm{V}" class="ltx_Math" display="inline" id="S5.I2.i3.p1.m2"><semantics><mi>𝑽</mi><annotation encoding="application/x-tex">\bm{V}</annotation><annotation encoding="application/x-llamapun">bold_italic_V</annotation></semantics></math> can be chosen so that the whitened matrix satisfies <math alttext="(\bm{Y}\bm{Y}^{\top})^{-1/2}\bm{Y}=\bm{W}[\bm{z}_{1},\dots,\bm{z}_{N}]" class="ltx_Math" display="inline" id="S5.I2.i3.p1.m3"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>𝒀</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒀</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒀</mi></mrow><mo>=</mo><mrow><mi>𝑾</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>N</mi></msub><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\bm{Y}\bm{Y}^{\top})^{-1/2}\bm{Y}=\bm{W}[\bm{z}_{1},\dots,\bm{z}_{N}]</annotation><annotation encoding="application/x-llamapun">( bold_italic_Y bold_italic_Y start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT bold_italic_Y = bold_italic_W [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]</annotation></semantics></math>, where <math alttext="\bm{W}" class="ltx_Math" display="inline" id="S5.I2.i3.p1.m4"><semantics><mi>𝑾</mi><annotation encoding="application/x-tex">\bm{W}</annotation><annotation encoding="application/x-llamapun">bold_italic_W</annotation></semantics></math> is an orthonormal matrix.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.5</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise5.p1">
<p class="ltx_p">Let <math alttext="X" class="ltx_Math" display="inline" id="Thmexercise5.p1.m1"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation><annotation encoding="application/x-llamapun">italic_X</annotation></semantics></math> and <math alttext="Y" class="ltx_Math" display="inline" id="Thmexercise5.p1.m2"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation><annotation encoding="application/x-llamapun">italic_Y</annotation></semantics></math> be zero-mean independent random variables.</p>
<ol class="ltx_enumerate" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p">Show that <math alttext="\mathop{\mathrm{kurt}}(X+Y)=\mathop{\mathrm{kurt}}(X)+\mathop{\mathrm{kurt}}(Y)" class="ltx_Math" display="inline" id="S5.I3.i1.p1.m1"><semantics><mrow><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo>+</mo><mi>Y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><mrow><mo lspace="0.1389em" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mo lspace="0em" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(X+Y)=\mathop{\mathrm{kurt}}(X)+\mathop{\mathrm{kurt}}(Y)</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_X + italic_Y ) = roman_kurt ( italic_X ) + roman_kurt ( italic_Y )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I3.i2.p1">
<p class="ltx_p">For any <math alttext="\alpha\in\mathbb{R}" class="ltx_Math" display="inline" id="S5.I3.i2.p1.m1"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha\in\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_α ∈ blackboard_R</annotation></semantics></math>, show that <math alttext="\mathop{\mathrm{kurt}}(\alpha X)=\alpha^{4}\mathop{\mathrm{kurt}}(X)" class="ltx_Math" display="inline" id="S5.I3.i2.p1.m2"><semantics><mrow><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>α</mi><mn>4</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(\alpha X)=\alpha^{4}\mathop{\mathrm{kurt}}(X)</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_α italic_X ) = italic_α start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_kurt ( italic_X )</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.6</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise6.p1">
<p class="ltx_p">Let <math alttext="f:\mathbb{R}^{d}\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmexercise6.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{d}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math> be a given twice-continuously-differentiable objective function. Consider the spherically-constrained optimization problem</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\|\bm{u}\|_{2}^{2}=1}\,f(\bm{u})." class="ltx_Math" display="block" id="S5.E1.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow></munder><mo lspace="0.337em">⁡</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\|\bm{u}\|_{2}^{2}=1}\,f(\bm{u}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT italic_f ( bold_italic_u ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this exercise, we will derive the expressions we gave in the FastICA derivation for maximizing kurtosis over the sphere via a gradient ascent algorithm.
These expressions are special cases of a rich theory of calculus and optimization on manifolds, of which the sphere is a particular example. A deep technical study of this field is out-of-scope for our purposes, so we only mention two key references for the interested reader: the pioneering textbook by Absil, Mahony, and Sepulchre
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx1" title="">AMS09</a>]</cite>, and a more recent introductory treatise by Boumal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx26" title="">Bou23</a>]</cite>.</p>
<ol class="ltx_enumerate" id="S5.I4">
<li class="ltx_item" id="S5.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I4.i1.p1">
<p class="ltx_p">For any constraint set <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> that is a differentiable submanifold of <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m2"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, the
<span class="ltx_text ltx_font_italic">tangent space</span> at a point <math alttext="\bm{u}\in\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m3"><semantics><mrow><mi>𝒖</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><annotation encoding="application/x-tex">\bm{u}\in\mathcal{M}</annotation><annotation encoding="application/x-llamapun">bold_italic_u ∈ caligraphic_M</annotation></semantics></math> is, informally, the best local linear approximation to the manifold <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m4"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> at the point <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m5"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math>.
In the important special case where <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m6"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> is defined locally at <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m7"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> as a level set of a function <math alttext="F:\mathbb{R}^{d}\to\mathbb{R}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m8"><semantics><mrow><mi>F</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">F:\mathbb{R}^{d}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_F : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math>,
that is</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="U\cap\mathcal{M}=F^{-1}(\{0\})" class="ltx_Math" display="block" id="S5.Ex1.m1"><semantics><mrow><mrow><mi>U</mi><mo>∩</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><mo>=</mo><mrow><msup><mi>F</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo stretchy="false">}</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">U\cap\mathcal{M}=F^{-1}(\{0\})</annotation><annotation encoding="application/x-llamapun">italic_U ∩ caligraphic_M = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { 0 } )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">for some open set <math alttext="U\subset\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m9"><semantics><mrow><mi>U</mi><mo>⊂</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><annotation encoding="application/x-tex">U\subset\mathcal{M}</annotation><annotation encoding="application/x-llamapun">italic_U ⊂ caligraphic_M</annotation></semantics></math> with <math alttext="\bm{u}\in U" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m10"><semantics><mrow><mi>𝒖</mi><mo>∈</mo><mi>U</mi></mrow><annotation encoding="application/x-tex">\bm{u}\in U</annotation><annotation encoding="application/x-llamapun">bold_italic_u ∈ italic_U</annotation></semantics></math>,
the tangent space to <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m11"><semantics><mi class="ltx_font_mathcaligraphic">ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation><annotation encoding="application/x-llamapun">caligraphic_M</annotation></semantics></math> at <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m12"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> can be calculated
via differentiation:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T_{\bm{u}}\mathcal{M}=\operatorname{Ker}(DF_{\bm{u}})." class="ltx_Math" display="block" id="S5.Ex2.m1"><semantics><mrow><mrow><mrow><msub><mi>T</mi><mi>𝒖</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">ℳ</mi></mrow><mo>=</mo><mrow><mi>Ker</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>F</mi><mi>𝒖</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">T_{\bm{u}}\mathcal{M}=\operatorname{Ker}(DF_{\bm{u}}).</annotation><annotation encoding="application/x-llamapun">italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT caligraphic_M = roman_Ker ( italic_D italic_F start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">It is easily seen that the sphere has the defining equation <math alttext="F(\bm{u})=\|\bm{u}\|_{2}^{2}-1" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m13"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">F(\bm{u})=\|\bm{u}\|_{2}^{2}-1</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_u ) = ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 1</annotation></semantics></math>.
Show, using these facts, that the tangent space to the sphere at <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m14"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T_{\bm{u}}\mathbb{S}^{d-1}=\{\bm{v}\in\mathbb{R}^{d}\mid\langle\bm{v},\bm{u}\rangle=0\}," class="ltx_Math" display="block" id="S5.Ex3.m1"><semantics><mrow><mrow><mrow><msub><mi>T</mi><mi>𝒖</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>𝒗</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mrow><mo stretchy="false">⟨</mo><mi>𝒗</mi><mo>,</mo><mi>𝒖</mi><mo stretchy="false">⟩</mo></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">T_{\bm{u}}\mathbb{S}^{d-1}=\{\bm{v}\in\mathbb{R}^{d}\mid\langle\bm{v},\bm{u}\rangle=0\},</annotation><annotation encoding="application/x-llamapun">italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT = { bold_italic_v ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ∣ ⟨ bold_italic_v , bold_italic_u ⟩ = 0 } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and that the orthogonal projection onto this subspace is <math alttext="\bm{P}_{\bm{u}}^{\perp}=\bm{I}-\bm{u}\bm{u}^{\top}" class="ltx_Math" display="inline" id="S5.I4.i1.p1.m15"><semantics><mrow><msubsup><mi>𝑷</mi><mi>𝒖</mi><mo>⟂</mo></msubsup><mo>=</mo><mrow><mi>𝑰</mi><mo>−</mo><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{P}_{\bm{u}}^{\perp}=\bm{I}-\bm{u}\bm{u}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT = bold_italic_I - bold_italic_u bold_italic_u start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I4.i2.p1">
<p class="ltx_p">The vector field</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{grad}\,f(\bm{u})=\bm{P}_{\bm{u}}^{\perp}\nabla f" class="ltx_Math" display="block" id="S5.E2.m1"><semantics><mrow><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>𝑷</mi><mi>𝒖</mi><mo>⟂</mo></msubsup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{u})=\bm{P}_{\bm{u}}^{\perp}\nabla f</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT ∇ italic_f</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is known as the <span class="ltx_text ltx_font_italic">Riemannian gradient</span> of the function <math alttext="f" class="ltx_Math" display="inline" id="S5.I4.i2.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> restricted to the sphere.
The <span class="ltx_text ltx_font_italic">first order optimality conditions</span> for the optimization problem (<a class="ltx_ref" href="#S5.E1" title="Equation 2.5.1 ‣ Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.1</span></a>) can be expressed in terms of the Riemann gradient:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{grad}\,f(\bm{u})=\mathbf{0}." class="ltx_Math" display="block" id="S5.Ex4.m1"><semantics><mrow><mrow><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{u})=\mathbf{0}.</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_u ) = bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Geometrically, this says that the Euclidean gradient of <math alttext="f" class="ltx_Math" display="inline" id="S5.I4.i2.p1.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> at <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i2.p1.m3"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> must be orthogonal to the tangent space to the sphere at <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i2.p1.m4"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math>.
Now suppose <math alttext="\bm{v}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S5.I4.i2.p1.m5"><semantics><mrow><mi>𝒗</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{v}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_v ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is nonzero. Show that</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{proj}_{\mathbb{S}^{d-1}}(\bm{v})\doteq\min_{\|\bm{u}\|_{2}^{2}=1}\,\|\bm{u}-\bm{v}\|_{2}=\frac{\bm{v}}{\|\bm{v}\|_{2}}," class="ltx_Math" display="block" id="S5.Ex5.m1"><semantics><mrow><mrow><mrow><msub><mi>proj</mi><msup><mi>𝕊</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒗</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><munder><mi>min</mi><mrow><msubsup><mrow><mo stretchy="false">‖</mo><mi>𝒖</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒖</mi><mo>−</mo><mi>𝒗</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><mo>=</mo><mfrac><mi>𝒗</mi><msub><mrow><mo stretchy="false">‖</mo><mi>𝒗</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{proj}_{\mathbb{S}^{d-1}}(\bm{v})\doteq\min_{\|\bm{u}\|_{2}^{2}=1}\,\|\bm{u}-\bm{v}\|_{2}=\frac{\bm{v}}{\|\bm{v}\|_{2}},</annotation><annotation encoding="application/x-llamapun">roman_proj start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_v ) ≐ roman_min start_POSTSUBSCRIPT ∥ bold_italic_u ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT ∥ bold_italic_u - bold_italic_v ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG bold_italic_v end_ARG start_ARG ∥ bold_italic_v ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">using the first-order optimality conditions.</p>
</div>
</li>
<li class="ltx_item" id="S5.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I4.i3.p1">
<p class="ltx_p">In optimization over <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S5.I4.i3.p1.m1"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, one checks the second-order optimality conditions (to determine whether a critical point is a maximizer, a minimizer, or a saddle point) using the <span class="ltx_text ltx_font_italic">Hessian matrix</span> <math alttext="\nabla^{2}f(\bm{u})" class="ltx_Math" display="inline" id="S5.I4.i3.p1.m2"><semantics><mrow><mrow><msup><mo>∇</mo><mn>2</mn></msup><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla^{2}f(\bm{u})</annotation><annotation encoding="application/x-llamapun">∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( bold_italic_u )</annotation></semantics></math>.
Show, by differentiating the Riemann gradient <math alttext="\mathrm{grad}\,f(\bm{u})" class="ltx_Math" display="inline" id="S5.I4.i3.p1.m3"><semantics><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{u})</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_u )</annotation></semantics></math> for the sphere with respect to <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S5.I4.i3.p1.m4"><semantics><mi>𝒖</mi><annotation encoding="application/x-tex">\bm{u}</annotation><annotation encoding="application/x-llamapun">bold_italic_u</annotation></semantics></math> as in the first part of this exercise, that the corresponding object for determining second-order optimality conditions for sphere-constrained optimization is the <span class="ltx_text ltx_font_italic">Riemannian Hessian</span>, defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Hess}\,f(\bm{u})=\bm{P}_{\bm{u}}^{\perp}\left(\nabla^{2}f(\bm{u})-\langle\nabla f(\bm{u}),\bm{u}\rangle\bm{I}\right)\bm{P}_{\bm{u}}^{\perp}." class="ltx_Math" display="block" id="S5.E3.m1"><semantics><mrow><mrow><mrow><mi>Hess</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>𝑷</mi><mi>𝒖</mi><mo>⟂</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mrow><msup><mo rspace="0.167em">∇</mo><mn>2</mn></msup><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mo stretchy="false">⟨</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒖</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝒖</mi><mo stretchy="false">⟩</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝑷</mi><mi>𝒖</mi><mo>⟂</mo></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{Hess}\,f(\bm{u})=\bm{P}_{\bm{u}}^{\perp}\left(\nabla^{2}f(\bm{u})-\langle\nabla f(\bm{u}),\bm{u}\rangle\bm{I}\right)\bm{P}_{\bm{u}}^{\perp}.</annotation><annotation encoding="application/x-llamapun">roman_Hess italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT ( ∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( bold_italic_u ) - ⟨ ∇ italic_f ( bold_italic_u ) , bold_italic_u ⟩ bold_italic_I ) bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.3)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.7</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise7.p1">
<p class="ltx_p">In this exercise, we sketch an argument referred to in the literature as a <span class="ltx_text ltx_font_italic">landscape analysis</span> for the spherically-constrained population kurtosis maximization problem (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>). We will show that when there is at least one independent component with positive kurtosis, its
global maximizers indeed lead to the recovery of one column of the dictionary <math alttext="\bm{U}" class="ltx_Math" display="inline" id="Thmexercise7.p1.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>.
For simplicity, we will assume that <math alttext="\mathop{\mathrm{kurt}}(z_{i})\neq 0" class="ltx_Math" display="inline" id="Thmexercise7.p1.m2"><semantics><mrow><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(z_{i})\neq 0</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≠ 0</annotation></semantics></math> for each <math alttext="i=1,\dots,d" class="ltx_Math" display="inline" id="Thmexercise7.p1.m3"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">i=1,\dots,d</annotation><annotation encoding="application/x-llamapun">italic_i = 1 , … , italic_d</annotation></semantics></math>.</p>
<ol class="ltx_enumerate" id="S5.I5">
<li class="ltx_item" id="S5.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I5.i1.p1">
<p class="ltx_p">Using the results of Part 1 of Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>,
show that the first-order optimality condition for (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>) is</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left(\sum_{i=1}^{d}\mathop{\mathrm{kurt}}(z_{i})w_{i}^{4}\right)\bm{w}=\mathop{\mathrm{kurt}}(\bm{z})\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{w}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}," class="ltx_Math" display="block" id="S5.E4.m1"><semantics><mrow><mrow><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mo lspace="0em" movablelimits="false" rspace="0em">kurt</mo><mrow><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>w</mi><mi>i</mi><mn>4</mn></msubsup></mrow></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝒘</mi></mrow><mo rspace="0.1389em">=</mo><mrow><mo lspace="0.1389em" movablelimits="false" rspace="0em">kurt</mo><mrow><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo rspace="0.222em" stretchy="false">)</mo></mrow><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><msup><mi>𝒘</mi><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\left(\sum_{i=1}^{d}\mathop{\mathrm{kurt}}(z_{i})w_{i}^{4}\right)\bm{w}=\mathop{\mathrm{kurt}}(\bm{z})\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}\bm{w}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3},</annotation><annotation encoding="application/x-llamapun">( ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) bold_italic_w = roman_kurt ( bold_italic_z ) ⊙ bold_italic_w start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the kurtosis is calculated elementwise, <math alttext="\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}" class="ltx_Math" display="inline" id="S5.I5.i1.p1.m1"><semantics><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊙</mo></mpadded><annotation encoding="application/x-tex">\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}</annotation><annotation encoding="application/x-llamapun">⊙</annotation></semantics></math> denotes elementwise multiplication of vectors and <math alttext="\bm{w}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}" class="ltx_Math" display="inline" id="S5.I5.i1.p1.m2"><semantics><msup><mi>𝒘</mi><mrow><mpadded depth="0.2pt" height="1.6pt" voffset="0.8pt" width="3.0pt"><mo class="ltx_markedasmath" mathsize="48%">⊙</mo></mpadded><mn>3</mn></mrow></msup><annotation encoding="application/x-tex">\bm{w}^{\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\odot$}}{\scalebox{0.8}{$\textstyle\odot$}}{\scalebox{0.8}{$\scriptstyle\odot$}}{\scalebox{0.8}{$\scriptscriptstyle\odot$}}$}}}3}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUPERSCRIPT ⊙ 3 end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the elementwise cube of its argument.</p>
</div>
</li>
<li class="ltx_item" id="S5.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I5.i2.p1">
<p class="ltx_p">Show that the vectors <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m1"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> with unit norm that also satisfy (<a class="ltx_ref" href="#S5.E4" title="Equation 2.5.4 ‣ Item 1 ‣ Exercise 2.7. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.4</span></a>)
all take the following form.
Let <math alttext="S^{+}=\{i\in[d]\mid\mathop{\mathrm{kurt}}(z_{i})&gt;0\}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m2"><semantics><mrow><msup><mi>S</mi><mo>+</mo></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo></mrow></mrow><mo fence="true" lspace="0em" rspace="0.0835em">∣</mo><mrow><mrow><mo lspace="0.0835em" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S^{+}=\{i\in[d]\mid\mathop{\mathrm{kurt}}(z_{i})&gt;0\}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { italic_i ∈ [ italic_d ] ∣ roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) &gt; 0 }</annotation></semantics></math>, and
<math alttext="S^{-}=\{i\in[d]\mid\mathop{\mathrm{kurt}}(z_{i})&lt;0\}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m3"><semantics><mrow><msup><mi>S</mi><mo>−</mo></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo></mrow></mrow><mo fence="true" lspace="0em" rspace="0.0835em">∣</mo><mrow><mrow><mo lspace="0.0835em" rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S^{-}=\{i\in[d]\mid\mathop{\mathrm{kurt}}(z_{i})&lt;0\}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT = { italic_i ∈ [ italic_d ] ∣ roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) &lt; 0 }</annotation></semantics></math>.
Let <math alttext="S" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m4"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math> be a subset either of <math alttext="S^{+}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m5"><semantics><msup><mi>S</mi><mo>+</mo></msup><annotation encoding="application/x-tex">S^{+}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math> or <math alttext="S^{-}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m6"><semantics><msup><mi>S</mi><mo>−</mo></msup><annotation encoding="application/x-tex">S^{-}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT</annotation></semantics></math>.
Then</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{w}_{S}=\sum_{i\in S}\pm\sqrt{\frac{1}{\mathop{\mathrm{kurt}}(z_{i})\sum_{j\in S}\frac{1}{\mathop{\mathrm{kurt}}(z_{j})}}}\bm{e}_{i}" class="ltx_Math" display="block" id="S5.E5.m1"><semantics><mrow><msub><mi>𝒘</mi><mi>S</mi></msub><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><mo lspace="0em">±</mo><mrow><msqrt><mfrac><mn>1</mn><mrow><mo rspace="0em">kurt</mo><mrow><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><msub><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>S</mi></mrow></msub><mfrac><mn>1</mn><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mfrac></msqrt><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒆</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{w}_{S}=\sum_{i\in S}\pm\sqrt{\frac{1}{\mathop{\mathrm{kurt}}(z_{i})\sum_{j\in S}\frac{1}{\mathop{\mathrm{kurt}}(z_{j})}}}\bm{e}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_w start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i ∈ italic_S end_POSTSUBSCRIPT ± square-root start_ARG divide start_ARG 1 end_ARG start_ARG roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∑ start_POSTSUBSCRIPT italic_j ∈ italic_S end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_kurt ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG end_ARG end_ARG bold_italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">satisfies (<a class="ltx_ref" href="#S5.E4" title="Equation 2.5.4 ‣ Item 1 ‣ Exercise 2.7. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.4</span></a>),
where <math alttext="\bm{e}_{i}" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m7"><semantics><msub><mi>𝒆</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{e}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the vector with a <math alttext="1" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m8"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation><annotation encoding="application/x-llamapun">1</annotation></semantics></math> in the <math alttext="i" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m9"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math>-th position and <math alttext="0" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m10"><mn>0</mn></math>s elsewhere, and the <math alttext="\pm" class="ltx_Math" display="inline" id="S5.I5.i2.p1.m11"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation><annotation encoding="application/x-llamapun">±</annotation></semantics></math> sign denotes the choice of either a positive or negative sign.</p>
</div>
</li>
<li class="ltx_item" id="S5.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I5.i3.p1">
<p class="ltx_p">Assume that there is at least one <math alttext="i" class="ltx_Math" display="inline" id="S5.I5.i3.p1.m1"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation><annotation encoding="application/x-llamapun">italic_i</annotation></semantics></math> such that <math alttext="\mathop{\mathrm{kurt}}(z_{i})&gt;0" class="ltx_Math" display="inline" id="S5.I5.i3.p1.m2"><semantics><mrow><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(z_{i})&gt;0</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) &gt; 0</annotation></semantics></math>. Using the results of Part 2 of Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>, show that the only local maxima of the objective of (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>) are the signed one-sparse vectors <math alttext="\pm\bm{e}_{i}" class="ltx_Math" display="inline" id="S5.I5.i3.p1.m3"><semantics><mrow><mo>±</mo><msub><mi>𝒆</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\pm\bm{e}_{i}</annotation><annotation encoding="application/x-llamapun">± bold_italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="i\in S^{+}" class="ltx_Math" display="inline" id="S5.I5.i3.p1.m4"><semantics><mrow><mi>i</mi><mo>∈</mo><msup><mi>S</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">i\in S^{+}</annotation><annotation encoding="application/x-llamapun">italic_i ∈ italic_S start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>. Conclude that the global maximizers of (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>) are the signed one-sparse vectors corresponding to components with maximum kurtosis. (<span class="ltx_text ltx_font_italic">Hint: count the number of positive and negative eigenvalues of the Riemannian Hessian (<a class="ltx_ref" href="#S5.E3" title="Equation 2.5.3 ‣ Item 3 ‣ Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.3</span></a>) at each critical point.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S5.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I5.i4.p1">
<p class="ltx_p">Now assume that <math alttext="\mathop{\mathrm{kurt}}(z_{j})&lt;0" class="ltx_Math" display="inline" id="S5.I5.i4.p1.m1"><semantics><mrow><mrow><mo rspace="0em">kurt</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathop{\mathrm{kurt}}(z_{j})&lt;0</annotation><annotation encoding="application/x-llamapun">roman_kurt ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) &lt; 0</annotation></semantics></math> for every <math alttext="j=1,\dots,d" class="ltx_Math" display="inline" id="S5.I5.i4.p1.m2"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">j=1,\dots,d</annotation><annotation encoding="application/x-llamapun">italic_j = 1 , … , italic_d</annotation></semantics></math>. This corresponds to an “over-deflated” instantiation of the kurtosis maximization problem. Using again the results of Part 2 of Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>, show that the only local maxima of the objective of (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>) are the signed dense vectors <math alttext="\sum_{i=1}^{d}\pm\bm{e}_{i}" class="ltx_Math" display="inline" id="S5.I5.i4.p1.m3"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mo lspace="0em">±</mo><msub><mi>𝒆</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{d}\pm\bm{e}_{i}</annotation><annotation encoding="application/x-llamapun">∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ± bold_italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
This shows that the optimization formulation (<a class="ltx_ref" href="#S2.E24" title="Equation 2.2.24 ‣ Incremental ICA: correctness and FastICA algorithm. ‣ 2.2.3 Connection to ICA and Kurtosis ‣ 2.2 A Mixture of Complete Low-Dimensional Subspaces ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.2.24</span></a>) cannot be applied naively.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_exercise" id="Thmexercise8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Exercise 2.8</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexercise8.p1">
<p class="ltx_p">This exercise follows the structure and formalism introduced in Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a>, but applies it instead to the orthogonal group <math alttext="\mathsf{O}(d)=\{\bm{U}\in\mathbb{R}^{d\times d}\mid\bm{U}^{\top}\bm{U}=\bm{I}\}" class="ltx_Math" display="inline" id="Thmexercise8.p1.m1"><semantics><mrow><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mrow><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑼</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{O}(d)=\{\bm{U}\in\mathbb{R}^{d\times d}\mid\bm{U}^{\top}\bm{U}=\bm{I}\}</annotation><annotation encoding="application/x-llamapun">sansserif_O ( italic_d ) = { bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT ∣ bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_U = bold_italic_I }</annotation></semantics></math>.
Consult the description of Exercise <a class="ltx_ref" href="#Thmexercise6" title="Exercise 2.6. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.6</span></a> for the necessary conceptual background; the formalism applies identically to the case where the ambient space is the set of <math alttext="d\times d" class="ltx_Math" display="inline" id="Thmexercise8.p1.m2"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d\times d</annotation><annotation encoding="application/x-llamapun">italic_d × italic_d</annotation></semantics></math> matrices as long as one recalls that the relevant inner product on matrices is <math alttext="\langle\bm{X},\bm{Y}\rangle=\operatorname{tr}(\bm{X}^{\top}\bm{Y})" class="ltx_Math" display="inline" id="Thmexercise8.p1.m3"><semantics><mrow><mrow><mo stretchy="false">⟨</mo><mi>𝑿</mi><mo>,</mo><mi>𝒀</mi><mo stretchy="false">⟩</mo></mrow><mo>=</mo><mrow><mi>tr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒀</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\langle\bm{X},\bm{Y}\rangle=\operatorname{tr}(\bm{X}^{\top}\bm{Y})</annotation><annotation encoding="application/x-llamapun">⟨ bold_italic_X , bold_italic_Y ⟩ = roman_tr ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Y )</annotation></semantics></math>.
An excellent general reference for facts about optimization on the orthogonal group is Edelman, Arias, and Smith <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx77" title="">EAS98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Thmexercise8.p2">
<p class="ltx_p">Let <math alttext="f:\mathbb{R}^{d\times d}\to\mathbb{R}" class="ltx_Math" display="inline" id="Thmexercise8.p2.m1"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^{d\times d}\to\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math> be a given twice-continuously-differentiable objective function. Consider the orthogonally-constrained optimization problem</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\bm{Q}^{\top}\bm{Q}=\bm{I}}\,f(\bm{Q})." class="ltx_Math" display="block" id="S5.E6.m1"><semantics><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow></munder><mo lspace="0.337em">⁡</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\max_{\bm{Q}^{\top}\bm{Q}=\bm{I}}\,f(\bm{Q}).</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_POSTSUBSCRIPT italic_f ( bold_italic_Q ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.6)</span></td>
</tr></tbody>
</table>
<ol class="ltx_enumerate" id="S5.I6">
<li class="ltx_item" id="S5.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I6.i1.p1">
<p class="ltx_p">It is easily seen that the orthogonal group has the defining equation <math alttext="F(\bm{Q})=\bm{Q}^{\top}\bm{Q}=\bm{I}" class="ltx_Math" display="inline" id="S5.I6.i1.p1.m1"><semantics><mrow><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑸</mi></mrow><mo>=</mo><mi>𝑰</mi></mrow><annotation encoding="application/x-tex">F(\bm{Q})=\bm{Q}^{\top}\bm{Q}=\bm{I}</annotation><annotation encoding="application/x-llamapun">italic_F ( bold_italic_Q ) = bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I</annotation></semantics></math>.
Show, using this fact, that the tangent space to the orthogonal group at <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S5.I6.i1.p1.m2"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T_{\bm{Q}}\mathsf{O}(d)=\{\bm{Q}\bm{\Omega}\in\mathbb{R}^{d\times d}\mid\bm{\Omega}^{\top}=-\bm{\Omega}\}," class="ltx_Math" display="block" id="S5.Ex6.m1"><semantics><mrow><mrow><mrow><msub><mi>T</mi><mi>𝑸</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mi>𝑸</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝛀</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><msup><mi>𝛀</mi><mo>⊤</mo></msup><mo>=</mo><mrow><mo>−</mo><mi>𝛀</mi></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">T_{\bm{Q}}\mathsf{O}(d)=\{\bm{Q}\bm{\Omega}\in\mathbb{R}^{d\times d}\mid\bm{\Omega}^{\top}=-\bm{\Omega}\},</annotation><annotation encoding="application/x-llamapun">italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) = { bold_italic_Q bold_Ω ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT ∣ bold_Ω start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = - bold_Ω } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and that the orthogonal projection onto this subspace is</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}(\bm{\Delta})=\bm{Q}\operatorname{skew}(\bm{Q}^{\top}\bm{\Delta})," class="ltx_Math" display="block" id="S5.Ex7.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>T</mi><mi>𝑸</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝚫</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑸</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>skew</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝚫</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}(\bm{\Delta})=\bm{Q}\operatorname{skew}(\bm{Q}^{\top}\bm{\Delta}),</annotation><annotation encoding="application/x-llamapun">caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_Δ ) = bold_italic_Q roman_skew ( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_Δ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\operatorname{skew}(\bm{\Delta})=\tfrac{1}{2}(\bm{\Delta}-\bm{\Delta}^{\top})" class="ltx_Math" display="inline" id="S5.I6.i1.p1.m3"><semantics><mrow><mrow><mi>skew</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝚫</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝚫</mi><mo>−</mo><msup><mi>𝚫</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{skew}(\bm{\Delta})=\tfrac{1}{2}(\bm{\Delta}-\bm{\Delta}^{\top})</annotation><annotation encoding="application/x-llamapun">roman_skew ( bold_Δ ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_Δ - bold_Δ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math> is the orthogonal projection onto the set of skew-symmetric matrices.
The vector field</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{grad}\,f(\bm{Q})=\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}\left(\nabla f(\bm{Q})\right)" class="ltx_Math" display="block" id="S5.E7.m1"><semantics><mrow><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>T</mi><mi>𝑸</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{Q})=\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}\left(\nabla f(\bm{Q})\right)</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_Q ) = caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( ∇ italic_f ( bold_italic_Q ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is known as the <span class="ltx_text ltx_font_italic">Riemannian gradient</span> of the function <math alttext="f" class="ltx_Math" display="inline" id="S5.I6.i1.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> restricted to the orthogonal group.
The <span class="ltx_text ltx_font_italic">first order optimality conditions</span> for the optimization problem (<a class="ltx_ref" href="#S5.E6" title="Equation 2.5.6 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.6</span></a>) can be expressed in terms of the Riemann gradient:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{grad}\,f(\bm{Q})=\mathbf{0}." class="ltx_Math" display="block" id="S5.Ex8.m1"><semantics><mrow><mrow><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{Q})=\mathbf{0}.</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_Q ) = bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S5.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I6.i2.p1">
<p class="ltx_p">Show, by differentiating the Riemann gradient <math alttext="\mathrm{grad}\,f(\bm{Q})" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m1"><semantics><mrow><mi>grad</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{grad}\,f(\bm{Q})</annotation><annotation encoding="application/x-llamapun">roman_grad italic_f ( bold_italic_Q )</annotation></semantics></math> for the orthogonal group with respect to <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m2"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> as in the first part of this exercise, that the <span class="ltx_text ltx_font_italic">Riemannian Hessian</span> is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Hess}\,f(\bm{Q})=\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}\left(\nabla^{2}f(\bm{Q})-\operatorname{sym}(\bm{Q}^{\top}\nabla f(\bm{Q}))\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}\bm{I}\right)\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}," class="ltx_Math" display="block" id="S5.E8.m1"><semantics><mrow><mrow><mrow><mi>Hess</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>T</mi><mi>𝑸</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mrow><msup><mo rspace="0.167em">∇</mo><mn>2</mn></msup><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mrow><mi>sym</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em" stretchy="false">)</mo></mrow></mrow><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊗</mo></mpadded><mi>𝑰</mi></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>T</mi><mi>𝑸</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{Hess}\,f(\bm{Q})=\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)}\left(\nabla^{2}f(\bm{Q})-\operatorname{sym}(\bm{Q}^{\top}\nabla f(\bm{Q}))\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}\bm{I}\right)\mathcal{P}_{T_{\bm{Q}}\mathsf{O}(d)},</annotation><annotation encoding="application/x-llamapun">roman_Hess italic_f ( bold_italic_Q ) = caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( ∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( bold_italic_Q ) - roman_sym ( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∇ italic_f ( bold_italic_Q ) ) ⊗ bold_italic_I ) caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\operatorname{sym}(\bm{\Delta})=\tfrac{1}{2}(\bm{\Delta}+\bm{\Delta}^{\top})" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m3"><semantics><mrow><mrow><mi>sym</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝚫</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝚫</mi><mo>+</mo><msup><mi>𝚫</mi><mo>⊤</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{sym}(\bm{\Delta})=\tfrac{1}{2}(\bm{\Delta}+\bm{\Delta}^{\top})</annotation><annotation encoding="application/x-llamapun">roman_sym ( bold_Δ ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( bold_Δ + bold_Δ start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT )</annotation></semantics></math> denotes the orthogonal projection onto the set of symmetric matrices, and <math alttext="\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m4"><semantics><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊗</mo></mpadded><annotation encoding="application/x-tex">\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}</annotation><annotation encoding="application/x-llamapun">⊗</annotation></semantics></math> denotes the Kronecker product of matrices.
Take care to interpret the operators appearing in the previous expression as <span class="ltx_text ltx_font_italic">linear transformations on <math alttext="{d\times d}" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m5"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">{d\times d}</annotation><annotation encoding="application/x-llamapun">italic_d × italic_d</annotation></semantics></math> matrices</span>, <span class="ltx_text ltx_font_bold">not</span> as <math alttext="d\times d" class="ltx_Math" display="inline" id="S5.I6.i2.p1.m6"><semantics><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d\times d</annotation><annotation encoding="application/x-llamapun">italic_d × italic_d</annotation></semantics></math> matrices themselves.
The <span class="ltx_text ltx_font_italic">second-order optimality conditions</span> for the optimization problem (<a class="ltx_ref" href="#S5.E6" title="Equation 2.5.6 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.6</span></a>) can be expressed in terms of the Riemann Hessian:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Hess}\,f(\bm{Q})\preceq\mathbf{0}." class="ltx_Math" display="block" id="S5.Ex9.m1"><semantics><mrow><mrow><mrow><mi>Hess</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑸</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⪯</mo><mn>𝟎</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{Hess}\,f(\bm{Q})\preceq\mathbf{0}.</annotation><annotation encoding="application/x-llamapun">roman_Hess italic_f ( bold_italic_Q ) ⪯ bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">For a minimization problem, the sign is reversed.</p>
</div>
<div class="ltx_para" id="S5.I6.i2.p2">
<p class="ltx_p">(<span class="ltx_text ltx_font_italic">Hint: The key is to manipulate one’s calcluations to obtain the form (<a class="ltx_ref" href="#S5.E8" title="Equation 2.5.8 ‣ Item 2 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.8</span></a>), which is as compact as possible. To this end, make use of the following isomorphism of the Kronecker product: if <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S5.I6.i2.p2.m1"><semantics><mi>𝐀</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S5.I6.i2.p2.m2"><semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, and <math alttext="\bm{B}" class="ltx_Math" display="inline" id="S5.I6.i2.p2.m3"><semantics><mi>𝐁</mi><annotation encoding="application/x-tex">\bm{B}</annotation><annotation encoding="application/x-llamapun">bold_italic_B</annotation></semantics></math> are matrices of compatible sizes, then one has</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\bm{B}^{\top}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}\bm{A})\operatorname{\mathrm{vec}}(\bm{X})=\operatorname{\mathrm{vec}}(\bm{A}\bm{X}\bm{B})," class="ltx_Math" display="block" id="S5.Ex10.m1"><semantics><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑩</mi><mo>⊤</mo></msup><mpadded class="ltx_markedasmath" depth="0.7pt" height="4.7pt" voffset="1.3pt" width="8.0pt"><mo class="ltx_markedasmath">⊗</mo></mpadded><mi>𝑨</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>vec</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>vec</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑩</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">(\bm{B}^{\top}\mathbin{\mathchoice{\raisebox{1.3pt}{$\displaystyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{1.3pt}{$\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.75pt}{$\scriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}{\raisebox{0.6pt}{$\scriptscriptstyle\mathchoice{\scalebox{0.8}{$\displaystyle\otimes$}}{\scalebox{0.8}{$\textstyle\otimes$}}{\scalebox{0.8}{$\scriptstyle\otimes$}}{\scalebox{0.8}{$\scriptscriptstyle\otimes$}}$}}}\bm{A})\operatorname{\mathrm{vec}}(\bm{X})=\operatorname{\mathrm{vec}}(\bm{A}\bm{X}\bm{B}),</annotation><annotation encoding="application/x-llamapun">( bold_italic_B start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⊗ bold_italic_A ) roman_vec ( bold_italic_X ) = roman_vec ( bold_italic_A bold_italic_X bold_italic_B ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\operatorname{\mathrm{vec}}(\bm{X})" class="ltx_Math" display="inline" id="S5.I6.i2.p2.m4"><semantics><mrow><mi>vec</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐗</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{\mathrm{vec}}(\bm{X})</annotation><annotation encoding="application/x-llamapun">roman_vec ( bold_italic_X )</annotation></semantics></math> denotes the “left-to-right” stacking of the columns of the matrix argument into a vector. We use this isomorphism in (<a class="ltx_ref" href="#S5.E8" title="Equation 2.5.8 ‣ Item 2 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.8</span></a>) in order to define the Kronecker product of two matrices as an operator on matrices in a canonical way.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S5.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I6.i3.p1">
<p class="ltx_p">Now suppose <math alttext="\bm{X}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S5.I6.i3.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{X}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is full-rank. In this and the next part of the exercise, we consider the projection onto the orthogonal group of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S5.I6.i3.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{proj}_{\mathsf{O}(d)}(\bm{X})\doteq\min_{\bm{Q}\in\mathsf{O}(d)}\,\|\bm{Q}-\bm{X}\|_{F}^{2}." class="ltx_Math" display="block" id="S5.E9.m1"><semantics><mrow><mrow><mrow><msub><mi>proj</mi><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><munder><mi>min</mi><mrow><mi>𝑸</mi><mo>∈</mo><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mo>⁡</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝑸</mi><mo>−</mo><mi>𝑿</mi></mrow><mo stretchy="false">‖</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{proj}_{\mathsf{O}(d)}(\bm{X})\doteq\min_{\bm{Q}\in\mathsf{O}(d)}\,\|\bm{Q}-\bm{X}\|_{F}^{2}.</annotation><annotation encoding="application/x-llamapun">roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ≐ roman_min start_POSTSUBSCRIPT bold_italic_Q ∈ sansserif_O ( italic_d ) end_POSTSUBSCRIPT ∥ bold_italic_Q - bold_italic_X ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2.5.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We will prove that the solution to this problem is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{proj}_{\mathsf{O}(d)}(\bm{X})=\bm{U}\bm{V}^{\top}," class="ltx_Math" display="block" id="S5.Ex11.m1"><semantics><mrow><mrow><mrow><msub><mi>proj</mi><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{proj}_{\mathsf{O}(d)}(\bm{X})=\bm{U}\bm{V}^{\top},</annotation><annotation encoding="application/x-llamapun">roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) = bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{X}=\bm{U}\bm{S}\bm{V}^{\top}" class="ltx_Math" display="inline" id="S5.I6.i3.p1.m3"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{S}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_italic_S bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math> is a singular value decomposition of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S5.I6.i3.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.I6.i3.p2">
<ol class="ltx_enumerate" id="S5.I6.i3.I1">
<li class="ltx_item" id="S5.I6.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S5.I6.i3.I1.i1.p1">
<p class="ltx_p">Using the first and second-order optimality conditions, show that every local minimizer <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i1.p1.m1"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> of (<a class="ltx_ref" href="#S5.E9" title="Equation 2.5.9 ‣ Item 3 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.9</span></a>) satisfies</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.S3.EGx12">
<tbody id="S5.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\left(\bm{Q}^{\top}\bm{X}\right)^{\top}" class="ltx_Math" display="inline" id="S5.Ex12.m1"><semantics><msup><mrow><mo>(</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>)</mo></mrow><mo>⊤</mo></msup><annotation encoding="application/x-tex">\displaystyle\left(\bm{Q}^{\top}\bm{X}\right)^{\top}</annotation><annotation encoding="application/x-llamapun">( bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{Q}^{\top}\bm{X}," class="ltx_Math" display="inline" id="S5.Ex12.m2"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{Q}^{\top}\bm{X},</annotation><annotation encoding="application/x-llamapun">= bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{Q}^{\top}\bm{X}" class="ltx_Math" display="inline" id="S5.Ex13.m1"><semantics><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">\displaystyle\bm{Q}^{\top}\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\succeq\mathbf{0}." class="ltx_Math" display="inline" id="S5.Ex13.m2"><semantics><mrow><mrow><mi></mi><mo>⪰</mo><mn>𝟎</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\succeq\mathbf{0}.</annotation><annotation encoding="application/x-llamapun">⪰ bold_0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">(<span class="ltx_text ltx_font_italic">Hint: use linearity of the Kronecker product in either of its two arguments when the other is fixed.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S5.I6.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="S5.I6.i3.I1.i2.p1">
<p class="ltx_p">Using these conditions, argue that at every local minimizer <math alttext="\bm{Q}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i2.p1.m1"><semantics><mi>𝑸</mi><annotation encoding="application/x-tex">\bm{Q}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q</annotation></semantics></math> of (<a class="ltx_ref" href="#S5.E9" title="Equation 2.5.9 ‣ Item 3 ‣ Exercise 2.8. ‣ 2.5 Exercises and Extensions ‣ Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2.5.9</span></a>), one has <math alttext="\bm{Q}^{\top}\bm{X}=(\bm{X}^{\top}\bm{X})^{1/2}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i2.p1.m2"><semantics><mrow><mrow><msup><mi>𝑸</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝑿</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑿</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{Q}^{\top}\bm{X}=(\bm{X}^{\top}\bm{X})^{1/2}</annotation><annotation encoding="application/x-llamapun">bold_italic_Q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X = ( bold_italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
(<span class="ltx_text ltx_font_italic">Hint: Use the fact from linear algebra that if <math alttext="\bm{S}\succeq\mathbf{0}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i2.p1.m3"><semantics><mrow><mi>𝐒</mi><mo>⪰</mo><mn>𝟎</mn></mrow><annotation encoding="application/x-tex">\bm{S}\succeq\mathbf{0}</annotation><annotation encoding="application/x-llamapun">bold_italic_S ⪰ bold_0</annotation></semantics></math> is a symmetric positive semidefinite matrix, then <math alttext="(\bm{S}^{\top}\bm{S})^{1/2}=\bm{S}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i2.p1.m4"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝐒</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝐒</mi></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>=</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">(\bm{S}^{\top}\bm{S})^{1/2}=\bm{S}</annotation><annotation encoding="application/x-llamapun">( bold_italic_S start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_S ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT = bold_italic_S</annotation></semantics></math>.</span>)</p>
</div>
</li>
<li class="ltx_item" id="S5.I6.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="S5.I6.i3.I1.i3.p1">
<p class="ltx_p">Using the singular value decomposition <math alttext="\bm{X}=\bm{U}\bm{S}\bm{V}^{\top}" class="ltx_Math" display="inline" id="S5.I6.i3.I1.i3.p1.m1"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑺</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{U}\bm{S}\bm{V}^{\top}</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_U bold_italic_S bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT</annotation></semantics></math>, conclude that</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{U}\bm{V}^{\top}=\mathrm{proj}_{\mathsf{O}(d)}(\bm{X})." class="ltx_Math" display="block" id="S5.Ex14.m1"><semantics><mrow><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑽</mi><mo>⊤</mo></msup></mrow><mo>=</mo><mrow><msub><mi>proj</mi><mrow><mi>𝖮</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{U}\bm{V}^{\top}=\mathrm{proj}_{\mathsf{O}(d)}(\bm{X}).</annotation><annotation encoding="application/x-llamapun">bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 12:37:23 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
