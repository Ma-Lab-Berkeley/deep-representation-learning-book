<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Mon Aug 18 09:48:41 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 18, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book.css" rel="stylesheet" type="text/css"/><script defer="defer" src="shared-ui.js"></script><script defer="defer" src="book.js"></script></head>
<body id="top">
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Preface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Declaration of Open Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Chx3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter ltx_ref_self">
<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></span>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="In Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Intelligence, Cybernetics, and Artificial Intelligence</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px1" title="In 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Emergence and evolution of intelligence.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px2" title="In 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Evolution of human intelligence.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px3" title="In 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Origin of machine intelligence – Cybernetics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px4" title="In 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Origin of Artificial Intelligence.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S1.SS0.SSS0.Px5" title="In 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">The renaissance of “Artificial Intelligence” or “Cybernetics”?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="In Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>What to Learn?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS1" title="In 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.1 </span>Predictability</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px1" title="In 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Scalar Case.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px2" title="In 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Multi-Variable Case.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px3" title="In 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Vector Case.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px4" title="In 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Controlled Prediction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS1.SSS0.Px5" title="In 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Continuous Processes.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="In 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2.2 </span>Low Dimensionality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px1" title="In 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Learn to Predict.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px2" title="In 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Predictability and Low-Dimensionality.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S2.SS2.SSS0.Px3" title="In 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Properties of Low-Dimensionality.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="In Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>How to Learn?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS1" title="In 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3.1 </span>Analytical Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS1.SSSx1" title="In 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Linear Dynamical Systems</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx1.Px1" title="In Linear Dynamical Systems ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Wiener Filter.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx1.Px2" title="In Linear Dynamical Systems ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Kalman Filter.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx1.Px3" title="In Linear Dynamical Systems ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Identification of Linear Dynamical Systems.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS1.SSSx2" title="In 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Linear and Mixed Linear Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx2.Px1" title="In Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Principal Component Analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx2.Px2" title="In Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Independent Component Analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx2.Px3" title="In Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Sparse Structures and Compressive Sensing.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx2.Px4" title="In Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Dictionary Learning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS1.SSSx3" title="In 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">General Distributions</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx3.Px1" title="In General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Denoising.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS1.SSSx3.Px2" title="In General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Entropy minimization.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="In 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3.2 </span>Empirical Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS2.SSSx1" title="In 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Classic Artificial Neural Networks</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx1.Px1" title="In Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Artificial neuron.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx1.Px2" title="In Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Artificial neural network.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx1.Px3" title="In Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Convolutional neural networks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx1.Px4" title="In Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Backpropagation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx1.Px5" title="In Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Compressive autoencoding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="#S3.SS2.SSSx2" title="In 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Modern Deep Neural Networks</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx2.Px1" title="In Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Classification and recognition.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx2.Px2" title="In Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Reinforcement learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx2.Px3" title="In Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Generation and prediction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx2.Px4" title="In Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Generation via discriminative approaches.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS2.SSSx2.Px5" title="In Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Generation via denoising and diffusion.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="In Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>A Unifying Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="In 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.1 </span>Learning Parsimonious Representations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px1" title="In 1.4.1 Learning Parsimonious Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Pursuing low-dimensionality via compression.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px2" title="In 1.4.1 Learning Parsimonious Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Computable measure of parsimony.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="In 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.2 </span>Learning Informative Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="In 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.3 </span>Learning Consistent Representations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS3.SSS0.Px1" title="In 1.4.3 Learning Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Bidirectional Autoencoding for Consistency.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS4" title="In 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.4 </span>Learning Self-Consistent Representations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="In Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Bridging Theory and Practice for Machine Intelligence</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S5.SS0.SSS0.Px1" title="In 1.5 Bridging Theory and Practice for Machine Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title">Back to Intelligence.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Learning Linear and Independent Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch3.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pursuing Low-Dimensional Distributions via Lossy Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch4.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Deep Representations from Unrolled Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch5.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Consistent and Self-Consistent Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch6.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Inference with Low-Dimensional Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch7.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Learning Representations for Real-World Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a class="ltx_ref" href="Ch8.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Study of Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A1.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Optimization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="A2.html" title="In Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Entropy, Diffusion, Denoising, and Lossy Coding</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
</header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 1 </span>Introduction</h1><div class="mini-toc"><div class="mini-toc-title">In this chapter</div><ul><li><a href="#S1">Intelligence, Cybernetics, and Artificial Intelligence</a></li><li><a href="#S2">What to Learn?</a><div class="mini-toc-sub"><a href="#S2.SS1">Predictability</a><a href="#S2.SS2">Low Dimensionality</a></div></li><li><a href="#S3">How to Learn?</a><div class="mini-toc-sub"><a href="#S3.SS1">Analytical Approaches</a><a href="#S3.SS2">Empirical Approaches</a></div></li><li><a href="#S4">A Unifying Approach</a><div class="mini-toc-sub"><a href="#S4.SS1">Learning Parsimonious Representations</a><a href="#S4.SS2">Learning Informative Representations</a><a href="#S4.SS3">Learning Consistent Representations</a><a href="#S4.SS4">Learning Self-Consistent Representations</a></div></li><li><a href="#S5">Bridging Theory and Practice for Machine Intelligence</a></li></ul></div>
<div class="ltx_para" id="p1">
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">Just as the constant increase of entropy is the basic law of the universe, so it is the basic law of life to be ever more highly structured and to struggle against entropy.</span>”</p>
<p class="ltx_p">  – Václav Havel</p>
</blockquote>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.1 </span>Intelligence, Cybernetics, and Artificial Intelligence</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">The world in which we are living is neither fully random nor completely unpredictable.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note there is no need for an intelligent being to learn or memorize anything if the world is fully random.</span></span></span> Instead, it follows certain orders, patterns, and laws that make it largely predictable.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Some deterministic and some probabilistic.</span></span></span> The very emergence and existence of life depend on a predictable living environment. Only by learning and memorizing what is predictable in the environment can life survive and thrive since good decisions and actions depend on reliable predictions. Because there seem to be unlimited things that are predictable about the world, intelligent beings, such as animals and humans, have continued to improve through evolution their capability to explore and exploit such predictability for a better and better life. To this end, they have developed increasingly more acute senses, including vision, audio, touch, taste, and smell, to perceive what is predictable in the external environment from these high-throughput sensory data. Hence a fundamental task for all intelligent beings is to be able to:</p>
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_italic">learn and memorize predictable information from massive sensed data.</span></p>
<p class="ltx_p">Before we may begin to understand how this is done, we need to consider a few related questions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">How to model and represent such predictable information in the data mathematically?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">How can such information be learned effectively and efficiently from the data computationally?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">How should such information be best organized for future prediction and inference?</p>
</div>
</li>
</ul>
<p class="ltx_p">This book aims to provide some answers to these questions. These answers will help us better understand intelligence, especially the computational principles and mechanisms that enable it. There is reason to believe that all forms of intelligence, from low-level intelligence seen in early primitive life to the highest form of intelligence, the practice of modern science, follow the same set of principles and mechanisms. We will elaborate more below.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Emergence and evolution of intelligence.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p">A necessary condition for the emergence of life on earth about 4 billion years ago is that the earth’s environment is largely predictable. In the environment, life has developed mechanisms that allow it to learn what is predictable about the environment, encode the information in a certain way, and use it for survival. Generally speaking, we call the ability to learn <span class="ltx_text ltx_font_italic">intelligence</span>. To a large extent, the evolution of life is the mechanism of intelligence at work. In nature, intelligence is mainly developed through two types of learning mechanisms: <span class="ltx_text ltx_font_italic">phylogenetic</span> and <span class="ltx_text ltx_font_italic">ontogenetic</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx293" title="">Wie61</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The phylogenetic intelligence</span> refers to learning through the evolution of species. Species inherit and survive mainly based on knowledge inherited from DNAs or genes of their parents. To a large extent, we may call DNAs nature’s pre-trained large models because they play a very similar role. The main characteristic of phylogenetic intelligence is that individuals do not have much learning capacity. Learning is carried out with a “trial-and-error” mechanism based on random mutation of the genes, and then species evolve based on the process of natural selection – survival of the fittest, as shown in Figure <a class="ltx_ref" href="#F1" title="Figure 1.1 ‣ Emergence and evolution of intelligence. ‣ 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.1</span></a>.
This can be viewed as nature’s way of implementing what is now known as “reinforcement learning.” However, such a “trial-and-error” learning process can be extremely slow, costly, and unpredictable: It is known that from the emergence of the first life forms, from about 4.4 to 3.8 billion years ago, life has relied on this form of evolution.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Astute readers may have noticed an uncanny similarity between how early life evolves and how large language models evolve today.</span></span></span></p>
</div>
<figure class="ltx_figure" id="F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.1 : Evolution of phylogenetic intelligence: Knowledge of the external world is encoded and passed on via DNAs (left), and it is decoded from DNA to RNA and to Protein etc. In the early stage of life evolution (right), intelligence develops knowledge at the species level via (random) gene mutation and natural selection – “may the fittest survive,” which can be viewed as a primitive form of reinforcement learning." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="200" id="F1.g1" src="chapters/chapter1/figs/DNAs.png" width="299"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.1 : Evolution of phylogenetic intelligence: Knowledge of the external world is encoded and passed on via DNAs (left), and it is decoded from DNA to RNA and to Protein etc. In the early stage of life evolution (right), intelligence develops knowledge at the species level via (random) gene mutation and natural selection – “may the fittest survive,” which can be viewed as a primitive form of reinforcement learning." class="ltx_graphics ltx_figure_panel ltx_img_square" height="203" id="F1.g2" src="chapters/chapter1/figs/Evolution.jpg" width="240"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.1</span>: </span><span class="ltx_text" style="font-size:90%;">Evolution of phylogenetic intelligence: Knowledge of the external world is encoded and passed on via DNAs (left), and it is decoded from DNA to RNA and to Protein etc. In the early stage of life evolution (right), intelligence develops knowledge at the species level via (random) gene mutation and natural selection – “may the fittest survive,” which can be viewed as a primitive form of reinforcement learning.</span></figcaption>
</figure>
<figure class="ltx_figure" id="F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.2 : Evolution of life, from the ancestor of all life today (named LUCA — last universal common ancestor), a single-cell-like organism which lived from 3.5-4.3 billion years ago, to the emergence of the first nervous system in worm-like species (middle), about 550 million years ago, to the explosion of life forms in the Cambrian period (right), about 530 million years ago." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="114" id="F2.g1" src="chapters/chapter1/figs/luca.jpeg" width="166"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.2 : Evolution of life, from the ancestor of all life today (named LUCA — last universal common ancestor), a single-cell-like organism which lived from 3.5-4.3 billion years ago, to the emergence of the first nervous system in worm-like species (middle), about 550 million years ago, to the explosion of life forms in the Cambrian period (right), about 530 million years ago." class="ltx_graphics ltx_figure_panel ltx_img_square" height="114" id="F2.g2" src="chapters/chapter1/figs/Worm.jpeg" width="133"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.2 : Evolution of life, from the ancestor of all life today (named LUCA — last universal common ancestor), a single-cell-like organism which lived from 3.5-4.3 billion years ago, to the emergence of the first nervous system in worm-like species (middle), about 550 million years ago, to the explosion of life forms in the Cambrian period (right), about 530 million years ago." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="114" id="F2.g3" src="chapters/chapter1/figs/Cambrian.jpg" width="251"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.2</span>: </span><span class="ltx_text" style="font-size:90%;">Evolution of life, from the ancestor of all life today (named LUCA — last universal common ancestor), a single-cell-like organism which lived from 3.5-4.3 billion years ago, to the emergence of the first nervous system in worm-like species (middle), about 550 million years ago, to the explosion of life forms in the Cambrian period (right), about 530 million years ago.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The ontogenetic intelligence</span> refers to the learning mechanisms that allow an individual to learn through its own senses, memories, and predictions within its specific living environment and to improve and adapt its behaviors. The ontogenetic learning became possible after the emergence of the nervous system about 550 to 600 million years ago (in worm-like organisms), shown in Figure <a class="ltx_ref" href="#F2" title="Figure 1.2 ‣ Emergence and evolution of intelligence. ‣ 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2</span></a> middle. That is, with a sensory and nervous system, an individual is capable of continuously forming and improving its own knowledge about the world, also known as a memory, in addition to what is inherited from its DNAs or genes. This capability has significantly enhanced the survival of the individual and attributed to the explosion of life forms in the Cambrian period about 530 million years ago. Compared to phylogenetic learning, ontogenetic learning is significantly more efficient and predictable, which can be realized with the resource limit of an individual in its lifespan.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p4">
<p class="ltx_p">Notice that both types of learning mechanisms rely on some form of feedback (from the external environment), in terms of a penalty (death) or reward (food), of a species or individuals’ actions<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Gene mutation of the species or actions made by the individual.</span></span></span> to learn. As a result, all intelligent beings, as species or as individuals, rely on a closed-loop feedback mechanism to learn and improve their knowledge about the world. We also notice that from plants, to fish, to birds, and to mammals, more advanced species rely more and more on their ontogenetic learning capabilities. They stay with and learn from their parents longer and longer after birth, because individuals of the same species need to survive in very diverse environments.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Evolution of human intelligence.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p">Since the emergence of homo sapiens about 315 thousand years ago, a new and higher form of intelligence emerged which evolves more efficiently and economically. Languages, first spoken<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>It was believed that Sanskrit was the first spoken language, dated as back as 5000 BC.</span></span></span> and then written<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Sumerian language is believed to be one of the oldest written language in existence, first attested about 3100 BC in southern Mesopotamia.</span></span></span>, were developed a few thousand years ago. See Figure <a class="ltx_ref" href="#F3" title="Figure 1.3 ‣ Evolution of human intelligence. ‣ 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3</span></a>. This allows individuals to communicate and share useful information with others. Therefore, a human community or society can behave like a single intelligent organism that can learn much faster and hold more knowledge than any individual. In a way, written languages, or texts, play a role similar to DNAs and genes as they allow human societies to accumulate and pass knowledge of the world onto next generations. We may refer to this type of intelligence as <span class="ltx_text ltx_font_italic">societal intelligence</span>, to distinguish it from the phylogenetic intelligence of species and the ontogenetic intelligence of individuals. This type of knowledge accumulation serves as the foundation of ancient civilizations.</p>
</div>
<figure class="ltx_figure" id="F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.3 : The development of verbal communication and spoken languages (between 10000-5000 BC), written languages (about 3000 BC), and mathematics (around 500-300 BC) mark three key milestones in the evolution of human intelligence." class="ltx_graphics ltx_figure_panel ltx_img_square" height="150" id="F3.g1" src="chapters/chapter1/figs/Spoken-language.jpg" width="146"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.3 : The development of verbal communication and spoken languages (between 10000-5000 BC), written languages (about 3000 BC), and mathematics (around 500-300 BC) mark three key milestones in the evolution of human intelligence." class="ltx_graphics ltx_figure_panel ltx_img_square" height="150" id="F3.g2" src="chapters/chapter1/figs/Cuneiform.png" width="151"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.3 : The development of verbal communication and spoken languages (between 10000-5000 BC), written languages (about 3000 BC), and mathematics (around 500-300 BC) mark three key milestones in the evolution of human intelligence." class="ltx_graphics ltx_figure_panel ltx_img_square" height="150" id="F3.g3" src="chapters/chapter1/figs/adopt-euclid1685-2.jpg" width="182"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.3</span>: </span><span class="ltx_text" style="font-size:90%;">The development of verbal communication and spoken languages (between 10000-5000 BC), written languages (about 3000 BC), and mathematics (around 500-300 BC) mark three key milestones in the evolution of human intelligence.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p">Quite miraculously, about a few thousand years ago, another quantum leap in human intelligence occurred, which allowed philosophers and mathematicians to develop knowledge that seem to go way beyond developing empirical knowledge. The development of abstract mathematical concepts and symbols, such as numbers, space and time, as well as mathematical logic, serve as a new precise language for modern science. In addition, the development of the ability to generate new hypotheses and verify their correctness based on logic deduction or scientific experimentation. This, for the first time, has enabled humans to proactively develop new knowledge beyond passive empirical means. The ability to conduct these high-level forms of knowledge development is believed to be unique to humans. This advanced form of intelligence is referred to as “artificial intelligence” (AI), coined by John McCarthy at the Dartmouth summer workshop in 1956.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p3">
<p class="ltx_p">Hence, from what we can learn from the nature, from now on, whenever we use the word “intelligence,” we need to be very specific about which level/form of intelligence we mean:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{{phylogentic}}\;\Longrightarrow\;\mbox{{ontogenetic}}\;\Longrightarrow\;\mbox{{societal}}\;\Longrightarrow\;\mbox{{artificial intelligence}}." class="ltx_Math" display="block" id="S1.E1.m1"><semantics><mrow><mrow><mtext class="ltx_mathvariant_bold">phylogentic</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">ontogenetic</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">societal</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">artificial intelligence</mtext></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{{phylogentic}}\;\Longrightarrow\;\mbox{{ontogenetic}}\;\Longrightarrow\;\mbox{{societal}}\;\Longrightarrow\;\mbox{{artificial intelligence}}.</annotation><annotation encoding="application/x-llamapun">phylogentic ⟹ ontogenetic ⟹ societal ⟹ artificial intelligence .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.1.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">A clear characterization and distinction are necessary and important because we want to study intelligence as a scientific and mathematical subject. It is highly likely that, even if they all may share the common objective of learning useful knowledge of the world, the specific computational mechanisms and physical implementations behind each level/form of intelligence could be different. We believe that the reader would better understand and appreciate these differences after having finished study this book. Therefore, we will leave more discussions about general intelligence to the last Chapter <a class="ltx_ref" href="Ch8.html" title="Chapter 8 Future Study of Intelligence ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Origin of machine intelligence – Cybernetics.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p1">
<p class="ltx_p">In 1940s, partly due to the war effort, intelligence in nature had inspired scientists in the 1940s to emulate animal intelligence by machines, which led to the “Cybernetics” movement advocated by Norbert Wiener. Wiener studied zoology at Harvard as an undergraduate but later became a mathematician and control theorist. Wiener had a life long passion in understanding and developing autonomous systems that could emulate intelligent behaviors of animals. Today, the Cybernetics program is often narrowly interpreted by people as mainly about feedback control systems for which Wiener indeed made his most significant technical contributions. But the Cybernetics program was much broader and deeper than that. It is more about understanding intelligence as a whole<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>At least at the level of animals.</span></span></span> and had actually influenced the work of a whole generation of renowned scientists, including Warren McCulloch, Walter Pitts, Claude Shannon, John von Neumann, and Alan Turing.</p>
</div>
<figure class="ltx_figure" id="F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.4 : The book “Cybernetics” by Norbert Wiener published in 1948 [ Wie48 ] (left) and the second edition in 1961 [ Wie61 ] (right)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="240" id="F4.g1" src="chapters/chapter1/figs/Cybernetics1.jpg" width="155"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.4 : The book “Cybernetics” by Norbert Wiener published in 1948 [ Wie48 ] (left) and the second edition in 1961 [ Wie61 ] (right)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="240" id="F4.g2" src="chapters/chapter1/figs/Cybernetics2.jpg" width="159"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.4</span>: </span><span class="ltx_text" style="font-size:90%;">The book “Cybernetics” by Norbert Wiener published in 1948 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx291" title="">Wie48</a>]</cite> (left) and the second edition in 1961 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx293" title="">Wie61</a>]</cite> (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p2">
<p class="ltx_p">Wiener was arguably the first person who studied intelligence as <span class="ltx_text ltx_font_italic">a system</span>, instead of paying attention to only one component or aspect of it. His comprehensive views on intelligence were elaborated in his famous 1948 book <span class="ltx_text ltx_font_italic">“Cybernetics: or Control and Communication in the Animal and the Machine”</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx291" title="">Wie48</a>]</cite>. In this book and its second edition published in 1961 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx293" title="">Wie61</a>]</cite> (see Figure <a class="ltx_ref" href="#F4" title="Figure 1.4 ‣ Origin of machine intelligence – Cybernetics. ‣ 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4</span></a>), he tried to identify several necessary characteristics and mechanisms of intelligent systems, which include (but are not limited to):</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p">How to <span class="ltx_text ltx_font_italic">measure and store</span> information (in the brain) and how to communicate with others. <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Norbert Wiener was the first to point out “information” is not matter nor energy, but an independent quantity for study.</span></span></span> This led to the formulation of information theory and coding theory by Claude Shannon in 1948.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p">How to <span class="ltx_text ltx_font_italic">correct errors</span> in prediction and estimation based on existing information. Norbert Wiener himself helped formalize the theory for control systems based on closed-loop feedback in the 1940s.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p">How to learn to <span class="ltx_text ltx_font_italic">make better decisions</span> from interacting with a potentially non-cooperative opponent or adversarial environment. This was formalized by John von Neumann as game theory in 1944.</p>
</div>
</li>
</ul>
<p class="ltx_p">In 1943, very much motivated by Wiener’s Cybernetics program, the psychiatrist Warren McCulloch and the logician Walter Pitts together formalized the first computational model for a neuron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx181" title="">MP43</a>]</cite>, called <span class="ltx_text ltx_font_italic">an artificial neuron</span>, as illustrated later in Figure <a class="ltx_ref" href="#F13" title="Figure 1.13 ‣ Artificial neuron. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.13</span></a>. Based on this model, in 1950s, Frank Rosenblatt built a physical machine, named the <span class="ltx_text ltx_font_italic">Mark I Perceptron</span>, with a network of hundreds of such artificial neurons. Perceptron was the first artificial neural network physically realized, see Figure <a class="ltx_ref" href="#F15" title="Figure 1.15 ‣ Artificial neural network. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.15</span></a>. Notably, John von Neumann’s universal computer architecture, proposed in 1945, was also designed to facilitate the goal of building <span class="ltx_text ltx_font_italic">computing machines</span> that can physically realize the mechanisms suggested by the Cybernetics program.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p3">
<p class="ltx_p">Acute readers probably have noticed that the 1940s was truly a magical era: So many fundamental ideas were invented and influential theories formalized in that era, including the mathematical model of neurons, artificial neural networks, information theory, control theory, game theory, and computing machines. Figure <a class="ltx_ref" href="#F5" title="Figure 1.5 ‣ Origin of machine intelligence – Cybernetics. ‣ 1.1 Intelligence, Cybernetics, and Artificial Intelligence ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.5</span></a> shows some of the pioneers of these theories. As we now know, each work above had grown to become the foundation of a scientific or engineering field for the following many decades and has tremendous impact on us. All these fundamental theories were inspired and motivated by the goal of trying to develop machines that can emulate intelligence in nature. Based on historical notes, Wiener’s Cybernetics movement had influenced almost all of these people and work. To a large extent, the Cybernetics program laid out by Wiener can be viewed as the true predecessor to the currently very popular “embodied” intelligence program. In fact, Wiender had described the program in much more concrete terms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx293" title="">Wie61</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Figure 1.5 : Pioneers of theoretical and computational foundations for intelligence: Norbert Wiener (cybernetics and control theory), Claude Shannon (information theory), John von Neumann (game theory), and Alan Turing (computing theory)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="180" id="F5.g1" src="chapters/chapter1/figs/Wiener.png" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Figure 1.5 : Pioneers of theoretical and computational foundations for intelligence: Norbert Wiener (cybernetics and control theory), Claude Shannon (information theory), John von Neumann (game theory), and Alan Turing (computing theory)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="180" id="F5.g2" src="chapters/chapter1/figs/Shannon.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Figure 1.5 : Pioneers of theoretical and computational foundations for intelligence: Norbert Wiener (cybernetics and control theory), Claude Shannon (information theory), John von Neumann (game theory), and Alan Turing (computing theory)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="180" id="F5.g3" src="chapters/chapter1/figs/neumann.jpg" width="143"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Figure 1.5 : Pioneers of theoretical and computational foundations for intelligence: Norbert Wiener (cybernetics and control theory), Claude Shannon (information theory), John von Neumann (game theory), and Alan Turing (computing theory)." class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="180" id="F5.g4" src="chapters/chapter1/figs/Turing.jpeg" width="137"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.5</span>: </span><span class="ltx_text" style="font-size:90%;">Pioneers of theoretical and computational foundations for intelligence: Norbert Wiener (cybernetics and control theory), Claude Shannon (information theory), John von Neumann (game theory), and Alan Turing (computing theory).</span></figcaption>
</figure>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p4">
<p class="ltx_p">Although Wiener had identified in his work many key characteristics and mechanisms of (embodied) intelligence, there was no indication that he knew how to properly integrate all these mechanisms together to build a complete autonomous intelligent system. Judging from today’s knowledge, some of his views on these mechanisms were not entirely accurate or complete. In particular, in the last chapter of the second edition of Cybernetics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx293" title="">Wie61</a>]</cite>, he pointed out that it is crucial to <span class="ltx_text ltx_font_italic">deal with nonlinearality</span> if a machine learning system is designed to emulate typical learning mechanisms in nature. But he did not provide any concrete and effective solutions to this difficult issue. To his defense though, at the time, few people knew how, since even the theory for dealing with linear models and systems was still in its infancy.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p5">
<p class="ltx_p">Nevertheless, we could not help but marvel at Wiener’s foresight about the importance of nonlinearity. As we will see in this book, the answer was found only recently: nonlinearity can be effectively dealt with through progressive linearization and transformation realized by deep neural networks (see Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>). In addition, we will attempt to show in this book how all these mechanisms listed above can be naturally integrated into a complete system which would exhibit characteristics of an autonomous intelligent system (see Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Origin of Artificial Intelligence.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px4.p1">
<p class="ltx_p">From the subtitle of Wiener’s Cybernetics book: <span class="ltx_text ltx_font_italic">“Control and Communication in the Animal and the Machine”</span>, one can tell that the studies in the 1940s mainly aimed to emulate intelligence at the level of animals. As we mentioned before, the research agendas about intelligence around the 1940s were very much dominated by Norbert Wiener’s Cybernetics movement.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px4.p2">
<p class="ltx_p">Alan Turing was one of the first to notice this limitation. In his famous 1950 paper “<span class="ltx_text ltx_font_italic">Computing Machinery and Intelligence</span>” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx267" title="">Tur50</a>]</cite>, Turing formally posted the question whether machines can imitate intelligence even at the human level, to the point of being indistinguishable from the intelligent capabilities of humans. This is now known as <span class="ltx_text ltx_font_italic">the Turing test</span>.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px4.p3">
<p class="ltx_p">Around 1955, a group of young and ambitious scientists tried to break away from the then dominating Cybernetics movement and research agendas so that they would have a chance to create their own legacy. They decided to take on Turing’s challenge of imitating human intelligence and proposed a workshop to be held at Dartmouth College in the summer of 1956. They made their intention clear with a statement in their proposal:</p>
<blockquote class="ltx_quote">
<p class="ltx_p">“<span class="ltx_text ltx_font_italic">The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves</span>.”</p>
</blockquote>
<p class="ltx_p">In essence, they wanted to formalize and study higher-level intelligence that differentiates humans from animals. The topics they considered ranged from abstraction, symbolic methods, natural languages, and deductive methods (including causal inference, logic deduction, etc.) The organizer of the workshop, John McCarthy, then a young assistant professor of Mathematics of the Dartmouth College, coined the now famous term “Artificial Intelligence” (AI) to encapsulate the set of characteristics or mechanisms that are believed to be <span class="ltx_text ltx_font_italic">unique to human intelligence</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">The renaissance of “Artificial Intelligence” or “Cybernetics”?</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px5.p1">
<p class="ltx_p">As the readers may have known, in the past decade or so, machine intelligence has undergone explosive development, powered mainly by the practice of deep artificial neural networks, triggered by the work of Geoffrey Hinton and students in 2012 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>. This era is also hailed as the “Renaissance” of Artificial Intelligence (AI). However, in terms of tasks that people have actually tried to tackle (recognition, generation, and prediction) and techniques that people have developed and implemented so far (reinforcing, imitating, encoding, decoding, denoising, and compression), we are very much just emulating the mechanisms that are common to the intelligence of early life and animals. Even in that regard, as we will try to clarify in this book, current “AI” models and systems have not correctly implemented all necessary mechanisms for intelligence at these levels, which were already known to the Cybernetics movement in the 1940s.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px5.p2">
<p class="ltx_p">Hence, strictly speaking, the advancement of machine intelligence in the past decade does not align well with the “Artificial Intelligence” program laid out in the 1956 Dartmouth workshop. Instead, what has been predominantly accomplished so far is more closely related to the objectives of the classic “Cybernetics” program laid out by Norbert Wiener in the 1940s. It is probably more appropriate to call the current era the “Renaissance of Cybernetics”.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>The recent rise of the so-called “Embodied AI” for autonomous intelligent robots share even more similarity with the goals of the Cybernetics program.</span></span></span> Only after we have fully understood what we have truly done from the scientific and mathematical perspective, can we truly know what remains to be done and which direction to go to pursue the true nature of intelligence. This is one of the main purposes of this book.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.2 </span>What to Learn?</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2.1 </span>Predictability</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at the heart of the theory of computing and very much led to the invention of computers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx268" title="">Tur36</a>]</cite>. The role of predictable sequences in (inductive) inference was studied by Ray Solomonoff, Andrey Kolmogorov, and many others in the 1960s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx141" title="">Kol98</a>]</cite> as a generalization to Claude Shannon’s classic Information Theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx242" title="">Sha48</a>]</cite>. To understand the concept of predictable sequences, let us first start with some concrete simple examples.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Scalar Case.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">The simplest predictable discrete sequence is arguably the sequence of natural numbers:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{S}=1,2,3,4,5,6,\ldots,n,n+1,\ldots" class="ltx_Math" display="block" id="S2.E1.m1"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo>,</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow><annotation encoding="application/x-tex">{S}=1,2,3,4,5,6,\ldots,n,n+1,\ldots</annotation><annotation encoding="application/x-llamapun">italic_S = 1 , 2 , 3 , 4 , 5 , 6 , … , italic_n , italic_n + 1 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">in which the next number <math alttext="x_{n+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1"><semantics><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{n+1}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math> is defined to be its previous number <math alttext="x_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m2"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_{n}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> plus 1:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+1}=x_{n}+1." class="ltx_Math" display="block" id="S2.E2.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>+</mo><mn>1</mn></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">x_{n+1}=x_{n}+1.</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">One may generalize the notion of predictability to any sequence <math alttext="\{x_{n}\}_{n=1}^{\infty}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m3"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><annotation encoding="application/x-tex">\{x_{n}\}_{n=1}^{\infty}</annotation><annotation encoding="application/x-llamapun">{ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="x_{n}\in\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m4"><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">x_{n}\in\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R</annotation></semantics></math> if the next number <math alttext="x_{n+1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m5"><semantics><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{n+1}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math> can always be computed from its previous one <math alttext="x_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m6"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_{n}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+1}=f(x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E3.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow><mo rspace="0.447em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n+1}=f(x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m7"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic">computable</span> (scalar) function.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Here we emphasize that the function <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="footnote10.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math> itself is computable, say it can be implemented as a program on a computer. </span></span></span> Note that here we emphasize that the function <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m8"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math> must be computable. There are many functions that can be defined but are not computable. Alan Turing’s seminal work in 1936 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx268" title="">Tur36</a>]</cite> gives a rigorous definition of computability. In practice, we often further assume that <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m9"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is efficiently computable and has nice properties such as being continuous and differentiable, etc. The necessity of these properties will become clear later once we understand more about more refined notions of computability, and their roles in machine learning and intelligence.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multi-Variable Case.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p">Of course, the value of the next number can also depend on two of its predecessors. For example, the famous <span class="ltx_text ltx_font_italic">Fibonacci sequence</span> is defined to be:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{S}=1,1,2,3,5,8,13,21,34,55,\ldots" class="ltx_Math" display="block" id="S2.E4.m1"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>8</mn><mo>,</mo><mn>13</mn><mo>,</mo><mn>21</mn><mo>,</mo><mn>34</mn><mo>,</mo><mn>55</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow><annotation encoding="application/x-tex">{S}=1,1,2,3,5,8,13,21,34,55,\ldots</annotation><annotation encoding="application/x-llamapun">italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where one can easily see:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+2}=x_{n+1}+x_{n},\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E5.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow><mo rspace="0.447em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n+2}=x_{n+1}+x_{n},\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Similarly, we may generalize this recursion to</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+2}=f(x_{n+1},x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E6.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow><mo rspace="0.447em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n+2}=f(x_{n+1},x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="f(\cdot,\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , ⋅ )</annotation></semantics></math> is any computable function that takes two variables as input. We can further generalize the notion of predictability to a sequence whose next value depends on say <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> of its predecessors:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+d}=f(x_{n+d-1},\ldots,x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E7.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mi>d</mi></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mrow><mi>n</mi><mo>+</mo><mi>d</mi></mrow><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow><mo rspace="0.447em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{n+d}=f(x_{n+d-1},\ldots,x_{n}),\quad x_{n}\in\mathbb{R},\;n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The number of predecessors <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> needed for the prediction is called the <span class="ltx_text ltx_font_italic">degree</span> of the recursive prediction. The above expression (<a class="ltx_ref" href="#S2.E7" title="Equation 1.2.7 ‣ Multi-Variable Case. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.7</span></a>) is also called an <span class="ltx_text ltx_font_italic">(auto) regression</span>. Such a sequence is also called a <span class="ltx_text ltx_font_italic">auto-regressive</span> sequence. If the function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is a linear function, we call it a linear (auto) regression.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Vector Case.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p">To simplify the notation, we may define a vector <math alttext="\bm{x}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m1"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> that collects <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> consecutive values in the sequence</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n}\doteq[x_{n+d-1},\ldots,x_{n}]^{\top},\quad\bm{x}_{n}\in\mathbb{R}^{d},\;n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E8.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mi>n</mi></msub><mo>≐</mo><msup><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mrow><mrow><mi>n</mi><mo>+</mo><mi>d</mi></mrow><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>𝒙</mi><mi>n</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="0.447em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{n}\doteq[x_{n+d-1},\ldots,x_{n}]^{\top},\quad\bm{x}_{n}\in\mathbb{R}^{d},\;n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≐ [ italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">With this notation, the recursive relation (<a class="ltx_ref" href="#S2.E7" title="Equation 1.2.7 ‣ Multi-Variable Case. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.7</span></a>) can be conveniently written as</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n+1}=g(\bm{x}_{n})\;\in\mathbb{R}^{d},\quad n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E9.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="1.167em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}=g(\bm{x}_{n})\;\in\mathbb{R}^{d},\quad n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_g ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the function <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m3"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ )</annotation></semantics></math> is uniquely defined by the function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> in (<a class="ltx_ref" href="#S2.E7" title="Equation 1.2.7 ‣ Multi-Variable Case. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.7</span></a>) and it takes a <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-dimensional vector as input. In different contexts, such a vector is sometimes referred to as a “state” or a “token”. Note that the equation in (<a class="ltx_ref" href="#S2.E7" title="Equation 1.2.7 ‣ Multi-Variable Case. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.7</span></a>) denotes a mapping <math alttext="\mathbb{R}^{d}\rightarrow\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m6"><semantics><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\mathbb{R}^{d}\rightarrow\mathbb{R}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R</annotation></semantics></math>, but the equation here is <math alttext="g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m7"><semantics><mrow><mi>g</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mi>d</mi></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">g:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">italic_g : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Controlled Prediction.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p">We may also define a predictable sequence that depends on another predictable sequence as input:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n+1}=f(\bm{x}_{n},\bm{u}_{n})\;\in\mathbb{R}^{d},\quad n=1,2,3,\ldots," class="ltx_Math" display="block" id="S2.E10.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo>,</mo><msub><mi>𝒖</mi><mi>n</mi></msub><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo rspace="1.167em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}=f(\bm{x}_{n},\bm{u}_{n})\;\in\mathbb{R}^{d},\quad n=1,2,3,\ldots,</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , … ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\{\bm{u}_{n}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m1"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒖</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{u}_{n}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> with <math alttext="\bm{u}_{n}\in\mathbb{R}^{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m2"><semantics><mrow><msub><mi>𝒖</mi><mi>n</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\bm{u}_{n}\in\mathbb{R}^{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> is a (computable) predictable sequence. In other words, the next vector <math alttext="\bm{x}_{n+1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m3"><semantics><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> depends on both <math alttext="\bm{x}_{n}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m4"><semantics><mrow><msub><mi>𝒙</mi><mi>n</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bm{x}_{n}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\bm{u}_{n}\in\mathbb{R}^{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m5"><semantics><mrow><msub><mi>𝒖</mi><mi>n</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\bm{u}_{n}\in\mathbb{R}^{k}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>. In the context of control theory, the sequence <math alttext="\{\bm{u}_{n}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m6"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒖</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{u}_{n}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> is often referred to as the “control input” and <math alttext="\bm{x}_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m7"><semantics><msub><mi>𝒙</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\bm{x}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> as the “state” or “output” of the system (<a class="ltx_ref" href="#S2.E10" title="Equation 1.2.10 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.10</span></a>). One classic example is a linear dynamical system:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n+1}=\bm{A}\bm{x}_{n}+\bm{B}\bm{u}_{n},\quad\bm{A}\in\mathbb{R}^{d\times d},\bm{B}\in\mathbb{R}^{d\times k}," class="ltx_Math" display="block" id="S2.E11.m1"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow><mo>+</mo><mrow><mi>𝑩</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>n</mi></msub></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mi>𝑩</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>k</mi></mrow></msup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}=\bm{A}\bm{x}_{n}+\bm{B}\bm{u}_{n},\quad\bm{A}\in\mathbb{R}^{d\times d},\bm{B}\in\mathbb{R}^{d\times k},</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT , bold_italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is widely studied in control theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx35" title="">CD91</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p2">
<p class="ltx_p">Very often the control input is given by a computable function of the state <math alttext="\bm{x}_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m1"><semantics><msub><mi>𝒙</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\bm{x}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> itself, say:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{u}_{n}=h(\bm{x}_{n}),\quad n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E12.m1"><semantics><mrow><mrow><msub><mi>𝒖</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{u}_{n}=h(\bm{x}_{n}),\quad n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As a result, the sequence <math alttext="\{\bm{x}_{n}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m2"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{n}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> is given by composing the two computable functions <math alttext="f" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m4"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n+1}=f\big{(}\bm{x}_{n},h(\bm{x}_{n})\big{)},\quad n=1,2,3,\ldots" class="ltx_Math" display="block" id="S2.E13.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">(</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo>,</mo><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>n</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}=f\big{(}\bm{x}_{n},h(\bm{x}_{n})\big{)},\quad n=1,2,3,\ldots</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) , italic_n = 1 , 2 , 3 , …</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this way, the sequence <math alttext="\{\bm{x}_{n}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m5"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{n}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> again becomes an auto-regressive predictable sequence. When the input <math alttext="\bm{u}_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m6"><semantics><msub><mi>𝒖</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\bm{u}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> depends on the output <math alttext="\bm{x}_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m7"><semantics><msub><mi>𝒙</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\bm{x}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, we say the resulting sequence is produced by a “closed-loop” system (<a class="ltx_ref" href="#S2.E13" title="Equation 1.2.13 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.13</span></a>). As the closed-loop system no longer depends on any external input, we say such a system has become <span class="ltx_text ltx_font_italic">autonomous</span>. It can be viewed as a special case of auto-regression. For instance, if we choose in the above linear system (<a class="ltx_ref" href="#S2.E11" title="Equation 1.2.11 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.11</span></a>), <math alttext="\bm{u}_{n}=\bm{F}\bm{x}_{n}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m8"><semantics><mrow><msub><mi>𝒖</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\bm{u}_{n}=\bm{F}\bm{x}_{n}</annotation><annotation encoding="application/x-llamapun">bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = bold_italic_F bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, the closed-loop system becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{n+1}=\bm{A}\bm{x}_{n}+\bm{B}\bm{u}_{n}=\bm{A}\bm{x}_{n}+\bm{B}\bm{F}\bm{x}_{n}=(\bm{A}+\bm{B}\bm{F})\bm{x}_{n}," class="ltx_Math" display="block" id="S2.E14.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow><mo>+</mo><mrow><mi>𝑩</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒖</mi><mi>n</mi></msub></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow><mo>+</mo><mrow><mi>𝑩</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>𝑨</mi><mo>+</mo><mrow><mi>𝑩</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑭</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝒙</mi><mi>n</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{n+1}=\bm{A}\bm{x}_{n}+\bm{B}\bm{u}_{n}=\bm{A}\bm{x}_{n}+\bm{B}\bm{F}\bm{x}_{n}=(\bm{A}+\bm{B}\bm{F})\bm{x}_{n},</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_F bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = ( bold_italic_A + bold_italic_B bold_italic_F ) bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is a linear auto-regression.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Continuous Processes.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p1">
<p class="ltx_p">Predictable sequences have their natural counterparts in the continuous case. We may refer to them as predictable processes. Similar to the sequence of natural numbers, the simplest predictable continuous process is time itself <math alttext="x=t" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p1.m1"><semantics><mrow><mi>x</mi><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">x=t</annotation><annotation encoding="application/x-llamapun">italic_x = italic_t</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p2">
<p class="ltx_p">More generally, we say a process, denoted by <math alttext="\bm{x}(t)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m1"><semantics><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}(t)</annotation><annotation encoding="application/x-llamapun">bold_italic_x ( italic_t )</annotation></semantics></math>, is predictable if at any time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m2"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>, the value of the process at <math alttext="t+\delta t" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m3"><semantics><mrow><mi>t</mi><mo>+</mo><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">t+\delta t</annotation><annotation encoding="application/x-llamapun">italic_t + italic_δ italic_t</annotation></semantics></math>, where <math alttext="\delta t" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m4"><semantics><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\delta t</annotation><annotation encoding="application/x-llamapun">italic_δ italic_t</annotation></semantics></math> is an infinitesimal increment, is determined by its value at <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m5"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation><annotation encoding="application/x-llamapun">italic_t</annotation></semantics></math>. Typically, the change in value <math alttext="\delta\bm{x}(t)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m6"><semantics><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta\bm{x}(t)</annotation><annotation encoding="application/x-llamapun">italic_δ bold_italic_x ( italic_t )</annotation></semantics></math> is continuous and smooth. So <math alttext="\delta\bm{x}(t)=\bm{x}(t+\delta t)-\bm{x}(t)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p2.m7"><semantics><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\delta\bm{x}(t)=\bm{x}(t+\delta t)-\bm{x}(t)</annotation><annotation encoding="application/x-llamapun">italic_δ bold_italic_x ( italic_t ) = bold_italic_x ( italic_t + italic_δ italic_t ) - bold_italic_x ( italic_t )</annotation></semantics></math> is infinitesimally small. Predictable processes are typically described by (multivariate) differential equations:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\dot{\bm{x}}(t)=f(\bm{x}(t)),\quad\bm{x}\in\mathbb{R}^{d}." class="ltx_Math" display="block" id="S2.E15.m1"><semantics><mrow><mrow><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>˙</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\dot{\bm{x}}(t)=f(\bm{x}(t)),\quad\bm{x}\in\mathbb{R}^{d}.</annotation><annotation encoding="application/x-llamapun">over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) ) , bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.15)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px5.p3">
<p class="ltx_p">In the context of systems theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx35" title="">CD91</a>, <a class="ltx_ref" href="bib.html#bibx238" title="">Sas99</a>]</cite>, the above equation is also known as a state-space model. Similar to the discrete case, a controlled process can be given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\dot{\bm{x}}(t)=f(\bm{x}(t),\bm{u}(t)),\quad\bm{x}\in\mathbb{R}^{d},\bm{u}\in\mathbb{R}^{k}," class="ltx_Math" display="block" id="S2.E16.m1"><semantics><mrow><mrow><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>˙</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><mo>,</mo><mrow><mi>𝒖</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\dot{\bm{x}}(t)=f(\bm{x}(t),\bm{u}(t)),\quad\bm{x}\in\mathbb{R}^{d},\bm{u}\in\mathbb{R}^{k},</annotation><annotation encoding="application/x-llamapun">over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) , bold_italic_u ( italic_t ) ) , bold_italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , bold_italic_u ∈ blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{u}(t)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px5.p3.m1"><semantics><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{u}(t)</annotation><annotation encoding="application/x-llamapun">bold_italic_u ( italic_t )</annotation></semantics></math> is a computable input process.</p>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 1.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample1.p1">
<p class="ltx_p">For example in physics, Newton’s second law of motion describes how to predict the trajectory <math alttext="\bm{x}(t)\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="Thmexample1.p1.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\bm{x}(t)\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> of a moving object under a force input <math alttext="\bm{F}(t)\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="Thmexample1.p1.m2"><semantics><mrow><mrow><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\bm{F}(t)\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">bold_italic_F ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m\ddot{\bm{x}}(t)=\bm{F}(t)." class="ltx_Math" display="block" id="S2.E17.m1"><semantics><mrow><mrow><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒙</mi><mo>¨</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">m\ddot{\bm{x}}(t)=\bm{F}(t).</annotation><annotation encoding="application/x-llamapun">italic_m over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">When there is no force <math alttext="\bm{F}(t)\equiv 0" class="ltx_Math" display="inline" id="Thmexample1.p1.m3"><semantics><mrow><mrow><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≡</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\bm{F}(t)\equiv 0</annotation><annotation encoding="application/x-llamapun">bold_italic_F ( italic_t ) ≡ 0</annotation></semantics></math>, the above law reduces to a special case, known as Newton’s first law: the object maintains a constant speed in a straight line:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\ddot{\bm{x}}(t)=\bm{0}\;\Leftrightarrow\;\dot{\bm{x}}(t)=\bm{v}" class="ltx_Math" display="block" id="S2.E18.m1"><semantics><mrow><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>¨</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟎</mn></mrow><mo lspace="0.558em" rspace="0.558em" stretchy="false">⇔</mo><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>˙</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>𝒗</mi></mrow></mrow><annotation encoding="application/x-tex">\ddot{\bm{x}}(t)=\bm{0}\;\Leftrightarrow\;\dot{\bm{x}}(t)=\bm{v}</annotation><annotation encoding="application/x-llamapun">over¨ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_0 ⇔ over˙ start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_v</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some constant velocity vector <math alttext="\bm{v}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="Thmexample1.p1.m4"><semantics><mrow><mi>𝒗</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\bm{v}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun">bold_italic_v ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample1.p1.m5"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2.2 </span>Low Dimensionality</h3>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Learn to Predict.</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p">Now suppose you have observed or are given many sequence segments:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}" class="ltx_Math" display="block" id="S2.E19.m1"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}</annotation><annotation encoding="application/x-llamapun">{ italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">all from some predictable sequence <math alttext="\{x_{n}\}_{n=1}^{\infty}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><annotation encoding="application/x-tex">\{x_{n}\}_{n=1}^{\infty}</annotation><annotation encoding="application/x-llamapun">{ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT</annotation></semantics></math>. Without loss of generality, we may assume the length of each segment is <math alttext="D\gg d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m2"><semantics><mrow><mi>D</mi><mo>≫</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">D\gg d</annotation><annotation encoding="application/x-llamapun">italic_D ≫ italic_d</annotation></semantics></math>. So each segment is of the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="S_{i}=[x_{j(i)},x_{j(i)+1},\ldots,x_{j(i)+D-1}]^{\top}\in\mathbb{R}^{D}" class="ltx_Math" display="block" id="S2.E20.m1"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><msup><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mrow><mrow><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>D</mi></mrow><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">S_{i}=[x_{j(i)},x_{j(i)+1},\ldots,x_{j(i)+D-1}]^{\top}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) + 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) + italic_D - 1 end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some <math alttext="j\in\mathbb{N}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m3"><semantics><mrow><mi>j</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">j\in\mathbb{N}</annotation><annotation encoding="application/x-llamapun">italic_j ∈ blackboard_N</annotation></semantics></math>. Then you are given a new segment <math alttext="S_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m4"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_{t}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and are asked to predict its future values.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p">One difficulty here is that you normally do not know the function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and the degree <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> from which the sequence is generated:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+d}=f(x_{n+d-1},\ldots,x_{n})." class="ltx_Math" display="block" id="S2.E21.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mi>d</mi></mrow></msub><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mrow><mi>n</mi><mo>+</mo><mi>d</mi></mrow><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">x_{n+d}=f(x_{n+d-1},\ldots,x_{n}).</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">So the hope is somehow “to learn” <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> from the given sample segments <math alttext="S_{1},S_{2},\ldots,S_{N}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m5"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">S_{1},S_{2},\ldots,S_{N}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>. Hence the central task of learning to predict is:</p>
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_italic">Given many sampled segments of a predictable sequence, how to effectively and efficiently identify the function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m6"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Predictability and Low-Dimensionality.</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p">To identify the predictive function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, we may notice a common characteristic of segments of any predictable sequence, say given by (<a class="ltx_ref" href="#S2.E21" title="Equation 1.2.21 ‣ Learn to Predict. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.21</span></a>). If we take a long segment, say with a length <math alttext="D\gg d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m2"><semantics><mrow><mi>D</mi><mo>≫</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">D\gg d</annotation><annotation encoding="application/x-llamapun">italic_D ≫ italic_d</annotation></semantics></math>, of the sequence and view it as a vector:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}_{i}=[x_{i},x_{i+1},\ldots x_{i+D-1}]^{\top}\in\mathbb{R}^{D}." class="ltx_Math" display="block" id="S2.E22.m1"><semantics><mrow><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>=</mo><msup><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mrow><mi>i</mi><mo>+</mo><mi>D</mi></mrow><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">]</mo></mrow><mo>⊤</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}_{i}=[x_{i},x_{i+1},\ldots x_{i+D-1}]^{\top}\in\mathbb{R}^{D}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , … italic_x start_POSTSUBSCRIPT italic_i + italic_D - 1 end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Then the set of all such vectors <math alttext="\{\bm{x}_{i}\}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m3"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> are far from random and hence cannot possibly occupy the entire space of <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m4"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Instead, they essentially have at most <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m5"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> degrees of freedom – given the first <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m6"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> entries of any <math alttext="\bm{x}_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m7"><semantics><msub><mi>𝒙</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{x}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, values of the rest of the entries are uniquely determined. In other words, all <math alttext="\{\bm{x}_{i}\}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m8"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}_{i}\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> lie on a <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m9"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-dimensional surface. In mathematics, such a surface is often called a submanifold, denoted as <math alttext="\mathcal{S}\subset\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m10"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}\subset\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_S ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="F6"><img alt="Figure 1.6 : A two-dimensional subspace in a ten-dimensional ambient space." class="ltx_graphics" id="F6.g1" src="chapters/chapter1/figs/two-dimensional-plane-in-R10.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.6</span>: </span><span class="ltx_text" style="font-size:90%;">A two-dimensional subspace in a ten-dimensional ambient space.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p">In practice, if we choose the segment length <math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m1"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math> to be large enough, then all segments sampled from the same predicting function lie on a surface with an intrinsic dimension <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, significantly lower than that of the ambient space <math alttext="D" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m3"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>. For example, if the sequence is given by the following linear autoregression:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{n+2}=a\cdot x_{n+1}+b\cdot x_{n}," class="ltx_Math" display="block" id="S2.E23.m1"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>a</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>b</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">x_{n+2}=a\cdot x_{n+1}+b\cdot x_{n},</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ⋅ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_b ⋅ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for some constants <math alttext="a,b\in\mathbb{R}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m4"><semantics><mrow><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">a,b\in\mathbb{R}</annotation><annotation encoding="application/x-llamapun">italic_a , italic_b ∈ blackboard_R</annotation></semantics></math>. If we sample segments of length <math alttext="D=10" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m5"><semantics><mrow><mi>D</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">D=10</annotation><annotation encoding="application/x-llamapun">italic_D = 10</annotation></semantics></math> from such a sequence, then all samples lie on a two-dimensional plane or subspace in <math alttext="\mathbb{R}^{10}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m6"><semantics><msup><mi>ℝ</mi><mn>10</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{10}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math>, as illustrated in Figure <a class="ltx_ref" href="#F6" title="Figure 1.6 ‣ Predictability and Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.6</span></a>. If we can identify this two-dimensional subspace, the constants <math alttext="a" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m7"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation><annotation encoding="application/x-llamapun">italic_a</annotation></semantics></math> and <math alttext="b" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.m8"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation><annotation encoding="application/x-llamapun">italic_b</annotation></semantics></math> in (<a class="ltx_ref" href="#S2.E23" title="Equation 1.2.23 ‣ Predictability and Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.23</span></a>) can be fully determined.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p">It is easy to see that if the predicting function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is linear, such as the case with the linear systems given in (<a class="ltx_ref" href="#S2.E11" title="Equation 1.2.11 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.11</span></a>) and (<a class="ltx_ref" href="#S2.E14" title="Equation 1.2.14 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.14</span></a>), the long segments always lie on certain low-dimensional linear subspace. Identifying the predicting function is largely equivalent to identifying this low-dimensional subspace, a problem generally known as principal component analysis. We will discuss such classic models and methods in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p4">
<p class="ltx_p">As it turns out, this is largely true for general predictable sequences too: if one can identify the low-dimensional surface on which all the segment samples lie, then one can identify the associated predictive function <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p4.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Under mild conditions, there is a one-to-one mapping between the low-dimensional surface and the function <math alttext="f" class="ltx_Math" display="inline" id="footnote11.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>. This fact has been exploited in problems such as system identification which we will discuss later.</span></span></span> We cannot over-emphasize the importance of this property of segments from a predictable sequence: <span class="ltx_text ltx_font_italic">All samples of long segments of a predictable sequence lie on a low-dimensional submanifold.</span> As we will see in this book, all modern learning methods essentially exploit this property, implicitly or explicitly.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p5">
<p class="ltx_p">In real-world scenarios, the data we observe often do not come from a single predictable sequence. Typically they contain observations of multiple predictable sequences. For example, a video sequence can contain multiple moving objects. It is easy to see that in such scenarios, the data lie on a mixture of multiple low-dimensional linear subspaces or nonlinear submanifolds, as illustrated in <a class="ltx_ref" href="#F7" title="In Predictability and Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1.7</span></a>.</p>
</div>
<figure class="ltx_figure" id="F7"><img alt="Figure 1.7 : Data distributed on a mixture of (orthogonal) subspaces (left) or submanifolds (right)." class="ltx_graphics" id="F7.g1" src="chapters/chapter1/figs/mixture.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.7</span>: </span><span class="ltx_text" style="font-size:90%;">Data distributed on a mixture of (orthogonal) subspaces (left) or submanifolds (right).</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Properties of Low-Dimensionality.</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p">Of course, temporal correlation in predictable sequences is not the only reason why data are low-dimensional. For example, the space of all images is humongous but most of the space consists of images that resemble structureless random images as shown in <a class="ltx_ref" href="#F8" title="In Properties of Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1.8</span></a> left. Natural images and videos however are highly redundant because there is a strong spatial and temporal correlation among all pixel values. This is the reason why we can easily recognize whether an image is noisy or clean, as shown in <a class="ltx_ref" href="#F8" title="In Properties of Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1.8</span></a> middle and right. Hence the distribution of natural images has a very low intrinsic dimension (relative to the total number of pixels of an image).</p>
</div>
<figure class="ltx_figure" id="F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.8 : An image of random noise versus a noisy image and the original clean image." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F8.g1" src="chapters/chapter1/figs/Gaussian-noise.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.8 : An image of random noise versus a noisy image and the original clean image." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F8.g2" src="chapters/chapter1/figs/Standard-test-image-Barbara-of-size-512-512-pixels-including-Gaussian-noise-with.png" width="180"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.8 : An image of random noise versus a noisy image and the original clean image." class="ltx_graphics ltx_figure_panel ltx_img_square" height="180" id="F8.g3" src="chapters/chapter1/figs/barbara.jpg" width="180"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.8</span>: </span><span class="ltx_text" style="font-size:90%;">An image of random noise versus a noisy image and the original clean image. </span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p2">
<p class="ltx_p">Due to the importance and ubiquity of the task of learning low-dimensional structures, the book <span class="ltx_text ltx_font_italic">“High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications”</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> has stated in the beginning with a statement: “<span class="ltx_text ltx_font_italic">The problem of identifying the low-dimensional structure of signals or data in high-dimensional
spaces is one of the most fundamental problems that, through a long
history, interweaves many engineering and mathematical fields such as system
theory, signal processing, pattern recognition, machine learning, and statistics.</span>”</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p3">
<p class="ltx_p">Note that by enforcing the observed data point <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to be on a low-dimensional surface, we essentially have made the entries of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> very dependent on each other and in some sense have made the entries very “predictable” from values of other entries. For example, if we know that the data are constrained on a <math alttext="d" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.m3"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>-dimensional surface in <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.m4"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, then it allows us to do many useful things with the data besides prediction:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">completion</span>: in general, given more than <math alttext="d" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> entries of a typical sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>, the rest of its entries usually can be <span class="ltx_text ltx_font_italic">uniquely</span> determined.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Prediction becomes a special case of this property.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">denoising</span>: suppose entries of a sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> are perturbed by <span class="ltx_text ltx_font_italic">small</span> noises, they can be effectively removed by projecting <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> back onto the surface.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">error correction</span>: suppose a small number of <span class="ltx_text ltx_font_italic">unknown</span> entries of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> are arbitrarily corrupted, they can be effectively and efficiently corrected.</p>
</div>
</li>
</ul>
<p class="ltx_p">Figure <a class="ltx_ref" href="#F9" title="Figure 1.9 ‣ Properties of Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.9</span></a> illustrates these properties with a low-dimensional linear structure: a one-dimensional line in a two-dimensional plane.</p>
</div>
<figure class="ltx_figure" id="F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.9 : Illustration of properties of a low-dimensional (linear) structure: it enables completion (left), denoising (middle), and error correction (right)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="168" id="F9.g1" src="chapters/chapter1/figs/Completion-low-dim.png" width="205"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.9 : Illustration of properties of a low-dimensional (linear) structure: it enables completion (left), denoising (middle), and error correction (right)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="168" id="F9.g2" src="chapters/chapter1/figs/Denoising-low-dim.png" width="205"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Figure 1.9 : Illustration of properties of a low-dimensional (linear) structure: it enables completion (left), denoising (middle), and error correction (right)." class="ltx_graphics ltx_figure_panel ltx_img_square" height="192" id="F9.g3" src="chapters/chapter1/figs/Correction-low-dim.png" width="167"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.9</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of properties of a low-dimensional (linear) structure: it enables completion (left), denoising (middle), and error correction (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p4">
<p class="ltx_p">In fact, under mild conditions, the above properties are generalizable to many other low-dimensional structures in high-dimensional spaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>. Interestingly, as we will see in this book, these useful properties such as completion and denoising will inspire effective methods to learn such low-dimensional structures.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p5">
<p class="ltx_p">In the above, for simplicity, we have only used the deterministic case to introduce the important notion of predictability and low-dimensionality. Hence, the data (or sampled segments) precisely lie on some geometric structures such as subspaces or surfaces. In practice, as we have alluded to before, there is always a certain level of uncertainty or randomness in the data. In this case, we may assume that the data have a probability distribution, with a probability density function <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.m1"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>. We say that a distribution is “low-dimensional” if its density concentrates around a geometric structure that is rather low-dimensional, say a subspace, a surface or a mixture of them, as shown in Figure <a class="ltx_ref" href="#F7" title="Figure 1.7 ‣ Predictability and Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.7</span></a>. Notice that, from a practical point of view, such a density function <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math>, once learned, can serve as a very powerful prior for estimating <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> based on partial, noising, or corrupted observation, say:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}=f(\bm{x})+\bm{n}," class="ltx_Math" display="block" id="S2.E24.m1"><semantics><mrow><mrow><mi>𝒚</mi><mo>=</mo><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝒏</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{y}=f(\bm{x})+\bm{n},</annotation><annotation encoding="application/x-llamapun">bold_italic_y = italic_f ( bold_italic_x ) + bold_italic_n ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.2.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">by computing the conditional estimation <math alttext="\hat{\bm{x}}(\bm{y})=\mathbb{E}(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.m4"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}(\bm{y})=\mathbb{E}(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ( bold_italic_y ) = blackboard_E ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math> or through sampling the conditional distribution <math alttext="\hat{\bm{x}}(\bm{y})\sim p(\bm{x}\mid\bm{y})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.m5"><semantics><mrow><mrow><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>∣</mo><mi>𝒚</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}(\bm{y})\sim p(\bm{x}\mid\bm{y})</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG ( bold_italic_y ) ∼ italic_p ( bold_italic_x ∣ bold_italic_y )</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Modern generative AI technologies such as (conditioned) image generation very much rely on this fact, as we will elaborate on in Chapter <a class="ltx_ref" href="Ch6.html" title="Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6</span></a>.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p6">
<p class="ltx_p">Our discussions above have led to the main and only assumption on which this book will make for a deductive approach to understand intelligence and deep networks in particular:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">The Main Assumption:</span> <span class="ltx_text ltx_font_italic">Any intelligent systems or learning methods should and could rely on is that the world is predictable hence the distribution of the observed high-dimensional data samples have low-dimensional supports.</span></p>
</blockquote>
<p class="ltx_p">The remaining question is how to learn such low-dimensional structures correctly from the high-dimensional data, via effective and efficient computable means. As we will see in this book, parametric models that were well studied in classic analytical approaches and non-parametric models such as deep networks that are popular in modern practice are merely different means to achieve the same goal.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.3 </span>How to Learn?</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3.1 </span>Analytical Approaches</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable or amenable to efficient solutions is to make explicit assumptions about the family of low-dimensional structures we are dealing with. Historically, due to limited computation and data, simple and idealistic analytical models were always the first to be studied as they often offer efficient closed-form or numerical solutions. In addition, they can provide insights to the more general problems and they often already provide useful solutions to important though limited cases. In old days when computational resource was scarce, analytical models that permitted efficient closed-form or numerical solutions were the only cases that could be implemented. <span class="ltx_text ltx_font_italic">Linear structures</span> became the first classes of models to be thoroughly studied.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">For example, arguably the simplest case is to assume the data is distributed around a single low-dimensional subspace in a high-dimensional space. Or somewhat equivalently, one may assume the data is distributed according to an almost degenerate low-dimensional Gaussian. Identifying such a subspace or Gaussian from a finite number of (noisy) samples is then the classical problem of principal component analysis (PCA) and effective algorithms have been developed for this class of models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx124" title="">Jol02</a>]</cite>. One can make the family of models increasingly more complex and expressive. For instance, one may assume the data are distributed around a certain mixture of low-dimensional components (subspaces or low-dimensional Gaussians), as in the case of independent component analysis (ICA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx9" title="">BJC85</a>]</cite>, dictionary learning (DL), generalized principal component analysis (GPCA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx277" title="">VMS05</a>]</cite>, or the even more general class of sparse low-dimensional models that have been studied extensively in recent years in fields such as compressive sensing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">Around all these analytical model families, the central problem of study is always how to identify <span class="ltx_text ltx_font_italic">the most compact</span> model within each family that best fits the given data. Below, we give a very brief account of these classical analytical models but leave a more systematic study to Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>. In theory, these analytical models have provided us with tremendous insights into the geometric and statistical properties of low-dimensional structures. They often give us closed-form solutions or efficient and scalable algorithms which are very useful for data whose distributions can be well approximated by such models. More importantly, for more general problems, they provide us with a general sense of how easy or difficult the problem of identifying low-dimensional structures can be, and what the basic ideas are to approach such a problem.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Linear Dynamical Systems</h4>
<section class="ltx_paragraph" id="S3.SS1.SSSx1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Wiener Filter.</h5>
<div class="ltx_para" id="S3.SS1.SSSx1.Px1.p1">
<p class="ltx_p">As we have discussed before in Section <a class="ltx_ref" href="#S2.SS1" title="1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.1</span></a>, a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a <span class="ltx_text ltx_font_italic">linear time-invariant</span> (LTI) process:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x[n]=h[n]*z[n]+\epsilon[n]," class="ltx_Math" display="block" id="S3.E1.m1"><semantics><mrow><mrow><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">∗</mo><mi>z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">x[n]=h[n]*z[n]+\epsilon[n],</annotation><annotation encoding="application/x-llamapun">italic_x [ italic_n ] = italic_h [ italic_n ] ∗ italic_z [ italic_n ] + italic_ϵ [ italic_n ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="z" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m1"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation><annotation encoding="application/x-llamapun">italic_z</annotation></semantics></math> is the input and <math alttext="h" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m2"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> is the impulse response function.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Normally <math alttext="h" class="ltx_Math" display="inline" id="footnote14.m1"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation><annotation encoding="application/x-llamapun">italic_h</annotation></semantics></math> is assumed to have certain nice structures, say finite length or banded spectrum.</span></span></span> Here <math alttext="\epsilon[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m3"><semantics><mrow><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\epsilon[n]</annotation><annotation encoding="application/x-llamapun">italic_ϵ [ italic_n ]</annotation></semantics></math> is some additive noise in the observations. The problem is that given the input process <math alttext="\{z[n]\}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m4"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{z[n]\}</annotation><annotation encoding="application/x-llamapun">{ italic_z [ italic_n ] }</annotation></semantics></math> and observations of the output process <math alttext="\{x[n]\}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m5"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{x[n]\}</annotation><annotation encoding="application/x-llamapun">{ italic_x [ italic_n ] }</annotation></semantics></math>, how to find the optimal <math alttext="h[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m6"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">h[n]</annotation><annotation encoding="application/x-llamapun">italic_h [ italic_n ]</annotation></semantics></math> such that <math alttext="\hat{x}[n]=h[n]*z[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m7"><semantics><mrow><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">∗</mo><mi>z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{x}[n]=h[n]*z[n]</annotation><annotation encoding="application/x-llamapun">over^ start_ARG italic_x end_ARG [ italic_n ] = italic_h [ italic_n ] ∗ italic_z [ italic_n ]</annotation></semantics></math> predicts <math alttext="x[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m8"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">x[n]</annotation><annotation encoding="application/x-llamapun">italic_x [ italic_n ]</annotation></semantics></math> is an optimal way. In general, we measure the goodness of the prediction by the minimum mean squared error (MMSE):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{h}\mathbb{E}\big{[}\epsilon[n]^{2}\big{]}=\mathbb{E}\big{[}\|x[n]-h[n]*z[n]\|_{2}^{2}\big{]}." class="ltx_Math" display="block" id="S3.E2.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>h</mi></munder><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>ϵ</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><mn>2</mn></msup></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo rspace="0.055em" stretchy="false">]</mo></mrow></mrow><mo rspace="0.222em">∗</mo><mi>z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{h}\mathbb{E}\big{[}\epsilon[n]^{2}\big{]}=\mathbb{E}\big{[}\|x[n]-h[n]*z[n]\|_{2}^{2}\big{]}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT blackboard_E [ italic_ϵ [ italic_n ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E [ ∥ italic_x [ italic_n ] - italic_h [ italic_n ] ∗ italic_z [ italic_n ] ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The optimal solution <math alttext="h[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px1.p1.m9"><semantics><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">h[n]</annotation><annotation encoding="application/x-llamapun">italic_h [ italic_n ]</annotation></semantics></math> is referred to as a (denoising) filter. Norbert Wiener, the same person initiated the Cybernetics movement, studied this problem in 1940s and gave an elegant closed-form solution known as the <span class="ltx_text ltx_font_italic">Wiener filter</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx290" title="">Wie42</a>, <a class="ltx_ref" href="bib.html#bibx292" title="">Wie49</a>]</cite>. This became one of the most fundamental results in the field of Signal Processing.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Kalman Filter.</h5>
<div class="ltx_para" id="S3.SS1.SSSx1.Px2.p1">
<p class="ltx_p">The idea of denoising or filtering for a dynamic process was later extended to a linear time-invariant system described by a (finite-dimensional) state-space model by Rudolph Kalman in the 1960s:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}[n]=\bm{A}\bm{z}[n-1]+\bm{B}\bm{u}[n]+\bm{\epsilon}[n]." class="ltx_Math" display="block" id="S3.E3.m1"><semantics><mrow><mrow><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝑩</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}[n]=\bm{A}\bm{z}[n-1]+\bm{B}\bm{u}[n]+\bm{\epsilon}[n].</annotation><annotation encoding="application/x-llamapun">bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bold_italic_B bold_italic_u [ italic_n ] + bold_italic_ϵ [ italic_n ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The problem is how we can estimate the state of the system <math alttext="\bm{z}[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px2.p1.m1"><semantics><mrow><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}[n]</annotation><annotation encoding="application/x-llamapun">bold_italic_z [ italic_n ]</annotation></semantics></math> from noisy observations of the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}[n]=\bm{C}\bm{z}[n]+\bm{w}[n]," class="ltx_Math" display="block" id="S3.E4.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑪</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>+</mo><mrow><mi>𝒘</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}[n]=\bm{C}\bm{z}[n]+\bm{w}[n],</annotation><annotation encoding="application/x-llamapun">bold_italic_x [ italic_n ] = bold_italic_C bold_italic_z [ italic_n ] + bold_italic_w [ italic_n ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{w}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px2.p1.m2"><semantics><mi>𝒘</mi><annotation encoding="application/x-tex">\bm{w}</annotation><annotation encoding="application/x-llamapun">bold_italic_w</annotation></semantics></math> is some (white) noise. The optimal causal<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Which means the estimation can only use observations up to the current time step <math alttext="n" class="ltx_Math" display="inline" id="footnote15.m1"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation><annotation encoding="application/x-llamapun">italic_n</annotation></semantics></math>. Kalman filter is always causal whereas Wiener filter needs not to be.</span></span></span> state estimator that minimizes the MMSE-type prediction error</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\mathbb{E}\big{[}\|\bm{x}[n]-\bm{C}\bm{z}[n]\|_{2}^{2}\big{]}" class="ltx_Math" display="block" id="S3.E5.m1"><semantics><mrow><mrow><mi>min</mi><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mrow><mi>𝑪</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><annotation encoding="application/x-tex">\min\mathbb{E}\big{[}\|\bm{x}[n]-\bm{C}\bm{z}[n]\|_{2}^{2}\big{]}</annotation><annotation encoding="application/x-llamapun">roman_min blackboard_E [ ∥ bold_italic_x [ italic_n ] - bold_italic_C bold_italic_z [ italic_n ] ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">is given in closed-form by the so-called <span class="ltx_text ltx_font_italic">Kalman filter</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx130" title="">Kal60</a>]</cite>. This is one of the corner stones of modern Control Theory because it allows us to estimate the state of a dynamical system from its noisy observations. Then one can subsequently introduce a (linear) state feedback, say of the form <math alttext="\bm{u}[n]=\bm{F}\hat{\bm{z}}[n]" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px2.p1.m3"><semantics><mrow><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑭</mi><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{u}[n]=\bm{F}\hat{\bm{z}}[n]</annotation><annotation encoding="application/x-llamapun">bold_italic_u [ italic_n ] = bold_italic_F over^ start_ARG bold_italic_z end_ARG [ italic_n ]</annotation></semantics></math>, and make the closed-loop system fully autonomous, as we saw in equation (<a class="ltx_ref" href="#S2.E13" title="Equation 1.2.13 ‣ Controlled Prediction. ‣ 1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.13</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Identification of Linear Dynamical Systems.</h5>
<div class="ltx_para" id="S3.SS1.SSSx1.Px3.p1">
<p class="ltx_p">To derive the Kalman filter, the system parameters <math alttext="(\bm{A},\bm{B},\bm{C})" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p1.m1"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑨</mi><mo>,</mo><mi>𝑩</mi><mo>,</mo><mi>𝑪</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{A},\bm{B},\bm{C})</annotation><annotation encoding="application/x-llamapun">( bold_italic_A , bold_italic_B , bold_italic_C )</annotation></semantics></math> are assumed to be known. If they are not given in advance, it would be a more challenging problem known as <span class="ltx_text ltx_font_italic">system identification</span>: how to <span class="ltx_text ltx_font_italic">learn</span> the parameters <math alttext="(\bm{A},\bm{B},\bm{C})" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p1.m2"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑨</mi><mo>,</mo><mi>𝑩</mi><mo>,</mo><mi>𝑪</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{A},\bm{B},\bm{C})</annotation><annotation encoding="application/x-llamapun">( bold_italic_A , bold_italic_B , bold_italic_C )</annotation></semantics></math> from (many samples of) the input sequence <math alttext="\{\bm{u}[n]\}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p1.m3"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{u}[n]\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_u [ italic_n ] }</annotation></semantics></math> and observation sequence <math alttext="\{\bm{x}[n]\}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p1.m4"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{x}[n]\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_x [ italic_n ] }</annotation></semantics></math>. This is a classic problem in systems theory. If the system is linear, it can be shown that the input and output sequences <math alttext="\{\bm{u}[n],\bm{x}[n]\}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p1.m5"><semantics><mrow><mo stretchy="false">{</mo><mrow><mi>𝒖</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mi>𝒙</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{u}[n],\bm{x}[n]\}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_u [ italic_n ] , bold_italic_x [ italic_n ] }</annotation></semantics></math> would jointly lie on certain low-dimensional subspace<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>which has the same dimension as the order of the state-space model (<a class="ltx_ref" href="#S3.E3" title="Equation 1.3.3 ‣ Kalman Filter. ‣ Linear Dynamical Systems ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.3</span></a>). </span></span></span>. Hence the identification problem is essentially equivalent to identifying this low-dimensional subspace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx270" title="">VM96</a>, <a class="ltx_ref" href="bib.html#bibx167" title="">LV09</a>, <a class="ltx_ref" href="bib.html#bibx168" title="">LV10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx1.Px3.p2">
<p class="ltx_p">Note that the above problems have two things in common: first, the (noise-free) sequences or signals are assumed to be generated by an explicit family of parametric models; second, these models essentially are all linear. So conceptually, let <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p2.m1"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> be a random variable whose “true” distribution is supported on a low-dimensional linear subspace, say <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p2.m2"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math>. To a large extent, Wiener filter and Kalman filter all try to estimate such an <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p2.m3"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> from its noisy observations:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{x}_{o}+\bm{\epsilon},\quad\bm{x}_{o}\sim S," class="ltx_Math" display="block" id="S3.E6.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>o</mi></msub><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>𝒙</mi><mi>o</mi></msub><mo>∼</mo><mi>S</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{x}_{o}+\bm{\epsilon},\quad\bm{x}_{o}\sim S,</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_ϵ , bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∼ italic_S ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{\epsilon}" class="ltx_Math" display="inline" id="S3.SS1.SSSx1.Px3.p2.m4"><semantics><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><annotation encoding="application/x-tex">\bm{\epsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ϵ</annotation></semantics></math> is typically a random Gaussian noise (or process). Hence, essentially, their solutions all rely on identifying a low-dimensional linear subspace that best fits the observed (noisy) data. Then by projecting the data onto this subspace, one obtains the optimal denoising operations, all in closed form.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Linear and Mixed Linear Models</h4>
<section class="ltx_paragraph" id="S3.SS1.SSSx2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Principal Component Analysis.</h5>
<div class="ltx_para" id="S3.SS1.SSSx2.Px1.p1">
<p class="ltx_p">From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a <span class="ltx_text ltx_font_italic">single</span> low-dimensional linear subspace. Mathematically, we may model such a structure as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{u}_{1}z_{1}+\bm{u}_{2}z_{2}+\cdots+\bm{u}_{d}z_{d}+\bm{\epsilon}=\bm{U}\bm{z}+\bm{\epsilon},\quad\bm{U}\in\mathbb{R}^{D\times d}" class="ltx_Math" display="block" id="S3.E7.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><msub><mi>𝒖</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>𝒖</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mn>2</mn></msub></mrow><mo>+</mo><mi mathvariant="normal">⋯</mi><mo>+</mo><mrow><msub><mi>𝒖</mi><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mi>d</mi></msub></mrow><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{u}_{1}z_{1}+\bm{u}_{2}z_{2}+\cdots+\bm{u}_{d}z_{d}+\bm{\epsilon}=\bm{U}\bm{z}+\bm{\epsilon},\quad\bm{U}\in\mathbb{R}^{D\times d}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bold_italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ⋯ + bold_italic_u start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + bold_italic_ϵ = bold_italic_U bold_italic_z + bold_italic_ϵ , bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{\epsilon}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p1.m1"><semantics><mrow><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\epsilon}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">bold_italic_ϵ ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> is some small random noise. Figure <a class="ltx_ref" href="#F10" title="Figure 1.10 ‣ Principal Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.10</span></a> illustrates such a distribution with two principal components.</p>
</div>
<figure class="ltx_figure" id="F10"><img alt="Figure 1.10 : A distribution with two principal components." class="ltx_graphics ltx_img_landscape" height="225" id="F10.g1" src="chapters/chapter1/figs/PCA.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.10</span>: </span><span class="ltx_text" style="font-size:90%;">A distribution with two principal components.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSSx2.Px1.p2">
<p class="ltx_p">The problem is to find the subspace basis <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m1"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> from many samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. A typical approach to estimate the subspace <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is to minimize the variance of the noise, also known as the minimum mean square error (MMSE):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\bm{U}}\mathbb{E}\big{[}\|\bm{\epsilon}\|_{2}^{2}\big{]}=\mathbb{E}\big{[}\|\bm{x}-\bm{U}\bm{z}\|_{2}^{2}\big{]}." class="ltx_Math" display="block" id="S3.E8.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>𝑼</mi></munder><mo lspace="0.167em">⁡</mo><mi>𝔼</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><msubsup><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\bm{U}}\mathbb{E}\big{[}\|\bm{\epsilon}\|_{2}^{2}\big{]}=\mathbb{E}\big{[}\|\bm{x}-\bm{U}\bm{z}\|_{2}^{2}\big{]}.</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT bold_italic_U end_POSTSUBSCRIPT blackboard_E [ ∥ bold_italic_ϵ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E [ ∥ bold_italic_x - bold_italic_U bold_italic_z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notice that this is essentially a denoising task: once the basis <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m4"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> is correctly found, we can denoise the noisy sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> by projecting it onto the low-dimensional subspace spanned by <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m6"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\rightarrow\hat{\bm{x}}=\bm{U}\bm{U}^{\top}\bm{x}." class="ltx_Math" display="block" id="S3.E9.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo stretchy="false">→</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\rightarrow\hat{\bm{x}}=\bm{U}\bm{U}^{\top}\bm{x}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x → over^ start_ARG bold_italic_x end_ARG = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_italic_x .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If the noise is small and if we learned the correct low-dimensional subspace <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m7"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math>, we should expect <math alttext="\bm{x}\approx\hat{\bm{x}}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m8"><semantics><mrow><mi>𝒙</mi><mo>≈</mo><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\bm{x}\approx\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ≈ over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math>. That is, PCA is a special case of the auto-encoding:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054pt\bm{U}^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\bm{U}\hskip 5.69054pt}\hat{\bm{x}}." class="ltx_Math" display="block" id="S3.E10.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑼</mo></mover><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054pt\bm{U}^{\top}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\bm{U}\hskip 5.69054pt}\hat{\bm{x}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT bold_italic_U end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_x end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Only here because of the simple data structure, the encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m9"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> and decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p2.m10"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math> both become simple linear operators ((projecting and lifting).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px1.p3">
<p class="ltx_p">This is a classic problem in statistics known as the Principal Component Analysis (PCA). It was first studied by Pearson in 1901 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx212" title="">Pea01</a>]</cite> and later independently by Hotelling in 1933 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx110" title="">Hot33</a>]</cite>. This topic is systematically summarized in the classic book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx125" title="">Jol86</a>, <a class="ltx_ref" href="bib.html#bibx124" title="">Jol02</a>]</cite>.
In addition, one may explicitly assume the data <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is distributed according to a single low-dimensional Gaussian:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\sim\mathcal{N}(\bm{0},\bm{U}\bm{U}^{\top}+\sigma\bm{I}),\quad\bm{U}\in\mathbb{R}^{D\times d}," class="ltx_Math" display="block" id="S3.E11.m1"><semantics><mrow><mrow><mrow><mi>𝒙</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝑼</mi><mo>⊤</mo></msup></mrow><mo>+</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝑼</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}\sim\mathcal{N}(\bm{0},\bm{U}\bm{U}^{\top}+\sigma\bm{I}),\quad\bm{U}\in\mathbb{R}^{D\times d},</annotation><annotation encoding="application/x-llamapun">bold_italic_x ∼ caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT + italic_σ bold_italic_I ) , bold_italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is equivalent to assuming that <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px1.p3.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> in the above PCA model (<a class="ltx_ref" href="#S3.E7" title="Equation 1.3.7 ‣ Principal Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.7</span></a>) is a standard normal distribution.
This is known as Probabilistic PCA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx261" title="">TB99</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px1.p4">
<p class="ltx_p">In this book, we will revisit PCA in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>, from the perspective of learning a low-dimensional distribution. Our goal is to use this simple and idealistic model to convey some of the most fundamental ideas for learning a compact representation for a low-dimensional distribution, including the important notion of compression via denoising and autoencoding for a consistent representation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Independent Component Analysis.</h5>
<div class="ltx_para" id="S3.SS1.SSSx2.Px2.p1">
<p class="ltx_p">Independent component analysis (ICA) was originally proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx9" title="">BJC85</a>]</cite> as a classic model for <span class="ltx_text ltx_font_italic">learning a good representation</span>. In fact it was originally proposed as a simple mathematical model for our memory. The ICA model takes a deceivingly similar form as the above PCA model (<a class="ltx_ref" href="#S3.E7" title="Equation 1.3.7 ‣ Principal Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.7</span></a>) by assuming that the observed random variable <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is a linear superposition of multiple independent components <math alttext="z_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m2"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{u}_{1}z_{1}+\bm{u}_{2}z_{2}+\cdots+\bm{u}_{d}z_{d}+\bm{\epsilon}=\bm{U}\bm{z}+\bm{\epsilon}." class="ltx_Math" display="block" id="S3.E12.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><msub><mi>𝒖</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>𝒖</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mn>2</mn></msub></mrow><mo>+</mo><mi mathvariant="normal">⋯</mi><mo>+</mo><mrow><msub><mi>𝒖</mi><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mi>d</mi></msub></mrow><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow><mo>=</mo><mrow><mrow><mi>𝑼</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{u}_{1}z_{1}+\bm{u}_{2}z_{2}+\cdots+\bm{u}_{d}z_{d}+\bm{\epsilon}=\bm{U}\bm{z}+\bm{\epsilon}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bold_italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ⋯ + bold_italic_u start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + bold_italic_ϵ = bold_italic_U bold_italic_z + bold_italic_ϵ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, here the components <math alttext="z_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m3"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are assumed to be independent <span class="ltx_text ltx_font_italic">non-Gaussian</span> variables. For example, a popular choice is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{i}=\sigma_{i}\cdot w_{i},\quad\sigma_{i}\sim B(1,p)," class="ltx_Math" display="block" id="S3.E13.m1"><semantics><mrow><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>σ</mi><mi>i</mi></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>σ</mi><mi>i</mi></msub><mo>∼</mo><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">z_{i}=\sigma_{i}\cdot w_{i},\quad\sigma_{i}\sim B(1,p),</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ italic_B ( 1 , italic_p ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\sigma_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m4"><semantics><msub><mi>σ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\sigma_{i}</annotation><annotation encoding="application/x-llamapun">italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a Bernoulli random variable and <math alttext="w_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m5"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_{i}</annotation><annotation encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> could be a constant value or another random variable, say Gaussian.<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>Even if <math alttext="w" class="ltx_Math" display="inline" id="footnote17.m1"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation><annotation encoding="application/x-llamapun">italic_w</annotation></semantics></math> is Gaussian, <math alttext="\sigma w" class="ltx_Math" display="inline" id="footnote17.m2"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">\sigma w</annotation><annotation encoding="application/x-llamapun">italic_σ italic_w</annotation></semantics></math> is no longer a Gaussian variable!</span></span></span> The ICA problem is trying to identify both <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m6"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> from observed samples of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p1.m8"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. Figure <a class="ltx_ref" href="#F11" title="Figure 1.11 ‣ Independent Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.11</span></a> illustrates the difference between ICA and PCA.</p>
</div>
<figure class="ltx_figure" id="F11"><img alt="Figure 1.11 : PCA (left) versus ICA (right)." class="ltx_graphics ltx_img_landscape" height="199" id="F11.g1" src="chapters/chapter1/figs/PCA_ICA.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.11</span>: </span><span class="ltx_text" style="font-size:90%;">PCA (left) versus ICA (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSSx2.Px2.p2">
<p class="ltx_p">Although the (decoding) mapping from <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> to <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> seems linear and easy once <math alttext="\bm{U}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m3"><semantics><mi>𝑼</mi><annotation encoding="application/x-tex">\bm{U}</annotation><annotation encoding="application/x-llamapun">bold_italic_U</annotation></semantics></math> and <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> are learned, the (encoding) mapping from <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> to <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> can be complicated and may not be represented by a simple linear mapping. Hence ICA generally gives an autoencoding of the form:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\bm{U}\hskip 5.69054pt}\hat{\bm{x}}." class="ltx_Math" display="block" id="S3.E14.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒛</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑼</mo></mover><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{z}\xrightarrow{\hskip 5.69054pt\bm{U}\hskip 5.69054pt}\hat{\bm{x}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z start_ARROW start_OVERACCENT bold_italic_U end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_x end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Hence, unlike PCA, ICA is a little more difficult to analyze and solve. In 1990s, folks like Erkki Oja and Aapo Hyvärinen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx118" title="">HO97</a>, <a class="ltx_ref" href="bib.html#bibx120" title="">HO00a</a>]</cite> have made significant theoretical and algorithmic contributions to ICA. In Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>, we will study and give a solution to ICA from which the encoding mapping <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px2.p2.m7"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> will become clear.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Sparse Structures and Compressive Sensing.</h5>
<div class="ltx_para" id="S3.SS1.SSSx2.Px3.p1">
<p class="ltx_p">As one may see, if <math alttext="p" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p1.m1"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> in (<a class="ltx_ref" href="#S3.E13" title="Equation 1.3.13 ‣ Independent Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.13</span></a>) is very small, the probability that any of the components is non-zero is small. In this case, we say <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is sparsely generated and it concentrates on a set of linear subspaces of dimension <math alttext="k=p\cdot d" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p1.m3"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mi>p</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">k=p\cdot d</annotation><annotation encoding="application/x-llamapun">italic_k = italic_p ⋅ italic_d</annotation></semantics></math>. Hence, to some extent, we may extend the above ICA model to a more general family of low-dimensional structures known as sparse models.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px3.p2">
<p class="ltx_p">A <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m1"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-sparse model is defined to be consisting of the set of all <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m2"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_italic">-sparse vectors</span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{Z}=\{\bm{z}\in\mathbb{R}^{n}\mid\|\bm{z}\|_{0}\leq k\}," class="ltx_Math" display="block" id="S3.E15.m1"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>𝒛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><mo>≤</mo><mi>k</mi></mrow><mo stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{Z}=\{\bm{z}\in\mathbb{R}^{n}\mid\|\bm{z}\|_{0}\leq k\},</annotation><annotation encoding="application/x-llamapun">caligraphic_Z = { bold_italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∣ ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ≤ italic_k } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\|\cdot\|_{0}" class="ltx_math_unparsed" display="inline" id="S3.SS1.SSSx2.Px3.p2.m3"><semantics><mrow><mo rspace="0em">∥</mo><mo lspace="0em" rspace="0em">⋅</mo><msub><mo lspace="0em">∥</mo><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\cdot\|_{0}</annotation><annotation encoding="application/x-llamapun">∥ ⋅ ∥ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="\ell^{0}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m4"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>0</mn></msup><annotation encoding="application/x-tex">\ell^{0}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm that counts the number of non-zero entries in a vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m5"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. That is, <math alttext="\mathcal{Z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m6"><semantics><mi class="ltx_font_mathcaligraphic">𝒵</mi><annotation encoding="application/x-tex">\mathcal{Z}</annotation><annotation encoding="application/x-llamapun">caligraphic_Z</annotation></semantics></math> is the union of all <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m7"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-dimensional subspaces that align with the coordinate axes, as illustrated in Figure <a class="ltx_ref" href="#F7" title="Figure 1.7 ‣ Predictability and Low-Dimensionality. ‣ 1.2.2 Low Dimensionality ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.7</span></a> left. One important problem in classic signal processing and statistics is how to recover a sparse vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m8"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> from its linear observations:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{A}\bm{z}+\bm{\epsilon},\quad\bm{A}\in\mathbb{R}^{m\times n}" class="ltx_Math" display="block" id="S3.E16.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow><mo>+</mo><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{A}\bm{z}+\bm{\epsilon},\quad\bm{A}\in\mathbb{R}^{m\times n}</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_A bold_italic_z + bold_italic_ϵ , bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m9"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is given but typically <math alttext="m&lt;n" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m10"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m&lt;n</annotation><annotation encoding="application/x-llamapun">italic_m &lt; italic_n</annotation></semantics></math> and <math alttext="\bm{\epsilon}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p2.m11"><semantics><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><annotation encoding="application/x-tex">\bm{\epsilon}</annotation><annotation encoding="application/x-llamapun">bold_italic_ϵ</annotation></semantics></math> is some small noise. This seemingly benign problem turns out to be NP-hard to compute and it is even hard to approximate (see the book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite> for details).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px3.p3">
<p class="ltx_p">So despite a very rich and long history of study which can be dated back as early as the 18th century <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx25" title="">Bos50</a>]</cite>, there was no provably efficient algorithm to solve this class of problems, although many heuristic algorithms have been proposed and developed between the 1960s and the 1990s. Some are rather effective in practice but without any rigorous justification. A major breakthrough came in the early 2000s when a few renowned mathematicians including David Donoho, Emmanuel Candès, and Terence Tao <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx74" title="">Don05</a>, <a class="ltx_ref" href="bib.html#bibx38" title="">CT05a</a>, <a class="ltx_ref" href="bib.html#bibx37" title="">CT05</a>]</cite> established a rigorous theoretical framework that allows us to characterize precise conditions under which the sparse recovery problem can be solved effectively and efficient, say via a convex <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p3.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> minimization:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\|\bm{z}\|_{1}\quad\mbox{subject to}\quad\|\bm{x}-\bm{A}\bm{z}\|_{2}\leq\epsilon," class="ltx_Math" display="block" id="S3.E17.m1"><semantics><mrow><mrow><mrow><mrow><mi>min</mi><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mi>𝒛</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>𝒙</mi><mo>−</mo><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒛</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><mo>≤</mo><mi>ϵ</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min\|\bm{z}\|_{1}\quad\mbox{subject to}\quad\|\bm{x}-\bm{A}\bm{z}\|_{2}\leq\epsilon,</annotation><annotation encoding="application/x-llamapun">roman_min ∥ bold_italic_z ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT subject to ∥ bold_italic_x - bold_italic_A bold_italic_z ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_ϵ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\|\cdot\|_{1}" class="ltx_math_unparsed" display="inline" id="S3.SS1.SSSx2.Px3.p3.m2"><semantics><mrow><mo rspace="0em">∥</mo><mo lspace="0em" rspace="0em">⋅</mo><msub><mo lspace="0em">∥</mo><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\|\cdot\|_{1}</annotation><annotation encoding="application/x-llamapun">∥ ⋅ ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is the sparsity-promoting <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p3.m3"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> norm of a vector and <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p3.m4"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> is some small constant. Any solution to this problem essentially gives a sparse coding mapping:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{z}." class="ltx_Math" display="block" id="S3.E18.m1"><semantics><mrow><mrow><mi>𝒙</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒛</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{x}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{z}.</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We will give a brief account of such an algorithm, hence mapping, in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a> which will reveal interesting fundamental connections between sparse coding and deep learning.<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>Although similarities between algorithms for sparse coding and deep networks were noticed as early as in 2010 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx95" title="">GL10</a>]</cite>.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px3.p4">
<p class="ltx_p">As it turns out, conditions under which <math alttext="\ell^{1}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p4.m1"><semantics><msup><mi mathvariant="normal">ℓ</mi><mn>1</mn></msup><annotation encoding="application/x-tex">\ell^{1}</annotation><annotation encoding="application/x-llamapun">roman_ℓ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math> minimization succeeds are surprisingly general. The minimum number of measurements <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p4.m2"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation><annotation encoding="application/x-llamapun">italic_m</annotation></semantics></math> required for a successful recovery is only proportional to the intrinsic dimension of the data <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px3.p4.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>. This is now known as the <span class="ltx_text ltx_font_italic">compressive sensing</span> phenomenon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx36" title="">Can06</a>]</cite>. Moreover, this phenomenon is not unique to sparse structures. It also applies to very broad families of low-dimensional structures such as low-rank matrices etc. These results have fundamentally changed our understanding about recovering low-dimensional structures in high-dimensional spaces. This dramatic reverse of fortune with high-dimensional data analysis was even celebrated as the <span class="ltx_text ltx_font_italic">“blessing of dimensionality”</span> by David Donoho <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx61" title="">D ̵D00</a>]</cite> as opposed to the typical pessimistic belief in “curse of dimensionality” for high-dimensional problems. This coherent and complete body of results has been systematically organized in the book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx295" title="">WM22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px3.p5">
<p class="ltx_p">From a computational perspective, one cannot over-estimate the significance of this new framework. It has fundamentally changed our views about an important class of problems previously believed to be largely intractable. It has enabled us to develop extremely efficient algorithms that scale gracefully with the dimension of the problem, hence making the problem of sparse recovery from:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{{intractable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}." class="ltx_Math" display="block" id="S3.E19.m1"><semantics><mrow><mrow><mtext class="ltx_mathvariant_bold">intractable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">tractable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">scalable</mtext></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{{intractable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}.</annotation><annotation encoding="application/x-llamapun">intractable ⟹ tractable ⟹ scalable .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">These algorithms also come with rigorous theoretical guarantees of their correctness given precise requirements in data and computation. The rigorous and precise nature of this approach is almost opposite to that of practicing deep neural networks, which is largely empirical. Yet, despite their seemingly opposite styles and standards, we now know both approaches share a common goal: <span class="ltx_text ltx_font_italic">trying to pursue low-dimensional structures in high-dimensional spaces.</span></p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Dictionary Learning.</h5>
<div class="ltx_para" id="S3.SS1.SSSx2.Px4.p1">
<p class="ltx_p">Conceptually, an even harder problem than the sparse coding problem (<a class="ltx_ref" href="#S3.E16" title="Equation 1.3.16 ‣ Sparse Structures and Compressive Sensing. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.16</span></a>) is when the observation matrix <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m1"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> is not even known in advance and we need to learn <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m2"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> from a set of (possibly noisy) observations, say <math alttext="\bm{X}=[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{n}]" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m3"><semantics><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒙</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{X}=[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{n}]</annotation><annotation encoding="application/x-llamapun">bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}=\bm{A}\bm{Z}+\bm{E},\quad\bm{A}\in\mathbb{R}^{m\times n}." class="ltx_Math" display="block" id="S3.E20.m1"><semantics><mrow><mrow><mrow><mi>𝑿</mi><mo>=</mo><mrow><mrow><mi>𝑨</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒁</mi></mrow><mo>+</mo><mi>𝑬</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>𝑨</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}=\bm{A}\bm{Z}+\bm{E},\quad\bm{A}\in\mathbb{R}^{m\times n}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , bold_italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here we are given only <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> but not the corresponding <math alttext="\bm{Z}=[\bm{z}_{1},\bm{z}_{2},\ldots,\bm{z}_{n}]" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m5"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝒛</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒛</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝒛</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=[\bm{z}_{1},\bm{z}_{2},\ldots,\bm{z}_{n}]</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> nor the noise term <math alttext="\bm{E}=[\bm{\epsilon}_{1},\bm{\epsilon}_{2},\ldots,\bm{\epsilon}_{n}]" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m6"><semantics><mrow><mi>𝑬</mi><mo>=</mo><mrow><mo stretchy="false">[</mo><msub><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mn>1</mn></msub><mo>,</mo><msub><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi class="ltx_mathvariant_bold-italic" mathvariant="bold-italic">ϵ</mi><mi>n</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{E}=[\bm{\epsilon}_{1},\bm{\epsilon}_{2},\ldots,\bm{\epsilon}_{n}]</annotation><annotation encoding="application/x-llamapun">bold_italic_E = [ bold_italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_ϵ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, except for knowing <math alttext="\bm{z}_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m7"><semantics><msub><mi>𝒛</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{z}_{i}</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>’s are sparse. This is known as the <span class="ltx_text ltx_font_italic">dictionary learning</span> problem, which can be viewed as a generalization to the ICA problem (<a class="ltx_ref" href="#S3.E12" title="Equation 1.3.12 ‣ Independent Component Analysis. ‣ Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.12</span></a>) discussed earlier. In other words, given the distribution of the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m8"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to be the image of a standard sparse distribution <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m9"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> under a linear transform <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m10"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math>, we like to learn <math alttext="\bm{A}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m11"><semantics><mi>𝑨</mi><annotation encoding="application/x-tex">\bm{A}</annotation><annotation encoding="application/x-llamapun">bold_italic_A</annotation></semantics></math> and its “inverse” mapping <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.SSSx2.Px4.p1.m12"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> such that we obtain an autoencoder:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\bm{A}\hskip 5.69054pt}\bm{X}?" class="ltx_Math" display="block" id="S3.E21.m1"><semantics><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑨</mo></mover><mrow><mi>𝑿</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">?</mi></mrow></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\bm{A}\hskip 5.69054pt}\bm{X}?</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT bold_italic_A end_OVERACCENT → end_ARROW bold_italic_X ?</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.21)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.Px4.p2">
<p class="ltx_p">One can see what is in common with PCA, ICA, and Dictionary Learning is that they all assume that the distribution of the data is supported around low-dimensional linear or mixed linear structures. They all require to learn a (global or local) basis of the linear structures, from probably noisy samples of the distribution. In Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>, we will study how to identify low-dimensional structures through these classical models. In particular, we will see an interesting and important fact: all these low-dimensional (piecewise) linear models can be learned effectively and efficiently by the same type of fast algorithms, known as <span class="ltx_text ltx_font_italic">power iteration</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx317" title="">ZMZ+20</a>]</cite>. Although the above linear or mixed linear models are somewhat too simplistic or idealistic for most real-world data, understanding these models is an important first step toward understanding more general low-dimensional distributions.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">General Distributions</h4>
<div class="ltx_para" id="S3.SS1.SSSx3.p1">
<p class="ltx_p">The distributions of real-world data such as images, videos, and audio are too complex to be modeled by above, somewhat idealistic, linear models or Gaussian processes. We normally do not know <span class="ltx_text ltx_font_italic">a priori</span> they are generated from which family of parametric models.<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>Although in history there had been many attempts to develop analytical models for these data, such as random fields or stochastic processes for imagery data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx194" title="">MG99</a>]</cite>, as we have discussed in the previous section.</span></span></span> In practice, we typically only have many samples from their distributions – the empirical distributions. Obviously, in such cases, we normally cannot expect to have any closed-form solutions for their low-dimensional structures, nor for the resulting denoising operators.<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>Unlike the cases of PCA, Wiener filter, and Kalman filter.</span></span></span> So we need to develop a more general solution to these empirical distributions, not necessarily in closed form but at least efficiently computable. If we did this correctly, solutions to the aforementioned linear models should become their special cases.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSSx3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Denoising.</h5>
<div class="ltx_para" id="S3.SS1.SSSx3.Px1.p1">
<p class="ltx_p">In the 1950s, statisticians became interested in the problem of denoising data drawn from an arbitrary distribution. Let <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m1"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> be a random variable with probability density function <math alttext="p_{o}(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m2"><semantics><mrow><msub><mi>p</mi><mi>o</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{o}(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math>. So if we observe a noisy version of <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m3"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}=\bm{x}_{o}+\sigma\bm{g}," class="ltx_Math" display="block" id="S3.E22.m1"><semantics><mrow><mrow><mi>𝒙</mi><mo>=</mo><mrow><msub><mi>𝒙</mi><mi>o</mi></msub><mo>+</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝒈</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x}=\bm{x}_{o}+\sigma\bm{g},</annotation><annotation encoding="application/x-llamapun">bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_σ bold_italic_g ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.22)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m4"><semantics><mrow><mi>𝒈</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{g}\sim\operatorname{\mathcal{N}}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">bold_italic_g ∼ caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> is standard Gaussian noise and <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m5"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation><annotation encoding="application/x-llamapun">italic_σ</annotation></semantics></math> is the noise level of the observation. Let <math alttext="p(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m6"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p ( ⋅ )</annotation></semantics></math> be the probability density function of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>,<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span>That is, <math alttext="p(\bm{x})=\int_{-\infty}^{\infty}\varphi_{\sigma}(\bm{x}-\bm{z})p_{o}(\bm{z})\mathrm{d}\bm{z}" class="ltx_Math" display="inline" id="footnote21.m1"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∫</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><mi mathvariant="normal">∞</mi></msubsup><mrow><msub><mi>φ</mi><mi>σ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>−</mo><mi>𝒛</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>p</mi><mi>o</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒛</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})=\int_{-\infty}^{\infty}\varphi_{\sigma}(\bm{x}-\bm{z})p_{o}(\bm{z})\mathrm{d}\bm{z}</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x ) = ∫ start_POSTSUBSCRIPT - ∞ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT italic_φ start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT ( bold_italic_x - bold_italic_z ) italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( bold_italic_z ) roman_d bold_italic_z</annotation></semantics></math>, where <math alttext="\varphi_{\sigma}" class="ltx_Math" display="inline" id="footnote21.m2"><semantics><msub><mi>φ</mi><mi>σ</mi></msub><annotation encoding="application/x-tex">\varphi_{\sigma}</annotation><annotation encoding="application/x-llamapun">italic_φ start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT</annotation></semantics></math> is the density function of the Gaussian distribution <math alttext="\mathcal{N}(\bm{0},\sigma^{2}\bm{I})" class="ltx_Math" display="inline" id="footnote21.m3"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝑰</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\bm{0},\sigma^{2}\bm{I})</annotation><annotation encoding="application/x-llamapun">caligraphic_N ( bold_0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I )</annotation></semantics></math>.</span></span></span> Amazingly, the posterior expectation of <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m8"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m9"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> can be calculated by an elegant formula, known as the Tweedie’s formula <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx230" title="">Rob56</a>]</cite>:<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span>Herbert Robbins gave the credits of this formula to Maurice Kenneth Tweedie from their personal correspondence.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{x}}_{o}=\mathbb{E}[\bm{x}_{o}\mid\bm{x}]=\bm{x}+\sigma^{2}\nabla\log p(\bm{x})." class="ltx_Math" display="block" id="S3.E23.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>o</mi></msub><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>𝒙</mi><mi>o</mi></msub><mo>∣</mo><mi>𝒙</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>𝒙</mi><mo>+</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{x}}_{o}=\mathbb{E}[\bm{x}_{o}\mid\bm{x}]=\bm{x}+\sigma^{2}\nabla\log p(\bm{x}).</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∣ bold_italic_x ] = bold_italic_x + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∇ roman_log italic_p ( bold_italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.23)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As can be seen, from the formula, the function <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m10"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math> plays a very special role in denoising the observation <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> here. The noise <math alttext="\bm{g}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m12"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">\bm{g}</annotation><annotation encoding="application/x-llamapun">bold_italic_g</annotation></semantics></math> can be explicitly estimated as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\bm{g}}=\frac{\bm{x}-\hat{\bm{x}}_{o}}{\sigma}=-\sigma\nabla\log p(\bm{x})," class="ltx_Math" display="block" id="S3.E24.m1"><semantics><mrow><mrow><mover accent="true"><mi>𝒈</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>𝒙</mi><mo>−</mo><msub><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><mi>o</mi></msub></mrow><mi>σ</mi></mfrac><mo>=</mo><mrow><mo>−</mo><mrow><mi>σ</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{\bm{g}}=\frac{\bm{x}-\hat{\bm{x}}_{o}}{\sigma}=-\sigma\nabla\log p(\bm{x}),</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_g end_ARG = divide start_ARG bold_italic_x - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_ARG start_ARG italic_σ end_ARG = - italic_σ ∇ roman_log italic_p ( bold_italic_x ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for which we only need to know the distribution <math alttext="p(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m13"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p ( ⋅ )</annotation></semantics></math> of <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m14"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> not the ground truth <math alttext="p_{o}(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m15"><semantics><mrow><msub><mi>p</mi><mi>o</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{o}(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> for <math alttext="\bm{x}_{o}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m16"><semantics><msub><mi>𝒙</mi><mi>o</mi></msub><annotation encoding="application/x-tex">\bm{x}_{o}</annotation><annotation encoding="application/x-llamapun">bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>. An important implication of this result is that if we add Gaussian noise to any distribution, the denoising process can be done easily if we can somehow get hold of the function <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p1.m17"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx3.Px1.p2">
<p class="ltx_p">Because this is such an important and useful result, it has been rediscovered and used in many different contexts and areas. For example, after Tweedie’s formula <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx230" title="">Rob56</a>]</cite>, it was rediscovered a few years later by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx188" title="">Miy61</a>]</cite>. In the early 2000s, the function <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px1.p2.m1"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math> was rediscovered again in the context of learning a general distribution and was named the “score function” by Aapo Hyvärinen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx117" title="">Hyv05</a>]</cite>. But its connection to (empirical Bayesian) denoising was soon recognized by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx278" title="">Vin11</a>]</cite>.
Generalizations to other measurement distributions (beyond Gaussian noise) have been made by Eero Simoncelli’s group <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx225" title="">RS11</a>]</cite>, and later applied to image denoising <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx129" title="">KS21</a>, <a class="ltx_ref" href="bib.html#bibx106" title="">HJA20</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="F12"><img alt="Figure 1.12 : Geometric interpretation of a score function ∇ log ⁡ p ​ ( 𝒙 ) \nabla\log p(\bm{x}) ∇ roman_log italic_p ( bold_italic_x ) for a distribution with density p ​ ( 𝒙 ) p(\bm{x}) italic_p ( bold_italic_x ) on the left. The operation generated by the score function pushes the distribution towards areas of higher density. The goal is that, by a certain measure of compactness (e.g. entropy or coding length), the resulting distribution is more “compressed”. Eventually, the distribution converges onto one that is with a lower-dimensional support, as p ∗ ​ ( 𝒙 ) p^{*}(\bm{x}) italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x ) shown on the right." class="ltx_graphics ltx_img_landscape" height="128" id="F12.g1" src="chapters/chapter1/figs/Density-compress.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.12</span>: </span><span class="ltx_text" style="font-size:90%;">Geometric interpretation of a score function <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="F12.m4"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math> for a distribution with density <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="F12.m5"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> on the left. The operation generated by the score function pushes the distribution towards areas of higher density. The goal is that, by a certain measure of compactness (e.g. entropy or coding length), the resulting distribution is more “compressed”. Eventually, the distribution converges onto one that is with a lower-dimensional support, as <math alttext="p^{*}(\bm{x})" class="ltx_Math" display="inline" id="F12.m6"><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x )</annotation></semantics></math> shown on the right.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSSx3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Entropy minimization.</h5>
<div class="ltx_para" id="S3.SS1.SSSx3.Px2.p1">
<p class="ltx_p">In fact, this function has a very intuitive information-theoretic and geometric interpretation. Note that in information theory <math alttext="-\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m1"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">-\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">- roman_log italic_p ( bold_italic_x )</annotation></semantics></math> generally corresponds to the number of bits needed to encode <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>at least in the case of a discrete variable, as we will explain in more details in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>.</span></span></span>. The gradient <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m3"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math> points to a direction in which the density is higher, as shown in Figure <a class="ltx_ref" href="#F12" title="Figure 1.12 ‣ Denoising. ‣ General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.12</span></a> left. The number of bits needed to encode <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> decreases if it moves in that direction. Hence, the overall effect of the operator <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m5"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math> is to push the distribution to “shrink” towards areas of higher density. Actually, one can formally show that the (differential) entropy of the distribution</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(\bm{x})=-\int p(\bm{w})\log p(\bm{w})\mathrm{d}\bm{w}" class="ltx_Math" display="block" id="S3.E25.m1"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒘</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{x})=-\int p(\bm{w})\log p(\bm{w})\mathrm{d}\bm{w}</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_x ) = - ∫ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w ) roman_d bold_italic_w</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">indeed decreases under such an operation (see Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a> and Appendix <a class="ltx_ref" href="A2.html" title="Appendix B Entropy, Diffusion, Denoising, and Lossy Coding ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">B</span></a>). Therefore, if we encode it with an optimal code book, the overall coding length/rate of the resulting distribution is reduced, hence more “compressed.”. Intuitively, one can imagine that, if we repeat such a denoising process indefinitely, the distribution will eventually shrunk to one whose mass is concentrated on a support of lower dimension. For example, the distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m6"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> shown on the left of Figure <a class="ltx_ref" href="#F12" title="Figure 1.12 ‣ Denoising. ‣ General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.12</span></a>, under the action of the score function <math alttext="\nabla\log p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m7"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla\log p(\bm{x})</annotation><annotation encoding="application/x-llamapun">∇ roman_log italic_p ( bold_italic_x )</annotation></semantics></math>, eventually will converge to the distribution <math alttext="p^{*}(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m8"><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x )</annotation></semantics></math> on the right<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>Strictly speaking, <math alttext="p^{*}(\bm{x})" class="ltx_Math" display="inline" id="footnote24.m1"><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x )</annotation></semantics></math> is a distribution whose density is a generalized function: <math alttext="p^{*}(\bm{x})=p^{*}(\bm{x}_{1})\delta(\bm{x}-\bm{x}_{1})+p^{*}(\bm{x}_{2})\delta(\bm{x}-\bm{x}_{2})" class="ltx_Math" display="inline" id="footnote24.m2"><semantics><mrow><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝒙</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒙</mi><mo>−</mo><msub><mi>𝒙</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x})=p^{*}(\bm{x}_{1})\delta(\bm{x}-\bm{x}_{1})+p^{*}(\bm{x}_{2})\delta(\bm{x}-\bm{x}_{2})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x ) = italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_δ ( bold_italic_x - bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_δ ( bold_italic_x - bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math>, with <math alttext="p^{*}(\bm{x}_{1})+p^{*}(\bm{x}_{2})=1" class="ltx_Math" display="inline" id="footnote24.m3"><semantics><mrow><mrow><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x}_{1})+p^{*}(\bm{x}_{2})=1</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = 1</annotation></semantics></math>. </span></span></span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(\bm{x})=-\int p(\bm{w})\log p(\bm{w})\mathrm{d}\bm{w}\quad\xrightarrow{\hskip 2.84526pt\mbox{decreasing}\hskip 2.84526pt}\quad H^{*}(\bm{x})=-\int p^{*}(\bm{w})\log p^{*}(\bm{w})\mathrm{d}\bm{w}." class="ltx_Math" display="block" id="S3.E26.m1"><semantics><mrow><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒘</mi></mrow></mrow></mrow></mrow><mspace width="1em"></mspace><mover accent="true"><mo stretchy="false">→</mo><mtext>decreasing</mtext></mover></mrow></mrow><mspace width="1em"></mspace><mrow><mrow><msup><mi>H</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msup><mi>p</mi><mo>∗</mo></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒘</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>𝒘</mi></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(\bm{x})=-\int p(\bm{w})\log p(\bm{w})\mathrm{d}\bm{w}\quad\xrightarrow{\hskip 2.84526pt\mbox{decreasing}\hskip 2.84526pt}\quad H^{*}(\bm{x})=-\int p^{*}(\bm{w})\log p^{*}(\bm{w})\mathrm{d}\bm{w}.</annotation><annotation encoding="application/x-llamapun">italic_H ( bold_italic_x ) = - ∫ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w ) roman_d bold_italic_w start_ARROW start_OVERACCENT decreasing end_OVERACCENT → end_ARROW italic_H start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x ) = - ∫ italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_w ) roman_log italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_w ) roman_d bold_italic_w .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Strictly speaking, as the distribution converges to <math alttext="p^{*}(\bm{x})" class="ltx_Math" display="inline" id="S3.SS1.SSSx3.Px2.p1.m9"><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^{*}(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_italic_x )</annotation></semantics></math>, it differential entropy converges to negative infinity. This is due to a technical difference between the definition of differential entropy for continuous random variable and discrete random variable, respectively. We will see how to resolve this technical difficulty in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, using a more uniform measure of <span class="ltx_text ltx_font_italic">rate distortion</span>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx3.Px2.p2">
<p class="ltx_p">We will discuss later in this chapter and in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, how such a seemingly simple concept of denoising and compression leads to a very unifying and powerful method for learning general low-dimensional distributions in a high-dimensional space, including the distribution of natural images.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3.2 </span>Empirical Approaches</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">In practice, for many important real-world data such as images, sounds, and texts, it is difficult to model them with idealistic linear or mixed linear models. For example, there has been a long and rich history in the fields of image processing and computer vision that tries to model the distribution of natural images analytically. David Mumford, a Fields Medalist, spent considerable effort in the 1990s trying to understand and model the statistics of natural images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx193" title="">Mum96</a>]</cite>. He and his students, including Song-Chun Zhu, drew inspiration and techniques from statistical physics and proposed many statistical and stochastic models for the distribution of natural images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx325" title="">ZWM97</a>, <a class="ltx_ref" href="bib.html#bibx326" title="">ZM97a</a>, <a class="ltx_ref" href="bib.html#bibx324" title="">ZM97</a>, <a class="ltx_ref" href="bib.html#bibx112" title="">HM99</a>, <a class="ltx_ref" href="bib.html#bibx194" title="">MG99</a>, <a class="ltx_ref" href="bib.html#bibx152" title="">LPM03</a>]</cite>. However, these analytical models met with limited success in producing samples that closely resemble natural images. Obviously, for real-world data like images, we need to develop more powerful and unifying methods to pursue their more general low-dimensional structures.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">Hence, historically, many empirical models have been proposed to model important real-world data, including images and texts. These models often drew inspiration from the characteristics of the biological nerve system because the brain of an animal or human seems to process these data extremely efficiently and effectively.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Classic Artificial Neural Networks</h4>
<section class="ltx_paragraph" id="S3.SS2.SSSx1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Artificial neuron.</h5>
<figure class="ltx_figure" id="F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="183" id="F13.g1" src="chapters/chapter1/figs/neuron.png" width="329"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="165" id="F13.g2" src="chapters/chapter1/figs/Artificial_neuron.png" width="240"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.13</span>: </span><span class="ltx_text" style="font-size:90%;">The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSSx1.Px1.p1">
<p class="ltx_p">Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span>known as the Linear Threshold Unit, or a perceptron.</span></span></span> was proposed by Warren McCulloch<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span>A professor of psychiatry at the University of Chicago at the time</span></span></span> and Walter Pitts in 1943 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx181" title="">MP43</a>]</cite>. It describes the relationship between the input <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px1.p1.m1"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and output <math alttext="o_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px1.p1.m2"><semantics><msub><mi>o</mi><mi>j</mi></msub><annotation encoding="application/x-tex">o_{j}</annotation><annotation encoding="application/x-llamapun">italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="o_{j}=\varphi\Big{(}\sum_{i}w_{ji}x_{i}\Big{)}," class="ltx_Math" display="block" id="S3.E27.m1"><semantics><mrow><mrow><msub><mi>o</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><munder><mo lspace="0em" movablelimits="false">∑</mo><mi>i</mi></munder><mrow><msub><mi>w</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">o_{j}=\varphi\Big{(}\sum_{i}w_{ji}x_{i}\Big{)},</annotation><annotation encoding="application/x-llamapun">italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_φ ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\varphi(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px1.p1.m3"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_φ ( ⋅ )</annotation></semantics></math> is some nonlinear activation, normally modeled by a threshold function. This model is illustrated in Figure <a class="ltx_ref" href="#F13" title="Figure 1.13 ‣ Artificial neuron. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.13</span></a>. As we can see, this form already shares the main characteristics of a basic unit in modern deep neural networks. The model is derived from observations of how a single neuron works in our nerve system. However, people did not know exactly what functions a collection of such neurons wants to realize and perform. On a more technical level, neither were they sure what nonlinear activation function <math alttext="\varphi(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px1.p1.m4"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_φ ( ⋅ )</annotation></semantics></math> should be used. Hence, historically many variants have been proposed.<span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span>Step function, Hard or soft thresholding, Rectified Linear Unit (ReLU), sigmoid, etc.
</span></span></span></p>
</div>
<figure class="ltx_figure" id="F14"><img alt="Figure 1.14 : A network with one single hidden layer (left) versus a deep network (right)." class="ltx_graphics ltx_img_landscape" height="195" id="F14.g1" src="chapters/chapter1/figs/single-deep.png" width="509"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.14</span>: </span><span class="ltx_text" style="font-size:90%;">A network with one single hidden layer (left) versus a deep network (right). </span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Artificial neural network.</h5>
<div class="ltx_para" id="S3.SS2.SSSx1.Px2.p1">
<p class="ltx_p">In the 1950s, Frank Rosenblatt was the first to build a machine with a <span class="ltx_text ltx_font_italic">network</span> of such artificial neurons, shown in Figure <a class="ltx_ref" href="#F15" title="Figure 1.15 ‣ Artificial neural network. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.15</span></a>. The machine is called Mark I Perceptron which consists of an input layer, an output layer, and a single hidden layer consisting of 512 artificial neurons, as shown in Figure <a class="ltx_ref" href="#F15" title="Figure 1.15 ‣ Artificial neural network. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.15</span></a> left, which is similar to what is illustrated in Figure <a class="ltx_ref" href="#F14" title="Figure 1.14 ‣ Artificial neuron. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.14</span></a> left. It was designed to classify optical images of letters. However, the capacity of a single-layer network is limited and can only learn linearly separable patterns. In a 1969 book <span class="ltx_text ltx_font_italic">Perceptrons: An Introduction to Computational Geometry</span> by Marvin Minsky and Seymour Papert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx186" title="">MP69</a>]</cite>, it was shown that the single-layer architecture of Mark I Perceptron cannot learn an XOR function. This result had significantly damped people’s interest in artificial neural networks, even though it was later proven that a multi-layer network is able to learn an XOR function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx235" title="">RHW86</a>]</cite>. In fact, a (big enough) multi-layer network, as shown in Figure <a class="ltx_ref" href="#F14" title="Figure 1.14 ‣ Artificial neuron. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.14</span></a> right, consisting of such simple neurons can simulate any finite-state machine, even the universal Turing machine.<span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span>Do not confuse what neural networks are capable of doing in principle with whether it is tractable or easy to learn a neural network that realizes certain desired functions.</span></span></span> Nevertheless, subsequently, the study of artificial neural networks went into its first winter in the 1970s.</p>
</div>
<figure class="ltx_figure" id="F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.15 : The Mark I Perceptron machine developed by Frank Rosenblatt in late 1950s." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="203" id="F15.g1" src="chapters/chapter1/figs/visu-large.jpg" width="269"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Figure 1.15 : The Mark I Perceptron machine developed by Frank Rosenblatt in late 1950s." class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="207" id="F15.g2" src="chapters/chapter1/figs/Original-Mark-I-perceptron-as-seen-in-its-operators-manual-20.ppm.png" width="269"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.15</span>: </span><span class="ltx_text" style="font-size:90%;">The Mark I Perceptron machine developed by Frank Rosenblatt in late 1950s.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Convolutional neural networks.</h5>
<div class="ltx_para" id="S3.SS2.SSSx1.Px3.p1">
<p class="ltx_p">The somewhat disappointing early experimentation with artificial neural networks like Mark I Perceptron in the 50s and 60s suggested that it might not be enough to simply connect neurons in a general fashion as multi-layer perceptrons (MLP). In order to build effective and efficient networks, we need to understand what purpose or function neurons in a network need to achieve collectively so that they should be organized and learned in a certain special way. Once again, the study of machine intelligence turned to draw inspiration from how the animal’s nerve system works.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.Px3.p2">
<p class="ltx_p">It is known that most of our brain is dedicated to process visual information. In 1950s and 1960s, David Hubel and Torsten Wiesel systematically studied the visual cortices of cats. It was discovered that the visual cortex contains different types of cells (known as simple cells and complex cells), which are sensitive to visual stimuli of different orientations and locations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx113" title="">HW59</a>]</cite>. Hubel and Wiesel won the 1981 Nobel Prize in Physiology or Medicine for their ground-breaking discovery.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.Px3.p3">
<p class="ltx_p">On the artificial neural network side, Hubel and Wiesel’s work had inspired Kunihiko Fukushima who designed the “neocognitron” network in 1980 which consists of artificial neurons that emulate biological neurons in the visual cortices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx88" title="">Fuk80</a>]</cite>. This is known as the first <span class="ltx_text ltx_font_italic">convolutional neural network</span> (CNN), and its architecture is illustrated in Figure <a class="ltx_ref" href="#F16" title="Figure 1.16 ‣ Convolutional neural networks. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.16</span></a>. Unlike the perceptron, the neocognitron had more than one hidden layer and could be viewed as a deep network, as compared in Figure <a class="ltx_ref" href="#F14" title="Figure 1.14 ‣ Artificial neuron. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.14</span></a> right.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.Px3.p4">
<p class="ltx_p">Also inspired by how neurons work in the cat’s visual cortex, he was also the first to introduce the use of <span class="ltx_text ltx_font_italic">rectified linear unit</span> (ReLU):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E28">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\varphi(x)=\max\{0,x\}=\begin{cases}x,&amp;\text{if}\,x&gt;0,\\
0,\quad&amp;\text{if}\,x\leq 0,\end{cases}" class="ltx_Math" display="block" id="S3.E28.m1"><semantics><mrow><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">}</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mi>x</mi><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext>if</mtext><mo lspace="0.170em" rspace="0em">​</mo><mi>x</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext>if</mtext><mo lspace="0.170em" rspace="0em">​</mo><mi>x</mi></mrow><mo>≤</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\varphi(x)=\max\{0,x\}=\begin{cases}x,&amp;\text{if}\,x&gt;0,\\
0,\quad&amp;\text{if}\,x\leq 0,\end{cases}</annotation><annotation encoding="application/x-llamapun">italic_φ ( italic_x ) = roman_max { 0 , italic_x } = { start_ROW start_CELL italic_x , end_CELL start_CELL if italic_x &gt; 0 , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL if italic_x ≤ 0 , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.28)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for the activation function <math alttext="\varphi(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px3.p4.m1"><semantics><mrow><mi>φ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_φ ( ⋅ )</annotation></semantics></math> in 1969 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx87" title="">Fuk69</a>]</cite>. But not until recent years has the ReLU become widely used activation function in modern deep (convolutional) neural networks. We will learn from this book why this is a good choice once we explain the main operations that deep networks try to implement: compression.</p>
</div>
<figure class="ltx_figure" id="F16"><img alt="Figure 1.16 : Origin of convolutional neural networks: the Neocognitron by Kunihiko Fukushim in 1980. Notice that the interleaving layers of convolutions and pooling try to emulate the functions of simple cells and complex cells discovered in the visual cortices of cats." class="ltx_graphics ltx_img_landscape" height="225" id="F16.g1" src="chapters/chapter1/figs/neocognitron.png" width="359"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.16</span>: </span><span class="ltx_text" style="font-size:90%;">Origin of convolutional neural networks: the Neocognitron by Kunihiko Fukushim in 1980. Notice that the interleaving layers of convolutions and pooling try to emulate the functions of simple cells and complex cells discovered in the visual cortices of cats.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSSx1.Px3.p5">
<p class="ltx_p">CNN-type networks continued to evolve in the 1980s and many different variants had been introduced and studied. However, despite the remarkable capacities of deep networks and the improved architectures inspired by neuroscience, it remained extremely difficult to train such deep networks for a real task such as image classification. How to get a network to work depended on many unexplainable heuristics and tricks that really limited the appeal and applicability of neural networks. One major breakthrough came around 1989 when Yann LeCun successfully used <span class="ltx_text ltx_font_italic">back propagation</span> (BP) to learn a deep convolutional neural network for recognizing hand-written digits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx149" title="">LBD+89</a>]</cite>, later known as the LeNet (see Figure <a class="ltx_ref" href="#F17" title="Figure 1.17 ‣ Convolutional neural networks. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.17</span></a>). After several years’ persistent development, his perseverance paid off: performance of the LeNet eventually became good enough for practical usage in late 1990s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx150" title="">LBB+98</a>]</cite>: it was used by the US Post Office for recognizing handwritten digits (for zip codes). The LeNet was considered as the “prototype” network for all modern deep neural networks, such as the AlexNet and ResNet, which we will discuss later. Due to this work, Yann LeCun was awarded the 2018 Turing Award.<span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span>Together with two other pioneers of deep networks, Yoshua Bengio and Geoffrey Hinton.</span></span></span></p>
</div>
<figure class="ltx_figure" id="F17"><img alt="Figure 1.17 : The LeNet-5 convolution neural network designed by Yann LeCun in 1989." class="ltx_graphics ltx_img_landscape" height="159" id="F17.g1" src="chapters/chapter1/figs/LeNet-5.png" width="568"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.17</span>: </span><span class="ltx_text" style="font-size:90%;">The LeNet-5 convolution neural network designed by Yann LeCun in 1989. </span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Backpropagation.</h5>
<div class="ltx_para" id="S3.SS2.SSSx1.Px4.p1">
<p class="ltx_p">In history, the fate of deep neural networks seems to be tied closely to how they can be trained easily and efficiently. Back propagation (BP) was introduced for this reason. We know that a multiple layer perceptron can be expressed as a composition of a sequence of linear mappings and nonlinear activations as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(\bm{W}_{1},\ldots,\bm{W}_{L})=f^{L}(\bm{W}_{L}f^{L-1}(\bm{W}_{L-1}\cdots f^{2}(\bm{W}_{2}f^{1}(\bm{W}_{1}\bm{x}))))." class="ltx_Math" display="block" id="S3.E29.m1"><semantics><mrow><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝑾</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝑾</mi><mi>L</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>f</mi><mi>L</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑾</mi><mi>L</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>f</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑾</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">⋯</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>f</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑾</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>f</mi><mn>1</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝑾</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">h(\bm{W}_{1},\ldots,\bm{W}_{L})=f^{L}(\bm{W}_{L}f^{L-1}(\bm{W}_{L-1}\cdots f^{2}(\bm{W}_{2}f^{1}(\bm{W}_{1}\bm{x})))).</annotation><annotation encoding="application/x-llamapun">italic_h ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) = italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT italic_L - 1 end_POSTSUBSCRIPT ⋯ italic_f start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_x ) ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In order to train the network weights <math alttext="\{\bm{W}_{l}\}_{l=1}^{L}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px4.p1.m1"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>𝑾</mi><mi>l</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding="application/x-tex">\{\bm{W}_{l}\}_{l=1}^{L}</annotation><annotation encoding="application/x-llamapun">{ bold_italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> to reduce the prediction/classification error based on a gradient descent algorithm, we need to evaluate the gradient <math alttext="{\partial h}/{\partial\bm{W}_{l}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px4.p1.m2"><semantics><mrow><mo rspace="0em">∂</mo><mrow><mi>h</mi><mo>/</mo><mrow><mo lspace="0em" rspace="0em">∂</mo><msub><mi>𝑾</mi><mi>l</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">{\partial h}/{\partial\bm{W}_{l}}</annotation><annotation encoding="application/x-llamapun">∂ italic_h / ∂ bold_italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. It has been well known, from the <span class="ltx_text ltx_font_italic">chain rule</span> in calculus, that gradients can be computed efficiently for this type of functions, later known as back propagation (BP). See Appendix <a class="ltx_ref" href="A1.html" title="Appendix A Optimization Methods ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">A</span></a> for a detailed description. The technique of back propagation was already known and practiced by people in fields such as optimal control and dynamic programming in the 1960s and 1970s. For example, it appeared in the 1974 PhD thesis of Dr. Paul Werbos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx287" title="">Wer74</a>, <a class="ltx_ref" href="bib.html#bibx288" title="">Wer94</a>]</cite>. In 1986, David Rumelhart et al. were the first to apply back propagation to train a multiple layer perceptron (MLP) network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx235" title="">RHW86</a>]</cite>. Since then, BP has become increasingly popular since it gives a <span class="ltx_text ltx_font_italic">scalable</span> algorithm to learn large deep neural networks.<span class="ltx_note ltx_role_footnote" id="footnote30"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span>as it can be efficiently implemented on computing platforms that facilitate parallel and distributed computing.</span></span></span> It is now an almost dominant technique for training deep neural networks today. However, it is believed that nature does not learn by back propagation, because such a mechanism is still way too expensive for physical implementation by nature<span class="ltx_note ltx_role_footnote" id="footnote31"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup><span class="ltx_tag ltx_tag_note">31</span>As we have discussed earlier, nature almost ubiquitously learns to correct error via closed-loop feedback.</span></span></span>. This obviously leaves tremendous room for improvement in the future, as we will discuss more.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.Px4.p2">
<p class="ltx_p">However, despite the above algorithmic progress and promising practice in the 1980s, training deep neural networks remained extremely finicky and expensive for computing systems in the 1980s and 1990s. In late 1990s, support vector machines (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx57" title="">CV95</a>]</cite> had become very popular as they were viewed as a better alternative to neural networks for tasks such as classification.<span class="ltx_note ltx_role_footnote" id="footnote32"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><span class="ltx_tag ltx_tag_note">32</span>In fact, similar ideas to solve classification problems can be traced back to the PhD dissertation work of Thomas Cover, which as condensed and published in a paper in 1964 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx59" title="">Cov64</a>]</cite>.</span></span></span> There were two main reasons: first, SVM was based on a rigorous statistical learning framework known as the Vapnik–Chervonenkis (VC) theory; and second, it leads to rather efficient algorithms based on convex optimization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx27" title="">BV04</a>]</cite>. The rise of SVM had brought a second winter to the study of neural networks around the early 2000s.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx1.Px5">
<h5 class="ltx_title ltx_title_paragraph">Compressive autoencoding.</h5>
<div class="ltx_para" id="S3.SS2.SSSx1.Px5.p1">
<p class="ltx_p">In the late 1980s and 1990s, artificial neural networks were already adopted to learn low-dimensional representations of high-dimensional data such as images. It had been shown that neural networks can be used to learn PCA from the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx200" title="">Oja82</a>, <a class="ltx_ref" href="bib.html#bibx14" title="">BH89</a>]</cite>, instead of using the classic methods discussed in Section <a class="ltx_ref" href="#S3.SS1.SSSx2" title="Linear and Mixed Linear Models ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.1</span></a>. It was also argued during late 1980s that due to its capability to model nonlinear transforms, neural networks were suggested to learn low-dimensional representations for data with nonlinear distributions. Similar to the linear PCA case, one can try to simultaneously learn a nonlinear dimension-reduction encoder <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and a decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p1.m2"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math>, each modeled by a deep neural network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx235" title="">RHW86</a>, <a class="ltx_ref" href="bib.html#bibx143" title="">Kra91</a>]</cite>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}}." class="ltx_Math" display="block" id="S3.E30.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑓</mo></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑔</mo></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.30)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By enforcing the decoded data <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p1.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> to be consistent with the original <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, say by minimizing an MMSE type reconstruction error<span class="ltx_note ltx_role_footnote" id="footnote33"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><span class="ltx_tag ltx_tag_note">33</span>Although MMSE type of errors are known to be problematic to imagery data that have complex nonlinear structures. As we will soon discuss, much of the recent work in generative methods, including GANs, has been to find surrogates of better distance functions between the original data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote33.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and the regenerated <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="footnote33.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>.</span></span></span>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{f,g}\big{\|}\bm{X}-\hat{\bm{X}}\big{\|}_{2}^{2}=\big{\|}\bm{X}-g(f(\bm{X}))\big{\|}_{2}^{2}," class="ltx_Math" display="block" id="S3.E31.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow></munder><mo>⁡</mo><msubsup><mrow><mo maxsize="120%" minsize="120%">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo maxsize="120%" minsize="120%">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>=</mo><msubsup><mrow><mo maxsize="120%" minsize="120%">‖</mo><mrow><mi>𝑿</mi><mo>−</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="120%" minsize="120%">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{f,g}\big{\|}\bm{X}-\hat{\bm{X}}\big{\|}_{2}^{2}=\big{\|}\bm{X}-g(f(\bm{X}))\big{\|}_{2}^{2},</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ∥ bold_italic_X - over^ start_ARG bold_italic_X end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ∥ bold_italic_X - italic_g ( italic_f ( bold_italic_X ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.31)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">an autoencoder can be learned from the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p1.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> themselves.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.Px5.p2">
<p class="ltx_p">But how can we guarantee that such an autoencoding indeed captures the true low-dimensional structures in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p2.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> instead of giving a trivial redundant representation? For example, we can simply choose <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p2.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p2.m3"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> to be the identity map and <math alttext="\bm{Z}=\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx1.Px5.p2.m4"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mi>𝑿</mi></mrow><annotation encoding="application/x-tex">\bm{Z}=\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = bold_italic_X</annotation></semantics></math>. So to ensure the autoencoding to be worthwhile, one wishes the resulting representation to be compressive, in terms of a certain computable measure of complexity. In 1993, Geoffrey Hinton and colleagues proposed to use the coding length as such a measure, hence the objective of autoencoding became to find such a representation that minimizes the coding length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx105" title="">HZ93</a>]</cite>. This work also established a fundamental connection between the principle of minimum description length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx229" title="">Ris78</a>]</cite> and free (Helmholtz) energy minimization. Later work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx104" title="">HS06</a>]</cite> from Hinton’s group showed empirically that such an autoencoding is capable of learning meaningful low-dimensional representations for real-world images. A more comprehensive survey of autoencoders was done by Pierre Baldi in 2011 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx13" title="">Bal11</a>]</cite>, just before deep networks became popular. We will discuss more about the measure of complexity and autoencoding later in Section <a class="ltx_ref" href="#S4" title="1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4</span></a>, and give a systematic study of compressive autoencoding to Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>, with a more unifying view.</p>
</div>
<figure class="ltx_figure" id="F18"><img alt="Figure 1.18 : Architecture of the LeNet [ LBD+89 ] versus that of the AlexNet [ KSH12 ] ." class="ltx_graphics ltx_img_landscape" height="359" id="F18.g1" src="chapters/chapter1/figs/Comparison_image_neural_networks.svg.png" width="479"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.18</span>: </span><span class="ltx_text" style="font-size:90%;">Architecture of the LeNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx149" title="">LBD+89</a>]</cite> versus that of the AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Modern Deep Neural Networks</h4>
<div class="ltx_para" id="S3.SS2.SSSx2.p1">
<p class="ltx_p">For nearly 30 years, from 1980s to 2010s, for the study of machine learning and machine intelligence, neural networks were not considered seriously by the mainstream. Early (deep) neural networks, such as the LeNet, have shown promising performance for small-scale classification problems such as recognizing digits. However, the design and practice of the networks were rather empirical, datasets available at the time were small, and the BP algorithm was a huge computational burden for computers then. These factors had resulted in the lack of interest in neural networks, and progress had been stagnant, with only a handful of researchers working on it.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSSx2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Classification and recognition.</h5>
<div class="ltx_para" id="S3.SS2.SSSx2.Px1.p1">
<p class="ltx_p">As it turns out, the tremendous potential of deep neural networks could only be unleashed once there are enough data and computing power. Fast forward to 2010s, much larger datasets such as ImageNet became available, and GPUs became powerful enough to make BP much more affordable, even for networks much larger than LeNet. Around 2012, a deep convolutional neural network known as the AlexNet drew attention as it surpassed extent classification methods by a significant margin with the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote34"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><span class="ltx_tag ltx_tag_note">34</span>In fact, before this, deep networks had demonstrated state of the art performance on speech recognition tasks. But it did not receive so much attention till their success with image classification.</span></span></span> Figure <a class="ltx_ref" href="#F18" title="Figure 1.18 ‣ Compressive autoencoding. ‣ Classic Artificial Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.18</span></a> shows the comparison between the AlexNet and the LeNet. The AlexNet shares many common characteristics as the LeNet, only it is bigger and has adopted ReLU as the nonlinear activation instead of the Sigmoid function used in LeNet. Partly due to the influence of this work, Geoffrey Hinton was awarded the 2018 Turing Award.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px1.p2">
<p class="ltx_p">This early success inspired the machine intelligence community, in the next few years, to explore new variations and improvements to the network design. In particular, people had discovered empirically that the larger and deeper the networks, the better the performance in tasks such as image classification. Many deep network architectures have been tried, tested, and popularized. A few notable ones include VGG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx246" title="">SZ15</a>]</cite>, GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx257" title="">SLJ+14</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx102" title="">HZR+16</a>]</cite>, and more recently Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx271" title="">VSP+17</a>]</cite> etc. Despite fast progress in empirical performance, there was a lack of theoretical explanation for these empirically discovered architectures, including relationships among them if any. One purpose of this book is to reveal what common objective all these networks may serve and why they share certain common characteristics, including multiple layers of linear operator interleaved with nonlinear activation (see Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Reinforcement learning.</h5>
<div class="ltx_para" id="S3.SS2.SSSx2.Px2.p1">
<p class="ltx_p">The early successes of deep networks were mainly for classification tasks in a supervised learning setting, such as speech recognition and image recognition. Deep networks were later adopted by the DeepMind team, led by Demis Hassabis, to learn decision-making or control policies for playing games. In this context, deep networks are used to model the optimal decision/control policy or the associated optimal value function, as shown in Figure <a class="ltx_ref" href="#F19" title="Figure 1.19 ‣ Reinforcement learning. ‣ Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.19</span></a>. These network parameters are incrementally optimized <span class="ltx_note ltx_role_footnote" id="footnote35"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><span class="ltx_tag ltx_tag_note">35</span>Say based on back propagation (BP).</span></span></span> based on reward returned from the success or failure of playing the game with the current policy. This learning method is generally referred to as <span class="ltx_text ltx_font_italic">reinforcement learning</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx256" title="">SB18</a>]</cite>, originated from the practice of control systems in the late 1960s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx282" title="">WF65</a>, <a class="ltx_ref" href="bib.html#bibx182" title="">MM70</a>]</cite>. Its earlier roots can be traced back to a much longer and richer history of <span class="ltx_text ltx_font_italic">dynamic programming</span> by Richard Bellman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx19" title="">Bel57</a>]</cite> and <span class="ltx_text ltx_font_italic">trial-and-error learning</span> by Marvin Minsky <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx185" title="">Min54</a>]</cite> in the 1950s.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px2.p2">
<p class="ltx_p">From an implementation perspective, the combination of deep networks and reinforcement learning turned out to be rather powerful: deep networks can be used to approximate control policy and value function for real-world environments that are difficult to model analytically. This practice had eventually led to the AlphaGo system, developed by the company DeepMind, which surprised the world in 2016 by beating a top human player Lee Sedol in the game Go and then the world champion Jie Ke in 2017.<span class="ltx_note ltx_role_footnote" id="footnote36"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><span class="ltx_tag ltx_tag_note">36</span>In 1996, IBM’s Deep Blue system made history by defeating Russian grandmaster Garry Kasparov in chess. It mainly used traditional machine learning techniques, such as tree search and pruning, which were not so scalable and had not been proven successful for more challenging games such as Go for over 20 years.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px2.p3">
<p class="ltx_p">The success of AlphaGo came as a big surprise to the computing society which generally believes that the state space for search is too prohibitively large to admit any efficient solution, in terms of computation and sample size. The only reasonable explanation for its success is that there must be very good structures in the optimal value/policy function of the game Go. Their intrinsic dimension is not so high and they can be approximated well by a neural network, learnable from not so prohibitively many samples.</p>
</div>
<figure class="ltx_figure" id="F19"><img alt="Figure 1.19 : AlphaGo: Using deep neural networks to model the optimal policy or the optimal value function for the game Go." class="ltx_graphics ltx_img_square" height="285" id="F19.g1" src="chapters/chapter1/figs/Policy-Value.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.19</span>: </span><span class="ltx_text" style="font-size:90%;">AlphaGo: Using deep neural networks to model the optimal policy or the optimal value function for the game Go. </span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Generation and prediction.</h5>
<div class="ltx_para" id="S3.SS2.SSSx2.Px3.p1">
<p class="ltx_p">One may view early practices of deep networks in the 2010s focused more on extracting relevant information from the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and encoding it for certain task-specific representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> (say <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m3"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> represent class labels in classification tasks):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054ptf\hskip 5.69054pt}\bm{Z}." class="ltx_Math" display="block" id="S3.E32.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑓</mo></mover><mi>𝒁</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054ptf\hskip 5.69054pt}\bm{Z}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT → end_ARROW bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.32)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this setting, the mapping <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m4"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> to be learned does not need to preserve most distributional information about <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> but only the sufficient statistics needed for a specific task. For example, a sample <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> in <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> could be an image of an apple, which is mapped by <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m8"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> to its class label <math alttext="\bm{z}=" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p1.m9"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\bm{z}=</annotation><annotation encoding="application/x-llamapun">bold_italic_z =</annotation></semantics></math> “apple.” The <span class="ltx_text ltx_font_italic">information bottleneck framework</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx262" title="">TZ15</a>]</cite> was proposed in 2015 to analyze the role of deep networks in such a context.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px3.p2">
<p class="ltx_p">However, in many modern situations such as those so-called large foundation models, people often need to decode <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> to recover the corresponding <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to a certain degree of precision:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}}." class="ltx_Math" display="block" id="S3.E33.m1"><semantics><mrow><mrow><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑔</mo></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.33)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> typically represents data observed from the external world, a good decoder would allow us to simulate or predict what happens in the world. For example, in a “text to image” or “text to video” task, <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m4"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> normally represents the texts that describe the content of a desired image <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m5"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. The decoder should be able to generate an <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m6"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> that has the same content as <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m7"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>. For example, given an object class <math alttext="\bm{z}=" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m8"><semantics><mrow><mi>𝒛</mi><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\bm{z}=</annotation><annotation encoding="application/x-llamapun">bold_italic_z =</annotation></semantics></math> “apple”, the decoder <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m9"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> should generate an image <math alttext="\hat{\bm{x}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m10"><semantics><mover accent="true"><mi>𝒙</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{x}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_x end_ARG</annotation></semantics></math> that looks like an apple, although not necessarily exactly the same as the original <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px3.p2.m11"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Generation via discriminative approaches.</h5>
<div class="ltx_para" id="S3.SS2.SSSx2.Px4.p1">
<p class="ltx_p">In order for the generated images <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p1.m1"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> to be similar to the true natural images <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we need to be able to evaluate and minimize some distance:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min d(\bm{X},\hat{\bm{X}})." class="ltx_Math" display="block" id="S3.E34.m1"><semantics><mrow><mrow><mrow><mi>min</mi><mo lspace="0.167em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min d(\bm{X},\hat{\bm{X}}).</annotation><annotation encoding="application/x-llamapun">roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.34)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As it turns out, most theoretically motivated distances are extremely difficult, if not impossible, to compute and optimize for distributions in high-dimensional space but with a low intrinsic dimension.<span class="ltx_note ltx_role_footnote" id="footnote37"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><span class="ltx_tag ltx_tag_note">37</span>This is the case even if a parametric family of distribution for <math alttext="\bm{X}" class="ltx_Math" display="inline" id="footnote37.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> is given. The distance often becomes ill-conditioned or ill-defined for distributions with low-dimensional supports. What could be even worse is that the chosen family might not be able to approximate well the true distribution of interest.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px4.p2">
<p class="ltx_p">In 2007, Zhuowen Tu, a former student of Song-Chun Zhu, probably disappointed by early analytical attempts to model and generate natural images (discussed earlier), decided to try a drastically different approach. In a paper published in CVPR 2007 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx266" title="">Tu07</a>]</cite>, he was the first to suggest that one could learn a generative model for images via a discriminative approach. The idea is simple: if it is difficult to evaluate the distance <math alttext="d(\bm{X},\hat{\bm{X}})" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m1"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\bm{X},\hat{\bm{X}})</annotation><annotation encoding="application/x-llamapun">italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG )</annotation></semantics></math>, one could try to learn a discriminator <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> to separate <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> from <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E35">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}},\bm{X}\xrightarrow{\hskip 5.69054ptd\hskip 5.69054pt}\bm{0},\bm{1}," class="ltx_Math" display="block" id="S3.E35.m1"><semantics><mrow><mrow><mrow><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑔</mo></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo>,</mo><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo>𝑑</mo></mover><mrow><mn>𝟎</mn><mo>,</mo><mn>𝟏</mn></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{Z}\xrightarrow{\hskip 5.69054ptg\hskip 5.69054pt}\hat{\bm{X}},\bm{X}\xrightarrow{\hskip 5.69054ptd\hskip 5.69054pt}\bm{0},\bm{1},</annotation><annotation encoding="application/x-llamapun">bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_d end_OVERACCENT → end_ARROW bold_0 , bold_1 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.35)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{0},\bm{1}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m5"><semantics><mrow><mn>𝟎</mn><mo>,</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">\bm{0},\bm{1}</annotation><annotation encoding="application/x-llamapun">bold_0 , bold_1</annotation></semantics></math> indicate an image is generated or not.
Intuitively, the harder we could separate <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m6"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> and <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p2.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, probably the closer they are.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px4.p3">
<p class="ltx_p">Tu’s work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx266" title="">Tu07</a>]</cite> was the first to demonstrate the feasibility of learning a generative model from a discriminative approach. However, the work adopted traditional methods to generate images and classify distributions (such as boosting), and they were slow and hard to implement. After 2012, deep neural networks became very popular for image classification. In 2014, Ian Goodfellow and colleagues proposed again to generate natural images with a discriminative approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx93" title="">GPM+14</a>]</cite>. They suggested using deep neural networks to model the generator <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m1"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> and the discriminator <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> instead. Moreover, they proposed to learn the generator <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m3"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> and discriminator <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m4"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> via a minimax game:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E36">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{g}\max_{d}\ell(\bm{X},\hat{\bm{X}})," class="ltx_Math" display="block" id="S3.E36.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mi>g</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mi>d</mi></munder><mo lspace="0.167em">⁡</mo><mi mathvariant="normal">ℓ</mi></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min_{g}\max_{d}\ell(\bm{X},\hat{\bm{X}}),</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT roman_ℓ ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.36)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\ell(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m5"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\cdot)</annotation><annotation encoding="application/x-llamapun">roman_ℓ ( ⋅ )</annotation></semantics></math> is some natural loss function associated with the classification. In words the discriminator <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m6"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math> tries to maximize its success in separating <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m8"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>, whereas the generator <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p3.m9"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> tries to do the opposite. For this reason, this approach is named as <span class="ltx_text ltx_font_italic">generative adversarial networks</span> (GANs). It was shown that once trained on a large dataset, GANs can indeed generate photo-realistic images. Partially due to the influence of this work, Yoshua Bengio was awarded the 2018 Turing Award.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.Px4.p4">
<p class="ltx_p">The discriminative approach seems to be a rather clever way to bypass a fundamental difficulty in distribution learning. However, rigorously speaking, this approach does not fully resolve this fundamental difficulty at all. It is shown by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx93" title="">GPM+14</a>]</cite> that with a properly chosen loss, the minimax formulation is mathematically equivalent to minimize <span class="ltx_text ltx_font_italic">the Jensen-Shannon distance</span> (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>) between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p4.m1"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px4.p4.m2"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math>. This is known to be a hard problem for two low-dimensional distributions in a high-dimensional space. As a result, in practice, GANs typically rely on many heuristics and engineering tricks and often suffer from instability issues such as <span class="ltx_text ltx_font_italic">mode collapsing</span>.<span class="ltx_note ltx_role_footnote" id="footnote38"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><span class="ltx_tag ltx_tag_note">38</span>Nevertheless, such a minimax formulation provides a practical approximation of the distance. It simplifies the implementation and avoids certain caveats in directly computing the distance.</span></span></span> Overall, there has been a lack of theoretical guidance on how to improve the practice of GANs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSSx2.Px5">
<h5 class="ltx_title ltx_title_paragraph">Generation via denoising and diffusion.</h5>
<div class="ltx_para" id="S3.SS2.SSSx2.Px5.p1">
<p class="ltx_p">In 2015, shortly after GAN was introduced and became popular, Surya Ganguli and his students realized and suggested that an iterative denoising process modeled by a deep network can be used to learn a general distribution, such as that of natural images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx247" title="">SWM+15</a>]</cite>. Their method was inspired by properties of the special Gaussian and binomial processes, studied by William Feller back in 1949 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx84" title="">Fel49</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote39"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup><span class="ltx_tag ltx_tag_note">39</span>Again, in the magical era of 1940s!</span></span></span></p>
</div>
<figure class="ltx_figure" id="F20"><img alt="Figure 1.20 : Illustration of an iterative denoising and compressing process that, starting from a Gaussian distribution q 0 = 𝒩 ​ ( 𝟎 , 𝑰 ) q^{0}=\mathcal{N}(\bm{0},\bm{I}) italic_q start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = caligraphic_N ( bold_0 , bold_italic_I ) , converges to an arbitrary low-dimensional data distribution q L = p ​ ( 𝒙 ) q^{L}=p(\bm{x}) italic_q start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = italic_p ( bold_italic_x ) ." class="ltx_graphics ltx_img_landscape" height="118" id="F20.g1" src="chapters/chapter1/figs/diffusion_pipeline.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.20</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of an iterative denoising and compressing process that, starting from a Gaussian distribution <math alttext="q^{0}=\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="F20.m3"><semantics><mrow><msup><mi>q</mi><mn>0</mn></msup><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">q^{0}=\mathcal{N}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">italic_q start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math>, converges to an arbitrary low-dimensional data distribution <math alttext="q^{L}=p(\bm{x})" class="ltx_Math" display="inline" id="F20.m4"><semantics><mrow><msup><mi>q</mi><mi>L</mi></msup><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">q^{L}=p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_q start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = italic_p ( bold_italic_x )</annotation></semantics></math>. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSSx2.Px5.p2">
<p class="ltx_p">Soon, denoising operators based on the score function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx117" title="">Hyv05</a>]</cite>, briefly introduced in Section <a class="ltx_ref" href="#S3.SS1.SSSx3" title="General Distributions ‣ 1.3.1 Analytical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.3.1</span></a>, were shown to be more general and unified the denoising and diffusion processes and algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx249" title="">SE19</a>, <a class="ltx_ref" href="bib.html#bibx250" title="">SSK+21</a>, <a class="ltx_ref" href="bib.html#bibx106" title="">HJA20</a>]</cite>. Figure <a class="ltx_ref" href="#F20" title="Figure 1.20 ‣ Generation via denoising and diffusion. ‣ Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.20</span></a> gives an illustration of the process that transforms a generic Gaussian distribution <math alttext="q^{0}=\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px5.p2.m1"><semantics><mrow><msup><mi>q</mi><mn>0</mn></msup><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">q^{0}=\mathcal{N}(\bm{0},\bm{I})</annotation><annotation encoding="application/x-llamapun">italic_q start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = caligraphic_N ( bold_0 , bold_italic_I )</annotation></semantics></math> to an (arbitrary) empirical distribution <math alttext="p(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSSx2.Px5.p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\bm{x})</annotation><annotation encoding="application/x-llamapun">italic_p ( bold_italic_x )</annotation></semantics></math> by performing a sequence of iterative denoising (or compressing) operations:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E37">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{z}^{0}\sim\mathcal{N}(\bm{0},\bm{I})\xrightarrow{\hskip 5.69054ptg^{0}\hskip 5.69054pt}\bm{z}^{1}\xrightarrow{\hskip 5.69054ptg^{1}\hskip 5.69054pt}\cdots\xrightarrow{\hskip 5.69054ptg^{L-1}\hskip 5.69054pt}\bm{z}^{L}\sim p(\bm{x})." class="ltx_Math" display="block" id="S3.E37.m1"><semantics><mrow><mrow><msup><mi>𝒛</mi><mn>0</mn></msup><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><msup><mi>g</mi><mn>0</mn></msup></mover><msup><mi>𝒛</mi><mn>1</mn></msup><mover accent="true"><mo stretchy="false">→</mo><msup><mi>g</mi><mn>1</mn></msup></mover><mi mathvariant="normal">⋯</mi><mover accent="true"><mo stretchy="false">→</mo><msup><mi>g</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup></mover><msup><mi>𝒛</mi><mi>L</mi></msup><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{z}^{0}\sim\mathcal{N}(\bm{0},\bm{I})\xrightarrow{\hskip 5.69054ptg^{0}\hskip 5.69054pt}\bm{z}^{1}\xrightarrow{\hskip 5.69054ptg^{1}\hskip 5.69054pt}\cdots\xrightarrow{\hskip 5.69054ptg^{L-1}\hskip 5.69054pt}\bm{z}^{L}\sim p(\bm{x}).</annotation><annotation encoding="application/x-llamapun">bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ∼ caligraphic_N ( bold_0 , bold_italic_I ) start_ARROW start_OVERACCENT italic_g start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_g start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW ⋯ start_ARROW start_OVERACCENT italic_g start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT end_OVERACCENT → end_ARROW bold_italic_z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∼ italic_p ( bold_italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.3.37)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By now, denoising (and diffusion) have replaced GANs and become a mainstream method for learning distributions of images and videos, leading to popular commercial image generating engines such as Midjourney and Stability.ai.
In Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a> we will systematically introduce and study the denoising and diffusion method for learning a general low-dimensional distribution.</p>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.4 </span>A Unifying Approach</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">So far, we have given a brief account of the main objective and history of machine intelligence and many important ideas and approaches associated with it. In recent years, after the empirical success of deep neural networks, tremendous efforts have been made to develop theoretical frameworks that can help us understand all the empirically designed deep neural networks, either certain seemingly necessary components (e.g., dropout, normalization, attention, etc.) or their overall behaviors (e.g., double descent, neural collapse, etc.).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">Partly motivated by this, this book aims to achieve several important and challenging goals:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p">Develop a theoretical framework that would allow us to derive rigorous mathematical interpretation of deep neural networks.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p">Ensure correctness of the learned data distribution and consistency with the learned representation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p">Demonstrate that the framework can lead to performant architectures and can guide further improvements in practice.</p>
</div>
</li>
</ul>
<p class="ltx_p">Within the past few years, there is mounting evidence that these goals can indeed be achieved, by leveraging the theory and solutions to the classical analytical low-dimensional models discussed briefly earlier (more thoroughly in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>) and by integrating fundamental ideas from a few related fields, namely information/coding theory, control/game theory, and optimization. This book aims to give a systematic introduction to this new approach.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.1 </span>Learning Parsimonious Representations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">One necessary condition for any learning task to be possible is that the sequences of interest must be <span class="ltx_text ltx_font_italic">computable</span>, at least in the sense of Alan Turing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx268" title="">Tur36</a>]</cite>. That is, a sequence can be computed via a program on a typical computer.<span class="ltx_note ltx_role_footnote" id="footnote40"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><span class="ltx_tag ltx_tag_note">40</span>There are indeed well-defined sequences that are not computable.</span></span></span> In addition to being computable, we require computation be <span class="ltx_text ltx_font_italic">tractable</span>.<span class="ltx_note ltx_role_footnote" id="footnote41"><sup class="ltx_note_mark">41</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">41</sup><span class="ltx_tag ltx_tag_note">41</span>We do not need to consider predicting things whose computational complexity is intractable, say grows exponentially in the length or dimension of the sequence.</span></span></span> That is, the computational cost (space and time) for learning and computing the sequence should not grow exponentially in length. Furthermore, as we see in nature (and in modern practice of machine intelligence), for most practical tasks, an intelligent system needs to learn what is predictable from massive data in a very high-dimensional space, such as from vision, sound, and touch. Hence, for intelligence, we do not need to consider all computable and tractable sequences or structures. We should focus only on predictable sequences and structures which admit <span class="ltx_text ltx_font_italic">scalable</span> realizations of its learning and computing algorithms:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{{computable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}." class="ltx_Math" display="block" id="S4.E1.m1"><semantics><mrow><mrow><mtext class="ltx_mathvariant_bold">computable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">tractable</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">scalable</mtext></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mbox{{computable}}\;\Longrightarrow\;\mbox{{tractable}}\;\Longrightarrow\;\mbox{{scalable}}.</annotation><annotation encoding="application/x-llamapun">computable ⟹ tractable ⟹ scalable .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">This is because whatever algorithms intelligent beings use to learn useful information must be <span class="ltx_text ltx_font_italic">scalable</span>. More specifically, the computational complexity of the algorithms would better scale gracefully, typically linear or even sublinear, in the size and dimension of the data. On the technical level, this requires that the operations that the algorithms rely on to learn could only utilize oracle information that can be efficiently computed from the data. More specifically, when the dimension is high and the scale is large, the only oracle one could afford to compute is either the first-order geometric information about the data<span class="ltx_note ltx_role_footnote" id="footnote42"><sup class="ltx_note_mark">42</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">42</sup><span class="ltx_tag ltx_tag_note">42</span>such as approximating a nonlinear structure locally with linear subspaces and computing the gradient of an objective function.</span></span></span> or the second-order statistic information<span class="ltx_note ltx_role_footnote" id="footnote43"><sup class="ltx_note_mark">43</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">43</sup><span class="ltx_tag ltx_tag_note">43</span>such as covariance or correlation of the data or their features.</span></span></span>.
The main goal of this book is to develop a theoretical and computational framework within which we can systematically develop efficient and effective solutions or algorithms with such scalable oracles and operations to <span class="ltx_text ltx_font_italic">learn</span> low-dimensional structures from the sampled data and subsequently the predictive function.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Pursuing low-dimensionality via compression.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p">From the examples of sequences we gave in Section <a class="ltx_ref" href="#S2.SS1" title="1.2.1 Predictability ‣ 1.2 What to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.2.1</span></a>, it is clear that some sequences are easy to model and compute and others are more difficult. Obviously, the computational cost of a sequence depends on how complex the predicting function <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is. The higher the degree of regression <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m2"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>, the more costly it is to compute. <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m3"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> can be a simple linear function, and it can also be a nonlinear function that can be arbitrarily difficult to specify and compute.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p">It is reasonable to believe that if a sequence is harder, by whatever measure we may choose, to specify and compute, then it will also be more difficult to learn from its sampled segments. Nevertheless, for any given predictable sequence, there are in fact many, often infinitely many, ways to specify it. For example, for a simple sequence <math alttext="x_{n+1}=ax_{n}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.m1"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow><annotation encoding="application/x-tex">x_{n+1}=ax_{n}</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_a italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, we could also define the same sequence with <math alttext="x_{n+1}=ax_{n}+bx_{n-1}-bx_{n-1}." class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.m2"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>+</mo><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>−</mo><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">x_{n+1}=ax_{n}+bx_{n-1}-bx_{n-1}.</annotation><annotation encoding="application/x-llamapun">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_a italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_b italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT - italic_b italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT .</annotation></semantics></math>
Hence it would be very useful if we could develop an objective and rigorous notion of “complexity” for any given computable sequence.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p3">
<p class="ltx_p">Andrey Kolmogorov, a Russian mathematician, was one of the first to give a definition of complexity for any computable sequence.<span class="ltx_note ltx_role_footnote" id="footnote44"><sup class="ltx_note_mark">44</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">44</sup><span class="ltx_tag ltx_tag_note">44</span>Many have contributed to this notion of sequence complexity, most notably including Ray Solomonoff and Greg Chaitin. All three are believed to have developed and studied algorithmic information theory independently, Ray Solomonoff in 1960, Andrey Kolmogorov in 1965 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx141" title="">Kol98</a>]</cite> and Gregory Chaitin around 1966 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx40" title="">Cha66</a>]</cite>.</span></span></span> He suggested that among all programs that can compute the same sequence, we may use the length of the shortest program as a measure for its complexity. This idea is very much in line with the famous “Occam’s Razor” principle of parsimony: <span class="ltx_text ltx_font_italic">one always chooses the simplest among all theories that can explain the same observation.</span> To be more precise, let <math alttext="p" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.m1"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation><annotation encoding="application/x-llamapun">italic_p</annotation></semantics></math> represent a compute program that can generate a sequence <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.m2"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math> on a universal computer <math alttext="\mathcal{U}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.m3"><semantics><mi class="ltx_font_mathcaligraphic">𝒰</mi><annotation encoding="application/x-tex">\mathcal{U}</annotation><annotation encoding="application/x-llamapun">caligraphic_U</annotation></semantics></math>. The Kolmogorov complexity of the sequence <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.m4"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math> is defined to be:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K(S)=\min_{p\,:\,\mathcal{U}(p)=S}L(p)." class="ltx_Math" display="block" id="S4.E2.m1"><semantics><mrow><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>min</mi><mrow><mi>p</mi><mo lspace="0.448em" rspace="0.448em">:</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒰</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>S</mi></mrow></mrow></munder><mo lspace="0.167em">⁡</mo><mi>L</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">K(S)=\min_{p\,:\,\mathcal{U}(p)=S}L(p).</annotation><annotation encoding="application/x-llamapun">italic_K ( italic_S ) = roman_min start_POSTSUBSCRIPT italic_p : caligraphic_U ( italic_p ) = italic_S end_POSTSUBSCRIPT italic_L ( italic_p ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, the complexity of a sequence is measured by how “parsimonously” we can specify or compute it. This definition of complexity, or parsimony, for sequences is of great conceptual importance and has many interesting properties. Historically, it has inspired many profound studies in the theory of computation, especially in the field of algorithmic information theory.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p4">
<p class="ltx_p">The length of the shortest program can be viewed as the ultimate compression of the sequence considered, providing a quantitative measure of how much we have gained by having learned the correct generative mechanism of the sequence. However, despite its theoretical importance, the Kolmogorov complexity is in general not a computable function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite> (even intractable to approximate accurately). As a result, this measure of complexity is of little practical use. Neither can it tell us in advance how difficult it is to learn a given sequence nor can it tell us how well we have learned.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Computable measure of parsimony.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p">Hence for practical purposes, we need an efficiently computable measure of complexity for sequences that are generated from the same predicting function.<span class="ltx_note ltx_role_footnote" id="footnote45"><sup class="ltx_note_mark">45</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">45</sup><span class="ltx_tag ltx_tag_note">45</span>Note that in practice, we typically care about learning the predicting function <math alttext="f" class="ltx_Math" display="inline" id="footnote45.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>, instead of any particular sequence generated by <math alttext="f" class="ltx_Math" display="inline" id="footnote45.m2"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>.</span></span></span> Note that part of the reason why Kolmogorov complexity is not computable is because its definition is non-constructive.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p">So to introduce a computable measure of complexity, we may take a more constructive approach, as advocated by Claude Shannon through the framework of information theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx242" title="">Sha48</a>, <a class="ltx_ref" href="bib.html#bibx58" title="">CT91</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote46"><sup class="ltx_note_mark">46</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">46</sup><span class="ltx_tag ltx_tag_note">46</span>which has successfully guided the engineering practice of the communication industry for the past over 80 years.</span></span></span> In essence, by assuming the sequence <math alttext="S" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.m1"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math> is drawn from a probabilistic distribution <math alttext="p(S)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.m2"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(S)</annotation><annotation encoding="application/x-llamapun">italic_p ( italic_S )</annotation></semantics></math>, the so-called <span class="ltx_text ltx_font_italic">entropy</span> of the distribution:<span class="ltx_note ltx_role_footnote" id="footnote47"><sup class="ltx_note_mark">47</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">47</sup><span class="ltx_tag ltx_tag_note">47</span>Here we consider differential entropy as we assume the sequence consists of continuous variables. If it consists of discrete variables, we could consider the entropy: <math alttext="H(S)=-\sum_{i}p(s_{i})\log p(s_{i})." class="ltx_Math" display="inline" id="footnote47.m1"><semantics><mrow><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">H(S)=-\sum_{i}p(s_{i})\log p(s_{i}).</annotation><annotation encoding="application/x-llamapun">italic_H ( italic_S ) = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) roman_log italic_p ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math> </span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h(S)\doteq-\int p(s)\log p(s)\mathrm{d}s" class="ltx_Math" display="block" id="S4.E3.m1"><semantics><mrow><mrow><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≐</mo><mrow><mo>−</mo><mrow><mo>∫</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>s</mi></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">h(S)\doteq-\int p(s)\log p(s)\mathrm{d}s</annotation><annotation encoding="application/x-llamapun">italic_h ( italic_S ) ≐ - ∫ italic_p ( italic_s ) roman_log italic_p ( italic_s ) roman_d italic_s</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">gives a natural measure of its complexity. This measure also has a natural interpretation of the average number of binary bits needed to encode such a sequence, as we will see in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p3">
<p class="ltx_p">To illustrate the main ideas of this view, let us take a large number of long sequence segments:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}\subset\mathbb{R}^{D}," class="ltx_Math" display="block" id="S4.E4.m1"><semantics><mrow><mrow><mrow><mo stretchy="false">{</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}\subset\mathbb{R}^{D},</annotation><annotation encoding="application/x-llamapun">{ italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">generated by a predicting function <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math>. Note that without loss of generality, here we have assumed that all sequences are of the same length <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m2"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>. Therefore, each sequence can be viewed as a vector in <math alttext="\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m3"><semantics><msup><mi>ℝ</mi><mi>D</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun">blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Secondly, we may introduce a coding scheme (with a code book), denoted as <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m4"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math>, that converts every segment <math alttext="S_{i}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m5"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding="application/x-tex">S_{i}</annotation><annotation encoding="application/x-llamapun">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to a unique stream of binary bits <math alttext="\mathcal{E}(S_{i})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m6"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}(S_{i})</annotation><annotation encoding="application/x-llamapun">caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>. The simplest coding scheme is to fill the space spanned by all segments with <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p3.m7"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math>-balls, as shown in Figure <a class="ltx_ref" href="#F21" title="Figure 1.21 ‣ Computable measure of parsimony. ‣ 1.4.1 Learning Parsimonious Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.21</span></a>. We then number all the balls. Each sequence is encoded as the (binary) number of its closest ball. Hence, each segment can be recovered<span class="ltx_note ltx_role_footnote" id="footnote48"><sup class="ltx_note_mark">48</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">48</sup><span class="ltx_tag ltx_tag_note">48</span>up to precision <math alttext="\epsilon" class="ltx_Math" display="inline" id="footnote48.m1"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation><annotation encoding="application/x-llamapun">italic_ϵ</annotation></semantics></math> as such an encoding scheme is lossy.</span></span></span> from its corresponding bit stream.</p>
</div>
<figure class="ltx_figure" id="F21"><img alt="Figure 1.21 : Comparison between two coding schemes. Image the true distribution of the data is around the two arrowed lines. One can code the samples from the two lines with the a code book consisting of all blue balls; one can also code the samples with a code book consisting of only the green balls. Obviously, the second coding schemes will result in much lower coding length/rate, subject to the same precision." class="ltx_graphics ltx_img_landscape" height="199" id="F21.g1" src="chapters/chapter1/figs/Coding-schemes.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.21</span>: </span><span class="ltx_text" style="font-size:90%;">Comparison between two coding schemes. Image the true distribution of the data is around the two arrowed lines. One can code the samples from the two lines with the a code book consisting of all blue balls; one can also code the samples with a code book consisting of only the green balls. Obviously, the second coding schemes will result in much lower coding length/rate, subject to the same precision.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p4">
<p class="ltx_p">Then the complexity of the predicting function <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p4.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> can be evaluated as the average coding length of all sequences, known as the coding rate:<span class="ltx_note ltx_role_footnote" id="footnote49"><sup class="ltx_note_mark">49</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">49</sup><span class="ltx_tag ltx_tag_note">49</span>One may make this more precise by taking <math alttext="R(f\mid\mathcal{E})" class="ltx_Math" display="inline" id="footnote49.m1"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>∣</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(f\mid\mathcal{E})</annotation><annotation encoding="application/x-llamapun">italic_R ( italic_f ∣ caligraphic_E )</annotation></semantics></math> to be the expected coding length for all segments of length <math alttext="D" class="ltx_Math" display="inline" id="footnote49.m2"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation><annotation encoding="application/x-llamapun">italic_D</annotation></semantics></math>. </span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R(f\mid\mathcal{E})=\mathbb{E}[L(\mathcal{E}(S))]\approx\frac{1}{N}\sum_{i=1}^{N}L(\mathcal{E}(S_{i}))." class="ltx_Math" display="block" id="S4.E5.m1"><semantics><mrow><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>∣</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≈</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">R(f\mid\mathcal{E})=\mathbb{E}[L(\mathcal{E}(S))]\approx\frac{1}{N}\sum_{i=1}^{N}L(\mathcal{E}(S_{i})).</annotation><annotation encoding="application/x-llamapun">italic_R ( italic_f ∣ caligraphic_E ) = blackboard_E [ italic_L ( caligraphic_E ( italic_S ) ) ] ≈ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Obviously, the coding rate measure will change if one uses a different coding scheme (or a code book). In practice, the better we know the low-dimensional structure around which the segments are distributed, the more efficient a code book we can design, as the example shown in Figure <a class="ltx_ref" href="#F21" title="Figure 1.21 ‣ Computable measure of parsimony. ‣ 1.4.1 Learning Parsimonious Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.21</span></a>. Acute readers may have recognized that conceptually the denoising process illustrated in Figure <a class="ltx_ref" href="#F20" title="Figure 1.20 ‣ Generation via denoising and diffusion. ‣ Modern Deep Neural Networks ‣ 1.3.2 Empirical Approaches ‣ 1.3 How to Learn? ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.20</span></a> resembles closely going from the coding scheme with the blue balls to that with the green ones.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p5">
<p class="ltx_p">Given two coding schemes <math alttext="\mathcal{E}_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{E}_{1}</annotation><annotation encoding="application/x-llamapun">caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{E}_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{E}_{2}</annotation><annotation encoding="application/x-llamapun">caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> for the segments, if the difference in the coding rates is positive:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R(f\mid\mathcal{E}_{1})-R(f\mid\mathcal{E}_{2})&gt;0," class="ltx_Math" display="block" id="S4.E6.m1"><semantics><mrow><mrow><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>∣</mo><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>∣</mo><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">R(f\mid\mathcal{E}_{1})-R(f\mid\mathcal{E}_{2})&gt;0,</annotation><annotation encoding="application/x-llamapun">italic_R ( italic_f ∣ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ∣ caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) &gt; 0 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">we may say the coding scheme <math alttext="\mathcal{E}_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m3"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{E}_{2}</annotation><annotation encoding="application/x-llamapun">caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is better. This difference can be viewed as a measure of how much more information <math alttext="\mathcal{E}_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m4"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{E}_{2}</annotation><annotation encoding="application/x-llamapun">caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> has over <math alttext="\mathcal{E}_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m5"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{E}_{1}</annotation><annotation encoding="application/x-llamapun">caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> about the distribution of the data. To a large extent, the goal of learning <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m6"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is equivalent to finding the most efficient encoding scheme that minimizes the coding rate:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\mathcal{E}}R(f\mid\mathcal{E})." class="ltx_Math" display="block" id="S4.E7.m1"><semantics><mrow><mrow><mrow><munder><mi>min</mi><mi class="ltx_font_mathcaligraphic">ℰ</mi></munder><mo lspace="0.167em">⁡</mo><mi>R</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo>∣</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min_{\mathcal{E}}R(f\mid\mathcal{E}).</annotation><annotation encoding="application/x-llamapun">roman_min start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT italic_R ( italic_f ∣ caligraphic_E ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">As we will see in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, the achievable minimal rate is closely related to the notion of entropy <math alttext="H(S)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p5.m7"><semantics><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(S)</annotation><annotation encoding="application/x-llamapun">italic_H ( italic_S )</annotation></semantics></math> (<a class="ltx_ref" href="#S4.E3" title="Equation 1.4.3 ‣ Computable measure of parsimony. ‣ 1.4.1 Learning Parsimonious Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4.3</span></a>).</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremark1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Remark 1.1</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmremark1.p1">
<p class="ltx_p">The perspective of measuring data complexity with explicit encoding schemes has motivated several learning objectives that were proposed to revise the Kolmogorov complexity for better computability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx281" title="">WD99</a>]</cite>, including the minimum message length (MML) proposed later in 1968 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx280" title="">WB68</a>]</cite> and the minimum description length (MDL) in 1978 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx229" title="">Ris78</a>, <a class="ltx_ref" href="bib.html#bibx99" title="">HY01</a>]</cite>. These objectives normally count the coding length for the coding scheme <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="Thmremark1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> itself (including its code book) in addition to the data <math alttext="S" class="ltx_Math" display="inline" id="Thmremark1.p1.m2"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation><annotation encoding="application/x-llamapun">italic_S</annotation></semantics></math> of interest: <math alttext="L(\mathcal{E}(S))+L(\mathcal{E})" class="ltx_Math" display="inline" id="Thmremark1.p1.m3"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\mathcal{E}(S))+L(\mathcal{E})</annotation><annotation encoding="application/x-llamapun">italic_L ( caligraphic_E ( italic_S ) ) + italic_L ( caligraphic_E )</annotation></semantics></math>. However, if the goal is to learn a finite-sized code book and apply it to a large number of sequences, the amortized cost of the code book can be ignored since</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{1}{N}\Big{(}L(\mathcal{E})+\sum_{i=1}^{N}L(\mathcal{E}(S_{i}))\Big{)}\approx\frac{1}{N}\sum_{i=1}^{N}L(\mathcal{E}(S_{i}))" class="ltx_Math" display="block" id="S4.Ex1.m1"><semantics><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="160%" minsize="160%">(</mo><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo maxsize="160%" minsize="160%">)</mo></mrow></mrow><mo>≈</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{N}\Big{(}L(\mathcal{E})+\sum_{i=1}^{N}L(\mathcal{E}(S_{i}))\Big{)}\approx\frac{1}{N}\sum_{i=1}^{N}L(\mathcal{E}(S_{i}))</annotation><annotation encoding="application/x-llamapun">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ( italic_L ( caligraphic_E ) + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ) ≈ divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as <math alttext="N" class="ltx_Math" display="inline" id="Thmremark1.p1.m4"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation><annotation encoding="application/x-llamapun">italic_N</annotation></semantics></math> becomes large.</p>
</div>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p6">
<p class="ltx_p">Again, one may view the resulting optimal coding scheme as the one that achieves the best compression of the observed data. In general, compared to the Kolmogorov complexity, the coding length given by any encoding scheme will always be larger:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K(S)&lt;L(\mathcal{E}(S))." class="ltx_Math" display="block" id="S4.E8.m1"><semantics><mrow><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">K(S)&lt;L(\mathcal{E}(S)).</annotation><annotation encoding="application/x-llamapun">italic_K ( italic_S ) &lt; italic_L ( caligraphic_E ( italic_S ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, minimizing the coding rate/length is essentially to minimize an upper bound of the otherwise uncomputable Kolmogorov complexity.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.2 </span>Learning Informative Representations</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">Note that if the goal was simply to compress the given data just for the sake of compression, then in theory the optimal codes that approach the Kolmogorov complexity would become nearly random or structureless <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx40" title="">Cha66</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote50"><sup class="ltx_note_mark">50</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">50</sup><span class="ltx_tag ltx_tag_note">50</span>Because any codes with structures can be further compressed.</span></span></span> However, our true purpose of learning the predictive function <math alttext="f" class="ltx_Math" display="inline" id="S4.SS2.p1.m1"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> is to use it repeatedly with ease in future predictions. Hence, while compression allows us to identify the low-dimensional distribution in the data, we would like to encode the distribution in a <span class="ltx_text ltx_font_italic">structured and organized</span> way so that the resulting representation is very informative and efficient to use.<span class="ltx_note ltx_role_footnote" id="footnote51"><sup class="ltx_note_mark">51</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">51</sup><span class="ltx_tag ltx_tag_note">51</span>For example, to sample the distribution under different conditions.</span></span></span> Figure <a class="ltx_ref" href="#F22" title="Figure 1.22 ‣ 1.4.2 Learning Informative Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.22</span></a> shows an example that explains intuitively why such a transformation is desired.</p>
</div>
<figure class="ltx_figure" id="F22"><img alt="Figure 1.22 : Transform the identified low-dimensional data distribution to a more informative and structured representation." class="ltx_graphics ltx_img_landscape" height="139" id="F22.g1" src="chapters/chapter1/figs/coding-transform.png" width="586"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.22</span>: </span><span class="ltx_text" style="font-size:90%;">Transform the identified low-dimensional data distribution to a more informative and structured representation.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">As we will show in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, these desired structures in the final representation can be precisely promoted by choosing a natural measure of <span class="ltx_text ltx_font_italic">information gain</span> based on the coding rates of the chosen coding schemes. As we see throughout this book, such an explicit and constructive coding approach provides a powerful computational framework for learning good representations of low-dimensional structures for real-world data, as in many cases of practical importance, the coding length function can be efficiently computed or approximated accurately. In some benign cases, we can even obtain closed-form formulae, e.g., subspace and Gaussian (see Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p">In addition, such a computational framework leads to a principled approach that naturally reveals the role that deep networks play in this learning process. As we will derive systematically in Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, the layers of a deep network are trying to perform operations that optimize the objective function of interest in an incremental manner. From this perspective, the role of deep networks can be precisely interpreted as to emulate a certain iterative optimization algorithm, say gradient descent, to optimize the objective of information gain. Layers of the resulting deep architectures can be endowed with precise statistical and geometric interpretation, namely performing incremental compressive encoding and decoding operations. As a result, the derived deep networks become transparent “white boxes” that are mathematically fully explainable.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.3 </span>Learning Consistent Representations</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">To summarize our discussions so far, let us denote the data as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}=\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}\subset\mathbb{R}^{D}," class="ltx_Math" display="block" id="S4.E9.m1"><semantics><mrow><mrow><mi>𝑿</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>S</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><mo>⊂</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}=\{S_{1},S_{2},\ldots,S_{i},\ldots,S_{N}\}\subset\mathbb{R}^{D},</annotation><annotation encoding="application/x-llamapun">bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ⊂ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and let <math alttext="\bm{Z}=\mathcal{E}(\bm{X})" class="ltx_Math" display="inline" id="S4.SS3.p1.m1"><semantics><mrow><mi>𝒁</mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{Z}=\mathcal{E}(\bm{X})</annotation><annotation encoding="application/x-llamapun">bold_italic_Z = caligraphic_E ( bold_italic_X )</annotation></semantics></math> be the codes of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> via some encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS3.p1.m3"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}." class="ltx_Math" display="block" id="S4.E10.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒁</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the machine learning context, <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.p1.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> is often called “features” or “latent representation”. Note that without knowing the underlying distribution of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.p1.m5"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, we do not know which encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS3.p1.m6"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> should be that can retain most useful information about the distribution of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.p1.m7"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. In practice, people often start with trying a certain compact encoding scheme that serves well for a specific task. In particular, they would try to learn an encoder that optimizes certain (empirical) measure of parsimony for the learned representation:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\rho(\bm{Z})." class="ltx_Math" display="block" id="S4.E11.m1"><semantics><mrow><mrow><mrow><mi>min</mi><mo lspace="0.167em">⁡</mo><mi>ρ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\min\rho(\bm{Z}).</annotation><annotation encoding="application/x-llamapun">roman_min italic_ρ ( bold_italic_Z ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.11)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_example" id="Thmexample2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_italic">Example 1.2</span></span><span class="ltx_text ltx_font_italic">.</span>
</h6>
<div class="ltx_para" id="Thmexample2.p1">
<p class="ltx_p">For example, image classification is such a case: we assign all images in the same class to a single code and images in different classes to different codes, say “one-hot” vectors:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x}\mapsto\bm{z}\in\{[1,0,0,\ldots,0,0],\;[0,1,0\ldots,0,0],\;\ldots,\;[0,0,0,\ldots,0,1].\}" class="ltx_math_unparsed" display="block" id="S4.E12.m1"><semantics><mrow><mi>𝒙</mi><mo stretchy="false">↦</mo><mi>𝒛</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="false">]</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="false">]</mo></mrow><mo rspace="0.447em">,</mo><mi mathvariant="normal">…</mi><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{x}\mapsto\bm{z}\in\{[1,0,0,\ldots,0,0],\;[0,1,0\ldots,0,0],\;\ldots,\;[0,0,0,\ldots,0,1].\}</annotation><annotation encoding="application/x-llamapun">bold_italic_x ↦ bold_italic_z ∈ { [ 1 , 0 , 0 , … , 0 , 0 ] , [ 0 , 1 , 0 … , 0 , 0 ] , … , [ 0 , 0 , 0 , … , 0 , 1 ] . }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, a classifier <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="Thmexample2.p1.m1"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ )</annotation></semantics></math> can be modeled as a function that predicts the probability of a given <math alttext="\bm{x}" class="ltx_Math" display="inline" id="Thmexample2.p1.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> belonging to each of the <math alttext="k" class="ltx_Math" display="inline" id="Thmexample2.p1.m3"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> classes: <math alttext="\hat{\bm{z}}=f(\bm{x})\in\mathbb{R}^{K}" class="ltx_Math" display="inline" id="Thmexample2.p1.m4"><semantics><mrow><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\hat{\bm{z}}=f(\bm{x})\in\mathbb{R}^{K}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG = italic_f ( bold_italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>. Then the “goodness” of a classifier can be measured by the so-called <span class="ltx_text ltx_font_italic">cross entropy</span>:<span class="ltx_note ltx_role_footnote" id="footnote52"><sup class="ltx_note_mark">52</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">52</sup><span class="ltx_tag ltx_tag_note">52</span>The cross entropy can be viewed as a distance measure between the ground truth distribution of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="footnote52.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and that of the prediction <math alttext="\hat{\bm{z}}" class="ltx_Math" display="inline" id="footnote52.m2"><semantics><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG</annotation></semantics></math>. It can also be viewed as the expected coding length of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="footnote52.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> if we use the optimal code book for <math alttext="\hat{\bm{z}}" class="ltx_Math" display="inline" id="footnote52.m4"><semantics><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG</annotation></semantics></math> to encode <math alttext="\bm{z}" class="ltx_Math" display="inline" id="footnote52.m5"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. The cross entropy reaches minimum when <math alttext="\bm{z}" class="ltx_Math" display="inline" id="footnote52.m6"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> and <math alttext="\hat{\bm{z}}" class="ltx_Math" display="inline" id="footnote52.m7"><semantics><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_z end_ARG</annotation></semantics></math> have the same distribution.</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(\hat{\bm{z}},\bm{z})=\sum_{k=1}^{K}-z_{k}\log\hat{z}_{k}," class="ltx_Math" display="block" id="S4.E13.m1"><semantics><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mo lspace="0em">−</mo><mrow><msub><mi>z</mi><mi>k</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>k</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">L(\hat{\bm{z}},\bm{z})=\sum_{k=1}^{K}-z_{k}\log\hat{z}_{k},</annotation><annotation encoding="application/x-llamapun">italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) = ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT - italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="z_{k}" class="ltx_Math" display="inline" id="Thmexample2.p1.m5"><semantics><msub><mi>z</mi><mi>k</mi></msub><annotation encoding="application/x-tex">z_{k}</annotation><annotation encoding="application/x-llamapun">italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> indicates the <math alttext="k" class="ltx_Math" display="inline" id="Thmexample2.p1.m6"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>-th entry of the vector <math alttext="\bm{z}" class="ltx_Math" display="inline" id="Thmexample2.p1.m7"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. As the early practice of deep networks indicated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx146" title="">KSH12</a>]</cite>, if enough data are given, such an encoding scheme can often be represented by a deep network and learned in an end-to-end fashion by optimizing the cross-entropy.
 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="Thmexample2.p1.m8"><semantics><mi mathvariant="normal">■</mi><annotation encoding="application/x-tex">\blacksquare</annotation><annotation encoding="application/x-llamapun">■</annotation></semantics></math></p>
</div>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p">The cross-entropy loss <math alttext="L(\hat{\bm{z}},\bm{z})" class="ltx_Math" display="inline" id="S4.SS3.p2.m1"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>𝒛</mi><mo>^</mo></mover><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\hat{\bm{z}},\bm{z})</annotation><annotation encoding="application/x-llamapun">italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z )</annotation></semantics></math> can be viewed as a special measure of parsimony <math alttext="\rho(\bm{z})" class="ltx_Math" display="inline" id="S4.SS3.p2.m2"><semantics><mrow><mi>ρ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\rho(\bm{z})</annotation><annotation encoding="application/x-llamapun">italic_ρ ( bold_italic_z )</annotation></semantics></math> associated with a particular family of encoding schemes that are suitable for classification. However, such an encoding is obviously <span class="ltx_text ltx_font_italic">very lossy</span>. The learned <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS3.p2.m3"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math> does not contain any other information about <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS3.p2.m4"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> except for its class type. For example, by assigning an image with (a code representing) the class label “apple”, we no longer know which specific type apple is in the original image from the label alone.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p">Of course, the other extreme is to require the coding scheme to be <span class="ltx_text ltx_font_italic">lossless</span>. That is, there is a one-to-one mapping between <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS3.p3.m1"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> and its code <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS3.p3.m2"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>. However, as we will see in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, lossless coding (or compression) is impractical unless <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS3.p3.m3"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is discrete. For a continuous random variable, we may only consider lossy coding schemes so that the coding length for the data can be finite. That is, we only encode the data up to a certain prescribed precision. As we will elaborate more in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>, lossy coding is not merely a practical choice, it plays a fundamental role in making learning the underlying continuous distribution possible from finite samples of the distribution.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p">For many purposes of learning, we want the feature <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S4.SS3.p4.m1"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation><annotation encoding="application/x-llamapun">bold_italic_z</annotation></semantics></math>, although <span class="ltx_text ltx_font_italic">lossy</span>, to keep more information about <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS3.p4.m2"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> than just its class type. In this book, we will introduce a more general measure of parsimony based on coding length/rate associated with a more general family of coding schemes – coding with a mixture of subspaces or Gaussians. This family has the capability to closely approximate arbitrary real-world distributions up to certain precision. As we will see in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a> and Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, such a measure will not only preserve most information about the distribution of <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.p4.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> but will also promote certain nice desired geometric and statistical structures for the learned representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.p4.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Bidirectional Autoencoding for Consistency.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p">In a broader learning context, the main goal of a compressive coding scheme <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> is to identify the low-dimensional structures in the data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> so that they can be used to predict things in the original data space. This requires that the learned encoding scheme <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m3"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> allows an efficient decoding scheme, denoted as <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m4"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math>. It maps <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>, often known as a latent representation, back to the data space:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}\hskip 5.69054pt}\hat{\bm{X}}." class="ltx_Math" display="block" id="S4.E14.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">𝒟</mo></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}\hskip 5.69054pt}\hat{\bm{X}}.</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In general, we call such an encoding and decoding pair <math alttext="(\mathcal{E},\mathcal{D})" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.m6"><semantics><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{E},\mathcal{D})</annotation><annotation encoding="application/x-llamapun">( caligraphic_E , caligraphic_D )</annotation></semantics></math> an <span class="ltx_text ltx_font_italic">autoencoding</span>. Figure <a class="ltx_ref" href="#F23" title="Figure 1.23 ‣ Bidirectional Autoencoding for Consistency. ‣ 1.4.3 Learning Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.23</span></a>
illustrates the process of such an autoencoder.</p>
</div>
<figure class="ltx_figure" id="F23"><img alt="Figure 1.23 : Illustration of the architecture of an autoencoder." class="ltx_graphics ltx_img_landscape" height="244" id="F23.g1" src="chapters/chapter1/figs/Autoencoder.jpg" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.23</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of the architecture of an autoencoder. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p">Generally, we would prefer that the decoding is approximately an “inverse” to the encoding such that the data (distribution) <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m1"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> decoded from <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m2"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> would be similar to the original data (distribution) <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> to some extent.<span class="ltx_note ltx_role_footnote" id="footnote53"><sup class="ltx_note_mark">53</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">53</sup><span class="ltx_tag ltx_tag_note">53</span>We will make it more precise what we mean by being similar later.</span></span></span> If so, we would be able to recover or predict from <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m4"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> what is going on in the original data space. In this case, we say the pair <math alttext="(\mathcal{E},\mathcal{D})" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m5"><semantics><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{E},\mathcal{D})</annotation><annotation encoding="application/x-llamapun">( caligraphic_E , caligraphic_D )</annotation></semantics></math> gives a <span class="ltx_text ltx_font_italic">consistent</span> autoencoding. For most practical purposes, not only do we need such a decoding to exist, but also can be efficiently realized and physically implemented. For example, if <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m6"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation><annotation encoding="application/x-llamapun">bold_italic_x</annotation></semantics></math> is a real-valued variable, quantification is needed in order for any decoding scheme to be realizable on a finite-state machine, as we will explain more in Chapter <a class="ltx_ref" href="Ch3.html" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">3</span></a>. Hence, in general, one should expect that realizable encoding and decoding schemes are necessarily lossy. A central question is how to learn a compact (lossy) representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> such that it can be used to predict <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p2.m8"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> well.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p3">
<p class="ltx_p">Generally speaking, as we will see, both encoder and decoder could be modeled and realized by deep networks and learned by solving an optimization problem of the following form:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\,d(\bm{X},\hat{\bm{X}})+\rho(\bm{Z})," class="ltx_Math" display="block" id="S4.E15.m1"><semantics><mrow><mrow><mrow><mrow><mi>min</mi><mo lspace="0.337em">⁡</mo><mi>d</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>ρ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\min\,d(\bm{X},\hat{\bm{X}})+\rho(\bm{Z}),</annotation><annotation encoding="application/x-llamapun">roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_ρ ( bold_italic_Z ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="d(\cdot,\cdot)" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.m1"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun">italic_d ( ⋅ , ⋅ )</annotation></semantics></math> is a certain distance function that promotes similarity between <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote54"><sup class="ltx_note_mark">54</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">54</sup><span class="ltx_tag ltx_tag_note">54</span>Either sample-wise or distribution-wise similar, depending on the choice of the distance function <math alttext="d" class="ltx_Math" display="inline" id="footnote54.m1"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation><annotation encoding="application/x-llamapun">italic_d</annotation></semantics></math>.</span></span></span> and <math alttext="\rho(\bm{Z})" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.m4"><semantics><mrow><mi>ρ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\rho(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_ρ ( bold_italic_Z )</annotation></semantics></math> is some measure that promotes parsimony and information richness of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>. The classic principal component analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="bib.html#bibx124" title="">Jol02</a>]</cite> is a typical example of a consistent autoencoding, which we will study in great detail in Chapter <a class="ltx_ref" href="Ch2.html" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">2</span></a>, as a precursor to more general low-dimensional structures. In Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>, we will study how to learn consistent autoencoding for general (say nonlinear) low-dimensional distributions.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.4 </span>Learning Self-Consistent Representations</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p">Note that in the above autoencoding objective, one needs to evaluate how close or consistent the decoded data <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p1.m1"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> is to the original <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>. This often requires some external supervision or knowledge on what similarity measure to use. Computing similarity between <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p1.m3"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> and <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p1.m4"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> can be very expensive, if not entirely impossible or intractable.<span class="ltx_note ltx_role_footnote" id="footnote55"><sup class="ltx_note_mark">55</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">55</sup><span class="ltx_tag ltx_tag_note">55</span>Say one wants to minimize certain distributional distance between the two.</span></span></span> Note that in nature, animals are capable of learning all by themselves without comparing their estimate <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p1.m5"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> with the ground truth <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p1.m6"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> in the data space. They typically do not even have that option.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p">Then how is a system able to learn autonomously without external supervision or comparison? How can they know that <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p2.m1"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> is consistent with <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p2.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> even without directly comparing them? That leads to the idea of “closing the loop”. As it turns out, under the mild conditions that we will make precise in Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>, to ensure <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p2.m3"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> and <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p2.m4"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> are consistent, one only has to encode <math alttext="\hat{\bm{X}}" class="ltx_Math" display="inline" id="S4.SS4.p2.m5"><semantics><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{X}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_X end_ARG</annotation></semantics></math> as <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S4.SS4.p2.m6"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> and check if <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS4.p2.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S4.SS4.p2.m8"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math> are consistent. We call this notion of consistency as <span class="ltx_text ltx_font_italic">self-consistency</span>, which can be illustrated by the following diagram:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}\hskip 5.69054pt}\hat{\bm{X}}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\hat{\bm{Z}}," class="ltx_Math" display="block" id="S4.E16.m1"><semantics><mrow><mrow><mi>𝑿</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mi>𝒁</mi><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">𝒟</mo></mover><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mover accent="true"><mo stretchy="false">→</mo><mo class="ltx_font_mathcaligraphic">ℰ</mo></mover><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{X}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\bm{Z}\xrightarrow{\hskip 5.69054pt\mathcal{D}\hskip 5.69054pt}\hat{\bm{X}}\xrightarrow{\hskip 5.69054pt\mathcal{E}\hskip 5.69054pt}\hat{\bm{Z}},</annotation><annotation encoding="application/x-llamapun">bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT → end_ARROW over^ start_ARG bold_italic_Z end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We refer to this process as a <span class="ltx_text ltx_font_italic">closed-loop transcription</span>,<span class="ltx_note ltx_role_footnote" id="footnote56"><sup class="ltx_note_mark">56</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">56</sup><span class="ltx_tag ltx_tag_note">56</span>Inspired by the transcription process between DNA and RNA or other proteins.</span></span></span> which is illustrated in Figure <a class="ltx_ref" href="#F24" title="Figure 1.24 ‣ 1.4.4 Learning Self-Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.24</span></a>.</p>
</div>
<figure class="ltx_figure" id="F24"><img alt="Figure 1.24 : Illustration of a closed-loop transcription. Here we use a mapping f f italic_f to represent the encoder ℰ \mathcal{E} caligraphic_E and g g italic_g to represent the decoder 𝒟 \mathcal{D} caligraphic_D ." class="ltx_graphics" id="F24.g1" src="chapters/chapter1/figs/diagrams_redu_gan_2.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1.24</span>: </span><span class="ltx_text" style="font-size:90%;">Illustration of a closed-loop transcription. Here we use a mapping <math alttext="f" class="ltx_Math" display="inline" id="F24.m5"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation><annotation encoding="application/x-llamapun">italic_f</annotation></semantics></math> to represent the encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="F24.m6"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation><annotation encoding="application/x-llamapun">caligraphic_E</annotation></semantics></math> and <math alttext="g" class="ltx_Math" display="inline" id="F24.m7"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation><annotation encoding="application/x-llamapun">italic_g</annotation></semantics></math> to represent the decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="F24.m8"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation><annotation encoding="application/x-llamapun">caligraphic_D</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p">It is arguably true that any autonomous intelligent being only needs to learn a self-consistent representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS4.p3.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> of the observed data <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p3.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>, because checking consistency in the original data space (often meaning in the external world) is either too expensive or even not physically feasible. The closed-loop formulation allows one to learn an optimal encoding <math alttext="f(\cdot,\theta)" class="ltx_Math" display="inline" id="S4.SS4.p3.m3"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\theta)</annotation><annotation encoding="application/x-llamapun">italic_f ( ⋅ , italic_θ )</annotation></semantics></math> and decoding <math alttext="g(\cdot,\eta)" class="ltx_Math" display="inline" id="S4.SS4.p3.m4"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot,\eta)</annotation><annotation encoding="application/x-llamapun">italic_g ( ⋅ , italic_η )</annotation></semantics></math> via a minmax game that depends only on the internal (learned) feature <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS4.p3.m5"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max_{\theta}\min_{\eta}\ell(\bm{Z},\hat{\bm{Z}})+\rho(\bm{Z})," class="ltx_Math" display="block" id="S4.E17.m1"><semantics><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi>θ</mi></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>min</mi><mi>η</mi></munder><mo lspace="0.167em">⁡</mo><mi mathvariant="normal">ℓ</mi></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>ρ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\max_{\theta}\min_{\eta}\ell(\bm{Z},\hat{\bm{Z}})+\rho(\bm{Z}),</annotation><annotation encoding="application/x-llamapun">roman_max start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_η end_POSTSUBSCRIPT roman_ℓ ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) + italic_ρ ( bold_italic_Z ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.4.17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\ell(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S4.SS4.p3.m6"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ell(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">roman_ℓ ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> is a loss function based on coding rates of the features <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS4.p3.m7"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> and <math alttext="\hat{\bm{Z}}" class="ltx_Math" display="inline" id="S4.SS4.p3.m8"><semantics><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\bm{Z}}</annotation><annotation encoding="application/x-llamapun">over^ start_ARG bold_italic_Z end_ARG</annotation></semantics></math>, which, as we will see, can be much easier to compute. Here again, <math alttext="\rho(\bm{Z})" class="ltx_Math" display="inline" id="S4.SS4.p3.m9"><semantics><mrow><mi>ρ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\rho(\bm{Z})</annotation><annotation encoding="application/x-llamapun">italic_ρ ( bold_italic_Z )</annotation></semantics></math> is some measure that promotes parsimony and information richness of <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S4.SS4.p3.m10"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math>. Somewhat surprisingly, as we will see in Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>, under rather mild conditions such as <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S4.SS4.p3.m11"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math> being sufficiently low-dimensional, self-consistency between <math alttext="(\bm{Z},\hat{\bm{Z}})" class="ltx_Math" display="inline" id="S4.SS4.p3.m12"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒁</mi><mo>,</mo><mover accent="true"><mi>𝒁</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{Z},\hat{\bm{Z}})</annotation><annotation encoding="application/x-llamapun">( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG )</annotation></semantics></math> implies consistency in <math alttext="(\bm{X},\hat{\bm{X}})" class="ltx_Math" display="inline" id="S4.SS4.p3.m13"><semantics><mrow><mo stretchy="false">(</mo><mi>𝑿</mi><mo>,</mo><mover accent="true"><mi>𝑿</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{X},\hat{\bm{X}})</annotation><annotation encoding="application/x-llamapun">( bold_italic_X , over^ start_ARG bold_italic_X end_ARG )</annotation></semantics></math>! In addition, we will also see that a closed-loop system will allow us to learn the distribution in a <span class="ltx_text ltx_font_italic">continuous and incremental</span> manner,<span class="ltx_note ltx_role_footnote" id="footnote57"><sup class="ltx_note_mark">57</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">57</sup><span class="ltx_tag ltx_tag_note">57</span>That is to learn with one class at a time or even one sample at a time.</span></span></span> without suffering from problems such as catastrophic forgetting associated with open-loop models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.5 </span>Bridging Theory and Practice for Machine Intelligence</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">So far, we have introduced three related frameworks for learning a compact and structured representation <math alttext="\bm{Z}" class="ltx_Math" display="inline" id="S5.p1.m1"><semantics><mi>𝒁</mi><annotation encoding="application/x-tex">\bm{Z}</annotation><annotation encoding="application/x-llamapun">bold_italic_Z</annotation></semantics></math> for a given data distribution <math alttext="\bm{X}" class="ltx_Math" display="inline" id="S5.p1.m2"><semantics><mi>𝑿</mi><annotation encoding="application/x-tex">\bm{X}</annotation><annotation encoding="application/x-llamapun">bold_italic_X</annotation></semantics></math>:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p">The open-ended encoding (<a class="ltx_ref" href="#S4.E10" title="Equation 1.4.10 ‣ 1.4.3 Learning Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4.10</span></a>);</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p">The bi-directional autoencoding (<a class="ltx_ref" href="#S4.E14" title="Equation 1.4.14 ‣ Bidirectional Autoencoding for Consistency. ‣ 1.4.3 Learning Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4.14</span></a>);</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p">The closed-loop transcription (<a class="ltx_ref" href="#S4.E16" title="Equation 1.4.16 ‣ 1.4.4 Learning Self-Consistent Representations ‣ 1.4 A Unifying Approach ‣ Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">1.4.16</span></a>).</p>
</div>
</li>
</ul>
<p class="ltx_p">In this book, we will systematically study all three frameworks, one after another:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mbox{{open-ended}}\;\Longrightarrow\;\mbox{{bi-directional}}\;\Longrightarrow\;\mbox{{closed-loop}}," class="ltx_Math" display="block" id="S5.E1.m1"><semantics><mrow><mrow><mtext class="ltx_mathvariant_bold">open-ended</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">bi-directional</mtext><mo lspace="0.558em" rspace="0.558em" stretchy="false">⟹</mo><mtext class="ltx_mathvariant_bold">closed-loop</mtext></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mbox{{open-ended}}\;\Longrightarrow\;\mbox{{bi-directional}}\;\Longrightarrow\;\mbox{{closed-loop}},</annotation><annotation encoding="application/x-llamapun">open-ended ⟹ bi-directional ⟹ closed-loop ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.5.1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">in Chapter <a class="ltx_ref" href="Ch4.html" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">4</span></a>, Section <a class="ltx_ref" href="Ch5.html#S1" title="5.1 Learning Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.1</span></a> and Section <a class="ltx_ref" href="Ch5.html#S2" title="5.2 Learning Self-Consistent Representations ‣ Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5.2</span></a> of Chapter <a class="ltx_ref" href="Ch5.html" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">5</span></a>, respectively.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p">In the past few years, many theoretical frameworks have been proposed and developed to help understand deep networks. However, many were unable to provide scalable solutions that matched the performance of empirical methods on real-world data and tasks. Many theories do not provide useful guidance on how to further improve practice. Chapters <a class="ltx_ref" href="Ch6.html" title="Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7</span></a> will show how the framework presented in this book may help bridge the gap between theory and practice. Chapter <a class="ltx_ref" href="Ch6.html" title="Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">6</span></a> will show how to use the learned distribution and its representation to conduct (Bayesian) inference for almost all practical tasks that depend on (conditional) generation, estimation, and prediction. Chapter <a class="ltx_ref" href="Ch7.html" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">7</span></a> will provide convincing experimental evidence that networks and systems designed from the first principles can achieve competitive and potentially better performance on a variety of tasks such as visual representation learning, image classification, image completion, segmentation, and text generation.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Back to Intelligence.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p">As we have mentioned in the beginning, a common and fundamental task of any intelligent being is to learn predictable information from its sensed data. Now we have understood a little about the computational nature of this task, and one should realize that this is a never-ending process, for the following reasons:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p">The knowledge learned so far from the data, say by the encoding and decoding schemes, is unlikely to be correct or optimal. Intelligence should have the ability to improve if there are still errors in predicting new observations.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p">The data observed so far do not yet cover all the predictable information. Intelligence should be able to recognize that current knowledge is inadequate and have the capability to learn and acquire new information whenever available.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p2">
<p class="ltx_p">Hence, intelligence is <span class="ltx_text ltx_font_italic">not</span> about simply collecting all data in advance and training a model to memorize all the predictable information in the data. In contrast, it is about being equipped with computational mechanisms that can constantly improve current knowledge and acquire new information when available and needed. That is, a fundamental characteristic of any intelligent being or system<span class="ltx_note ltx_role_footnote" id="footnote58"><sup class="ltx_note_mark">58</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">58</sup><span class="ltx_tag ltx_tag_note">58</span>An animal, a human being, an intelligent robot, the scientific community, and even the entire civilization.</span></span></span> is <span class="ltx_text ltx_font_italic">being able to continuously improve or gain information (or knowledge) on its own</span>. Conceptually, we may illustrate symbolically the relationship between intelligence and information (or knowledge) as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{Intelligence}(t)=\frac{\mathrm{d}}{\mathrm{d}t}\operatorname{Information}(t),\qquad\operatorname{Information}(t)=\int_{0}^{t}\operatorname{Intelligence}(s)\mathrm{d}s." class="ltx_Math" display="block" id="S5.E2.m1"><semantics><mrow><mrow><mrow><mrow><mi>Intelligence</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi mathvariant="normal">d</mi><mrow><mi mathvariant="normal">d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>Information</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mi>Information</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∫</mo><mn>0</mn><mi>t</mi></msubsup><mrow><mrow><mi>Intelligence</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">d</mo><mi>s</mi></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\operatorname{Intelligence}(t)=\frac{\mathrm{d}}{\mathrm{d}t}\operatorname{Information}(t),\qquad\operatorname{Information}(t)=\int_{0}^{t}\operatorname{Intelligence}(s)\mathrm{d}s.</annotation><annotation encoding="application/x-llamapun">roman_Intelligence ( italic_t ) = divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG roman_Information ( italic_t ) , roman_Information ( italic_t ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT roman_Intelligence ( italic_s ) roman_d italic_s .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1.5.2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We believe that the closed-loop framework is a universal mechanism that enables self-improving and self-learning, via feedback<span class="ltx_note ltx_role_footnote" id="footnote59"><sup class="ltx_note_mark">59</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">59</sup><span class="ltx_tag ltx_tag_note">59</span>Reinforcement can be viewed as a primitive form of feedback, say the natural selection by the nature.</span></span></span> or gaming<span class="ltx_note ltx_role_footnote" id="footnote60"><sup class="ltx_note_mark">60</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">60</sup><span class="ltx_tag ltx_tag_note">60</span>Scientific inquiries can be viewed as a most advanced form of gaming, through hypothesis formulation and verification.</span></span></span>. All intelligent beings or systems in nature utilize closed-loop mechanisms for learning at all levels and on all scales. Its ubiquity had inspired early studies that tried to model and emulate intelligence by machines and computers, particularly the Cybernetics movement initiated by Norbert Wiener in the 1940s.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p3">
<p class="ltx_p">We hope that this book will help people better understand the objectives, principles, and computational mechanisms behind intelligence. It serves as a foundation for further study of higher-level human intelligence, the true “artificial” intelligence, in the future, which we will layout several significant open problems in these new directions at the end of the book in Chapter <a class="ltx_ref" href="Ch8.html" title="Chapter 8 Future Study of Intelligence ‣ Learning Deep Representations of Data Distributions"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
</section>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug 18 09:48:41 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
